Directory structure:
└── modal-labs-modal-client.git/
    ├── README.md
    ├── CHANGELOG.md
    ├── LICENSE
    ├── conftest.py
    ├── pyproject.toml
    ├── requirements.dev.txt
    ├── tasks.py
    ├── .pre-commit-config.yaml
    ├── modal/
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── _clustered_functions.py
    │   ├── _container_entrypoint.py
    │   ├── _functions.py
    │   ├── _ipython.py
    │   ├── _location.py
    │   ├── _object.py
    │   ├── _output.py
    │   ├── _partial_function.py
    │   ├── _proxy_tunnel.py
    │   ├── _pty.py
    │   ├── _resolver.py
    │   ├── _resources.py
    │   ├── _serialization.py
    │   ├── _traceback.py
    │   ├── _tunnel.py
    │   ├── _watcher.py
    │   ├── app.py
    │   ├── call_graph.py
    │   ├── client.py
    │   ├── cloud_bucket_mount.py
    │   ├── cls.py
    │   ├── config.py
    │   ├── container_process.py
    │   ├── dict.py
    │   ├── environments.py
    │   ├── exception.py
    │   ├── experimental.py
    │   ├── file_io.py
    │   ├── file_pattern_matcher.py
    │   ├── functions.py
    │   ├── gpu.py
    │   ├── image.py
    │   ├── io_streams.py
    │   ├── mount.py
    │   ├── network_file_system.py
    │   ├── object.py
    │   ├── output.py
    │   ├── parallel_map.py
    │   ├── partial_function.py
    │   ├── proxy.py
    │   ├── py.typed
    │   ├── queue.py
    │   ├── retries.py
    │   ├── runner.py
    │   ├── running_app.py
    │   ├── sandbox.py
    │   ├── schedule.py
    │   ├── scheduler_placement.py
    │   ├── secret.py
    │   ├── serving.py
    │   ├── snapshot.py
    │   ├── stream_type.py
    │   ├── token_flow.py
    │   ├── volume.py
    │   ├── _runtime/
    │   │   ├── __init__.py
    │   │   ├── asgi.py
    │   │   ├── container_io_manager.py
    │   │   ├── execution_context.py
    │   │   ├── gpu_memory_snapshot.py
    │   │   ├── telemetry.py
    │   │   └── user_code_imports.py
    │   ├── _utils/
    │   │   ├── __init__.py
    │   │   ├── app_utils.py
    │   │   ├── async_utils.py
    │   │   ├── blob_utils.py
    │   │   ├── bytes_io_segment_payload.py
    │   │   ├── deprecation.py
    │   │   ├── docker_utils.py
    │   │   ├── function_utils.py
    │   │   ├── grpc_testing.py
    │   │   ├── grpc_utils.py
    │   │   ├── hash_utils.py
    │   │   ├── http_utils.py
    │   │   ├── logger.py
    │   │   ├── mount_utils.py
    │   │   ├── name_utils.py
    │   │   ├── package_utils.py
    │   │   ├── pattern_utils.py
    │   │   ├── rand_pb_testing.py
    │   │   └── shell_utils.py
    │   ├── _vendor/
    │   │   ├── __init__.py
    │   │   ├── a2wsgi_wsgi.py
    │   │   ├── cloudpickle.py
    │   │   └── tblib.py
    │   ├── cli/
    │   │   ├── __init__.py
    │   │   ├── _download.py
    │   │   ├── _traceback.py
    │   │   ├── app.py
    │   │   ├── config.py
    │   │   ├── container.py
    │   │   ├── dict.py
    │   │   ├── entry_point.py
    │   │   ├── environment.py
    │   │   ├── import_refs.py
    │   │   ├── launch.py
    │   │   ├── network_file_system.py
    │   │   ├── profile.py
    │   │   ├── queues.py
    │   │   ├── run.py
    │   │   ├── secret.py
    │   │   ├── token.py
    │   │   ├── utils.py
    │   │   ├── volume.py
    │   │   └── programs/
    │   │       ├── __init__.py
    │   │       ├── run_jupyter.py
    │   │       └── vscode.py
    │   ├── extensions/
    │   │   ├── __init__.py
    │   │   └── ipython.py
    │   └── requirements/
    │       ├── README.md
    │       ├── 2023.12.312.txt
    │       ├── 2023.12.txt
    │       ├── 2024.04.txt
    │       ├── 2024.10.txt
    │       └── base-images.json
    ├── modal_docs/
    │   ├── __init__.py
    │   ├── gen_cli_docs.py
    │   ├── gen_reference_docs.py
    │   └── mdmd/
    │       ├── __init__.py
    │       ├── mdmd.py
    │       └── signatures.py
    ├── modal_global_objects/
    │   ├── __init__.py
    │   ├── images/
    │   │   ├── __init__.py
    │   │   └── base_images.py
    │   └── mounts/
    │       ├── __init__.py
    │       ├── modal_client_package.py
    │       └── python_standalone.py
    ├── modal_proto/
    │   ├── __init__.py
    │   ├── api.proto
    │   ├── options.proto
    │   └── py.typed
    ├── modal_version/
    │   ├── __init__.py
    │   ├── __main__.py
    │   └── _version_generated.py
    ├── protoc_plugin/
    │   └── plugin.py
    ├── test/
    │   ├── __init__.py
    │   ├── aio_test.py
    │   ├── app_composition_test.py
    │   ├── app_test.py
    │   ├── asgi_wrapper_test.py
    │   ├── async_utils_test.py
    │   ├── blob_test.py
    │   ├── cli_imports_test.py
    │   ├── cli_test.py
    │   ├── client_test.py
    │   ├── cloud_bucket_mount_test.py
    │   ├── cls_test.py
    │   ├── config_test.py
    │   ├── conftest.py
    │   ├── container_app_test.py
    │   ├── container_buffer_test.py
    │   ├── container_test.py
    │   ├── decorator_test.py
    │   ├── deprecation_test.py
    │   ├── dict_test.py
    │   ├── docker_utils_test.py
    │   ├── e2e_test.py
    │   ├── error_test.py
    │   ├── file_io_test.py
    │   ├── file_pattern_matcher_test.py
    │   ├── fork_test.py
    │   ├── function_retry_test.py
    │   ├── function_serialization_test.py
    │   ├── function_test.py
    │   ├── function_utils_test.py
    │   ├── gpu_fallbacks_test.py
    │   ├── gpu_test.py
    │   ├── grpc_utils_test.py
    │   ├── helpers.py
    │   ├── i6pn_clustered_test.py
    │   ├── image_test.py
    │   ├── io_streams_test.py
    │   ├── live_reload_test.py
    │   ├── lookup_test.py
    │   ├── mdmd_test.py
    │   ├── mount_test.py
    │   ├── mount_utils_test.py
    │   ├── mounted_files_test.py
    │   ├── network_file_system_test.py
    │   ├── notebook_test.py
    │   ├── object_test.py
    │   ├── package_utils_test.py
    │   ├── queue_test.py
    │   ├── resolver_test.py
    │   ├── retries_test.py
    │   ├── runner_test.py
    │   ├── sandbox_test.py
    │   ├── schedule_test.py
    │   ├── scheduler_placement_test.py
    │   ├── secret_test.py
    │   ├── serialization_test.py
    │   ├── shutdown_test.py
    │   ├── slow_dependencies_test.py
    │   ├── static_types_test.py
    │   ├── telemetry_test.py
    │   ├── token_flow_test.py
    │   ├── traceback_test.py
    │   ├── tunnel_test.py
    │   ├── user_code_import_test.py
    │   ├── utils_test.py
    │   ├── version_test.py
    │   ├── volume_test.py
    │   ├── watcher_test.py
    │   ├── web_server_proxy_test.py
    │   ├── webhook_test.py
    │   ├── mdmd_data/
    │   │   ├── foo-expected.md
    │   │   └── foo.py
    │   ├── supports/
    │   │   ├── assert_package.py
    │   │   ├── base_class.py
    │   │   ├── class_hierarchy.py
    │   │   ├── class_with_image.py
    │   │   ├── common.py
    │   │   ├── consumed_map.py
    │   │   ├── experimental.py
    │   │   ├── forking.py
    │   │   ├── function_without_app.py
    │   │   ├── functions.py
    │   │   ├── hello.py
    │   │   ├── image_run_function.py
    │   │   ├── import_and_filter_source.py
    │   │   ├── import_modal_from_thread.py
    │   │   ├── imports_ast.py
    │   │   ├── imports_six.py
    │   │   ├── lazy_hydration.py
    │   │   ├── missing_main_conditional.py
    │   │   ├── module_1.py
    │   │   ├── module_2.py
    │   │   ├── mount_dedupe.py
    │   │   ├── multiapp.py
    │   │   ├── multiapp_privately_decorated.py
    │   │   ├── multiapp_privately_decorated_named_app.py
    │   │   ├── multiapp_same_name.py
    │   │   ├── multiapp_serialized_func.py
    │   │   ├── package_mount.py
    │   │   ├── progress_info.py
    │   │   ├── pyproject.toml
    │   │   ├── raise_error.py
    │   │   ├── sandbox.py
    │   │   ├── script.py
    │   │   ├── serialize_class.py
    │   │   ├── sibling_hydration_app.py
    │   │   ├── skip.py
    │   │   ├── slow_dependencies_container.py
    │   │   ├── slow_dependencies_local.py
    │   │   ├── special_poetry.lock
    │   │   ├── standalone_file.py
    │   │   ├── startup_failure.py
    │   │   ├── test-conda-environment.yml
    │   │   ├── test-dockerfile
    │   │   ├── test-pyproject.toml
    │   │   ├── test-requirements.txt
    │   │   ├── type_assertions.py
    │   │   ├── type_assertions_negative.py
    │   │   ├── unconsumed_map.py
    │   │   ├── volume_local.py
    │   │   ├── webhook_forgot_function.py
    │   │   ├── app_run_tests/
    │   │   │   ├── app_was_once_stub.py
    │   │   │   ├── app_with_lookups.py
    │   │   │   ├── app_with_multiple_functions.py
    │   │   │   ├── async_app.py
    │   │   │   ├── cli_args.py
    │   │   │   ├── cls.py
    │   │   │   ├── custom_app.py
    │   │   │   ├── default_app.py
    │   │   │   ├── file_with_global_lookups.py
    │   │   │   ├── generator.py
    │   │   │   ├── local_entrypoint.py
    │   │   │   ├── local_entrypoint_async.py
    │   │   │   ├── local_entrypoint_invalid.py
    │   │   │   ├── main_thread_assertion.py
    │   │   │   ├── prints_desc_app.py
    │   │   │   ├── raises_error.py
    │   │   │   ├── returns_data.py
    │   │   │   ├── variadic_args.py
    │   │   │   ├── webhook.py
    │   │   │   └── multifile/
    │   │   │       ├── __init__.py
    │   │   │       ├── main.py
    │   │   │       └── util.py
    │   │   ├── multifile_project/
    │   │   │   ├── __init__.py
    │   │   │   ├── a.py
    │   │   │   ├── b.py
    │   │   │   ├── c.py
    │   │   │   └── main.py
    │   │   ├── notebooks/
    │   │   │   └── simple.notebook.py
    │   │   ├── pkg_a/
    │   │   │   ├── __init__.py
    │   │   │   ├── a.py
    │   │   │   ├── d.py
    │   │   │   ├── package.py
    │   │   │   ├── script.py
    │   │   │   ├── serialized_fn.py
    │   │   │   └── b/
    │   │   │       ├── c.py
    │   │   │       └── e.py
    │   │   ├── pkg_b/
    │   │   │   ├── __init__.py
    │   │   │   ├── f.py
    │   │   │   └── g/
    │   │   │       └── h.py
    │   │   ├── pkg_c/
    │   │   │   ├── __init__.py
    │   │   │   ├── i.py
    │   │   │   └── j/
    │   │   │       └── k.py
    │   │   ├── pkg_d/
    │   │   │   ├── __init__.py
    │   │   │   ├── main.py
    │   │   │   └── sibling.py
    │   │   └── user_code_import_samples/
    │   │       ├── __init__.py
    │   │       ├── cls.py
    │   │       └── func.py
    │   └── telemetry/
    │       ├── tracing_module_1.py
    │       └── tracing_module_2.py
    └── .github/
        ├── pull_request_template.md
        ├── actions/
        │   └── setup-cached-python/
        │       └── action.yml
        └── workflows/
            ├── check.yml
            ├── ci-cd.yml
            ├── docs.yml
            └── sast-codeql.yml

================================================
File: README.md
================================================
# Modal Python Library

[![PyPI Version](https://img.shields.io/pypi/v/modal.svg)](https://pypi.org/project/modal/)
[![License](https://img.shields.io/badge/license-apache_2.0-darkviolet.svg)](https://github.com/modal-labs/modal-client/blob/master/LICENSE)
[![Tests](https://github.com/modal-labs/modal-client/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/modal-labs/modal-client/actions/workflows/ci-cd.yml)
[![Slack](https://img.shields.io/badge/slack-join-blue.svg?logo=slack)](https://modal.com/slack)

The [Modal](https://modal.com/) Python library provides convenient, on-demand
access to serverless cloud compute from Python scripts on your local computer.

## Documentation

See the [online documentation](https://modal.com/docs/guide) for many
[example applications](https://modal.com/docs/examples),
a [user guide](https://modal.com/docs/guide), and the detailed
[API reference](https://modal.com/docs/reference).

## Installation

**This library requires Python 3.9 – 3.13.**

Install the package with `pip`:

```bash
pip install modal
```

You can create a Modal account (or link your existing one) directly on the
command line:

```bash
python3 -m modal setup
```

## Support

For usage questions and other support, please reach out on the
[Modal Slack](https://modal.com/slack).


================================================
File: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS


================================================
File: conftest.py
================================================
# Copyright Modal Labs 2022
import pytest


def pytest_markdown_docs_globals():
    import math

    import modal

    return {
        "modal": modal,
        "app": modal.App.lookup("my-app", create_if_missing=True),
        "math": math,
        "__name__": "runtest",
        "web_endpoint": modal.web_endpoint,
        "asgi_app": modal.asgi_app,
        "wsgi_app": modal.wsgi_app,
        "__file__": "xyz.py",
    }


@pytest.fixture(autouse=True)
def disable_auto_mount(monkeypatch):
    monkeypatch.setenv("MODAL_AUTOMOUNT", "0")
    yield


================================================
File: pyproject.toml
================================================
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "modal"
description = "Python client library for Modal"
readme = "README.md"
dynamic = ["version"]
requires-python = ">=3.9"
license = {text = "Apache-2.0"}

authors = [
    { name = "Modal Labs", email = "support@modal.com" }
]
dependencies = [
    "aiohttp",
    "certifi",
    "click>=8.1.0",
    "fastapi",
    "grpclib==0.4.7",
    "protobuf>=3.19,<6.0,!=4.24.0",
    "rich>=12.0.0",
    "synchronicity~=0.9.10",
    "toml",
    "typer>=0.9",
    "types-certifi",
    "types-toml",
    "watchfiles",
    "typing_extensions~=4.6"
]
keywords = ["modal", "client", "cloud", "serverless", "infrastructure"]
classifiers = [
    "Topic :: System :: Distributed Computing",
    "Operating System :: OS Independent",
    "License :: OSI Approved :: Apache Software License",
    "Programming Language :: Python :: 3"
]

[project.urls]
Homepage = "https://modal.com"
Source = "https://github.com/modal-labs/modal-client"
Documentation = "https://modal.com/docs"
"Issue Tracker" = "https://github.com/modal-labs/modal-client/issues"

[project.scripts]
modal = "modal.__main__:main"

[tool.setuptools.packages.find]
include = ["modal", "modal.*", "modal_docs", "modal_docs.*", "modal_version", "modal_proto"]
exclude = ["test*", "modal_global_objects"]

[tool.setuptools.package-data]
modal = ["requirements/*.md", "requirements/*.txt", "requirements/*.json", "py.typed", "*.pyi"]
modal_proto = ["*.proto", "py.typed", "*.pyi"]

[tool.setuptools.dynamic]
version = {attr = "modal_version.__version__"}

[tool.mypy]
python_version = "3.11"
exclude = "build"
ignore_missing_imports = true
check_untyped_defs = true
no_strict_optional = true
namespace_packages = true

[[tool.mypy.overrides]]
module = [
    "modal/_vendor/cloudpickle",
    "modal/_vendor/tblib",
    "modal/_vendor/a2wsgi_wsgi",
]
ignore_errors = true

[tool.pytest.ini_options]
timeout = 300
addopts = "--ignore=modal/cli/programs"
filterwarnings = [
    "error::DeprecationWarning",
    "ignore:Type google._upb.*MapContainer uses PyType_Spec.*Python 3.14:DeprecationWarning",
    "error::modal.exception.DeprecationError",
    "ignore::DeprecationWarning:pytest.*:",
    "ignore::DeprecationWarning:pkg_resources.*:",
    "ignore::DeprecationWarning:google.rpc.*:",
    "ignore:.*pkg_resources.*:DeprecationWarning::",
]

[tool.ruff]
extend-include = ["*.pyi"]
exclude = [
    '.venv',
    '.git',
    '__pycache__',
    'proto',
    'build',
    'modal_proto',
    'modal/_vendor',
]
line-length = 120
lint.ignore = ['E741']
lint.select = ['E', 'F', 'W', 'I']

[tool.ruff.lint.per-file-ignores]
"*_test.py" = ['E712']
"test/supports/notebooks/*.py" = ['E402']

[tool.ruff.lint.isort]
combine-as-imports = true
known-first-party = [
    "modal",
    "modal_global_objects",
    "modal_proto",
    "modal_version",
]
extra-standard-library = ["pytest"]

[tool.pyright]
reportUnnecessaryComparison = true


================================================
File: requirements.dev.txt
================================================
# Development requirements

black==23.11.0
flaky~=3.7
grpcio-tools==1.48.0;python_version<'3.11'  # TODO: remove when we drop client support for Protobuf 3.19
grpcio-tools==1.59.2;python_version>='3.11' and python_version<'3.13'
grpcio-tools==1.66.2;python_version>='3.13'
grpclib==0.4.7
httpx~=0.23.0
invoke~=2.2
mypy~=1.11.2
mypy-protobuf~=3.3.0  # TODO: can't use mypy-protobuf>=3.4 because of protobuf==3.19 support
pre-commit>=2.21,<4
pytest~=8.0.0
pytest-asyncio @ git+https://github.com/modal-labs/pytest-asyncio.git@b535db05f6e43019700483c442ab6686f132a415
pytest-env~=0.6.2
pytest-markdown-docs==0.7.1
pytest-timeout~=2.1.0
python-dotenv~=1.0.0;python_version>='3.8'
requests~=2.31.0
ruff==0.9.6
types-croniter~=1.0.8
types-python-dateutil~=2.8.10
types-requests~=2.31.0
types-setuptools~=57.4.11
types-six==1.16.21
types-toml~=0.10.4
twine~=5.1.1
wheel~=0.37.1
nbclient==0.6.8
notebook==6.5.1
jupytext==1.14.1
pyright==1.1.351
python-json-logger==2.0.7  # unpinned transitive dependency of jupytext breaking on later versions
pdm==2.12.4  # used for testing pdm cache behavior w/ automounts
console-ctrl==0.1.0



================================================
File: tasks.py
================================================
# Copyright Modal Labs 2022
# Copyright (c) Modal Labs 2022

import ast
import datetime
import importlib
import os
import pkgutil
import re
import subprocess
import sys
from collections.abc import Generator
from contextlib import contextmanager
from datetime import date
from pathlib import Path
from tempfile import NamedTemporaryFile
from typing import Optional

import requests
from invoke import task
from rich.console import Console
from rich.table import Table

# Set working directory to the root of the client repository.
original_cwd = Path.cwd()
project_root = Path(os.path.dirname(__file__))
os.chdir(project_root)


year = datetime.date.today().year
copyright_header_start = "# Copyright Modal Labs"
copyright_header_full = f"{copyright_header_start} {year}"


@contextmanager
def python_file_as_executable(path: Path) -> Generator[Path, None, None]:
    if sys.platform == "win32":
        # windows can't just run shebang:ed python files, so we create a .bat file that calls it
        src = f"""@echo off
{sys.executable} {path}
"""
        with NamedTemporaryFile(mode="w", suffix=".bat", encoding="ascii", delete=False) as f:
            f.write(src)

        try:
            yield Path(f.name)
        finally:
            Path(f.name).unlink()
    else:
        yield path


@task
def protoc(ctx):
    protoc_cmd = f"{sys.executable} -m grpc_tools.protoc"
    input_files = "modal_proto/api.proto modal_proto/options.proto"
    py_protoc = (
        protoc_cmd + " --python_out=. --grpclib_python_out=." + " --grpc_python_out=. --mypy_out=. --mypy_grpc_out=."
    )
    print(py_protoc)
    # generate grpcio and grpclib proto files:
    ctx.run(f"{py_protoc} -I . {input_files}")

    # generate modal-specific wrapper around grpclib api stub using custom plugin:
    grpc_plugin_pyfile = Path("protoc_plugin/plugin.py")

    with python_file_as_executable(grpc_plugin_pyfile) as grpc_plugin_executable:
        ctx.run(
            f"{protoc_cmd} --plugin=protoc-gen-modal-grpclib-python={grpc_plugin_executable}"
            + f" --modal-grpclib-python_out=. -I . {input_files}"
        )


@task
def lint(ctx, fix=False):
    ctx.run(f"ruff check . {'--fix' if fix else ''}", pty=True)


@task
def lint_protos(ctx):
    proto_fname = "modal_proto/api.proto"
    with open(proto_fname) as f:
        proto_text = f.read()

    sections = ["import", "enum", "message", "service"]
    section_regex = "|".join(sections)
    matches = re.findall(rf"^((?:{section_regex})\s+(?:\w+))", proto_text, flags=re.MULTILINE)
    entities = [tuple(e.split()) for e in matches]

    console = Console()

    def get_first_lineno_with_prefix(text: str, prefix: str) -> int:
        lines = text.split("\n")
        for lineno, line in enumerate(lines):
            if re.match(rf"^{prefix}", line):
                return lineno
        raise RuntimeError(f"Failed to find line starting with `{prefix}` (this shouldn't happen)")

    section_order = {key: i for i, key in enumerate(sections)}
    for (a_type, a_name), (b_type, b_name) in zip(entities[:-1], entities[1:]):
        if (section_order[a_type] > section_order[b_type]) or (a_type == b_type and a_name > b_name):
            # This is a simplistic and sort of hacky of way of identifying the "out of order" entity,
            # as the latter one may be the one that is misplaced. Doesn't seem worth the effort though.
            lineno = get_first_lineno_with_prefix(proto_text, f"{a_type} {a_name}")
            console.print(f"[bold red]Proto lint error:[/bold red] {proto_fname}:{lineno}")
            console.print(f"\nThe {a_name} {a_type} proto is out of order relative to the {b_name} {b_type}.")
            console.print(
                "\nProtos should be organized into the following sections:", *sections, sep="\n - ", style="dim"
            )
            console.print("\nWithin sections, protos should be lexicographically sorted by name.", style="dim")
            sys.exit(1)

    service_chunks = re.findall(r"service \w+ {(.+)}", proto_text, flags=re.DOTALL)
    for service_text in service_chunks:
        rpcs = re.findall(r"^\s*rpc\s+(\w+)", service_text, flags=re.MULTILINE)
        for rpc_a, rpc_b in zip(rpcs[:-1], rpcs[1:]):
            if rpc_a > rpc_b:
                lineno = get_first_lineno_with_prefix(proto_text, rf"\s*rpc\s+{rpc_a}")
                console.print(f"[bold red]Proto lint error:[/bold red] {proto_fname}:{lineno}")
                console.print(f"\nThe {rpc_a} rpc proto is out of order relative to the {rpc_b} rpc.")
                console.print("\nRPC definitions should be ordered within each service proto.", style="dim")
                sys.exit(1)


@task
def type_check(ctx):
    type_stubs(ctx)
    # mypy will not check the *implementation* (.py) for files that also have .pyi type stubs
    mypy_exclude_list = [
        "playground",
        "venv312",
        "venv311",
        "venv310",
        "venv39",
        "venv38",
        "test/cls_test.py",  # blocked by mypy bug: https://github.com/python/mypy/issues/16527
        "test/supports/sibling_hydration_app.py",  # blocked by mypy bug: https://github.com/python/mypy/issues/16527
        "test/supports/type_assertions_negative.py",
    ]
    excludes = " ".join(f"--exclude {path}" for path in mypy_exclude_list)
    ctx.run(f"mypy . {excludes}", pty=True)

    # use pyright for checking implementation of those files
    pyright_allowlist = [
        "modal/_functions.py",
        "modal/_runtime/asgi.py",
        "modal/_utils/__init__.py",
        "modal/_utils/async_utils.py",
        "modal/_utils/grpc_testing.py",
        "modal/_utils/hash_utils.py",
        "modal/_utils/http_utils.py",
        "modal/_utils/name_utils.py",
        "modal/_utils/logger.py",
        "modal/_utils/mount_utils.py",
        "modal/_utils/package_utils.py",
        "modal/_utils/rand_pb_testing.py",
        "modal/_utils/shell_utils.py",
        "test/cls_test.py",  # see mypy bug above - but this works with pyright, so we run that instead
        "modal/_runtime/container_io_manager.py",
        "modal/io_streams.py",
        "modal/image.py",
        "modal/file_io.py",
        "modal/cli/import_refs.py",
        "modal/snapshot.py",
        "modal/config.py",
        "modal/object.py",
    ]
    ctx.run(f"pyright {' '.join(pyright_allowlist)}", pty=True)


@task
def check_copyright(ctx, fix=False):
    invalid_files = []
    for root, dirs, files in os.walk("."):
        fns = [
            os.path.join(root, fn)
            for fn in files
            if (
                fn.endswith(".py")
                # jupytext notebook formatted .py files can't be detected as notebooks if we put a
                # copyright comment at the top
                and not fn.endswith(".notebook.py")
                # ignore generated protobuf code
                and "/modal_proto" not in root
                # vendored code has a different copyright
                and "_vendor" not in root
                and "protoc_plugin" not in root
                # third-party code (i.e., in a local venv) has a different copyright
                and "/site-packages/" not in root
                and "/build/" not in root
                and "/.venv/" not in root
                and not re.search(r"/venv[0-9]*/", root)
            )
        ]
        for fn in fns:
            first_line = open(fn).readline()
            if not first_line.startswith(copyright_header_start):
                if fix:
                    print(f"Fixing {fn}...")
                    content = copyright_header_full + "\n" + open(fn).read()
                    with open(fn, "w") as g:
                        g.write(content)
                else:
                    invalid_files.append(fn)

    if invalid_files:
        for fn in invalid_files:
            print("Missing copyright:", fn)

        raise Exception(f"{len(invalid_files)} are missing copyright headers! Please run `inv check-copyright --fix`")


@task
def publish_base_mounts(ctx, no_confirm=False):
    from urllib.parse import urlparse

    from modal import config

    server_url = config.config["server_url"]
    if "localhost" not in urlparse(server_url).netloc and not no_confirm:
        answer = input(f"Modal server URL is '{server_url}' not localhost. Continue operation? [y/N]: ")
        if answer.upper() not in ["Y", "YES"]:
            exit("Aborting task.")
    for mount in ["modal_client_package", "python_standalone"]:
        ctx.run(f"{sys.executable} modal_global_objects/mounts/{mount}.py", pty=True)


@task
def update_build_number(ctx, new_build_number: Optional[int] = None):
    from modal_version import build_number as current_build_number

    new_build_number = int(new_build_number) if new_build_number else current_build_number + 1
    assert new_build_number > current_build_number

    # Add the current Git SHA to the file, so concurrent publish actions of the
    # client package result in merge conflicts.
    git_sha = ctx.run("git rev-parse --short=7 HEAD", hide="out").stdout.rstrip()

    with open("modal_version/_version_generated.py", "w") as f:
        f.write(
            f"""\
{copyright_header_full}

# Note: Reset this value to -1 whenever you make a minor `0.X` release of the client.
build_number = {new_build_number}  # git: {git_sha}
"""
        )


@task
def type_stubs(ctx):
    # We only generate type stubs for modules that contain synchronicity wrapped types
    from synchronicity.synchronizer import SYNCHRONIZER_ATTR

    stubs_to_remove = []
    for root, _, files in os.walk("modal"):
        for file in files:
            if file.endswith(".pyi"):
                stubs_to_remove.append(os.path.abspath(os.path.join(root, file)))
    for path in sorted(stubs_to_remove):
        os.remove(path)
        print(f"Removed {path}")

    def find_modal_modules(root: str = "modal"):
        modules = []
        path = importlib.import_module(root).__path__
        for _, name, is_pkg in pkgutil.iter_modules(path):
            full_name = f"{root}.{name}"
            if is_pkg:
                modules.extend(find_modal_modules(full_name))
            else:
                modules.append(full_name)
        return modules

    def get_wrapped_types(module_name: str) -> list[str]:
        module = importlib.import_module(module_name)
        return [
            name
            for name, obj in vars(module).items()
            if not module_name.startswith("modal.cli.")  # TODO we don't handle typer-wrapped functions well
            and hasattr(obj, "__module__")
            and obj.__module__ == module_name
            and not name.startswith("_")  # Avoid deprecation of _App.__getattr__
            and hasattr(obj, SYNCHRONIZER_ATTR)
        ]

    modules = [m for m in find_modal_modules() if len(get_wrapped_types(m))]
    subprocess.check_call(["python", "-m", "synchronicity.type_stubs", *modules])
    ctx.run("ruff format modal/ --exclude=*.py --no-respect-gitignore", pty=True)


@task
def update_changelog(ctx, sha: str = ""):
    # Parse a commit message for a GitHub PR number, defaulting to most recent commit
    res = ctx.run(f"git log --pretty=format:%s -n 1 {sha}", hide="stdout", warn=True)
    if res.exited:
        print("Failed to extract changelog update!")
        print("Last 5 commits:")
        res = ctx.run("git log --pretty=oneline -n 5")
        return
    m = re.search(r"\(#(\d+)\)$", res.stdout)
    if m:
        pull_number = m.group(1)
    else:
        print("Aborting: No PR number in commit message")
        return

    # Get the corresponding PR description via the GitHub API
    url = f"https://api.github.com/repos/modal-labs/modal-client/pulls/{pull_number}"
    headers = {"Authorization": f"Bearer {os.environ['GITHUB_TOKEN']}", "Accept": "application/vnd.github.v3+json"}
    response = requests.get(url, headers=headers).json()
    pr_description = response.get("body")
    if pr_description is None:
        print("Aborting: No PR description in response from GitHub API")
        return

    # Parse the PR description to get a changelog update
    comment_pattern = r"<!--.+?-->"
    pr_description = re.sub(comment_pattern, "", pr_description, flags=re.DOTALL)

    changelog_pattern = r"## Changelog\s*(.+)$"
    m = re.search(changelog_pattern, pr_description, flags=re.DOTALL)
    if m:
        update = m.group(1).strip()
    else:
        print("Aborting: No changelog section in PR description")
        return
    if not update:
        print("Aborting: Empty changelog in PR description")
        return

    # Read the existing changelog and split after the header so we can prepend new content
    with open("CHANGELOG.md") as fid:
        content = fid.read()
    token_pattern = "<!-- NEW CONTENT GENERATED BELOW. PLEASE PRESERVE THIS COMMENT. -->"
    m = re.search(token_pattern, content)
    if m:
        break_idx = m.span()[1]
        header = content[:break_idx]
        previous_changelog = content[break_idx:]
    else:
        print("Aborting: Could not find token in existing changelog to mark insertion spot")
        return

    # Build the new changelog and write it out
    from modal_version import __version__

    date = datetime.datetime.now().strftime("%Y-%m-%d")
    new_section = f"### {__version__} ({date})\n\n{update}"
    final_content = f"{header}\n\n{new_section}\n\n{previous_changelog}"
    with open("CHANGELOG.md", "w") as fid:
        fid.write(final_content)


@task
def show_deprecations(ctx):
    def get_modal_source_files() -> list[str]:
        source_files: list[str] = []
        for root, _, files in os.walk("modal"):
            for file in files:
                if file.endswith(".py"):
                    source_files.append(os.path.join(root, file))
        return source_files

    class FunctionCallVisitor(ast.NodeVisitor):
        def __init__(self, fname):
            self.fname = fname
            self.deprecations = []
            self.assignments = {}
            self.current_class = None
            self.current_function = None

        def visit_ClassDef(self, node):
            self.current_class = node.name
            self.generic_visit(node)
            self.current_class = None

        def visit_FunctionDef(self, node):
            self.current_function = node.name
            self.assignments["__doc__"] = ast.get_docstring(node)
            self.generic_visit(node)
            self.current_function = None
            self.assignments.pop("__doc__", None)

        def visit_Assign(self, node):
            for target in node.targets:
                if isinstance(target, ast.Name):
                    self.assignments[target.id] = node.value
            self.generic_visit(node)

        def visit_Attribute(self, node):
            self.assignments[node.attr] = node.value
            self.generic_visit(node)

        def visit_Call(self, node):
            func_name_to_level = {
                "deprecation_warning": "[yellow]warning[/yellow]",
                "deprecation_error": "[red]error[/red]",
                # We may add a flag to make renamed_parameter error instead of warn
                # in which case this would get a little bit more complicated.
                "renamed_parameter": "[yellow]warning[/yellow]",
            }
            if (
                isinstance(node.func, ast.Name)
                and node.func.id in func_name_to_level
                and isinstance(node.args[0], ast.Tuple)
            ):
                depr_date = date(*(getattr(elt, "n") for elt in node.args[0].elts))
                function = (
                    f"{self.current_class}.{self.current_function}" if self.current_class else self.current_function
                )
                if node.func.id == "renamed_parameter":
                    old_name = getattr(node.args[1], "s")
                    new_name = getattr(node.args[2], "s")
                    message = f"Renamed parameter: {old_name} -> {new_name}"
                else:
                    message = node.args[1]
                    # Handle a few different ways that the message can get passed to the deprecation helper
                    # since it's not always a literal string (e.g. it's often a functions .__doc__ attribute)
                    if isinstance(message, ast.Name):
                        message = self.assignments.get(message.id, "")
                    if isinstance(message, ast.Attribute):
                        message = self.assignments.get(message.attr, "")
                    if isinstance(message, ast.Constant):
                        message = message.s
                    elif isinstance(message, ast.JoinedStr):
                        message = "".join(v.s for v in message.values if isinstance(v, ast.Constant))
                    else:
                        message = str(message)
                    message = message.replace("\n", " ")
                    if len(message) > (max_length := 80):
                        message = message[:max_length] + "..."

                level = func_name_to_level[node.func.id]
                self.deprecations.append((str(depr_date), level, f"{self.fname}:{node.lineno}", function, message))

    files = get_modal_source_files()
    deprecations = []
    for fname in files:
        with open(fname) as f:
            tree = ast.parse(f.read())
        visitor = FunctionCallVisitor(fname)
        visitor.visit(tree)
        deprecations.extend(visitor.deprecations)

    console = Console()
    table = Table("Date", "Level", "Location", "Function", "Message")
    for row in sorted(deprecations, key=lambda r: r[0]):
        table.add_row(*row)
    console.print(table)


================================================
File: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: "v0.9.6"
    hooks:
      - id: ruff
        # Autofix, and respect `exclude` and `extend-exclude` settings.
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format


================================================
File: modal/__init__.py
================================================
# Copyright Modal Labs 2022
import sys

if sys.version_info[:2] < (3, 9):
    raise RuntimeError("This version of Modal requires at least Python 3.9")
if sys.version_info[:2] >= (3, 14):
    raise RuntimeError("This version of Modal does not support Python 3.14+")

from modal_version import __version__

try:
    from ._runtime.execution_context import current_function_call_id, current_input_id, interact, is_local
    from ._tunnel import Tunnel, forward
    from .app import App, Stub
    from .client import Client
    from .cloud_bucket_mount import CloudBucketMount
    from .cls import Cls, parameter
    from .dict import Dict
    from .exception import Error
    from .file_pattern_matcher import FilePatternMatcher
    from .functions import Function, FunctionCall
    from .image import Image
    from .mount import Mount
    from .network_file_system import NetworkFileSystem
    from .output import enable_output
    from .partial_function import asgi_app, batched, build, enter, exit, method, web_endpoint, web_server, wsgi_app
    from .proxy import Proxy
    from .queue import Queue
    from .retries import Retries
    from .sandbox import Sandbox
    from .schedule import Cron, Period
    from .scheduler_placement import SchedulerPlacement
    from .secret import Secret
    from .snapshot import SandboxSnapshot
    from .volume import Volume
except Exception:
    print()
    print("#" * 80)
    print("#" + "Something with the Modal installation seems broken.".center(78) + "#")
    print("#" + "Please email support@modal.com and we will try to help!".center(78) + "#")
    print("#" * 80)
    print()
    raise

__all__ = [
    "__version__",
    "App",
    "Client",
    "Cls",
    "Cron",
    "Dict",
    "Error",
    "FilePatternMatcher",
    "Function",
    "FunctionCall",
    "Image",
    "Mount",
    "NetworkFileSystem",
    "Period",
    "Proxy",
    "Queue",
    "Retries",
    "CloudBucketMount",
    "Sandbox",
    "SandboxSnapshot",
    "SchedulerPlacement",
    "Secret",
    "Stub",
    "Tunnel",
    "Volume",
    "asgi_app",
    "batched",
    "build",
    "current_function_call_id",
    "current_input_id",
    "enable_output",
    "enter",
    "exit",
    "forward",
    "is_local",
    "interact",
    "method",
    "parameter",
    "web_endpoint",
    "web_server",
    "wsgi_app",
]


================================================
File: modal/__main__.py
================================================
# Copyright Modal Labs 2022
import sys

from ._traceback import reduce_traceback_to_user_code
from .cli._traceback import highlight_modal_deprecation_warnings, setup_rich_traceback
from .cli.entry_point import entrypoint_cli
from .cli.import_refs import _CliUserExecutionError
from .config import config


def main():
    # Setup rich tracebacks, but only on user's end, when using the Modal CLI.
    setup_rich_traceback()
    highlight_modal_deprecation_warnings()

    try:
        entrypoint_cli()

    except _CliUserExecutionError as exc:
        if config.get("traceback"):
            raise

        assert exc.__cause__  # We should always raise this class from another error
        tb = reduce_traceback_to_user_code(exc.__cause__.__traceback__, exc.user_source)
        sys.excepthook(type(exc.__cause__), exc.__cause__, tb)
        sys.exit(1)

    except Exception as exc:
        if (
            # User has asked to alway see full tracebacks
            config.get("traceback")
            # The exception message is empty, so we need to provide _some_ actionable information
            or not str(exc)
        ):
            raise

        from grpclib import GRPCError, Status
        from rich.console import Console
        from rich.panel import Panel
        from rich.text import Text

        if isinstance(exc, GRPCError):
            status_map = {
                Status.ABORTED: "Aborted",
                Status.ALREADY_EXISTS: "Already exists",
                Status.CANCELLED: "Cancelled",
                Status.DATA_LOSS: "Data loss",
                Status.DEADLINE_EXCEEDED: "Deadline exceeded",
                Status.FAILED_PRECONDITION: "Failed precondition",
                Status.INTERNAL: "Internal",
                Status.INVALID_ARGUMENT: "Invalid",
                Status.NOT_FOUND: "Not found",
                Status.OUT_OF_RANGE: "Out of range",
                Status.PERMISSION_DENIED: "Permission denied",
                Status.RESOURCE_EXHAUSTED: "Resource exhausted",
                Status.UNAUTHENTICATED: "Unauthenticaed",
                Status.UNAVAILABLE: "Unavailable",
                Status.UNIMPLEMENTED: "Unimplemented",
                Status.UNKNOWN: "Unknown",
            }
            title = f"Error: {status_map.get(exc.status, 'Unknown')}"
            content = str(exc.message)
            if exc.details:
                content += f"\n\nDetails: {exc.details}"
        else:
            title = "Error"
            content = str(exc)
            if notes := getattr(exc, "__notes__", []):
                content = f"{content}\n\nNote: {' '.join(notes)}"

        console = Console(stderr=True)
        panel = Panel(Text(content), title=title, title_align="left", border_style="red")
        console.print(panel, highlight=False)
        sys.exit(1)


if __name__ == "__main__":
    main()


================================================
File: modal/_clustered_functions.py
================================================
# Copyright Modal Labs 2024
import os
import socket
from dataclasses import dataclass
from typing import Optional

from modal._utils.async_utils import synchronize_api
from modal._utils.grpc_utils import retry_transient_errors
from modal.client import _Client
from modal.exception import InvalidError
from modal_proto import api_pb2


@dataclass
class ClusterInfo:
    rank: int
    container_ips: list[str]


cluster_info: Optional[ClusterInfo] = None


def get_cluster_info() -> ClusterInfo:
    if cluster_info is None:
        raise InvalidError(
            "Cluster info not initialized. Please ensure that you are "
            "calling get_cluster_info() from a clustered function."
        )
    return cluster_info


async def _initialize_clustered_function(client: _Client, task_id: str, world_size: int):
    global cluster_info

    def get_i6pn():
        """Returns the ipv6 address assigned to this container."""
        return socket.getaddrinfo("i6pn.modal.local", None, socket.AF_INET6)[0][4][0]

    hostname = socket.gethostname()
    container_ip = get_i6pn()

    # nccl's default host ID is $(hostname)$(cat /proc/sys/kernel/random/boot_id).
    # on runc, if two i6pn-linked containers get scheduled on the same worker,
    # their boot ID and hostname will both be identical, causing nccl to break.
    # As a workaround, we can explicitly specify a unique host ID here.
    # See MOD-4067.
    os.environ["NCCL_HOSTID"] = f"{hostname}{container_ip}"

    # We found these settings to work well in most cases. You may be able to achieve
    # better performance by tuning these settings.
    if os.environ["MODAL_CLOUD_PROVIDER"] in ("CLOUD_PROVIDER_GCP", "CLOUD_PROVIDER_OCI"):
        os.environ["NCCL_SOCKET_NTHREADS"] = "4"
        os.environ["NCCL_NSOCKS_PERTHREAD"] = "1"
    elif os.environ["MODAL_CLOUD_PROVIDER"] == "CLOUD_PROVIDER_AWS":
        os.environ["NCCL_SOCKET_NTHREADS"] = "2"
        os.environ["NCCL_NSOCKS_PERTHREAD"] = "8"
    else:
        os.environ["NCCL_SOCKET_NTHREADS"] = "1"
        os.environ["NCCL_NSOCKS_PERTHREAD"] = "1"

    if world_size > 1:
        resp: api_pb2.TaskClusterHelloResponse = await retry_transient_errors(
            client.stub.TaskClusterHello,
            api_pb2.TaskClusterHelloRequest(
                task_id=task_id,
                container_ip=container_ip,
            ),
        )
        cluster_info = ClusterInfo(
            rank=resp.cluster_rank,
            container_ips=resp.container_ips,
        )
    else:
        cluster_info = ClusterInfo(
            rank=0,
            container_ips=[container_ip],
        )


initialize_clustered_function = synchronize_api(_initialize_clustered_function)


================================================
File: modal/_container_entrypoint.py
================================================
# Copyright Modal Labs 2022
# ruff: noqa: E402
import os

from modal._runtime.user_code_imports import Service, import_class_service, import_single_function_service

telemetry_socket = os.environ.get("MODAL_TELEMETRY_SOCKET")
if telemetry_socket:
    from ._runtime.telemetry import instrument_imports

    instrument_imports(telemetry_socket)

import asyncio
import concurrent.futures
import inspect
import queue
import signal
import sys
import threading
import time
from collections.abc import Sequence
from typing import TYPE_CHECKING, Any, Callable, Optional

from google.protobuf.message import Message

from modal._clustered_functions import initialize_clustered_function
from modal._partial_function import (
    _find_callables_for_obj,
    _PartialFunctionFlags,
)
from modal._proxy_tunnel import proxy_tunnel
from modal._serialization import deserialize_params
from modal._utils.async_utils import TaskContext, synchronizer
from modal._utils.function_utils import (
    callable_has_non_self_params,
)
from modal.app import App, _App
from modal.client import Client, _Client
from modal.config import logger
from modal.exception import ExecutionError, InputCancellation, InvalidError
from modal.running_app import RunningApp, running_app_from_layout
from modal_proto import api_pb2

from ._runtime.container_io_manager import (
    ContainerIOManager,
    IOContext,
    UserException,
    _ContainerIOManager,
)
from ._runtime.execution_context import _set_current_context_ids

if TYPE_CHECKING:
    import modal._object
    import modal._runtime.container_io_manager


class DaemonizedThreadPool:
    # Used instead of ThreadPoolExecutor, since the latter won't allow
    # the interpreter to shut down before the currently running tasks
    # have finished
    def __init__(self, max_threads: int):
        self.max_threads = max_threads

    def __enter__(self):
        self.spawned_workers = 0
        self.inputs: queue.Queue[Any] = queue.Queue()
        self.finished = threading.Event()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.finished.set()

        if exc_type is None:
            self.inputs.join()
        else:
            # special case - allows us to exit the
            if self.inputs.unfinished_tasks:
                logger.info(
                    f"Exiting DaemonizedThreadPool with {self.inputs.unfinished_tasks} active "
                    f"inputs due to exception: {repr(exc_type)}"
                )

    def submit(self, func, *args):
        def worker_thread():
            while not self.finished.is_set():
                try:
                    _func, _args = self.inputs.get(timeout=1)
                except queue.Empty:
                    continue
                try:
                    _func(*_args)
                except BaseException:
                    logger.exception(f"Exception raised by {_func} in DaemonizedThreadPool worker!")
                self.inputs.task_done()

        if self.spawned_workers < self.max_threads:
            threading.Thread(target=worker_thread, daemon=True).start()
            self.spawned_workers += 1

        self.inputs.put((func, args))


class UserCodeEventLoop:
    """Run an async event loop as a context manager and handle signals.

    This will run all *user supplied* async code, i.e. async functions, as well as async enter/exit managers

    The following signals are handled while a coroutine is running on the event loop until
    completion (and then handlers are deregistered):

    - `SIGUSR1`: converted to an async task cancellation. Note that this only affects the event
      loop, and the signal handler defined here doesn't run for sync functions.
    - `SIGINT`: Unless the global signal handler has been set to SIGIGN, the loop's signal handler
        is set to cancel the current task and raise KeyboardInterrupt to the caller.
    """

    def __enter__(self):
        self.loop = asyncio.new_event_loop()
        self.tasks = set()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self.loop.run_until_complete(self.loop.shutdown_asyncgens())
        if sys.version_info[:2] >= (3, 9):
            self.loop.run_until_complete(self.loop.shutdown_default_executor())  # Introduced in Python 3.9

        for task in self.tasks:
            task.cancel()

        self.loop.close()

    def create_task(self, coro):
        task = self.loop.create_task(coro)
        self.tasks.add(task)
        task.add_done_callback(self.tasks.discard)
        return task

    def run(self, coro):
        task = asyncio.ensure_future(coro, loop=self.loop)
        self._sigints = 0

        def _sigint_handler():
            # cancel the task in order to have run_until_complete return soon and
            # prevent a bunch of unwanted tracebacks when shutting down the
            # event loop.

            # this basically replicates the sigint handler installed by asyncio.run()
            self._sigints += 1
            if self._sigints == 1:
                # first sigint is graceful
                task.cancel()
                return

            # this should normally not happen, but the second sigint would "hard kill" the event loop!
            raise KeyboardInterrupt()

        ignore_sigint = signal.getsignal(signal.SIGINT) == signal.SIG_IGN
        if not ignore_sigint:
            self.loop.add_signal_handler(signal.SIGINT, _sigint_handler)

        # Before Python 3.9 there is no argument to Task.cancel
        if sys.version_info[:2] >= (3, 9):
            self.loop.add_signal_handler(signal.SIGUSR1, task.cancel, "Input was cancelled by user")
        else:
            self.loop.add_signal_handler(signal.SIGUSR1, task.cancel)

        try:
            return self.loop.run_until_complete(task)
        except asyncio.CancelledError:
            if self._sigints > 0:
                raise KeyboardInterrupt()
        finally:
            self.loop.remove_signal_handler(signal.SIGUSR1)
            if not ignore_sigint:
                self.loop.remove_signal_handler(signal.SIGINT)


def call_function(
    user_code_event_loop: UserCodeEventLoop,
    container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager",
    finalized_functions: dict[str, "modal._runtime.user_code_imports.FinalizedFunction"],
    batch_max_size: int,
    batch_wait_ms: int,
):
    async def run_input_async(io_context: IOContext) -> None:
        started_at = time.time()
        input_ids, function_call_ids = io_context.input_ids, io_context.function_call_ids
        reset_context = _set_current_context_ids(input_ids, function_call_ids)
        async with container_io_manager.handle_input_exception.aio(io_context, started_at):
            res = io_context.call_finalized_function()
            # TODO(erikbern): any exception below shouldn't be considered a user exception
            if io_context.finalized_function.is_generator:
                if not inspect.isasyncgen(res):
                    raise InvalidError(f"Async generator function returned value of type {type(res)}")

                # Send up to this many outputs at a time.
                generator_queue: asyncio.Queue[Any] = await container_io_manager._queue_create.aio(1024)
                generator_output_task = asyncio.create_task(
                    container_io_manager.generator_output_task.aio(
                        function_call_ids[0],
                        io_context.finalized_function.data_format,
                        generator_queue,
                    )
                )

                item_count = 0
                async for value in res:
                    await container_io_manager._queue_put.aio(generator_queue, value)
                    item_count += 1

                await container_io_manager._queue_put.aio(generator_queue, _ContainerIOManager._GENERATOR_STOP_SENTINEL)
                await generator_output_task  # Wait to finish sending generator outputs.
                message = api_pb2.GeneratorDone(items_total=item_count)
                await container_io_manager.push_outputs.aio(
                    io_context,
                    started_at,
                    message,
                    api_pb2.DATA_FORMAT_GENERATOR_DONE,
                )
            else:
                if not inspect.iscoroutine(res) or inspect.isgenerator(res) or inspect.isasyncgen(res):
                    raise InvalidError(
                        f"Async (non-generator) function returned value of type {type(res)}"
                        " You might need to use @app.function(..., is_generator=True)."
                    )
                value = await res
                await container_io_manager.push_outputs.aio(
                    io_context,
                    started_at,
                    value,
                    io_context.finalized_function.data_format,
                )
        reset_context()

    def run_input_sync(io_context: IOContext) -> None:
        started_at = time.time()
        input_ids, function_call_ids = io_context.input_ids, io_context.function_call_ids
        reset_context = _set_current_context_ids(input_ids, function_call_ids)
        with container_io_manager.handle_input_exception(io_context, started_at):
            res = io_context.call_finalized_function()

            # TODO(erikbern): any exception below shouldn't be considered a user exception
            if io_context.finalized_function.is_generator:
                if not inspect.isgenerator(res):
                    raise InvalidError(f"Generator function returned value of type {type(res)}")

                # Send up to this many outputs at a time.
                generator_queue: asyncio.Queue[Any] = container_io_manager._queue_create(1024)
                generator_output_task: concurrent.futures.Future = container_io_manager.generator_output_task(  # type: ignore
                    function_call_ids[0],
                    io_context.finalized_function.data_format,
                    generator_queue,
                    _future=True,  # type: ignore  # Synchronicity magic to return a future.
                )

                item_count = 0
                for value in res:
                    container_io_manager._queue_put(generator_queue, value)
                    item_count += 1

                container_io_manager._queue_put(generator_queue, _ContainerIOManager._GENERATOR_STOP_SENTINEL)
                generator_output_task.result()  # Wait to finish sending generator outputs.
                message = api_pb2.GeneratorDone(items_total=item_count)
                container_io_manager.push_outputs(io_context, started_at, message, api_pb2.DATA_FORMAT_GENERATOR_DONE)
            else:
                if inspect.iscoroutine(res) or inspect.isgenerator(res) or inspect.isasyncgen(res):
                    raise InvalidError(
                        f"Sync (non-generator) function return value of type {type(res)}."
                        " You might need to use @app.function(..., is_generator=True)."
                    )
                container_io_manager.push_outputs(
                    io_context, started_at, res, io_context.finalized_function.data_format
                )
        reset_context()

    if container_io_manager.target_concurrency > 1:
        with DaemonizedThreadPool(max_threads=container_io_manager.max_concurrency) as thread_pool:

            def make_async_cancel_callback(task):
                def f():
                    user_code_event_loop.loop.call_soon_threadsafe(task.cancel)

                return f

            did_sigint = False

            def cancel_callback_sync():
                nonlocal did_sigint
                # We only want one sigint even if multiple inputs are cancelled
                # A second sigint would forcibly shut down the event loop and spew
                # out a bunch of tracebacks, which we only want to happen in case
                # the worker kills this process after a failed self-termination
                if not did_sigint:
                    did_sigint = True
                    logger.warning(
                        "User cancelling input of non-async functions with allow_concurrent_inputs > 1.\n"
                        "This shuts down the container, causing concurrently running inputs to be "
                        "rescheduled in other containers."
                    )
                    os.kill(os.getpid(), signal.SIGINT)

            async def run_concurrent_inputs():
                # all run_input coroutines will have completed by the time we leave the execution context
                # but the wrapping *tasks* may not yet have been resolved, so we add a 0.01s
                # for them to resolve gracefully:
                async with TaskContext(0.01) as task_context:
                    async for io_context in container_io_manager.run_inputs_outputs.aio(
                        finalized_functions, batch_max_size, batch_wait_ms
                    ):
                        # Note that run_inputs_outputs will not return until all the input slots are released
                        # so that they can be acquired by the run_inputs_outputs finalizer
                        # This prevents leaving the task_context before outputs have been created
                        # TODO: refactor to make this a bit more easy to follow?
                        if io_context.finalized_function.is_async:
                            input_task = task_context.create_task(run_input_async(io_context))
                            io_context.set_cancel_callback(make_async_cancel_callback(input_task))
                        else:
                            # run sync input in thread
                            thread_pool.submit(run_input_sync, io_context)
                            io_context.set_cancel_callback(cancel_callback_sync)

            user_code_event_loop.run(run_concurrent_inputs())
    else:
        for io_context in container_io_manager.run_inputs_outputs(finalized_functions, batch_max_size, batch_wait_ms):
            # This goes to a registered signal handler for sync Modal functions, or to the
            # `UserCodeEventLoop` for async functions.
            #
            # We only send this signal on functions that do not have concurrent inputs enabled.
            # This allows us to do fine-grained input cancellation. On sync functions, the
            # SIGUSR1 signal should interrupt the main thread where user code is running,
            # raising an InputCancellation() exception. On async functions, the signal should
            # reach a handler in UserCodeEventLoop, which cancels the task.
            io_context.set_cancel_callback(lambda: os.kill(os.getpid(), signal.SIGUSR1))

            if io_context.finalized_function.is_async:
                user_code_event_loop.run(run_input_async(io_context))
            else:
                # Set up a custom signal handler for `SIGUSR1`, which gets translated to an InputCancellation
                # during function execution. This is sent to cancel inputs from the user
                def _cancel_input_signal_handler(signum, stackframe):
                    raise InputCancellation("Input was cancelled by user")

                usr1_handler = signal.signal(signal.SIGUSR1, _cancel_input_signal_handler)
                # run this sync code in the main thread, blocking the "userland" event loop
                # this lets us cancel it using a signal handler that raises an exception
                try:
                    run_input_sync(io_context)
                finally:
                    signal.signal(signal.SIGUSR1, usr1_handler)  # reset signal handler


def get_active_app_fallback(function_def: api_pb2.Function) -> _App:
    # This branch is reached in the special case that the imported function/class is:
    # 1) not serialized, and
    # 2) isn't a FunctionHandle - i.e, not decorated at definition time
    # Look at all instantiated apps - if there is only one with the indicated name, use that one
    app_name: Optional[str] = function_def.app_name or None  # coalesce protobuf field to None
    matching_apps = _App._all_apps.get(app_name, [])
    if len(matching_apps) == 1:
        active_app: _App = matching_apps[0]
        return active_app

    if len(matching_apps) > 1:
        if app_name is not None:
            warning_sub_message = f"app with the same name ('{app_name}')"
        else:
            warning_sub_message = "unnamed app"
        logger.warning(
            f"You have more than one {warning_sub_message}. "
            "It's recommended to name all your Apps uniquely when using multiple apps"
        )

    # If we don't have an active app, create one on the fly
    # The app object is used to carry the app layout etc
    return _App()


def call_lifecycle_functions(
    event_loop: UserCodeEventLoop,
    container_io_manager,  #: ContainerIOManager,  TODO: this type is generated at runtime
    funcs: Sequence[Callable[..., Any]],
) -> None:
    """Call function(s), can be sync or async, but any return values are ignored."""
    with container_io_manager.handle_user_exception():
        for func in funcs:
            # We are deprecating parametrized exit methods but want to gracefully handle old code.
            # We can remove this once the deprecation in the actual @exit decorator is enforced.
            args = (None, None, None) if callable_has_non_self_params(func) else ()
            # in case func is non-async, it's executed here and sigint will by default
            # interrupt it using a KeyboardInterrupt exception
            res = func(*args)
            if inspect.iscoroutine(res):
                # if however func is async, we have to jump through some hoops
                event_loop.run(res)


def main(container_args: api_pb2.ContainerArguments, client: Client):
    # This is a bit weird but we need both the blocking and async versions of ContainerIOManager.
    # At some point, we should fix that by having built-in support for running "user code"
    container_io_manager = ContainerIOManager(container_args, client)
    active_app: _App
    service: Service
    function_def = container_args.function_def
    is_auto_snapshot: bool = function_def.is_auto_snapshot
    # The worker sets this flag to "1" for snapshot and restore tasks. Otherwise, this flag is unset,
    # in which case snapshots should be disabled.
    is_snapshotting_function = (
        function_def.is_checkpointing_function and os.environ.get("MODAL_ENABLE_SNAP_RESTORE", "0") == "1"
    )

    _client: _Client = synchronizer._translate_in(client)  # TODO(erikbern): ugly

    # Call ContainerHello - currently a noop but might be used later for things
    container_io_manager.hello()

    with container_io_manager.heartbeats(is_snapshotting_function), UserCodeEventLoop() as event_loop:
        # If this is a serialized function, fetch the definition from the server
        if function_def.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED:
            ser_cls, ser_fun = container_io_manager.get_serialized_function()
        else:
            ser_cls, ser_fun = None, None

        # Initialize the function, importing user code.
        with container_io_manager.handle_user_exception():
            if container_args.serialized_params:
                param_args, param_kwargs = deserialize_params(container_args.serialized_params, function_def, _client)
            else:
                param_args = ()
                param_kwargs = {}

            if function_def.is_class:
                service = import_class_service(
                    function_def,
                    ser_cls,
                    param_args,
                    param_kwargs,
                )
            else:
                service = import_single_function_service(
                    function_def,
                    ser_cls,
                    ser_fun,
                    param_args,
                    param_kwargs,
                )

            # If the cls/function decorator was applied in local scope, but the app is global, we can look it up
            if service.app is not None:
                active_app = service.app
            else:
                # if the app can't be inferred by the imported function, use name-based fallback
                active_app = get_active_app_fallback(function_def)

            if function_def.pty_info.pty_type == api_pb2.PTYInfo.PTY_TYPE_SHELL:
                # Concurrency and batching doesn't apply for `modal shell`.
                batch_max_size = 0
                batch_wait_ms = 0
            else:
                batch_max_size = function_def.batch_max_size or 0
                batch_wait_ms = function_def.batch_linger_ms or 0

        # Get ids and metadata for objects (primarily functions and classes) on the app
        container_app: RunningApp = running_app_from_layout(container_args.app_id, container_args.app_layout)

        # Initialize objects on the app.
        # This is basically only functions and classes - anything else is deprecated and will be unsupported soon
        app: App = synchronizer._translate_out(active_app)
        app._init_container(client, container_app)

        # Hydrate all function dependencies.
        # TODO(erikbern): we an remove this once we
        # 1. Enable lazy hydration for all objects
        # 2. Fully deprecate .new() objects
        if service.service_deps is not None:  # this is not set for serialized or non-global scope functions
            dep_object_ids: list[str] = [dep.object_id for dep in function_def.object_dependencies]
            if len(service.service_deps) != len(dep_object_ids):
                raise ExecutionError(
                    f"Function has {len(service.service_deps)} dependencies"
                    f" but container got {len(dep_object_ids)} object ids.\n"
                    f"Code deps: {service.service_deps}\n"
                    f"Object ids: {dep_object_ids}"
                )
            for object_id, obj in zip(dep_object_ids, service.service_deps):
                metadata: Message = container_app.object_handle_metadata[object_id]
                obj._hydrate(object_id, _client, metadata)

        # Initialize clustered functions.
        if function_def._experimental_group_size > 0:
            initialize_clustered_function(
                client,
                container_args.task_id,
                function_def._experimental_group_size,
            )

        # Identify all "enter" methods that need to run before we snapshot.
        if service.user_cls_instance is not None and not is_auto_snapshot:
            pre_snapshot_methods = _find_callables_for_obj(
                service.user_cls_instance, _PartialFunctionFlags.ENTER_PRE_SNAPSHOT
            )
            call_lifecycle_functions(event_loop, container_io_manager, list(pre_snapshot_methods.values()))

        # If this container is being used to create a checkpoint, checkpoint the container after
        # global imports and initialization. Checkpointed containers run from this point onwards.
        if is_snapshotting_function:
            container_io_manager.memory_snapshot()

        # Install hooks for interactive functions.
        def breakpoint_wrapper():
            # note: it would be nice to not have breakpoint_wrapper() included in the backtrace
            container_io_manager.interact(from_breakpoint=True)
            import pdb

            frame = inspect.currentframe().f_back

            pdb.Pdb().set_trace(frame)

        sys.breakpointhook = breakpoint_wrapper

        # Identify the "enter" methods to run after resuming from a snapshot.
        if service.user_cls_instance is not None and not is_auto_snapshot:
            post_snapshot_methods = _find_callables_for_obj(
                service.user_cls_instance, _PartialFunctionFlags.ENTER_POST_SNAPSHOT
            )
            call_lifecycle_functions(event_loop, container_io_manager, list(post_snapshot_methods.values()))

        with container_io_manager.handle_user_exception():
            finalized_functions = service.get_finalized_functions(function_def, container_io_manager)
        # Execute the function.
        lifespan_background_tasks = []
        try:
            for finalized_function in finalized_functions.values():
                if finalized_function.lifespan_manager:
                    lifespan_background_tasks.append(
                        event_loop.create_task(finalized_function.lifespan_manager.background_task())
                    )
                    with container_io_manager.handle_user_exception():
                        event_loop.run(finalized_function.lifespan_manager.lifespan_startup())
            call_function(
                event_loop,
                container_io_manager,
                finalized_functions,
                batch_max_size,
                batch_wait_ms,
            )
        finally:
            # Run exit handlers. From this point onward, ignore all SIGINT signals that come from
            # graceful shutdowns originating on the worker, as well as stray SIGUSR1 signals that
            # may have been sent to cancel inputs.
            int_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)
            usr1_handler = signal.signal(signal.SIGUSR1, signal.SIG_IGN)

            try:
                try:
                    # run lifespan shutdown for asgi apps
                    for finalized_function in finalized_functions.values():
                        if finalized_function.lifespan_manager:
                            with container_io_manager.handle_user_exception():
                                event_loop.run(finalized_function.lifespan_manager.lifespan_shutdown())
                finally:
                    # no need to keep the lifespan asgi call around - we send it no more messages
                    for lifespan_background_task in lifespan_background_tasks:
                        lifespan_background_task.cancel()  # prevent dangling tasks

                    # Identify "exit" methods and run them.
                    # want to make sure this is called even if the lifespan manager fails
                    if service.user_cls_instance is not None and not is_auto_snapshot:
                        exit_methods = _find_callables_for_obj(service.user_cls_instance, _PartialFunctionFlags.EXIT)
                        call_lifecycle_functions(event_loop, container_io_manager, list(exit_methods.values()))

                # Finally, commit on exit to catch uncommitted volume changes and surface background
                # commit errors.
                container_io_manager.volume_commit(
                    [v.volume_id for v in function_def.volume_mounts if v.allow_background_commits]
                )
            finally:
                # Restore the original signal handler, needed for container_test hygiene since the
                # test runs `main()` multiple times in the same process.
                signal.signal(signal.SIGINT, int_handler)
                signal.signal(signal.SIGUSR1, usr1_handler)


if __name__ == "__main__":
    logger.debug("Container: starting")

    container_args = api_pb2.ContainerArguments()
    container_arguments_path: Optional[str] = os.environ.get("MODAL_CONTAINER_ARGUMENTS_PATH")
    if container_arguments_path is None:
        raise RuntimeError("No path to the container arguments file provided!")
    container_args.ParseFromString(open(container_arguments_path, "rb").read())

    # Note that we're creating the client in a synchronous context, but it will be running in a separate thread.
    # This is good because if the function is long running then we the client can still send heartbeats
    # The only caveat is a bunch of calls will now cross threads, which adds a bit of overhead?
    client = Client.from_env()

    try:
        with proxy_tunnel(container_args.proxy_info):
            try:
                main(container_args, client)
            except UserException:
                logger.info("User exception caught, exiting")
    except KeyboardInterrupt:
        logger.debug("Container: interrupted")

    # Detect if any non-daemon threads are still running, which will prevent the Python interpreter
    # from shutting down. The sleep(0) here is needed for finished ThreadPoolExecutor resources to
    # shut down without triggering this warning (e.g., `@wsgi_app()`).
    time.sleep(0)
    lingering_threads: list[threading.Thread] = []
    for thread in threading.enumerate():
        current_thread = threading.get_ident()
        if thread.ident is not None and thread.ident != current_thread and not thread.daemon and thread.is_alive():
            lingering_threads.append(thread)
    if lingering_threads:
        thread_names = ", ".join(t.name for t in lingering_threads)
        logger.warning(
            f"Detected {len(lingering_threads)} background thread(s) [{thread_names}] still running "
            "after container exit. This will prevent runner shutdown for up to 30 seconds."
        )

    logger.debug("Container: done")


================================================
File: modal/_ipython.py
================================================
# Copyright Modal Labs 2022
import sys


def is_notebook(stdout=None):
    ipykernel_iostream = sys.modules.get("ipykernel.iostream")
    if ipykernel_iostream is None:
        return False
    if stdout is None:
        stdout = sys.stdout
    return isinstance(stdout, ipykernel_iostream.OutStream)


================================================
File: modal/_location.py
================================================
# Copyright Modal Labs 2022
import modal_proto.api_pb2


def display_location(cloud_provider: "modal_proto.api_pb2.CloudProvider.V") -> str:
    if cloud_provider == modal_proto.api_pb2.CLOUD_PROVIDER_GCP:
        return "GCP (us-east1)"
    elif cloud_provider == modal_proto.api_pb2.CLOUD_PROVIDER_AWS:
        return "AWS (us-east-1)"
    else:
        return ""


================================================
File: modal/_object.py
================================================
# Copyright Modal Labs 2022
import typing
import uuid
from collections.abc import Awaitable, Hashable, Sequence
from functools import wraps
from typing import Callable, ClassVar, Optional

from google.protobuf.message import Message
from typing_extensions import Self

from ._resolver import Resolver
from ._utils.async_utils import aclosing
from ._utils.deprecation import deprecation_warning
from .client import _Client
from .config import config, logger
from .exception import ExecutionError, InvalidError

EPHEMERAL_OBJECT_HEARTBEAT_SLEEP: int = 300


def _get_environment_name(environment_name: Optional[str] = None, resolver: Optional[Resolver] = None) -> Optional[str]:
    if environment_name:
        return environment_name
    elif resolver and resolver.environment_name:
        return resolver.environment_name
    else:
        return config.get("environment")


class _Object:
    _type_prefix: ClassVar[Optional[str]] = None
    _prefix_to_type: ClassVar[dict[str, type]] = {}

    # For constructors
    _load: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]]
    _preload: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]]
    _rep: str
    _is_another_app: bool
    _hydrate_lazily: bool
    _deps: Optional[Callable[..., Sequence["_Object"]]]
    _deduplication_key: Optional[Callable[[], Awaitable[Hashable]]] = None

    # For hydrated objects
    _object_id: Optional[str]
    _client: Optional[_Client]
    _is_hydrated: bool
    _is_rehydrated: bool

    @classmethod
    def __init_subclass__(cls, type_prefix: Optional[str] = None):
        super().__init_subclass__()
        if type_prefix is not None:
            cls._type_prefix = type_prefix
            cls._prefix_to_type[type_prefix] = cls

    def __init__(self, *args, **kwargs):
        raise InvalidError(f"Class {type(self).__name__} has no constructor. Use class constructor methods instead.")

    def _init(
        self,
        rep: str,
        load: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]] = None,
        is_another_app: bool = False,
        preload: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]] = None,
        hydrate_lazily: bool = False,
        deps: Optional[Callable[..., Sequence["_Object"]]] = None,
        deduplication_key: Optional[Callable[[], Awaitable[Hashable]]] = None,
    ):
        self._local_uuid = str(uuid.uuid4())
        self._load = load
        self._preload = preload
        self._rep = rep
        self._is_another_app = is_another_app
        self._hydrate_lazily = hydrate_lazily
        self._deps = deps
        self._deduplication_key = deduplication_key

        self._object_id = None
        self._client = None
        self._is_hydrated = False
        self._is_rehydrated = False

        self._initialize_from_empty()

    def _unhydrate(self):
        self._object_id = None
        self._client = None
        self._is_hydrated = False

    def _initialize_from_empty(self):
        # default implementation, can be overriden in subclasses
        pass

    def _initialize_from_other(self, other):
        # default implementation, can be overriden in subclasses
        self._object_id = other._object_id
        self._is_hydrated = other._is_hydrated
        self._client = other._client

    def _hydrate(self, object_id: str, client: _Client, metadata: Optional[Message]):
        assert isinstance(object_id, str) and self._type_prefix is not None
        if not object_id.startswith(self._type_prefix):
            raise ExecutionError(
                f"Can not hydrate {type(self)}:"
                f" it has type prefix {self._type_prefix}"
                f" but the object_id starts with {object_id[:3]}"
            )
        self._object_id = object_id
        self._client = client
        self._hydrate_metadata(metadata)
        self._is_hydrated = True

    def _hydrate_metadata(self, metadata: Optional[Message]):
        # override this is subclasses that need additional data (other than an object_id) for a functioning Handle
        pass

    def _get_metadata(self) -> Optional[Message]:
        # return the necessary metadata from this handle to be able to re-hydrate in another context if one is needed
        # used to provide a handle's handle_metadata for serializing/pickling a live handle
        # the object_id is already provided by other means
        return None

    def _validate_is_hydrated(self):
        if not self._is_hydrated:
            object_type = self.__class__.__name__.strip("_")
            if hasattr(self, "_app") and getattr(self._app, "_running_app", "") is None:  # type: ignore
                # The most common cause of this error: e.g., user called a Function without using App.run()
                reason = ", because the App it is defined on is not running"
            else:
                # Technically possible, but with an ambiguous cause.
                reason = ""
            raise ExecutionError(
                f"{object_type} has not been hydrated with the metadata it needs to run on Modal{reason}."
            )

    def clone(self) -> Self:
        """mdmd:hidden Clone a given hydrated object."""

        # Object to clone must already be hydrated, otherwise from_loader is more suitable.
        self._validate_is_hydrated()
        obj = type(self).__new__(type(self))
        obj._initialize_from_other(self)
        return obj

    @classmethod
    def _from_loader(
        cls,
        load: Callable[[Self, Resolver, Optional[str]], Awaitable[None]],
        rep: str,
        is_another_app: bool = False,
        preload: Optional[Callable[[Self, Resolver, Optional[str]], Awaitable[None]]] = None,
        hydrate_lazily: bool = False,
        deps: Optional[Callable[..., Sequence["_Object"]]] = None,
        deduplication_key: Optional[Callable[[], Awaitable[Hashable]]] = None,
    ):
        # TODO(erikbern): flip the order of the two first arguments
        obj = _Object.__new__(cls)
        obj._init(rep, load, is_another_app, preload, hydrate_lazily, deps, deduplication_key)
        return obj

    @staticmethod
    def _get_type_from_id(object_id: str) -> type["_Object"]:
        parts = object_id.split("-")
        if len(parts) != 2:
            raise InvalidError(f"Object id {object_id} has no dash in it")
        prefix = parts[0]
        if prefix not in _Object._prefix_to_type:
            raise InvalidError(f"Object prefix {prefix} does not correspond to a type")
        return _Object._prefix_to_type[prefix]

    @classmethod
    def _is_id_type(cls, object_id) -> bool:
        return cls._get_type_from_id(object_id) == cls

    @classmethod
    def _new_hydrated(
        cls, object_id: str, client: _Client, handle_metadata: Optional[Message], is_another_app: bool = False
    ) -> Self:
        obj_cls: type[Self]
        if cls._type_prefix is not None:
            # This is called directly on a subclass, e.g. Secret.from_id
            # validate the id matching the expected id type of the Object subclass
            if not object_id.startswith(cls._type_prefix + "-"):
                raise InvalidError(f"Object {object_id} does not start with {cls._type_prefix}")

            obj_cls = cls
        else:
            # this means the method is used directly on _Object
            # typically during deserialization of objects
            obj_cls = typing.cast(type[Self], cls._get_type_from_id(object_id))

        # Instantiate provider
        obj = _Object.__new__(obj_cls)
        rep = f"Object({object_id})"  # TODO(erikbern): dumb
        obj._init(rep, is_another_app=is_another_app)
        obj._hydrate(object_id, client, handle_metadata)

        return obj

    def _hydrate_from_other(self, other: Self):
        self._hydrate(other.object_id, other.client, other._get_metadata())

    def __repr__(self):
        return self._rep

    @property
    def local_uuid(self):
        """mdmd:hidden"""
        return self._local_uuid

    @property
    def object_id(self) -> str:
        """mdmd:hidden"""
        if self._object_id is None:
            raise AttributeError(f"Attempting to get object_id of unhydrated {self}")
        return self._object_id

    @property
    def client(self) -> _Client:
        """mdmd:hidden"""
        if self._client is None:
            raise AttributeError(f"Attempting to get client of unhydrated {self}")
        return self._client

    @property
    def is_hydrated(self) -> bool:
        """mdmd:hidden"""
        return self._is_hydrated

    @property
    def deps(self) -> Callable[..., Sequence["_Object"]]:
        """mdmd:hidden"""

        def default_deps(*args, **kwargs) -> Sequence["_Object"]:
            return []

        return self._deps if self._deps is not None else default_deps

    async def resolve(self, client: Optional[_Client] = None):
        """mdmd:hidden"""
        obj = self.__class__.__name__.strip("_")
        deprecation_warning(
            (2025, 1, 16),
            f"The `{obj}.resolve` method is deprecated and will be removed in a future release."
            f" Please use `{obj}.hydrate()` or `await {obj}.hydrate.aio()` instead."
            "\n\nNote that it is rarely necessary to explicitly hydrate objects, as most methods"
            " will lazily hydrate when needed.",
            show_source=False,  # synchronicity interferes with attributing source correctly
        )
        await self.hydrate(client)

    async def hydrate(self, client: Optional[_Client] = None) -> Self:
        """Synchronize the local object with its identity on the Modal server.

        It is rarely necessary to call this method explicitly, as most operations
        will lazily hydrate when needed. The main use case is when you need to
        access object metadata, such as its ID.
        """
        if self._is_hydrated:
            if self.client._snapshotted and not self._is_rehydrated:
                # memory snapshots capture references which must be rehydrated
                # on restore to handle staleness.
                logger.debug(f"rehydrating {self} after snapshot")
                self._is_hydrated = False  # un-hydrate and re-resolve
                c = client if client is not None else await _Client.from_env()
                resolver = Resolver(c)
                await resolver.load(typing.cast(_Object, self))
                self._is_rehydrated = True
                logger.debug(f"rehydrated {self} with client {id(c)}")
        elif not self._hydrate_lazily:
            # TODO(michael) can remove _hydrate lazily? I think all objects support it now?
            self._validate_is_hydrated()
        else:
            c = client if client is not None else await _Client.from_env()
            resolver = Resolver(c)
            await resolver.load(self)
        return self


def live_method(method):
    @wraps(method)
    async def wrapped(self, *args, **kwargs):
        await self.hydrate()
        return await method(self, *args, **kwargs)

    return wrapped


def live_method_gen(method):
    @wraps(method)
    async def wrapped(self, *args, **kwargs):
        await self.hydrate()
        async with aclosing(method(self, *args, **kwargs)) as stream:
            async for item in stream:
                yield item

    return wrapped


================================================
File: modal/_output.py
================================================
# Copyright Modal Labs 2022
from __future__ import annotations

import asyncio
import contextlib
import functools
import io
import platform
import re
import socket
import sys
from collections.abc import Generator
from datetime import timedelta
from typing import Callable, ClassVar

from grpclib.exceptions import GRPCError, StreamTerminatedError
from rich.console import Console, Group, RenderableType
from rich.live import Live
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    DownloadColumn,
    MofNCompleteColumn,
    Progress,
    ProgressColumn,
    TaskID,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)
from rich.spinner import Spinner
from rich.text import Text

from modal_proto import api_pb2

from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors
from ._utils.shell_utils import stream_from_stdin
from .client import _Client
from .config import logger

if platform.system() == "Windows":
    default_spinner = "line"
else:
    default_spinner = "dots"


class FunctionQueuingColumn(ProgressColumn):
    """Renders time elapsed, including task.completed as additional elapsed time."""

    def __init__(self):
        self.lag = 0
        super().__init__()

    def render(self, task) -> Text:
        self.lag = max(task.completed - task.elapsed, self.lag)
        if task.finished:
            elapsed = max(task.finished_time, task.completed)
        else:
            elapsed = task.elapsed + self.lag
        delta = timedelta(seconds=int(elapsed))
        return Text(str(delta), style="progress.elapsed")


def download_progress_bar() -> Progress:
    """
    Returns a progress bar suitable for showing file download progress.
    Requires passing a `path: str` data field for rendering.
    """
    return Progress(
        TextColumn("[bold white]{task.fields[path]}", justify="right"),
        BarColumn(bar_width=None),
        "[progress.percentage]{task.percentage:>3.1f}%",
        "•",
        DownloadColumn(),
        "•",
        TransferSpeedColumn(),
        "•",
        TimeRemainingColumn(),
        transient=True,
    )


class LineBufferedOutput(io.StringIO):
    """Output stream that buffers lines and passes them to a callback."""

    LINE_REGEX = re.compile("(\r\n|\r|\n)")

    def __init__(self, callback: Callable[[str], None]):
        self._callback = callback
        self._buf = ""

    def write(self, data: str):
        chunks = self.LINE_REGEX.split(self._buf + data)

        # re.split("(<exp>)") returns the matched groups, and also the separators.
        # e.g. re.split("(+)", "a+b") returns ["a", "+", "b"].
        # This means that chunks is guaranteed to be odd in length.

        completed_lines = "".join(chunks[:-1])
        remainder = chunks[-1]

        # Partially completed lines end with a carriage return. Append a newline so that they
        # are not overwritten by the `rich.Live` and prefix the inverse operation to the remaining
        # buffer. Note that this is not perfect -- when stdout and stderr are interleaved, the results
        # can have unexpected spacing.
        if completed_lines.endswith("\r"):
            completed_lines = completed_lines[:-1] + "\n"
            # Prepend cursor up + carriage return.
            remainder = "\x1b[1A\r" + remainder

        self._callback(completed_lines)
        self._buf = remainder

    def flush(self):
        pass

    def finalize(self):
        if self._buf:
            self._callback(self._buf)
            self._buf = ""


class OutputManager:
    _instance: ClassVar[OutputManager | None] = None

    _console: Console
    _task_states: dict[str, int]
    _task_progress_items: dict[tuple[str, int], TaskID]
    _current_render_group: Group | None
    _function_progress: Progress | None
    _function_queueing_progress: Progress | None
    _snapshot_progress: Progress | None
    _line_buffers: dict[int, LineBufferedOutput]
    _status_spinner: Spinner
    _app_page_url: str | None
    _show_image_logs: bool
    _status_spinner_live: Live | None

    def __init__(
        self,
        *,
        stdout: io.TextIOWrapper | None = None,
        status_spinner_text: str = "Running app...",
    ):
        self._stdout = stdout or sys.stdout
        self._console = Console(file=stdout, highlight=False)
        self._task_states = {}
        self._task_progress_items = {}
        self._current_render_group = None
        self._function_progress = None
        self._function_queueing_progress = None
        self._snapshot_progress = None
        self._line_buffers = {}
        self._status_spinner = OutputManager.step_progress(status_spinner_text)
        self._app_page_url = None
        self._show_image_logs = False
        self._status_spinner_live = None

    @classmethod
    def disable(cls):
        cls._instance.flush_lines()
        if cls._instance._status_spinner_live:
            cls._instance._status_spinner_live.stop()
        cls._instance = None

    @classmethod
    def get(cls) -> OutputManager | None:
        return cls._instance

    @classmethod
    @contextlib.contextmanager
    def enable_output(cls, show_progress: bool = True) -> Generator[None]:
        if show_progress:
            cls._instance = OutputManager()
        try:
            yield
        finally:
            cls._instance = None

    @staticmethod
    def step_progress(text: str = "") -> Spinner:
        """Returns the element to be rendered when a step is in progress."""
        return Spinner(default_spinner, text, style="blue")

    @staticmethod
    def step_completed(message: str) -> RenderableType:
        return f"[green]✓[/green] {message}"

    @staticmethod
    def substep_completed(message: str) -> RenderableType:
        return f"🔨 {message}"

    def print(self, renderable) -> None:
        self._console.print(renderable)

    def make_live(self, renderable: RenderableType) -> Live:
        """Creates a customized `rich.Live` instance with the given renderable. The renderable
        is placed in a `rich.Group` to allow for dynamic additions later."""
        self._function_progress = None
        self._current_render_group = Group(renderable)
        return Live(self._current_render_group, console=self._console, transient=True, refresh_per_second=4)

    def enable_image_logs(self):
        self._show_image_logs = True

    @property
    def function_progress(self) -> Progress:
        """Creates a `rich.Progress` instance with custom columns for function progress,
        and adds it to the current render group."""
        if not self._function_progress:
            self._function_progress = Progress(
                TextColumn("[progress.description][white]{task.description}[/white]"),
                BarColumn(),
                MofNCompleteColumn(),
                TimeRemainingColumn(),
                console=self._console,
            )
            if self._current_render_group:
                self._current_render_group.renderables.append(Panel(self._function_progress, style="gray50"))
        return self._function_progress

    @property
    def snapshot_progress(self) -> Progress:
        """Creates a `rich.Progress` instance with custom columns for image snapshot progress,
        and adds it to the current render group."""
        if not self._snapshot_progress:
            self._snapshot_progress = Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                DownloadColumn(),
                TimeElapsedColumn(),
                console=self._console,
                transient=True,
            )
            if self._current_render_group:
                # Appear above function progress renderables.
                self._current_render_group.renderables.insert(0, self._snapshot_progress)
        return self._snapshot_progress

    @property
    def function_queueing_progress(self) -> Progress:
        """Creates a `rich.Progress` instance with custom columns for function queue waiting progress
        and adds it to the current render group."""
        if not self._function_queueing_progress:
            self._function_queueing_progress = Progress(
                TextColumn("[progress.description]{task.description}"),
                FunctionQueuingColumn(),
                console=self._console,
                transient=True,
            )
            if self._current_render_group:
                self._current_render_group.renderables.append(self._function_queueing_progress)
        return self._function_queueing_progress

    def function_progress_callback(self, tag: str, total: int | None) -> Callable[[int, int], None]:
        """Adds a task to the current function_progress instance, and returns a callback
        to update task progress with new completed and total counts."""

        progress_task = self.function_progress.add_task(tag, total=total)

        def update_counts(completed: int, total: int):
            self.function_progress.update(progress_task, completed=completed, total=total)

        return update_counts

    def _print_log(self, fd: int, data: str) -> None:
        if fd == api_pb2.FILE_DESCRIPTOR_STDOUT:
            style = "blue"
        elif fd == api_pb2.FILE_DESCRIPTOR_STDERR:
            style = "red"
        elif fd == api_pb2.FILE_DESCRIPTOR_INFO:
            style = "yellow"
        else:
            raise Exception(f"Weird file descriptor {fd} for log output")

        self._console.out(data, style=style, end="")

    def update_app_page_url(self, app_page_url: str) -> None:
        self._app_page_url = app_page_url

    def update_task_state(self, task_id: str, state: int):
        """Updates the state of a task, sets the new task status string."""
        self._task_states[task_id] = state

        all_states = self._task_states.values()
        states_set = set(all_states)

        def tasks_at_state(state):
            return sum(x == state for x in all_states)

        # The most advanced state that's present informs the message.
        if api_pb2.TASK_STATE_ACTIVE in states_set or api_pb2.TASK_STATE_IDLE in states_set:
            # Note that as of writing the server no longer uses TASK_STATE_ACTIVE, but we'll
            # make the numerator the sum of active / idle in case that is revived at some point in the future.
            tasks_running = tasks_at_state(api_pb2.TASK_STATE_ACTIVE) + tasks_at_state(api_pb2.TASK_STATE_IDLE)
            tasks_not_completed = len(self._task_states) - tasks_at_state(api_pb2.TASK_STATE_COMPLETED)
            message = f"Running ({tasks_running}/{tasks_not_completed} containers active)..."
        elif api_pb2.TASK_STATE_LOADING_IMAGE in states_set:
            tasks_loading = tasks_at_state(api_pb2.TASK_STATE_LOADING_IMAGE)
            message = f"Loading images ({tasks_loading} containers initializing)..."
        elif api_pb2.TASK_STATE_WORKER_ASSIGNED in states_set:
            message = "Worker assigned..."
        elif api_pb2.TASK_STATE_COMPLETED in states_set:
            tasks_completed = tasks_at_state(api_pb2.TASK_STATE_COMPLETED)
            message = f"Running ({tasks_completed} containers finished)..."
        else:
            message = "Running..."

        message = f"[blue]{message}[/blue] [grey70]View app at [underline]{self._app_page_url}[/underline][/grey70]"

        # Set the new message
        self._status_spinner.update(text=message)

    def update_snapshot_progress(self, image_id: str, task_progress: api_pb2.TaskProgress) -> None:
        # TODO(erikbern): move this to sit on the resolver object, mostly
        completed = task_progress.pos
        total = task_progress.len

        task_key = (image_id, api_pb2.IMAGE_SNAPSHOT_UPLOAD)
        if task_key in self._task_progress_items:
            progress_task_id = self._task_progress_items[task_key]
        else:
            progress_task_id = self.snapshot_progress.add_task("[yellow]Uploading image snapshot…", total=total)
            self._task_progress_items[task_key] = progress_task_id

        try:
            self.snapshot_progress.update(progress_task_id, completed=completed, total=total)
            if completed == total:
                self.snapshot_progress.remove_task(progress_task_id)
        except KeyError:
            # Rich throws a KeyError if the task has already been removed.
            pass

    def update_queueing_progress(
        self, *, function_id: str, completed: int, total: int | None, description: str | None
    ) -> None:
        """Handle queueing updates, ignoring completion updates for functions that have no queue progress bar."""
        task_key = (function_id, api_pb2.FUNCTION_QUEUED)
        task_description = description or f"'{function_id}' function waiting on worker"
        task_desc = f"[yellow]{task_description}. Time in queue:"
        if task_key in self._task_progress_items:
            progress_task_id = self._task_progress_items[task_key]
            try:
                self.function_queueing_progress.update(progress_task_id, completed=completed, total=total)
                if completed == total:
                    del self._task_progress_items[task_key]
                    self.function_queueing_progress.remove_task(progress_task_id)
            except KeyError:
                pass
        elif completed != total:  # Create new bar for queued function
            progress_task_id = self.function_queueing_progress.add_task(task_desc, start=True, total=None)
            self._task_progress_items[task_key] = progress_task_id

    async def put_log_content(self, log: api_pb2.TaskLogs):
        stream = self._line_buffers.get(log.file_descriptor)
        if stream is None:
            stream = LineBufferedOutput(functools.partial(self._print_log, log.file_descriptor))
            self._line_buffers[log.file_descriptor] = stream
        stream.write(log.data)

    def flush_lines(self):
        for stream in self._line_buffers.values():
            stream.finalize()

    @contextlib.contextmanager
    def show_status_spinner(self):
        self._status_spinner_live = self.make_live(self._status_spinner)
        with self._status_spinner_live:
            yield


class ProgressHandler:
    live: Live
    _type: str
    _spinner: Spinner
    _overall_progress: Progress
    _download_progress: Progress
    _overall_progress_task_id: TaskID
    _total_tasks: int
    _completed_tasks: int

    def __init__(self, type: str, console: Console):
        self._type = type

        if self._type == "download":
            title = "Downloading file(s) to local..."
        elif self._type == "upload":
            title = "Uploading file(s) to volume..."
        else:
            raise NotImplementedError(f"Progress handler of type: `{type}` not yet implemented")

        self._spinner = OutputManager.step_progress(title)

        self._overall_progress = Progress(
            TextColumn(f"[bold white]{title}", justify="right"),
            TimeElapsedColumn(),
            BarColumn(bar_width=None),
            TextColumn("[bold white]{task.description}"),
            transient=True,
            console=console,
        )
        self._download_progress = Progress(
            TextColumn("[bold white]{task.fields[path]}", justify="right"),
            BarColumn(bar_width=None),
            "[progress.percentage]{task.percentage:>3.1f}%",
            "•",
            DownloadColumn(),
            "•",
            TransferSpeedColumn(),
            "•",
            TimeRemainingColumn(),
            transient=True,
            console=console,
        )

        self.live = Live(
            Group(self._spinner, self._overall_progress, self._download_progress), transient=True, refresh_per_second=4
        )

        self._overall_progress_task_id = self._overall_progress.add_task(".", start=True)
        self._total_tasks = 0
        self._completed_tasks = 0

    def _add_sub_task(self, name: str, size: float) -> TaskID:
        task_id = self._download_progress.add_task(self._type, path=name, start=True, total=size)
        self._total_tasks += 1
        self._overall_progress.update(self._overall_progress_task_id, total=self._total_tasks)
        return task_id

    def _reset_sub_task(self, task_id: TaskID):
        self._download_progress.reset(task_id)

    def _complete_progress(self):
        # TODO: we could probably implement some callback progression from the server
        # to get progress reports for the post processing too
        # so we don't have to just spin here
        self._overall_progress.remove_task(self._overall_progress_task_id)
        self._spinner.update(text="Post processing...")

    def _complete_sub_task(self, task_id: TaskID):
        self._completed_tasks += 1
        self._download_progress.remove_task(task_id)
        self._overall_progress.update(
            self._overall_progress_task_id,
            advance=1,
            description=f"({self._completed_tasks} out of {self._total_tasks} files completed)",
        )

    def _advance_sub_task(self, task_id: TaskID, advance: float):
        self._download_progress.update(task_id, advance=advance)

    def progress(
        self,
        task_id: TaskID | None = None,
        advance: float | None = None,
        name: str | None = None,
        size: float | None = None,
        reset: bool | None = False,
        complete: bool | None = False,
    ) -> TaskID | None:
        try:
            if task_id is not None:
                if reset:
                    return self._reset_sub_task(task_id)
                elif complete:
                    return self._complete_sub_task(task_id)
                elif advance is not None:
                    return self._advance_sub_task(task_id, advance)
            elif name is not None and size is not None:
                return self._add_sub_task(name, size)
            elif complete:
                return self._complete_progress()
        except Exception as exc:
            # Liberal exception handling to avoid crashing downloads and uploads.
            logger.error(f"failed progress update: {exc}")
        raise NotImplementedError(
            "Unknown action to take with args: "
            + f"name={name} "
            + f"size={size} "
            + f"task_id={task_id} "
            + f"advance={advance} "
            + f"reset={reset} "
            + f"complete={complete} "
        )


async def stream_pty_shell_input(client: _Client, exec_id: str, finish_event: asyncio.Event):
    """
    Streams stdin to the given exec id until finish_event is triggered
    """

    async def _handle_input(data: bytes, message_index: int):
        await retry_transient_errors(
            client.stub.ContainerExecPutInput,
            api_pb2.ContainerExecPutInputRequest(
                exec_id=exec_id, input=api_pb2.RuntimeInputMessage(message=data, message_index=message_index)
            ),
            total_timeout=10,
        )

    async with stream_from_stdin(_handle_input, use_raw_terminal=True):
        await finish_event.wait()


async def put_pty_content(log: api_pb2.TaskLogs, stdout):
    if hasattr(stdout, "buffer"):
        # If we're not showing progress, there's no need to buffer lines,
        # because the progress spinner can't interfere with output.

        data = log.data.encode("utf-8")
        written = 0
        n_retries = 0
        while written < len(data):
            try:
                written += stdout.buffer.write(data[written:])
                stdout.flush()
            except BlockingIOError:
                if n_retries >= 5:
                    raise
                n_retries += 1
                await asyncio.sleep(0.1)
    else:
        # `stdout` isn't always buffered (e.g. %%capture in Jupyter notebooks redirects it to
        # io.StringIO).
        stdout.write(log.data)
        stdout.flush()


async def get_app_logs_loop(
    client: _Client,
    output_mgr: OutputManager,
    app_id: str | None = None,
    task_id: str | None = None,
    app_logs_url: str | None = None,
):
    last_log_batch_entry_id = ""

    pty_shell_stdout = None
    pty_shell_finish_event: asyncio.Event | None = None
    pty_shell_task_id: str | None = None

    async def stop_pty_shell():
        nonlocal pty_shell_finish_event
        if pty_shell_finish_event:
            print("\r", end="")  # move cursor to beginning of line
            pty_shell_finish_event.set()
            pty_shell_finish_event = None
            await asyncio.sleep(0)  # yield to handle_exec_input() so it can disable raw terminal

    async def _put_log(log_batch: api_pb2.TaskLogsBatch, log: api_pb2.TaskLogs):
        if log.task_state:
            output_mgr.update_task_state(log_batch.task_id, log.task_state)
            if log.task_state == api_pb2.TASK_STATE_WORKER_ASSIGNED:
                # Close function's queueing progress bar (if it exists)
                output_mgr.update_queueing_progress(
                    function_id=log_batch.function_id, completed=1, total=1, description=None
                )
        elif log.task_progress.len or log.task_progress.pos:
            if log.task_progress.progress_type == api_pb2.FUNCTION_QUEUED:
                output_mgr.update_queueing_progress(
                    function_id=log_batch.function_id,
                    completed=log.task_progress.pos,
                    total=log.task_progress.len,
                    description=log.task_progress.description,
                )
            else:  # Ensure forward-compatible with new types.
                logger.debug(f"Received unrecognized progress type: {log.task_progress.progress_type}")
        elif log.data:
            if pty_shell_finish_event:
                await put_pty_content(log, pty_shell_stdout)
            else:
                await output_mgr.put_log_content(log)

    async def _get_logs():
        nonlocal last_log_batch_entry_id
        nonlocal pty_shell_stdout, pty_shell_finish_event, pty_shell_task_id

        request = api_pb2.AppGetLogsRequest(
            app_id=app_id or "",
            task_id=task_id or "",
            timeout=55,
            last_entry_id=last_log_batch_entry_id,
        )
        log_batch: api_pb2.TaskLogsBatch
        async for log_batch in client.stub.AppGetLogs.unary_stream(request):
            if log_batch.entry_id:
                # log_batch entry_id is empty for fd="server" messages from AppGetLogs
                last_log_batch_entry_id = log_batch.entry_id
            if log_batch.app_done:
                logger.debug("App logs are done")
                last_log_batch_entry_id = None
                break
            elif log_batch.image_id and not output_mgr._show_image_logs:
                # Ignore image logs while app is creating objects.
                # These logs are fetched through ImageJoinStreaming instead.
                # Logs from images built "dynamically" (after the app has started)
                # are printed through this loop.
                # TODO (akshat): have a better way of differentiating between
                # statically and dynamically built images.
                pass
            elif log_batch.pty_exec_id:
                # This corresponds to the `modal run -i` use case where a breakpoint
                # triggers and the task drops into an interactive PTY mode
                if pty_shell_finish_event:
                    print("ERROR: concurrent PTY shells are not supported.")
                else:
                    pty_shell_stdout = output_mgr._stdout
                    pty_shell_finish_event = asyncio.Event()
                    pty_shell_task_id = log_batch.task_id
                    output_mgr.disable()
                    asyncio.create_task(stream_pty_shell_input(client, log_batch.pty_exec_id, pty_shell_finish_event))
            else:
                for log in log_batch.items:
                    await _put_log(log_batch, log)

            if log_batch.eof and log_batch.task_id == pty_shell_task_id:
                await stop_pty_shell()

        output_mgr.flush_lines()

    while True:
        try:
            await _get_logs()
        except (GRPCError, StreamTerminatedError, socket.gaierror, AttributeError) as exc:
            if isinstance(exc, GRPCError):
                if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                    # Try again if we had a temporary connection drop,
                    # for example if computer went to sleep.
                    logger.debug("Log fetching timed out. Retrying ...")
                    continue
            elif isinstance(exc, StreamTerminatedError):
                logger.debug("Stream closed. Retrying ...")
                continue
            elif isinstance(exc, socket.gaierror):
                logger.debug("Lost connection. Retrying ...")
                continue
            elif isinstance(exc, AttributeError):
                if "_write_appdata" in str(exc):
                    # Happens after losing connection
                    # StreamTerminatedError are not properly raised in grpclib<=0.4.7
                    # fixed in https://github.com/vmagamedov/grpclib/issues/185
                    # TODO: update to newer version (>=0.4.8) once stable
                    logger.debug("Lost connection. Retrying ...")
                    continue
            raise

        if last_log_batch_entry_id is None:
            break

    await stop_pty_shell()

    logger.debug("Logging exited gracefully")


================================================
File: modal/_partial_function.py
================================================
# Copyright Modal Labs 2023
import enum
import inspect
import typing
from collections.abc import Coroutine, Iterable
from typing import (
    Any,
    Callable,
    Optional,
    Union,
)

import typing_extensions

from modal_proto import api_pb2

from ._functions import _Function
from ._utils.async_utils import synchronizer
from ._utils.deprecation import deprecation_error, deprecation_warning
from ._utils.function_utils import callable_has_non_self_non_default_params, callable_has_non_self_params
from .config import logger
from .exception import InvalidError

MAX_MAX_BATCH_SIZE = 1000
MAX_BATCH_WAIT_MS = 10 * 60 * 1000  # 10 minutes

if typing.TYPE_CHECKING:
    import modal.partial_function


class _PartialFunctionFlags(enum.IntFlag):
    FUNCTION = 1
    BUILD = 2
    ENTER_PRE_SNAPSHOT = 4
    ENTER_POST_SNAPSHOT = 8
    EXIT = 16
    BATCHED = 32
    CLUSTERED = 64  # Experimental: Clustered functions

    @staticmethod
    def all() -> int:
        return ~_PartialFunctionFlags(0)


P = typing_extensions.ParamSpec("P")
ReturnType = typing_extensions.TypeVar("ReturnType", covariant=True)
OriginalReturnType = typing_extensions.TypeVar("OriginalReturnType", covariant=True)


class _PartialFunction(typing.Generic[P, ReturnType, OriginalReturnType]):
    """Intermediate function, produced by @enter, @build, @method, @web_endpoint, or @batched"""

    raw_f: Callable[P, ReturnType]
    flags: _PartialFunctionFlags
    webhook_config: Optional[api_pb2.WebhookConfig]
    is_generator: bool
    keep_warm: Optional[int]
    batch_max_size: Optional[int]
    batch_wait_ms: Optional[int]
    force_build: bool
    cluster_size: Optional[int]  # Experimental: Clustered functions
    build_timeout: Optional[int]

    def __init__(
        self,
        raw_f: Callable[P, ReturnType],
        flags: _PartialFunctionFlags,
        webhook_config: Optional[api_pb2.WebhookConfig] = None,
        is_generator: Optional[bool] = None,
        keep_warm: Optional[int] = None,
        batch_max_size: Optional[int] = None,
        batch_wait_ms: Optional[int] = None,
        cluster_size: Optional[int] = None,  # Experimental: Clustered functions
        force_build: bool = False,
        build_timeout: Optional[int] = None,
    ):
        self.raw_f = raw_f
        self.flags = flags
        self.webhook_config = webhook_config
        if is_generator is None:
            # auto detect - doesn't work if the function *returns* a generator
            final_is_generator = inspect.isgeneratorfunction(raw_f) or inspect.isasyncgenfunction(raw_f)
        else:
            final_is_generator = is_generator

        self.is_generator = final_is_generator
        self.keep_warm = keep_warm
        self.wrapped = False  # Make sure that this was converted into a FunctionHandle
        self.batch_max_size = batch_max_size
        self.batch_wait_ms = batch_wait_ms
        self.cluster_size = cluster_size  # Experimental: Clustered functions
        self.force_build = force_build
        self.build_timeout = build_timeout

    def _get_raw_f(self) -> Callable[P, ReturnType]:
        return self.raw_f

    def _is_web_endpoint(self) -> bool:
        if self.webhook_config is None:
            return False
        return self.webhook_config.type != api_pb2.WEBHOOK_TYPE_UNSPECIFIED

    def __get__(self, obj, objtype=None) -> _Function[P, ReturnType, OriginalReturnType]:
        # to type checkers, any @method or similar function on a modal class, would appear to be
        # of the type PartialFunction and this descriptor would be triggered when accessing it,
        #
        # However, modal classes are *actually* Cls instances (which isn't reflected in type checkers
        # due to Python's lack of type chekcing intersection types), so at runtime the Cls instance would
        # use its __getattr__ rather than this descriptor.
        k = self.raw_f.__name__
        if obj:  # accessing the method on an instance of a class, e.g. `MyClass().fun``
            if hasattr(obj, "_modal_functions"):
                # This happens inside "local" user methods when they refer to other methods,
                # e.g. Foo().parent_method.remote() calling self.other_method.remote()
                return getattr(obj, "_modal_functions")[k]
            else:
                # special edge case: referencing a method of an instance of an
                # unwrapped class (not using app.cls()) with @methods
                # not sure what would be useful here, but let's return a bound version of the underlying function,
                # since the class is just a vanilla class at this point
                # This wouldn't let the user access `.remote()` and `.local()` etc. on the function
                return self.raw_f.__get__(obj, objtype)

        else:  # accessing a method directly on the class, e.g. `MyClass.fun`
            # This happens mainly during serialization of the wrapped underlying class of a Cls
            # since we don't have the instance info here we just return the PartialFunction itself
            # to let it be bound to a variable and become a Function later on
            return self  # type: ignore  # this returns a PartialFunction in a special internal case

    def __del__(self):
        if (self.flags & _PartialFunctionFlags.FUNCTION) and self.wrapped is False:
            logger.warning(
                f"Method or web function {self.raw_f} was never turned into a function."
                " Did you forget a @app.function or @app.cls decorator?"
            )

    def add_flags(self, flags) -> "_PartialFunction":
        # Helper method used internally when stacking decorators
        self.wrapped = True
        return _PartialFunction(
            raw_f=self.raw_f,
            flags=(self.flags | flags),
            webhook_config=self.webhook_config,
            keep_warm=self.keep_warm,
            batch_max_size=self.batch_max_size,
            batch_wait_ms=self.batch_wait_ms,
            force_build=self.force_build,
            build_timeout=self.build_timeout,
        )


def _find_partial_methods_for_user_cls(user_cls: type[Any], flags: int) -> dict[str, _PartialFunction]:
    """Grabs all method on a user class, and returns partials. Includes legacy methods."""
    from .partial_function import PartialFunction  # wrapped type

    partial_functions: dict[str, _PartialFunction] = {}
    for parent_cls in reversed(user_cls.mro()):
        if parent_cls is not object:
            for k, v in parent_cls.__dict__.items():
                if isinstance(v, PartialFunction):  # type: ignore[reportArgumentType]   # synchronicity wrapper types
                    _partial_function: _PartialFunction = typing.cast(_PartialFunction, synchronizer._translate_in(v))
                    if _partial_function.flags & flags:
                        partial_functions[k] = _partial_function

    return partial_functions


def _find_callables_for_obj(user_obj: Any, flags: int) -> dict[str, Callable[..., Any]]:
    """Grabs all methods for an object, and binds them to the class"""
    user_cls: type = type(user_obj)
    return {k: pf.raw_f.__get__(user_obj) for k, pf in _find_partial_methods_for_user_cls(user_cls, flags).items()}


class _MethodDecoratorType:
    @typing.overload
    def __call__(
        self,
        func: "modal.partial_function.PartialFunction[typing_extensions.Concatenate[Any, P], ReturnType, OriginalReturnType]",  # noqa
    ) -> "modal.partial_function.PartialFunction[P, ReturnType, OriginalReturnType]": ...

    @typing.overload
    def __call__(
        self, func: "Callable[typing_extensions.Concatenate[Any, P], Coroutine[Any, Any, ReturnType]]"
    ) -> "modal.partial_function.PartialFunction[P, ReturnType, Coroutine[Any, Any, ReturnType]]": ...

    @typing.overload
    def __call__(
        self, func: "Callable[typing_extensions.Concatenate[Any, P], ReturnType]"
    ) -> "modal.partial_function.PartialFunction[P, ReturnType, ReturnType]": ...

    def __call__(self, func): ...


# TODO(elias): fix support for coroutine type unwrapping for methods (static typing)
def _method(
    _warn_parentheses_missing=None,
    *,
    # Set this to True if it's a non-generator function returning
    # a [sync/async] generator object
    is_generator: Optional[bool] = None,
    keep_warm: Optional[int] = None,  # Deprecated: Use keep_warm on @app.cls() instead
) -> _MethodDecoratorType:
    """Decorator for methods that should be transformed into a Modal Function registered against this class's App.

    **Usage:**

    ```python
    @app.cls(cpu=8)
    class MyCls:

        @modal.method()
        def f(self):
            ...
    ```
    """
    if _warn_parentheses_missing is not None:
        raise InvalidError("Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@method()`.")

    if keep_warm is not None:
        deprecation_warning(
            (2024, 6, 10),
            (
                "`keep_warm=` is no longer supported per-method on Modal classes. "
                "All methods and web endpoints of a class use the same set of containers now. "
                "Use keep_warm via the @app.cls() decorator instead. "
            ),
            pending=True,
        )

    def wrapper(raw_f: Callable[..., Any]) -> _PartialFunction:
        nonlocal is_generator
        if isinstance(raw_f, _PartialFunction) and raw_f.webhook_config:
            raw_f.wrapped = True  # suppress later warning
            raise InvalidError(
                "Web endpoints on classes should not be wrapped by `@method`. "
                "Suggestion: remove the `@method` decorator."
            )
        if isinstance(raw_f, _PartialFunction) and raw_f.batch_max_size is not None:
            raw_f.wrapped = True  # suppress later warning
            raise InvalidError(
                "Batched function on classes should not be wrapped by `@method`. "
                "Suggestion: remove the `@method` decorator."
            )
        return _PartialFunction(raw_f, _PartialFunctionFlags.FUNCTION, is_generator=is_generator, keep_warm=keep_warm)

    return wrapper  # type: ignore   # synchronicity issue with wrapped vs unwrapped types and protocols


def _parse_custom_domains(custom_domains: Optional[Iterable[str]] = None) -> list[api_pb2.CustomDomainConfig]:
    assert not isinstance(custom_domains, str), "custom_domains must be `Iterable[str]` but is `str` instead."
    _custom_domains: list[api_pb2.CustomDomainConfig] = []
    if custom_domains is not None:
        for custom_domain in custom_domains:
            _custom_domains.append(api_pb2.CustomDomainConfig(name=custom_domain))

    return _custom_domains


def _web_endpoint(
    _warn_parentheses_missing=None,
    *,
    method: str = "GET",  # REST method for the created endpoint.
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    docs: bool = False,  # Whether to enable interactive documentation for this endpoint at /docs.
    custom_domains: Optional[
        Iterable[str]
    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name (FQDN).
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
    wait_for_response: bool = True,  # DEPRECATED: this must always be True now
) -> Callable[[Callable[P, ReturnType]], _PartialFunction[P, ReturnType, ReturnType]]:
    """Register a basic web endpoint with this application.

    This is the simple way to create a web endpoint on Modal. The function
    behaves as a [FastAPI](https://fastapi.tiangolo.com/) handler and should
    return a response object to the caller.

    Endpoints created with `@app.web_endpoint` are meant to be simple, single
    request handlers and automatically have
    [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS) enabled.
    For more flexibility, use `@app.asgi_app`.

    To learn how to use Modal with popular web frameworks, see the
    [guide on web endpoints](https://modal.com/docs/guide/webhooks).
    """
    if isinstance(_warn_parentheses_missing, str):
        # Probably passing the method string as a positional argument.
        raise InvalidError('Positional arguments are not allowed. Suggestion: `@web_endpoint(method="GET")`.')
    elif _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@web_endpoint()`."
        )

    def wrapper(raw_f: Callable[..., Any]) -> _PartialFunction:
        if isinstance(raw_f, _Function):
            raw_f = raw_f.get_raw_f()
            raise InvalidError(
                f"Applying decorators for {raw_f} in the wrong order!\nUsage:\n\n"
                "@app.function()\n@app.web_endpoint()\ndef my_webhook():\n    ..."
            )
        if not wait_for_response:
            deprecation_error(
                (2024, 5, 13),
                "wait_for_response=False has been deprecated on web endpoints. See "
                "https://modal.com/docs/guide/webhook-timeouts#polling-solutions for alternatives.",
            )

        # self._loose_webhook_configs.add(raw_f)

        return _PartialFunction(
            raw_f,
            _PartialFunctionFlags.FUNCTION,
            api_pb2.WebhookConfig(
                type=api_pb2.WEBHOOK_TYPE_FUNCTION,
                method=method,
                web_endpoint_docs=docs,
                requested_suffix=label,
                async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
                custom_domains=_parse_custom_domains(custom_domains),
                requires_proxy_auth=requires_proxy_auth,
            ),
        )

    return wrapper


def _asgi_app(
    _warn_parentheses_missing=None,
    *,
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
    wait_for_response: bool = True,  # DEPRECATED: this must always be True now
) -> Callable[[Callable[..., Any]], _PartialFunction]:
    """Decorator for registering an ASGI app with a Modal function.

    Asynchronous Server Gateway Interface (ASGI) is a standard for Python
    synchronous and asynchronous apps, supported by all popular Python web
    libraries. This is an advanced decorator that gives full flexibility in
    defining one or more web endpoints on Modal.

    **Usage:**

    ```python
    from typing import Callable

    @app.function()
    @modal.asgi_app()
    def create_asgi() -> Callable:
        ...
    ```

    To learn how to use Modal with popular web frameworks, see the
    [guide on web endpoints](https://modal.com/docs/guide/webhooks).
    """
    if isinstance(_warn_parentheses_missing, str):
        raise InvalidError('Positional arguments are not allowed. Suggestion: `@asgi_app(label="foo")`.')
    elif _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@asgi_app()`."
        )

    def wrapper(raw_f: Callable[..., Any]) -> _PartialFunction:
        if callable_has_non_self_params(raw_f):
            if callable_has_non_self_non_default_params(raw_f):
                raise InvalidError(
                    f"ASGI app function {raw_f.__name__} can't have parameters. See https://modal.com/docs/guide/webhooks#asgi."
                )
            else:
                deprecation_warning(
                    (2024, 9, 4),
                    f"ASGI app function {raw_f.__name__} has default parameters, but shouldn't have any parameters - "
                    f"Modal will drop support for default parameters in a future release.",
                )

        if inspect.iscoroutinefunction(raw_f):
            raise InvalidError(
                f"ASGI app function {raw_f.__name__} is an async function. Only sync Python functions are supported."
            )

        if not wait_for_response:
            deprecation_error(
                (2024, 5, 13),
                "wait_for_response=False has been deprecated on web endpoints. See "
                "https://modal.com/docs/guide/webhook-timeouts#polling-solutions for alternatives",
            )

        return _PartialFunction(
            raw_f,
            _PartialFunctionFlags.FUNCTION,
            api_pb2.WebhookConfig(
                type=api_pb2.WEBHOOK_TYPE_ASGI_APP,
                requested_suffix=label,
                async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
                custom_domains=_parse_custom_domains(custom_domains),
                requires_proxy_auth=requires_proxy_auth,
            ),
        )

    return wrapper


def _wsgi_app(
    _warn_parentheses_missing=None,
    *,
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
    wait_for_response: bool = True,  # DEPRECATED: this must always be True now
) -> Callable[[Callable[..., Any]], _PartialFunction]:
    """Decorator for registering a WSGI app with a Modal function.

    Web Server Gateway Interface (WSGI) is a standard for synchronous Python web apps.
    It has been [succeeded by the ASGI interface](https://asgi.readthedocs.io/en/latest/introduction.html#wsgi-compatibility)
    which is compatible with ASGI and supports additional functionality such as web sockets.
    Modal supports ASGI via [`asgi_app`](/docs/reference/modal.asgi_app).

    **Usage:**

    ```python
    from typing import Callable

    @app.function()
    @modal.wsgi_app()
    def create_wsgi() -> Callable:
        ...
    ```

    To learn how to use this decorator with popular web frameworks, see the
    [guide on web endpoints](https://modal.com/docs/guide/webhooks).
    """
    if isinstance(_warn_parentheses_missing, str):
        raise InvalidError('Positional arguments are not allowed. Suggestion: `@wsgi_app(label="foo")`.')
    elif _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@wsgi_app()`."
        )

    def wrapper(raw_f: Callable[..., Any]) -> _PartialFunction:
        if callable_has_non_self_params(raw_f):
            if callable_has_non_self_non_default_params(raw_f):
                raise InvalidError(
                    f"WSGI app function {raw_f.__name__} can't have parameters. See https://modal.com/docs/guide/webhooks#wsgi."
                )
            else:
                deprecation_warning(
                    (2024, 9, 4),
                    f"WSGI app function {raw_f.__name__} has default parameters, but shouldn't have any parameters - "
                    f"Modal will drop support for default parameters in a future release.",
                )

        if inspect.iscoroutinefunction(raw_f):
            raise InvalidError(
                f"WSGI app function {raw_f.__name__} is an async function. Only sync Python functions are supported."
            )

        if not wait_for_response:
            deprecation_error(
                (2024, 5, 13),
                "wait_for_response=False has been deprecated on web endpoints. See "
                "https://modal.com/docs/guide/webhook-timeouts#polling-solutions for alternatives",
            )

        return _PartialFunction(
            raw_f,
            _PartialFunctionFlags.FUNCTION,
            api_pb2.WebhookConfig(
                type=api_pb2.WEBHOOK_TYPE_WSGI_APP,
                requested_suffix=label,
                async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
                custom_domains=_parse_custom_domains(custom_domains),
                requires_proxy_auth=requires_proxy_auth,
            ),
        )

    return wrapper


def _web_server(
    port: int,
    *,
    startup_timeout: float = 5.0,  # Maximum number of seconds to wait for the web server to start.
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[[Callable[..., Any]], _PartialFunction]:
    """Decorator that registers an HTTP web server inside the container.

    This is similar to `@asgi_app` and `@wsgi_app`, but it allows you to expose a full HTTP server
    listening on a container port. This is useful for servers written in other languages like Rust,
    as well as integrating with non-ASGI frameworks like aiohttp and Tornado.

    **Usage:**

    ```python
    import subprocess

    @app.function()
    @modal.web_server(8000)
    def my_file_server():
        subprocess.Popen("python -m http.server -d / 8000", shell=True)
    ```

    The above example starts a simple file server, displaying the contents of the root directory.
    Here, requests to the web endpoint will go to external port 8000 on the container. The
    `http.server` module is included with Python, but you could run anything here.

    Internally, the web server is transparently converted into a web endpoint by Modal, so it has
    the same serverless autoscaling behavior as other web endpoints.

    For more info, see the [guide on web endpoints](https://modal.com/docs/guide/webhooks).
    """
    if not isinstance(port, int) or port < 1 or port > 65535:
        raise InvalidError("First argument of `@web_server` must be a local port, such as `@web_server(8000)`.")
    if startup_timeout <= 0:
        raise InvalidError("The `startup_timeout` argument of `@web_server` must be positive.")

    def wrapper(raw_f: Callable[..., Any]) -> _PartialFunction:
        return _PartialFunction(
            raw_f,
            _PartialFunctionFlags.FUNCTION,
            api_pb2.WebhookConfig(
                type=api_pb2.WEBHOOK_TYPE_WEB_SERVER,
                requested_suffix=label,
                async_mode=api_pb2.WEBHOOK_ASYNC_MODE_AUTO,
                custom_domains=_parse_custom_domains(custom_domains),
                web_server_port=port,
                web_server_startup_timeout=startup_timeout,
                requires_proxy_auth=requires_proxy_auth,
            ),
        )

    return wrapper


def _disallow_wrapping_method(f: _PartialFunction, wrapper: str) -> None:
    if f.flags & _PartialFunctionFlags.FUNCTION:
        f.wrapped = True  # Hack to avoid warning about not using @app.cls()
        raise InvalidError(f"Cannot use `@{wrapper}` decorator with `@method`.")


def _build(
    _warn_parentheses_missing=None, *, force: bool = False, timeout: int = 86400
) -> Callable[[Union[Callable[[Any], Any], _PartialFunction]], _PartialFunction]:
    """
    Decorator for methods that execute at _build time_ to create a new Image layer.

    **Deprecated**: This function is deprecated. We recommend using `modal.Volume`
    to store large assets (such as model weights) instead of writing them to the
    Image during the build process. For other use cases, you can replace this
    decorator with the `Image.run_function` method.

    **Usage**

    ```python notest
    @app.cls(gpu="A10G")
    class AlpacaLoRAModel:
        @build()
        def download_models(self):
            model = LlamaForCausalLM.from_pretrained(
                base_model,
            )
            PeftModel.from_pretrained(model, lora_weights)
            LlamaTokenizer.from_pretrained(base_model)
    ```
    """
    if _warn_parentheses_missing is not None:
        raise InvalidError("Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@build()`.")

    deprecation_warning(
        (2025, 1, 15),
        "The `@modal.build` decorator is deprecated and will be removed in a future release."
        "\n\nWe now recommend storing large assets (such as model weights) using a `modal.Volume`"
        " instead of writing them directly into the `modal.Image` filesystem."
        " For other use cases we recommend using `Image.run_function` instead."
        "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
    )

    def wrapper(f: Union[Callable[[Any], Any], _PartialFunction]) -> _PartialFunction:
        if isinstance(f, _PartialFunction):
            _disallow_wrapping_method(f, "build")
            f.force_build = force
            f.build_timeout = timeout
            return f.add_flags(_PartialFunctionFlags.BUILD)
        else:
            return _PartialFunction(f, _PartialFunctionFlags.BUILD, force_build=force, build_timeout=timeout)

    return wrapper


def _enter(
    _warn_parentheses_missing=None,
    *,
    snap: bool = False,
) -> Callable[[Union[Callable[[Any], Any], _PartialFunction]], _PartialFunction]:
    """Decorator for methods which should be executed when a new container is started.

    See the [lifeycle function guide](https://modal.com/docs/guide/lifecycle-functions#enter) for more information."""
    if _warn_parentheses_missing is not None:
        raise InvalidError("Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@enter()`.")

    if snap:
        flag = _PartialFunctionFlags.ENTER_PRE_SNAPSHOT
    else:
        flag = _PartialFunctionFlags.ENTER_POST_SNAPSHOT

    def wrapper(f: Union[Callable[[Any], Any], _PartialFunction]) -> _PartialFunction:
        if isinstance(f, _PartialFunction):
            _disallow_wrapping_method(f, "enter")
            return f.add_flags(flag)
        else:
            return _PartialFunction(f, flag)

    return wrapper


ExitHandlerType = Union[
    # NOTE: return types of these callables should be `Union[None, Awaitable[None]]` but
    #       synchronicity type stubs would strip Awaitable so we use Any for now
    # Original, __exit__ style method signature (now deprecated)
    Callable[[Any, Optional[type[BaseException]], Optional[BaseException], Any], Any],
    # Forward-looking unparametrized method
    Callable[[Any], Any],
]


def _exit(_warn_parentheses_missing=None) -> Callable[[ExitHandlerType], _PartialFunction]:
    """Decorator for methods which should be executed when a container is about to exit.

    See the [lifeycle function guide](https://modal.com/docs/guide/lifecycle-functions#exit) for more information."""
    if _warn_parentheses_missing is not None:
        raise InvalidError("Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@exit()`.")

    def wrapper(f: ExitHandlerType) -> _PartialFunction:
        if isinstance(f, _PartialFunction):
            _disallow_wrapping_method(f, "exit")

        return _PartialFunction(f, _PartialFunctionFlags.EXIT)

    return wrapper


def _batched(
    _warn_parentheses_missing=None,
    *,
    max_batch_size: int,
    wait_ms: int,
) -> Callable[[Callable[..., Any]], _PartialFunction]:
    """Decorator for functions or class methods that should be batched.

    **Usage**

    ```python notest
    @app.function()
    @modal.batched(max_batch_size=4, wait_ms=1000)
    async def batched_multiply(xs: list[int], ys: list[int]) -> list[int]:
        return [x * y for x, y in zip(xs, xs)]

    # call batched_multiply with individual inputs
    batched_multiply.remote.aio(2, 100)
    ```

    See the [dynamic batching guide](https://modal.com/docs/guide/dynamic-batching) for more information.
    """
    if _warn_parentheses_missing is not None:
        raise InvalidError(
            "Positional arguments are not allowed. Did you forget parentheses? Suggestion: `@batched()`."
        )
    if max_batch_size < 1:
        raise InvalidError("max_batch_size must be a positive integer.")
    if max_batch_size >= MAX_MAX_BATCH_SIZE:
        raise InvalidError(f"max_batch_size must be less than {MAX_MAX_BATCH_SIZE}.")
    if wait_ms < 0:
        raise InvalidError("wait_ms must be a non-negative integer.")
    if wait_ms >= MAX_BATCH_WAIT_MS:
        raise InvalidError(f"wait_ms must be less than {MAX_BATCH_WAIT_MS}.")

    def wrapper(raw_f: Callable[..., Any]) -> _PartialFunction:
        if isinstance(raw_f, _Function):
            raw_f = raw_f.get_raw_f()
            raise InvalidError(
                f"Applying decorators for {raw_f} in the wrong order!\nUsage:\n\n"
                "@app.function()\n@modal.batched()\ndef batched_function():\n    ..."
            )
        return _PartialFunction(
            raw_f,
            _PartialFunctionFlags.FUNCTION | _PartialFunctionFlags.BATCHED,
            batch_max_size=max_batch_size,
            batch_wait_ms=wait_ms,
        )

    return wrapper


================================================
File: modal/_proxy_tunnel.py
================================================
# Copyright Modal Labs 2022
import contextlib
import os
import subprocess
import tempfile

from modal_proto import api_pb2


@contextlib.contextmanager
def proxy_tunnel(info: api_pb2.ProxyInfo):
    if not info.elastic_ip:
        yield
        return

    with tempfile.NamedTemporaryFile(suffix=".pem") as t:
        f = open(t.name, "w")
        f.write(info.proxy_key)
        f.close()

        if info.remote_addr:
            # Forward port from address.
            args = [
                "-L",
                f"{info.remote_port}:{info.remote_addr}:{info.remote_port}",
            ]
        else:
            # Set up SOCKS proxy.
            # TODO: add a local_port column and proxy_type (this is all being rewritten anyway)
            args = ["-D", f"{info.remote_port}"]
            os.environ["HTTP_PROXY"] = f"socks5://localhost:{info.remote_port}"

            # https://github.com/getsentry/sentry-python/issues/1049
            os.environ["NO_PROXY"] = "sentry.io"

        cmd = [
            "autossh",
            "-M 0",  # don't use a monitoring port for autossh
            "-i",
            t.name,  # use pem file
            "-T",  # ignore the tty
            "-n",  # no input
            "-N",  # don't execute a command
            *args,
            f"ubuntu@{info.elastic_ip}",
            "-o",
            "StrictHostKeyChecking=no",  # avoid prompt for host
            "-o",
            "ServerAliveInterval=5",  # seconds before client sends keepalive to server
            "-o",
            "ServerAliveCountMax=1",  # number of failures before terminating ssh (autossh will restart)
            "-o",
            "LogLevel=ERROR",  # disable warning "Permanently added '...' to the list of known hosts."
        ]
        p = subprocess.Popen(cmd)

        import time

        time.sleep(3)
        try:
            yield
        finally:
            p.kill()


================================================
File: modal/_pty.py
================================================
# Copyright Modal Labs 2022
import contextlib
import os
import sys
from typing import Optional

from modal_proto import api_pb2


def get_winsz(fd) -> tuple[Optional[int], Optional[int]]:
    try:
        import fcntl
        import struct
        import termios

        return struct.unpack("hh", fcntl.ioctl(fd, termios.TIOCGWINSZ, "1234"))  # type: ignore
    except Exception:
        return None, None


def set_nonblocking(fd: int):
    import fcntl

    fl = fcntl.fcntl(fd, fcntl.F_GETFL)
    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)


@contextlib.contextmanager
def raw_terminal():
    import termios
    import tty

    fd = sys.stdin.fileno()
    old_settings = termios.tcgetattr(fd)

    try:
        tty.setraw(fd, termios.TCSADRAIN)
        yield
    finally:
        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)


def get_pty_info(shell: bool) -> api_pb2.PTYInfo:
    rows, cols = get_winsz(sys.stdin.fileno())
    return api_pb2.PTYInfo(
        enabled=True,  # TODO(erikbern): deprecated
        winsz_rows=rows,
        winsz_cols=cols,
        env_term=os.environ.get("TERM"),
        env_colorterm=os.environ.get("COLORTERM"),
        env_term_program=os.environ.get("TERM_PROGRAM"),
        pty_type=api_pb2.PTYInfo.PTY_TYPE_SHELL if shell else api_pb2.PTYInfo.PTY_TYPE_FUNCTION,
    )


================================================
File: modal/_resolver.py
================================================
# Copyright Modal Labs 2023
import asyncio
import contextlib
import typing
from asyncio import Future
from collections.abc import Hashable
from typing import TYPE_CHECKING, Optional

from grpclib import GRPCError, Status

from ._utils.async_utils import TaskContext
from .client import _Client
from .exception import NotFoundError

if TYPE_CHECKING:
    from rich.tree import Tree

    import modal._object


class StatusRow:
    def __init__(self, progress: "typing.Optional[Tree]"):
        self._spinner = None
        self._step_node = None
        if progress is not None:
            from ._output import OutputManager

            self._spinner = OutputManager.step_progress()
            self._step_node = progress.add(self._spinner)

    def message(self, message):
        if self._spinner is not None:
            self._spinner.update(text=message)

    def finish(self, message):
        if self._step_node is not None and self._spinner is not None:
            from ._output import OutputManager

            self._spinner.update(text=message)
            self._step_node.label = OutputManager.substep_completed(message)


class Resolver:
    _local_uuid_to_future: dict[str, Future]
    _environment_name: Optional[str]
    _app_id: Optional[str]
    _deduplication_cache: dict[Hashable, Future]
    _client: _Client

    def __init__(
        self,
        client: _Client,
        *,
        environment_name: Optional[str] = None,
        app_id: Optional[str] = None,
    ):
        try:
            # TODO(michael) If we don't clean this up more thoroughly, it would probably
            # be good to have a single source of truth for "rich is installed" rather than
            # doing a try/catch everywhere we want to use it.
            from rich.tree import Tree

            from ._output import OutputManager

            tree = Tree(OutputManager.step_progress("Creating objects..."), guide_style="gray50")
        except ImportError:
            tree = None

        self._local_uuid_to_future = {}
        self._tree = tree
        self._client = client
        self._app_id = app_id
        self._environment_name = environment_name
        self._deduplication_cache = {}

    @property
    def app_id(self) -> Optional[str]:
        return self._app_id

    @property
    def client(self):
        return self._client

    @property
    def environment_name(self):
        return self._environment_name

    async def preload(self, obj, existing_object_id: Optional[str]):
        if obj._preload is not None:
            await obj._preload(obj, self, existing_object_id)

    async def load(self, obj: "modal._object._Object", existing_object_id: Optional[str] = None):
        if obj._is_hydrated and obj._is_another_app:
            # No need to reload this, it won't typically change
            if obj.local_uuid not in self._local_uuid_to_future:
                # a bit dumb - but we still need to store a reference to the object here
                # to be able to include all referenced objects when setting up the app
                fut: Future = Future()
                fut.set_result(obj)
                self._local_uuid_to_future[obj.local_uuid] = fut
            return obj

        deduplication_key: Optional[Hashable] = None
        if obj._deduplication_key:
            deduplication_key = await obj._deduplication_key()

        cached_future = self._local_uuid_to_future.get(obj.local_uuid)

        if not cached_future and deduplication_key is not None:
            # deduplication cache makes sure duplicate mounts are resolved only
            # once, even if they are different instances - as long as they have
            # the same content
            cached_future = self._deduplication_cache.get(deduplication_key)
            if cached_future:
                hydrated_object = await cached_future
                obj._hydrate(hydrated_object.object_id, self._client, hydrated_object._get_metadata())
                return obj

        if not cached_future:
            # don't run any awaits within this if-block to prevent race conditions
            async def loader():
                # Wait for all its dependencies
                # TODO(erikbern): do we need existing_object_id for those?
                await TaskContext.gather(*[self.load(dep) for dep in obj.deps()])

                # Load the object itself
                if not obj._load:
                    raise Exception(f"Object {obj} has no loader function")
                try:
                    await obj._load(obj, self, existing_object_id)
                except GRPCError as exc:
                    if exc.status == Status.NOT_FOUND:
                        raise NotFoundError(exc.message)
                    raise

                # Check that the id of functions didn't change
                # Persisted refs are ignored because their life cycle is managed independently.
                if (
                    not obj._is_another_app
                    and existing_object_id is not None
                    and existing_object_id.startswith("fu-")
                    and obj.object_id != existing_object_id
                ):
                    raise Exception(
                        f"Tried creating an object using existing id {existing_object_id} but it has id {obj.object_id}"
                    )

                return obj

            cached_future = asyncio.create_task(loader())
            self._local_uuid_to_future[obj.local_uuid] = cached_future
            if deduplication_key is not None:
                self._deduplication_cache[deduplication_key] = cached_future

        # TODO(elias): print original exception/trace rather than the Resolver-internal trace
        return await cached_future

    def objects(self) -> list["modal._object._Object"]:
        unique_objects: dict[str, "modal._object._Object"] = {}
        for fut in self._local_uuid_to_future.values():
            if not fut.done():
                # this will raise an exception if not all loads have been awaited, but that *should* never happen
                raise RuntimeError(
                    "All loaded objects have not been resolved yet, can't get all objects for the resolver!"
                )
            obj = fut.result()
            unique_objects.setdefault(obj.object_id, obj)
        return list(unique_objects.values())

    @contextlib.contextmanager
    def display(self):
        # TODO(erikbern): get rid of this wrapper
        from .output import _get_output_manager

        if self._tree and (output_mgr := _get_output_manager()):
            with output_mgr.make_live(self._tree):
                yield
            self._tree.label = output_mgr.step_completed("Created objects.")
            output_mgr.print(self._tree)
        else:
            yield

    def add_status_row(self) -> StatusRow:
        return StatusRow(self._tree)


================================================
File: modal/_resources.py
================================================
# Copyright Modal Labs 2024
from typing import Optional, Union

from modal_proto import api_pb2

from .exception import InvalidError
from .gpu import GPU_T, parse_gpu_config


def convert_fn_config_to_resources_config(
    *,
    cpu: Optional[Union[float, tuple[float, float]]],
    memory: Optional[Union[int, tuple[int, int]]],
    gpu: GPU_T,
    ephemeral_disk: Optional[int],
) -> api_pb2.Resources:
    gpu_config = parse_gpu_config(gpu)
    if cpu and isinstance(cpu, tuple):
        if not cpu[0]:
            raise InvalidError("CPU request must be a positive number")
        elif not cpu[1]:
            raise InvalidError("CPU limit must be a positive number")
        milli_cpu = int(1000 * cpu[0])
        milli_cpu_max = int(1000 * cpu[1])
        if milli_cpu_max < milli_cpu:
            raise InvalidError(f"Cannot specify a CPU limit lower than request: {milli_cpu_max} < {milli_cpu}")
    elif cpu and isinstance(cpu, (float, int)):
        milli_cpu = int(1000 * cpu)
        milli_cpu_max = None
    else:
        milli_cpu = None
        milli_cpu_max = None

    if memory and isinstance(memory, int):
        memory_mb = memory
        memory_mb_max = 0  # no limit
    elif memory and isinstance(memory, tuple):
        memory_mb, memory_mb_max = memory
        if memory_mb_max < memory_mb:
            raise InvalidError(f"Cannot specify a memory limit lower than request: {memory_mb_max} < {memory_mb}")
    else:
        memory_mb = 0
        memory_mb_max = 0
    return api_pb2.Resources(
        milli_cpu=milli_cpu,
        milli_cpu_max=milli_cpu_max,
        gpu_config=gpu_config,
        memory_mb=memory_mb,
        memory_mb_max=memory_mb_max,
        ephemeral_disk_mb=ephemeral_disk,
    )


================================================
File: modal/_serialization.py
================================================
# Copyright Modal Labs 2022
import io
import pickle
import typing
from dataclasses import dataclass
from typing import Any

from modal._utils.async_utils import synchronizer
from modal_proto import api_pb2

from ._object import _Object
from ._vendor import cloudpickle
from .config import logger
from .exception import DeserializationError, ExecutionError, InvalidError
from .object import Object

if typing.TYPE_CHECKING:
    import modal.client

PICKLE_PROTOCOL = 4  # Support older Python versions.


class Pickler(cloudpickle.Pickler):
    def __init__(self, buf):
        super().__init__(buf, protocol=PICKLE_PROTOCOL)

    def persistent_id(self, obj):
        from modal.partial_function import PartialFunction

        if isinstance(obj, _Object):
            flag = "_o"
        elif isinstance(obj, Object):
            flag = "o"
        elif isinstance(obj, PartialFunction):
            # Special case for PartialObject since it's a synchronicity wrapped object
            # that's set on serialized classes.
            # The resulting pickled instance can't be deserialized without this in a
            # new process, since the original referenced synchronizer will have different
            # values for `._original_attr` etc.

            impl_object = synchronizer._translate_in(obj)
            attributes = impl_object.__dict__.copy()
            # ugly - we remove the `._wrapped_attr` attribute from the implementation instance
            # to avoid referencing and therefore pickling the wrapped instance despite having
            # translated it to the implementation type

            # it would be nice if we could avoid this by not having the wrapped instances
            # be directly linked from objects and instead having a lookup table in the Synchronizer:
            if synchronizer._wrapped_attr and synchronizer._wrapped_attr in attributes:
                attributes.pop(synchronizer._wrapped_attr)

            return ("sync", (impl_object.__class__, attributes))
        else:
            return
        if not obj.is_hydrated:
            raise InvalidError(f"Can't serialize object {obj} which hasn't been hydrated.")
        return (obj.object_id, flag, obj._get_metadata())


class Unpickler(pickle.Unpickler):
    def __init__(self, client, buf):
        self.client = client
        super().__init__(buf)

    def persistent_load(self, pid):
        if len(pid) == 2:
            # more general protocol
            obj_type, obj_data = pid
            if obj_type == "sync":  # synchronicity wrapped object
                # not actually a proto object in this case but the underlying object of a synchronicity object
                impl_class, attributes = obj_data
                impl_instance = impl_class.__new__(impl_class)
                impl_instance.__dict__.update(attributes)
                return synchronizer._translate_out(impl_instance)
            else:
                raise ExecutionError("Unknown serialization format")

        # old protocol, always a 3-tuple
        (object_id, flag, handle_proto) = pid
        if flag in ("o", "p", "h"):
            return Object._new_hydrated(object_id, self.client, handle_proto)
        elif flag in ("_o", "_p", "_h"):
            return _Object._new_hydrated(object_id, self.client, handle_proto)
        else:
            raise InvalidError("bad flag")


def serialize(obj: Any) -> bytes:
    """Serializes object and replaces all references to the client class by a placeholder."""
    buf = io.BytesIO()
    Pickler(buf).dump(obj)
    return buf.getvalue()


def deserialize(s: bytes, client) -> Any:
    """Deserializes object and replaces all client placeholders by self."""
    from ._runtime.execution_context import is_local  # Avoid circular import

    env = "local" if is_local() else "remote"
    try:
        return Unpickler(client, io.BytesIO(s)).load()
    except AttributeError as exc:
        # We use a different cloudpickle version pre- and post-3.11. Unfortunately cloudpickle
        # doesn't expose some kind of serialization version number, so we have to guess based
        # on the error message.
        if "Can't get attribute '_make_function'" in str(exc):
            raise DeserializationError(
                "Deserialization failed due to a version mismatch between local and remote environments. "
                "Try changing the Python version in your Modal image to match your local Python version. "
            ) from exc
        else:
            # On Python 3.10+, AttributeError has `.name` and `.obj` attributes for better custom reporting
            raise DeserializationError(
                f"Deserialization failed with an AttributeError, {exc}. This is probably because"
                " you have different versions of a library in your local and remote environments."
            ) from exc
    except ModuleNotFoundError as exc:
        raise DeserializationError(
            f"Deserialization failed because the '{exc.name}' module is not available in the {env} environment."
        ) from exc
    except Exception as exc:
        if env == "remote":
            # We currently don't always package the full traceback from errors in the remote entrypoint logic.
            # So try to include as much information as we can in the main error message.
            more = f": {type(exc)}({str(exc)})"
        else:
            # When running locally, we can just rely on standard exception chaining.
            more = " (see above for details)"
        raise DeserializationError(
            f"Encountered an error when deserializing an object in the {env} environment{more}."
        ) from exc


def _serialize_asgi(obj: Any) -> api_pb2.Asgi:
    def flatten_headers(obj):
        return [s for k, v in obj for s in (k, v)]

    if obj is None:
        return api_pb2.Asgi()

    msg_type = obj.get("type")

    if msg_type == "http":
        return api_pb2.Asgi(
            http=api_pb2.Asgi.Http(
                http_version=obj.get("http_version", "1.1"),
                method=obj["method"],
                scheme=obj.get("scheme", "http"),
                path=obj["path"],
                query_string=obj.get("query_string"),
                headers=flatten_headers(obj.get("headers", [])),
                client_host=obj["client"][0] if obj.get("client") else None,
                client_port=obj["client"][1] if obj.get("client") else None,
            )
        )
    elif msg_type == "http.request":
        return api_pb2.Asgi(
            http_request=api_pb2.Asgi.HttpRequest(
                body=obj.get("body"),
                more_body=obj.get("more_body"),
            )
        )
    elif msg_type == "http.response.start":
        return api_pb2.Asgi(
            http_response_start=api_pb2.Asgi.HttpResponseStart(
                status=obj["status"],
                headers=flatten_headers(obj.get("headers", [])),
                trailers=obj.get("trailers"),
            )
        )
    elif msg_type == "http.response.body":
        return api_pb2.Asgi(
            http_response_body=api_pb2.Asgi.HttpResponseBody(
                body=obj.get("body"),
                more_body=obj.get("more_body"),
            )
        )
    elif msg_type == "http.response.trailers":
        return api_pb2.Asgi(
            http_response_trailers=api_pb2.Asgi.HttpResponseTrailers(
                headers=flatten_headers(obj.get("headers", [])),
                more_trailers=obj.get("more_trailers"),
            )
        )
    elif msg_type == "http.disconnect":
        return api_pb2.Asgi(http_disconnect=api_pb2.Asgi.HttpDisconnect())

    elif msg_type == "websocket":
        return api_pb2.Asgi(
            websocket=api_pb2.Asgi.Websocket(
                http_version=obj.get("http_version", "1.1"),
                scheme=obj.get("scheme", "ws"),
                path=obj["path"],
                query_string=obj.get("query_string"),
                headers=flatten_headers(obj.get("headers", [])),
                client_host=obj["client"][0] if obj.get("client") else None,
                client_port=obj["client"][1] if obj.get("client") else None,
                subprotocols=obj.get("subprotocols"),
            )
        )
    elif msg_type == "websocket.connect":
        return api_pb2.Asgi(
            websocket_connect=api_pb2.Asgi.WebsocketConnect(),
        )
    elif msg_type == "websocket.accept":
        return api_pb2.Asgi(
            websocket_accept=api_pb2.Asgi.WebsocketAccept(
                subprotocol=obj.get("subprotocol"),
                headers=flatten_headers(obj.get("headers", [])),
            )
        )
    elif msg_type == "websocket.receive":
        return api_pb2.Asgi(
            websocket_receive=api_pb2.Asgi.WebsocketReceive(
                bytes=obj.get("bytes"),
                text=obj.get("text"),
            )
        )
    elif msg_type == "websocket.send":
        return api_pb2.Asgi(
            websocket_send=api_pb2.Asgi.WebsocketSend(
                bytes=obj.get("bytes"),
                text=obj.get("text"),
            )
        )
    elif msg_type == "websocket.disconnect":
        return api_pb2.Asgi(
            websocket_disconnect=api_pb2.Asgi.WebsocketDisconnect(
                code=obj.get("code"),
            )
        )
    elif msg_type == "websocket.close":
        return api_pb2.Asgi(
            websocket_close=api_pb2.Asgi.WebsocketClose(
                code=obj.get("code"),
                reason=obj.get("reason"),
            )
        )

    else:
        logger.debug("skipping serialization of unknown ASGI message type %r", msg_type)
        return api_pb2.Asgi()


def _deserialize_asgi(asgi: api_pb2.Asgi) -> Any:
    def unflatten_headers(obj):
        return list(zip(obj[::2], obj[1::2]))

    msg_type = asgi.WhichOneof("type")

    if msg_type == "http":
        return {
            "type": "http",
            "http_version": asgi.http.http_version,
            "method": asgi.http.method,
            "scheme": asgi.http.scheme,
            "path": asgi.http.path,
            "query_string": asgi.http.query_string,
            "headers": unflatten_headers(asgi.http.headers),
            **({"client": (asgi.http.client_host, asgi.http.client_port)} if asgi.http.HasField("client_host") else {}),
            "extensions": {
                "http.response.trailers": {},
            },
        }
    elif msg_type == "http_request":
        return {
            "type": "http.request",
            "body": asgi.http_request.body,
            "more_body": asgi.http_request.more_body,
        }
    elif msg_type == "http_response_start":
        return {
            "type": "http.response.start",
            "status": asgi.http_response_start.status,
            "headers": unflatten_headers(asgi.http_response_start.headers),
            "trailers": asgi.http_response_start.trailers,
        }
    elif msg_type == "http_response_body":
        return {
            "type": "http.response.body",
            "body": asgi.http_response_body.body,
            "more_body": asgi.http_response_body.more_body,
        }
    elif msg_type == "http_response_trailers":
        return {
            "type": "http.response.trailers",
            "headers": unflatten_headers(asgi.http_response_trailers.headers),
            "more_trailers": asgi.http_response_trailers.more_trailers,
        }
    elif msg_type == "http_disconnect":
        return {"type": "http.disconnect"}

    elif msg_type == "websocket":
        return {
            "type": "websocket",
            "http_version": asgi.websocket.http_version,
            "scheme": asgi.websocket.scheme,
            "path": asgi.websocket.path,
            "query_string": asgi.websocket.query_string,
            "headers": unflatten_headers(asgi.websocket.headers),
            **(
                {"client": (asgi.websocket.client_host, asgi.websocket.client_port)}
                if asgi.websocket.HasField("client_host")
                else {}
            ),
            "subprotocols": list(asgi.websocket.subprotocols),
        }
    elif msg_type == "websocket_connect":
        return {"type": "websocket.connect"}
    elif msg_type == "websocket_accept":
        return {
            "type": "websocket.accept",
            "subprotocol": asgi.websocket_accept.subprotocol if asgi.websocket_accept.HasField("subprotocol") else None,
            "headers": unflatten_headers(asgi.websocket_accept.headers),
        }
    elif msg_type == "websocket_receive":
        return {
            "type": "websocket.receive",
            "bytes": asgi.websocket_receive.bytes if asgi.websocket_receive.HasField("bytes") else None,
            "text": asgi.websocket_receive.text if asgi.websocket_receive.HasField("text") else None,
        }
    elif msg_type == "websocket_send":
        return {
            "type": "websocket.send",
            "bytes": asgi.websocket_send.bytes if asgi.websocket_send.HasField("bytes") else None,
            "text": asgi.websocket_send.text if asgi.websocket_send.HasField("text") else None,
        }
    elif msg_type == "websocket_disconnect":
        return {
            "type": "websocket.disconnect",
            "code": asgi.websocket_disconnect.code if asgi.websocket_disconnect.HasField("code") else 1005,
        }
    elif msg_type == "websocket_close":
        return {
            "type": "websocket.close",
            "code": asgi.websocket_close.code if asgi.websocket_close.HasField("code") else 1000,
            "reason": asgi.websocket_close.reason,
        }

    else:
        assert msg_type is None
        return None


def serialize_data_format(obj: Any, data_format: int) -> bytes:
    """Similar to serialize(), but supports other data formats."""
    if data_format == api_pb2.DATA_FORMAT_PICKLE:
        return serialize(obj)
    elif data_format == api_pb2.DATA_FORMAT_ASGI:
        return _serialize_asgi(obj).SerializeToString(deterministic=True)
    elif data_format == api_pb2.DATA_FORMAT_GENERATOR_DONE:
        assert isinstance(obj, api_pb2.GeneratorDone)
        return obj.SerializeToString(deterministic=True)
    else:
        raise InvalidError(f"Unknown data format {data_format!r}")


def deserialize_data_format(s: bytes, data_format: int, client) -> Any:
    if data_format == api_pb2.DATA_FORMAT_PICKLE:
        return deserialize(s, client)
    elif data_format == api_pb2.DATA_FORMAT_ASGI:
        return _deserialize_asgi(api_pb2.Asgi.FromString(s))
    elif data_format == api_pb2.DATA_FORMAT_GENERATOR_DONE:
        return api_pb2.GeneratorDone.FromString(s)
    else:
        raise InvalidError(f"Unknown data format {data_format!r}")


class ClsConstructorPickler(pickle.Pickler):
    def __init__(self, buf):
        super().__init__(buf, protocol=PICKLE_PROTOCOL)

    def persistent_id(self, obj):
        if isinstance(obj, (_Object, Object)):
            if not obj.object_id:
                raise InvalidError(f"Can't serialize object {obj} which hasn't been created.")
            return True


def check_valid_cls_constructor_arg(key, obj):
    # Basically pickle, but with support for modal objects
    buf = io.BytesIO()
    try:
        ClsConstructorPickler(buf).dump(obj)
        return True
    except (AttributeError, ValueError):
        raise ValueError(
            f"Only pickle-able types are allowed in remote class constructors: argument {key} of type {type(obj)}."
        )


@dataclass
class ParamTypeInfo:
    default_field: str
    proto_field: str
    converter: typing.Callable[[str], typing.Any]


PARAM_TYPE_MAPPING = {
    api_pb2.PARAM_TYPE_STRING: ParamTypeInfo(default_field="string_default", proto_field="string_value", converter=str),
    api_pb2.PARAM_TYPE_INT: ParamTypeInfo(default_field="int_default", proto_field="int_value", converter=int),
}


def serialize_proto_params(python_params: dict[str, Any], schema: typing.Sequence[api_pb2.ClassParameterSpec]) -> bytes:
    proto_params: list[api_pb2.ClassParameterValue] = []
    for schema_param in schema:
        type_info = PARAM_TYPE_MAPPING.get(schema_param.type)
        if not type_info:
            raise ValueError(f"Unsupported parameter type: {schema_param.type}")
        proto_param = api_pb2.ClassParameterValue(
            name=schema_param.name,
            type=schema_param.type,
        )
        python_value = python_params.get(schema_param.name)
        if python_value is None:
            if schema_param.has_default:
                python_value = getattr(schema_param, type_info.default_field)
            else:
                raise ValueError(f"Missing required parameter: {schema_param.name}")
        try:
            converted_value = type_info.converter(python_value)
        except ValueError as exc:
            raise ValueError(f"Invalid type for parameter {schema_param.name}: {exc}")
        setattr(proto_param, type_info.proto_field, converted_value)
        proto_params.append(proto_param)
    proto_bytes = api_pb2.ClassParameterSet(parameters=proto_params).SerializeToString(deterministic=True)
    return proto_bytes


def deserialize_proto_params(serialized_params: bytes, schema: list[api_pb2.ClassParameterSpec]) -> dict[str, Any]:
    proto_struct = api_pb2.ClassParameterSet()
    proto_struct.ParseFromString(serialized_params)
    value_by_name = {p.name: p for p in proto_struct.parameters}
    python_params = {}
    for schema_param in schema:
        if schema_param.name not in value_by_name:
            # TODO: handle default values? Could just be a flag on the FunctionParameter schema spec,
            #  allowing it to not be supplied in the FunctionParameterSet?
            raise AttributeError(f"Constructor arguments don't match declared parameters (missing {schema_param.name})")
        param_value = value_by_name[schema_param.name]
        if schema_param.type != param_value.type:
            raise ValueError(
                "Constructor arguments types don't match declared parameters "
                f"({schema_param.name}: type {schema_param.type} != type {param_value.type})"
            )
        python_value: Any
        if schema_param.type == api_pb2.PARAM_TYPE_STRING:
            python_value = param_value.string_value
        elif schema_param.type == api_pb2.PARAM_TYPE_INT:
            python_value = param_value.int_value
        else:
            # TODO(elias): based on `parameters` declared types, we could add support for
            #  custom non proto types encoded as bytes in the proto, e.g. PARAM_TYPE_PYTHON_PICKLE
            raise NotImplementedError("Only strings and ints are supported parameter value types at the moment")

        python_params[schema_param.name] = python_value

    return python_params


def deserialize_params(serialized_params: bytes, function_def: api_pb2.Function, _client: "modal.client._Client"):
    if function_def.class_parameter_info.format in (
        api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_UNSPECIFIED,
        api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PICKLE,
    ):
        # legacy serialization format - pickle of `(args, kwargs)` w/ support for modal object arguments
        param_args, param_kwargs = deserialize(serialized_params, _client)
    elif function_def.class_parameter_info.format == api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PROTO:
        param_args = ()
        param_kwargs = deserialize_proto_params(serialized_params, list(function_def.class_parameter_info.schema))
    else:
        raise ExecutionError(
            f"Unknown class parameter serialization format: {function_def.class_parameter_info.format}"
        )

    return param_args, param_kwargs


================================================
File: modal/_traceback.py
================================================
# Copyright Modal Labs 2022
"""Helper functions related to operating on exceptions, warnings, and traceback objects.

Functions related to *displaying* tracebacks should go in `modal/cli/_traceback.py`
so that Rich is not a dependency of the container Client.
"""

import re
import sys
import traceback
import warnings
from types import TracebackType
from typing import Any, Iterable, Optional

from modal_proto import api_pb2

from ._vendor.tblib import Traceback as TBLibTraceback
from .exception import ServerWarning

TBDictType = dict[str, Any]
LineCacheType = dict[tuple[str, str], str]


def extract_traceback(exc: BaseException, task_id: str) -> tuple[TBDictType, LineCacheType]:
    """Given an exception, extract a serializable traceback (with task ID markers included),
    and a line cache that maps (filename, lineno) to line contents. The latter is used to show
    a helpful traceback to the user, even if they don't have packages installed locally that
    are referenced in the traceback."""

    tb = TBLibTraceback(exc.__traceback__)
    # Prefix traceback file paths with <task_id>. This lets us attribute which parts of
    # the traceback came from specific remote containers, while still fitting in the TracebackType
    # spec. Real paths can never start with <; we can use this to extract task_ids from filenames
    # at the client.
    cur = tb
    while cur is not None:
        file = cur.tb_frame.f_code.co_filename

        # Paths starting with < indicate that they're from a traceback from a remote
        # container. This means we've reached the end of the local traceback.
        if file.startswith("<"):
            break
        # We rely on this specific filename format when inferring where the exception was raised
        # in various other exception-related code
        cur.tb_frame.f_code.co_filename = f"<{task_id}>:{file}"
        cur = cur.tb_next

    tb_dict = tb.to_dict()

    line_cache = getattr(exc, "__line_cache__", {})

    for frame in traceback.extract_tb(exc.__traceback__):
        line_cache[(frame.filename, frame.lineno)] = frame.line

    return tb_dict, line_cache


def append_modal_tb(exc: BaseException, tb_dict: TBDictType, line_cache: LineCacheType) -> None:
    tb = TBLibTraceback.from_dict(tb_dict).as_traceback()

    # Filter out the prefix corresponding to internal Modal frames, and then make
    # the remote traceback from a Modal function the starting point of the current
    # exception's traceback.

    while tb is not None:
        if "/pkg/modal/" not in tb.tb_frame.f_code.co_filename:
            break
        tb = tb.tb_next

    exc.__traceback__ = tb

    setattr(exc, "__line_cache__", line_cache)


def reduce_traceback_to_user_code(tb: Optional[TracebackType], user_source: str) -> TracebackType:
    """Return a traceback that does not contain modal entrypoint or synchronicity frames."""

    # Step forward all the way through the traceback and drop any "Modal support" frames
    def skip_frame(filename: str) -> bool:
        return "/site-packages/synchronicity/" in filename or "modal/_utils/deprecation" in filename

    tb_root = tb
    while tb is not None:
        while tb.tb_next is not None:
            if skip_frame(tb.tb_next.tb_frame.f_code.co_filename):
                tb.tb_next = tb.tb_next.tb_next
            else:
                break
        tb = tb.tb_next
    tb = tb_root

    # Now step forward again until we get to first frame of user code
    if user_source.endswith(".py"):
        while tb is not None and tb.tb_frame.f_code.co_filename != user_source:
            tb = tb.tb_next
    else:
        while tb is not None and tb.tb_frame.f_code.co_name != "<module>":
            tb = tb.tb_next
    if tb is None:
        # In case we didn't find a frame that matched the user source, revert to the original root
        tb = tb_root

    return tb


def traceback_contains_remote_call(tb: Optional[TracebackType]) -> bool:
    """Inspect the traceback stack to determine whether an error was raised locally or remotely."""
    while tb is not None:
        if re.match(r"^<ta-[0-9A-Z]{26}>:", tb.tb_frame.f_code.co_filename):
            return True
        tb = tb.tb_next
    return False


def print_exception(exc: Optional[type[BaseException]], value: Optional[BaseException], tb: Optional[TracebackType]):
    """Add backwards compatibility for printing exceptions with "notes" for Python<3.11."""
    traceback.print_exception(exc, value, tb)
    if sys.version_info < (3, 11) and value is not None:
        notes = getattr(value, "__notes__", [])
        print(*notes, sep="\n", file=sys.stderr)


def print_server_warnings(server_warnings: Iterable[api_pb2.Warning]):
    """Issue a warning originating from the server with empty metadata about local origin.

    When using the Modal CLI, these warnings should get caught and coerced into Rich panels.
    """
    for warning in server_warnings:
        warnings.warn_explicit(warning.message, ServerWarning, "<modal-server>", 0)


================================================
File: modal/_tunnel.py
================================================
# Copyright Modal Labs 2023
"""Client for Modal relay servers, allowing users to expose TLS."""

from collections.abc import AsyncIterator
from dataclasses import dataclass
from typing import Optional

from grpclib import GRPCError, Status
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from .client import _Client
from .exception import InvalidError, RemoteError


@dataclass(frozen=True)
class Tunnel:
    """A port forwarded from within a running Modal container. Created by `modal.forward()`.

    **Important:** This is an experimental API which may change in the future.
    """

    host: str
    port: int
    unencrypted_host: str
    unencrypted_port: int

    @property
    def url(self) -> str:
        """Get the public HTTPS URL of the forwarded port."""
        value = f"https://{self.host}"
        if self.port != 443:
            value += f":{self.port}"
        return value

    @property
    def tls_socket(self) -> tuple[str, int]:
        """Get the public TLS socket as a (host, port) tuple."""
        return (self.host, self.port)

    @property
    def tcp_socket(self) -> tuple[str, int]:
        """Get the public TCP socket as a (host, port) tuple."""
        if not self.unencrypted_host:
            raise InvalidError(
                "This tunnel is not configured for unencrypted TCP. Please use `forward(..., unencrypted=True)`."
            )
        return (self.unencrypted_host, self.unencrypted_port)


@asynccontextmanager
async def _forward(port: int, *, unencrypted: bool = False, client: Optional[_Client] = None) -> AsyncIterator[Tunnel]:
    """Expose a port publicly from inside a running Modal container, with TLS.

    If `unencrypted` is set, this also exposes the TCP socket without encryption on a random port
    number. This can be used to SSH into a container (see example below). Note that it is on the public Internet, so
    make sure you are using a secure protocol over TCP.

    **Important:** This is an experimental API which may change in the future.

    **Usage:**

    ```python notest
    import modal
    from flask import Flask

    app = modal.App(image=modal.Image.debian_slim().pip_install("Flask"))
    flask_app = Flask(__name__)


    @flask_app.route("/")
    def hello_world():
        return "Hello, World!"


    @app.function()
    def run_app():
        # Start a web server inside the container at port 8000. `modal.forward(8000)` lets us
        # expose that port to the world at a random HTTPS URL.
        with modal.forward(8000) as tunnel:
            print("Server listening at", tunnel.url)
            flask_app.run("0.0.0.0", 8000)

        # When the context manager exits, the port is no longer exposed.
    ```

    **Raw TCP usage:**

    ```python
    import socket
    import threading

    import modal


    def run_echo_server(port: int):
        \"""Run a TCP echo server listening on the given port.\"""
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(("0.0.0.0", port))
        sock.listen(1)

        while True:
            conn, addr = sock.accept()
            print("Connection from:", addr)

            # Start a new thread to handle the connection
            def handle(conn):
                with conn:
                    while True:
                        data = conn.recv(1024)
                        if not data:
                            break
                        conn.sendall(data)

            threading.Thread(target=handle, args=(conn,)).start()


    app = modal.App()


    @app.function()
    def tcp_tunnel():
        # This exposes port 8000 to public Internet traffic over TCP.
        with modal.forward(8000, unencrypted=True) as tunnel:
            # You can connect to this TCP socket from outside the container, for example, using `nc`:
            #  nc <HOST> <PORT>
            print("TCP tunnel listening at:", tunnel.tcp_socket)
            run_echo_server(8000)
    ```

    **SSH example:**
    This assumes you have a rsa keypair in `~/.ssh/id_rsa{.pub}`, this is a bare-bones example
    letting you SSH into a Modal container.

    ```python
    import subprocess
    import time

    import modal

    app = modal.App()
    image = (
        modal.Image.debian_slim()
        .apt_install("openssh-server")
        .run_commands("mkdir /run/sshd")
        .add_local_file("~/.ssh/id_rsa.pub", "/root/.ssh/authorized_keys", copy=True)
    )


    @app.function(image=image, timeout=3600)
    def some_function():
        subprocess.Popen(["/usr/sbin/sshd", "-D", "-e"])
        with modal.forward(port=22, unencrypted=True) as tunnel:
            hostname, port = tunnel.tcp_socket
            connection_cmd = f'ssh -p {port} root@{hostname}'
            print(f"ssh into container using: {connection_cmd}")
            time.sleep(3600)  # keep alive for 1 hour or until killed
    ```

    If you intend to use this more generally, a suggestion is to put the subprocess and port
    forwarding code in an `@enter` lifecycle method of an @app.cls, to only make a single
    ssh server and port for each container (and not one for each input to the function).
    """

    if not isinstance(port, int):
        raise InvalidError(f"The port argument should be an int, not {port!r}")
    if port < 1 or port > 65535:
        raise InvalidError(f"Invalid port number {port}")

    if not client:
        client = await _Client.from_env()

    if client.client_type != api_pb2.CLIENT_TYPE_CONTAINER:
        raise InvalidError("Forwarding ports only works inside a Modal container")

    try:
        response = await client.stub.TunnelStart(api_pb2.TunnelStartRequest(port=port, unencrypted=unencrypted))
    except GRPCError as exc:
        if exc.status == Status.ALREADY_EXISTS:
            raise InvalidError(f"Port {port} is already forwarded")
        elif exc.status == Status.UNAVAILABLE:
            raise RemoteError("Relay server is unavailable") from exc
        else:
            raise

    try:
        yield Tunnel(response.host, response.port, response.unencrypted_host, response.unencrypted_port)
    finally:
        await client.stub.TunnelStop(api_pb2.TunnelStopRequest(port=port))


forward = synchronize_api(_forward)


================================================
File: modal/_watcher.py
================================================
# Copyright Modal Labs 2022
from collections import defaultdict
from collections.abc import AsyncGenerator
from pathlib import Path
from typing import Optional

from rich.tree import Tree
from watchfiles import Change, DefaultFilter, awatch

from modal.mount import _Mount

from .output import _get_output_manager

_TIMEOUT_SENTINEL = object()


class AppFilesFilter(DefaultFilter):
    def __init__(
        self,
        *,
        # A directory filter is used to only watch certain files within a directory.
        # Watching specific files is discouraged on Linux, so to watch a file we watch its
        # containing directory and then filter that directory's changes for relevant files.
        # https://github.com/notify-rs/notify/issues/394
        dir_filters: dict[Path, Optional[set[Path]]],
    ) -> None:
        self.dir_filters = dir_filters
        super().__init__()

    def __call__(self, change: Change, path: str) -> bool:
        p = Path(path).absolute()
        if p.name == ".DS_Store":
            return False
        # Vim creates this temporary file to see whether it can write
        # into a target directory.
        elif p.name == "4913":
            return False

        allowlists = set()

        for root, allowlist in self.dir_filters.items():
            # For every filter path that's a parent of the current path...
            if root in p.parents:
                # If allowlist is None, we're watching the directory and we have a match.
                if allowlist is None:
                    return super().__call__(change, path)

                # If not, it's specific files, and we could have a match.
                else:
                    allowlists |= allowlist

        if allowlists and p not in allowlists:
            return False

        return super().__call__(change, path)


async def _watch_paths(paths: set[Path], watch_filter: AppFilesFilter) -> AsyncGenerator[set[str], None]:
    try:
        async for changes in awatch(*paths, step=500, watch_filter=watch_filter):
            changed_paths = {stringpath for _, stringpath in changes}
            yield changed_paths
    except RuntimeError:
        # Thrown by watchfiles from Rust, when the generator is closed externally.
        pass


def _print_watched_paths(paths: set[Path]):
    msg = "️️⚡️ Serving... hit Ctrl-C to stop!"

    output_tree = Tree(msg, guide_style="gray50")

    for path in paths:
        output_tree.add(f"Watching {path}.")

    if output_mgr := _get_output_manager():
        output_mgr.print(output_tree)


def _watch_args_from_mounts(mounts: list[_Mount]) -> tuple[set[Path], AppFilesFilter]:
    paths = set()
    dir_filters: dict[Path, Optional[set[Path]]] = defaultdict(set)
    for mount in mounts:
        # TODO(elias): Make this part of the mount class instead, since it uses so much internals
        for entry in mount._entries:
            path, filter_file = entry.watch_entry()
            path = path.absolute().resolve()
            paths.add(path)
            if filter_file is None:
                dir_filters[path] = None
            elif dir_filters[path] is not None:
                dir_filters[path].add(filter_file.absolute().resolve())

    watch_filter = AppFilesFilter(dir_filters=dict(dir_filters))
    return paths, watch_filter


async def watch(mounts: list[_Mount]) -> AsyncGenerator[set[str], None]:
    paths, watch_filter = _watch_args_from_mounts(mounts)

    _print_watched_paths(paths)

    async for updated_paths in _watch_paths(paths, watch_filter):
        yield updated_paths


================================================
File: modal/app.py
================================================
# Copyright Modal Labs 2022
import inspect
import typing
import warnings
from collections.abc import AsyncGenerator, Coroutine, Sequence
from pathlib import PurePosixPath
from textwrap import dedent
from typing import (
    Any,
    Callable,
    ClassVar,
    Optional,
    Union,
    overload,
)

import typing_extensions
from google.protobuf.message import Message
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._functions import _Function
from ._ipython import is_notebook
from ._object import _get_environment_name, _Object
from ._partial_function import (
    _find_partial_methods_for_user_cls,
    _PartialFunction,
    _PartialFunctionFlags,
)
from ._utils.async_utils import synchronize_api
from ._utils.deprecation import (
    deprecation_error,
    deprecation_warning,
    renamed_parameter,
    warn_on_renamed_autoscaler_settings,
)
from ._utils.function_utils import FunctionInfo, is_global_object, is_method_fn
from ._utils.grpc_utils import retry_transient_errors
from ._utils.mount_utils import validate_volumes
from ._utils.name_utils import check_object_name
from .client import _Client
from .cloud_bucket_mount import _CloudBucketMount
from .cls import _Cls, parameter
from .config import logger
from .exception import ExecutionError, InvalidError
from .functions import Function
from .gpu import GPU_T
from .image import _Image
from .mount import _Mount
from .network_file_system import _NetworkFileSystem
from .partial_function import PartialFunction
from .proxy import _Proxy
from .retries import Retries
from .running_app import RunningApp
from .schedule import Schedule
from .scheduler_placement import SchedulerPlacement
from .secret import _Secret
from .volume import _Volume

_default_image: _Image = _Image.debian_slim()


class _LocalEntrypoint:
    _info: FunctionInfo
    _app: "_App"

    def __init__(self, info: FunctionInfo, app: "_App") -> None:
        self._info = info
        self._app = app

    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        return self._info.raw_f(*args, **kwargs)

    @property
    def info(self) -> FunctionInfo:
        return self._info

    @property
    def app(self) -> "_App":
        return self._app

    @property
    def stub(self) -> "_App":
        # Deprecated soon, only for backwards compatibility
        return self._app


LocalEntrypoint = synchronize_api(_LocalEntrypoint)


def check_sequence(items: typing.Sequence[typing.Any], item_type: type[typing.Any], error_msg: str) -> None:
    if not isinstance(items, (list, tuple)):
        raise InvalidError(error_msg)
    if not all(isinstance(v, item_type) for v in items):
        raise InvalidError(error_msg)


CLS_T = typing.TypeVar("CLS_T", bound=type[Any])


P = typing_extensions.ParamSpec("P")
ReturnType = typing.TypeVar("ReturnType")
OriginalReturnType = typing.TypeVar("OriginalReturnType")


class _FunctionDecoratorType:
    @overload
    def __call__(
        self, func: PartialFunction[P, ReturnType, OriginalReturnType]
    ) -> Function[P, ReturnType, OriginalReturnType]: ...  # already wrapped by a modal decorator, e.g. web_endpoint

    @overload
    def __call__(
        self, func: Callable[P, Coroutine[Any, Any, ReturnType]]
    ) -> Function[P, ReturnType, Coroutine[Any, Any, ReturnType]]: ...  # decorated async function

    @overload
    def __call__(
        self, func: Callable[P, ReturnType]
    ) -> Function[P, ReturnType, ReturnType]: ...  # decorated non-async function

    def __call__(self, func): ...


class _App:
    """A Modal App is a group of functions and classes that are deployed together.

    The app serves at least three purposes:

    * A unit of deployment for functions and classes.
    * Syncing of identities of (primarily) functions and classes across processes
      (your local Python interpreter and every Modal container active in your application).
    * Manage log collection for everything that happens inside your code.

    **Registering functions with an app**

    The most common way to explicitly register an Object with an app is through the
    `@app.function()` decorator. It both registers the annotated function itself and
    other passed objects, like schedules and secrets, with the app:

    ```python
    import modal

    app = modal.App()

    @app.function(
        secrets=[modal.Secret.from_name("some_secret")],
        schedule=modal.Period(days=1),
    )
    def foo():
        pass
    ```

    In this example, the secret and schedule are registered with the app.
    """

    _all_apps: ClassVar[dict[Optional[str], list["_App"]]] = {}
    _container_app: ClassVar[Optional["_App"]] = None

    _name: Optional[str]
    _description: Optional[str]
    _functions: dict[str, _Function]
    _classes: dict[str, _Cls]

    _image: Optional[_Image]
    _mounts: Sequence[_Mount]
    _secrets: Sequence[_Secret]
    _volumes: dict[Union[str, PurePosixPath], _Volume]
    _web_endpoints: list[str]  # Used by the CLI
    _local_entrypoints: dict[str, _LocalEntrypoint]

    # Running apps only (container apps or running local)
    _app_id: Optional[str]  # Kept after app finishes
    _running_app: Optional[RunningApp]  # Various app info
    _client: Optional[_Client]

    _include_source_default: Optional[bool] = None

    def __init__(
        self,
        name: Optional[str] = None,
        *,
        image: Optional[_Image] = None,  # default image for all functions (default is `modal.Image.debian_slim()`)
        mounts: Sequence[_Mount] = [],  # default mounts for all functions
        secrets: Sequence[_Secret] = [],  # default secrets for all functions
        volumes: dict[Union[str, PurePosixPath], _Volume] = {},  # default volumes for all functions
        include_source: Optional[bool] = None,
    ) -> None:
        """Construct a new app, optionally with default image, mounts, secrets, or volumes.

        ```python notest
        image = modal.Image.debian_slim().pip_install(...)
        secret = modal.Secret.from_name("my-secret")
        volume = modal.Volume.from_name("my-data")
        app = modal.App(image=image, secrets=[secret], volumes={"/mnt/data": volume})
        ```
        """
        if name is not None and not isinstance(name, str):
            raise InvalidError("Invalid value for `name`: Must be string.")

        self._name = name
        self._description = name
        self._include_source_default = include_source

        check_sequence(mounts, _Mount, "`mounts=` has to be a list or tuple of `modal.Mount` objects")
        check_sequence(secrets, _Secret, "`secrets=` has to be a list or tuple of `modal.Secret` objects")
        validate_volumes(volumes)

        if image is not None and not isinstance(image, _Image):
            raise InvalidError("`image=` has to be a `modal.Image` object")

        self._functions = {}
        self._classes = {}
        self._image = image
        self._mounts = mounts
        self._secrets = secrets
        self._volumes = volumes
        self._local_entrypoints = {}
        self._web_endpoints = []

        self._app_id = None
        self._running_app = None  # Set inside container, OR during the time an app is running locally
        self._client = None

        # Register this app. This is used to look up the app in the container, when we can't get it from the function
        _App._all_apps.setdefault(self._name, []).append(self)

    @property
    def name(self) -> Optional[str]:
        """The user-provided name of the App."""
        return self._name

    @property
    def is_interactive(self) -> bool:
        """Whether the current app for the app is running in interactive mode."""
        # return self._name
        if self._running_app:
            return self._running_app.interactive
        else:
            return False

    @property
    def app_id(self) -> Optional[str]:
        """Return the app_id of a running or stopped app."""
        return self._app_id

    @property
    def description(self) -> Optional[str]:
        """The App's `name`, if available, or a fallback descriptive identifier."""
        return self._description

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def lookup(
        name: str,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_App":
        """Look up an App with a given name, creating a new App if necessary.

        Note that Apps created through this method will be in a deployed state,
        but they will not have any associated Functions or Classes. This method
        is mainly useful for creating an App to associate with a Sandbox:

        ```python
        app = modal.App.lookup("my-app", create_if_missing=True)
        modal.Sandbox.create("echo", "hi", app=app)
        ```
        """
        check_object_name(name, "App")

        if client is None:
            client = await _Client.from_env()

        environment_name = _get_environment_name(environment_name)

        request = api_pb2.AppGetOrCreateRequest(
            app_name=name,
            environment_name=environment_name,
            object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
        )

        response = await retry_transient_errors(client.stub.AppGetOrCreate, request)

        app = _App(name)
        app._app_id = response.app_id
        app._client = client
        app._running_app = RunningApp(response.app_id, interactive=False)
        return app

    def set_description(self, description: str):
        self._description = description

    def _validate_blueprint_value(self, key: str, value: Any):
        if not isinstance(value, _Object):
            raise InvalidError(f"App attribute `{key}` with value {value!r} is not a valid Modal object")

    @property
    def image(self) -> _Image:
        return self._image

    @image.setter
    def image(self, value):
        self._image = value

    def _uncreate_all_objects(self):
        # TODO(erikbern): this doesn't unhydrate objects that aren't tagged
        for obj in self._functions.values():
            obj._unhydrate()
        for obj in self._classes.values():
            obj._unhydrate()

    @asynccontextmanager
    async def _set_local_app(self, client: _Client, running_app: RunningApp) -> AsyncGenerator[None, None]:
        self._app_id = running_app.app_id
        self._running_app = running_app
        self._client = client
        try:
            yield
        finally:
            self._running_app = None
            self._client = None
            self._uncreate_all_objects()

    @asynccontextmanager
    async def run(
        self,
        client: Optional[_Client] = None,
        show_progress: Optional[bool] = None,
        detach: bool = False,
        interactive: bool = False,
        environment_name: Optional[str] = None,
    ) -> AsyncGenerator["_App", None]:
        """Context manager that runs an app on Modal.

        Use this as the main entry point for your Modal application. All calls
        to Modal functions should be made within the scope of this context
        manager, and they will correspond to the current app.

        **Example**

        ```python notest
        with app.run():
            some_modal_function.remote()
        ```

        To enable output printing, use `modal.enable_output()`:

        ```python notest
        with modal.enable_output():
            with app.run():
                some_modal_function.remote()
        ```

        Note that you cannot invoke this in global scope of a file where you have
        Modal functions or Classes, since that would run the block when the function
        or class is imported in your containers as well. If you want to run it as
        your entrypoint, consider wrapping it:

        ```python
        if __name__ == "__main__":
            with app.run():
                some_modal_function.remote()
        ```

        You can then run your script with:

        ```shell
        python app_module.py
        ```

        Note that this method used to return a separate "App" object. This is
        no longer useful since you can use the app itself for access to all
        objects. For backwards compatibility reasons, it returns the same app.
        """
        from .runner import _run_app  # Defer import of runner.py, which imports a lot from Rich

        # See Github discussion here: https://github.com/modal-labs/modal-client/pull/2030#issuecomment-2237266186

        if show_progress is True:
            deprecation_error(
                (2024, 11, 20),
                "`show_progress=True` is no longer supported. Use `with modal.enable_output():` instead.",
            )
        elif show_progress is False:
            deprecation_warning((2024, 11, 20), "`show_progress=False` is deprecated (and has no effect)")

        async with _run_app(
            self, client=client, detach=detach, interactive=interactive, environment_name=environment_name
        ):
            yield self

    def _get_default_image(self):
        if self._image:
            return self._image
        else:
            return _default_image

    def _get_watch_mounts(self):
        if not self._running_app:
            raise ExecutionError("`_get_watch_mounts` requires a running app.")

        all_mounts = [
            *self._mounts,
        ]
        for function in self.registered_functions.values():
            all_mounts.extend(function._serve_mounts)

        return [m for m in all_mounts if m.is_local()]

    def _add_function(self, function: _Function, is_web_endpoint: bool):
        if old_function := self._functions.get(function.tag, None):
            if old_function is function:
                return  # already added the same exact instance, ignore

            if not is_notebook():
                logger.warning(
                    f"Warning: function name '{function.tag}' collision!"
                    " Overriding existing function "
                    f"[{old_function._info.module_name}].{old_function._info.function_name}"
                    f" with new function [{function._info.module_name}].{function._info.function_name}"
                )
        if function.tag in self._classes:
            logger.warning(f"Warning: tag {function.tag} exists but is overridden by function")

        if self._running_app:
            # If this is inside a container, then objects can be defined after app initialization.
            # So we may have to initialize objects once they get bound to the app.
            if function.tag in self._running_app.function_ids:
                object_id: str = self._running_app.function_ids[function.tag]
                metadata: Message = self._running_app.object_handle_metadata[object_id]
                function._hydrate(object_id, self._client, metadata)

        self._functions[function.tag] = function
        if is_web_endpoint:
            self._web_endpoints.append(function.tag)

    def _add_class(self, tag: str, cls: _Cls):
        if self._running_app:
            # If this is inside a container, then objects can be defined after app initialization.
            # So we may have to initialize objects once they get bound to the app.
            if tag in self._running_app.class_ids:
                object_id: str = self._running_app.class_ids[tag]
                metadata: Message = self._running_app.object_handle_metadata[object_id]
                cls._hydrate(object_id, self._client, metadata)

        self._classes[tag] = cls

    def _init_container(self, client: _Client, running_app: RunningApp):
        self._app_id = running_app.app_id
        self._running_app = running_app
        self._client = client

        _App._container_app = self

        # Hydrate function objects
        for tag, object_id in running_app.function_ids.items():
            if tag in self._functions:
                obj = self._functions[tag]
                handle_metadata = running_app.object_handle_metadata[object_id]
                obj._hydrate(object_id, client, handle_metadata)

        # Hydrate class objects
        for tag, object_id in running_app.class_ids.items():
            if tag in self._classes:
                obj = self._classes[tag]
                handle_metadata = running_app.object_handle_metadata[object_id]
                obj._hydrate(object_id, client, handle_metadata)

    @property
    def registered_functions(self) -> dict[str, _Function]:
        """All modal.Function objects registered on the app."""
        return self._functions

    @property
    def registered_classes(self) -> dict[str, _Cls]:
        """All modal.Cls objects registered on the app."""
        return self._classes

    @property
    def registered_entrypoints(self) -> dict[str, _LocalEntrypoint]:
        """All local CLI entrypoints registered on the app."""
        return self._local_entrypoints

    @property
    def indexed_objects(self) -> dict[str, _Object]:
        deprecation_warning(
            (2024, 11, 25),
            "`app.indexed_objects` is deprecated! Use `app.registered_functions` or `app.registered_classes` instead.",
        )
        return dict(**self._functions, **self._classes)

    @property
    def registered_web_endpoints(self) -> list[str]:
        """Names of web endpoint (ie. webhook) functions registered on the app."""
        return self._web_endpoints

    def local_entrypoint(
        self, _warn_parentheses_missing: Any = None, *, name: Optional[str] = None
    ) -> Callable[[Callable[..., Any]], _LocalEntrypoint]:
        """Decorate a function to be used as a CLI entrypoint for a Modal App.

        These functions can be used to define code that runs locally to set up the app,
        and act as an entrypoint to start Modal functions from. Note that regular
        Modal functions can also be used as CLI entrypoints, but unlike `local_entrypoint`,
        those functions are executed remotely directly.

        **Example**

        ```python
        @app.local_entrypoint()
        def main():
            some_modal_function.remote()
        ```

        You can call the function using `modal run` directly from the CLI:

        ```shell
        modal run app_module.py
        ```

        Note that an explicit [`app.run()`](/docs/reference/modal.App#run) is not needed, as an
        [app](/docs/guide/apps) is automatically created for you.

        **Multiple Entrypoints**

        If you have multiple `local_entrypoint` functions, you can qualify the name of your app and function:

        ```shell
        modal run app_module.py::app.some_other_function
        ```

        **Parsing Arguments**

        If your entrypoint function take arguments with primitive types, `modal run` automatically parses them as
        CLI options.
        For example, the following function can be called with `modal run app_module.py --foo 1 --bar "hello"`:

        ```python
        @app.local_entrypoint()
        def main(foo: int, bar: str):
            some_modal_function.call(foo, bar)
        ```

        Currently, `str`, `int`, `float`, `bool`, and `datetime.datetime` are supported.
        Use `modal run app_module.py --help` for more information on usage.

        """
        if _warn_parentheses_missing:
            raise InvalidError("Did you forget parentheses? Suggestion: `@app.local_entrypoint()`.")
        if name is not None and not isinstance(name, str):
            raise InvalidError("Invalid value for `name`: Must be string.")

        def wrapped(raw_f: Callable[..., Any]) -> _LocalEntrypoint:
            info = FunctionInfo(raw_f)
            tag = name if name is not None else raw_f.__qualname__
            if tag in self._local_entrypoints:
                # TODO: get rid of this limitation.
                raise InvalidError(f"Duplicate local entrypoint name: {tag}. Local entrypoint names must be unique.")
            entrypoint = self._local_entrypoints[tag] = _LocalEntrypoint(info, self)
            return entrypoint

        return wrapped

    @warn_on_renamed_autoscaler_settings
    def function(
        self,
        _warn_parentheses_missing: Any = None,
        *,
        image: Optional[_Image] = None,  # The image to run as the container for the function
        schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function
        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
        gpu: Union[
            GPU_T, list[GPU_T]
        ] = None,  # GPU request as string ("any", "T4", ...), object (`modal.GPU.A100()`, ...), or a list of either
        serialized: bool = False,  # Whether to send the function over using cloudpickle.
        mounts: Sequence[_Mount] = (),  # Modal Mounts added to the container
        network_file_systems: dict[
            Union[str, PurePosixPath], _NetworkFileSystem
        ] = {},  # Mountpoints for Modal NetworkFileSystems
        volumes: dict[
            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
        # Specify, in fractional CPU cores, how many CPU cores to request.
        # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
        # CPU throttling will prevent a container from exceeding its specified limit.
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        # Specify, in MiB, a memory request which is the minimum memory required.
        # Or, pass (request, limit) to additionally specify a hard limit in MiB.
        memory: Optional[Union[int, tuple[int, int]]] = None,
        ephemeral_disk: Optional[int] = None,  # Specify, in MiB, the ephemeral disk size for the Function.
        min_containers: Optional[int] = None,  # Minimum number of containers to keep warm, even when Function is idle.
        max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
        buffer_containers: Optional[int] = None,  # Number of additional idle containers to maintain under active load.
        scaledown_window: Optional[int] = None,  # Max amount of time a container can remain idle before scaling down.
        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
        name: Optional[str] = None,  # Sets the Modal name of the function within the app
        is_generator: Optional[
            bool
        ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object
        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
        region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the function on.
        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
        block_network: bool = False,  # Whether to block network access
        # Maximum number of inputs a container should handle before shutting down.
        # With `max_inputs = 1`, containers will be single-use.
        max_inputs: Optional[int] = None,
        i6pn: Optional[bool] = None,  # Whether to enable IPv6 container networking within the region.
        # Whether the function's home package should be included in the image - defaults to True
        include_source: Optional[bool] = None,
        # Parameters below here are experimental. Use with caution!
        _experimental_scheduler_placement: Optional[
            SchedulerPlacement
        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
        _experimental_proxy_ip: Optional[str] = None,  # IP address of proxy
        _experimental_custom_scaling_factor: Optional[float] = None,  # Custom scaling factor
        _experimental_enable_gpu_snapshot: bool = False,  # Experimentally enable GPU memory snapshots.
        # Parameters below here are deprecated. Please update your code as suggested
        keep_warm: Optional[int] = None,  # Replaced with `min_containers`
        concurrency_limit: Optional[int] = None,  # Replaced with `max_containers`
        container_idle_timeout: Optional[int] = None,  # Replaced with `scaledown_window`
        _experimental_buffer_containers: Optional[int] = None,  # Now stable API with `buffer_containers`
    ) -> _FunctionDecoratorType:
        """Decorator to register a new Modal [Function](/docs/reference/modal.Function) with this App."""
        if isinstance(_warn_parentheses_missing, _Image):
            # Handle edge case where maybe (?) some users passed image as a positional arg
            raise InvalidError("`image` needs to be a keyword argument: `@app.function(image=image)`.")
        if _warn_parentheses_missing:
            raise InvalidError("Did you forget parentheses? Suggestion: `@app.function()`.")

        if image is None:
            image = self._get_default_image()

        secrets = [*self._secrets, *secrets]

        def wrapped(
            f: Union[_PartialFunction, Callable[..., Any], None],
        ) -> _Function:
            nonlocal is_generator, cloud, serialized

            # Check if the decorated object is a class
            if inspect.isclass(f):
                raise TypeError(
                    "The `@app.function` decorator cannot be used on a class. Please use `@app.cls` instead."
                )

            if isinstance(f, _PartialFunction):
                # typically for @function-wrapped @web_endpoint, @asgi_app, or @batched
                f.wrapped = True

                # but we don't support @app.function wrapping a method.
                if is_method_fn(f.raw_f.__qualname__):
                    raise InvalidError(
                        "The `@app.function` decorator cannot be used on class methods. "
                        "Swap with `@modal.method` or `@modal.web_endpoint`, or drop the `@app.function` decorator. "
                        "Example: "
                        "\n\n"
                        "```python\n"
                        "@app.cls()\n"
                        "class MyClass:\n"
                        "    @modal.web_endpoint()\n"
                        "    def f(self, x):\n"
                        "        ...\n"
                        "```\n"
                    )
                i6pn_enabled = i6pn or (f.flags & _PartialFunctionFlags.CLUSTERED)
                cluster_size = f.cluster_size  # Experimental: Clustered functions

                info = FunctionInfo(f.raw_f, serialized=serialized, name_override=name)
                raw_f = f.raw_f
                webhook_config = f.webhook_config
                is_generator = f.is_generator
                batch_max_size = f.batch_max_size
                batch_wait_ms = f.batch_wait_ms
            else:
                if not is_global_object(f.__qualname__) and not serialized:
                    raise InvalidError(
                        dedent(
                            """
                            The `@app.function` decorator must apply to functions in global scope,
                            unless `serialize=True` is set.
                            If trying to apply additional decorators, they may need to use `functools.wraps`.
                            """
                        )
                    )

                if is_method_fn(f.__qualname__):
                    raise InvalidError(
                        dedent(
                            """
                            The `@app.function` decorator cannot be used on class methods.
                            Please use `@app.cls` with `@modal.method` instead. Example:

                            ```python
                            @app.cls()
                            class MyClass:
                                @modal.method()
                                def f(self, x):
                                    ...
                            ```
                            """
                        )
                    )

                info = FunctionInfo(f, serialized=serialized, name_override=name)
                webhook_config = None
                batch_max_size = None
                batch_wait_ms = None
                raw_f = f

                cluster_size = None  # Experimental: Clustered functions
                i6pn_enabled = i6pn

            if info.function_name.endswith(".app"):
                warnings.warn(
                    "Beware: the function name is `app`. Modal will soon rename `Stub` to `App`, "
                    "so you might run into issues if you have code like `app = modal.App()` in the same scope"
                )

            if is_generator is None:
                is_generator = inspect.isgeneratorfunction(raw_f) or inspect.isasyncgenfunction(raw_f)

            scheduler_placement: Optional[SchedulerPlacement] = _experimental_scheduler_placement
            if region:
                if scheduler_placement:
                    raise InvalidError("`region` and `_experimental_scheduler_placement` cannot be used together")
                scheduler_placement = SchedulerPlacement(region=region)

            function = _Function.from_local(
                info,
                app=self,
                image=image,
                secrets=secrets,
                schedule=schedule,
                is_generator=is_generator,
                gpu=gpu,
                mounts=[*self._mounts, *mounts],
                network_file_systems=network_file_systems,
                allow_cross_region_volumes=allow_cross_region_volumes,
                volumes={**self._volumes, **volumes},
                cpu=cpu,
                memory=memory,
                ephemeral_disk=ephemeral_disk,
                proxy=proxy,
                retries=retries,
                min_containers=min_containers,
                max_containers=max_containers,
                buffer_containers=buffer_containers,
                scaledown_window=scaledown_window,
                allow_concurrent_inputs=allow_concurrent_inputs,
                batch_max_size=batch_max_size,
                batch_wait_ms=batch_wait_ms,
                timeout=timeout,
                cloud=cloud,
                webhook_config=webhook_config,
                enable_memory_snapshot=enable_memory_snapshot,
                block_network=block_network,
                max_inputs=max_inputs,
                scheduler_placement=scheduler_placement,
                _experimental_proxy_ip=_experimental_proxy_ip,
                i6pn_enabled=i6pn_enabled,
                cluster_size=cluster_size,  # Experimental: Clustered functions
                include_source=include_source if include_source is not None else self._include_source_default,
                _experimental_enable_gpu_snapshot=_experimental_enable_gpu_snapshot,
            )

            self._add_function(function, webhook_config is not None)

            return function

        return wrapped

    @typing_extensions.dataclass_transform(field_specifiers=(parameter,), kw_only_default=True)
    @warn_on_renamed_autoscaler_settings
    def cls(
        self,
        _warn_parentheses_missing: Optional[bool] = None,
        *,
        image: Optional[_Image] = None,  # The image to run as the container for the function
        secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
        gpu: Union[
            GPU_T, list[GPU_T]
        ] = None,  # GPU request as string ("any", "T4", ...), object (`modal.GPU.A100()`, ...), or a list of either
        serialized: bool = False,  # Whether to send the function over using cloudpickle.
        mounts: Sequence[_Mount] = (),
        network_file_systems: dict[
            Union[str, PurePosixPath], _NetworkFileSystem
        ] = {},  # Mountpoints for Modal NetworkFileSystems
        volumes: dict[
            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
        ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
        allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.
        # Specify, in fractional CPU cores, how many CPU cores to request.
        # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
        # CPU throttling will prevent a container from exceeding its specified limit.
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        # Specify, in MiB, a memory request which is the minimum memory required.
        # Or, pass (request, limit) to additionally specify a hard limit in MiB.
        memory: Optional[Union[int, tuple[int, int]]] = None,
        ephemeral_disk: Optional[int] = None,  # Specify, in MiB, the ephemeral disk size for the Function.
        min_containers: Optional[int] = None,  # Minimum number of containers to keep warm, even when Function is idle.
        max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
        buffer_containers: Optional[int] = None,  # Number of additional idle containers to maintain under active load.
        scaledown_window: Optional[int] = None,  # Max amount of time a container can remain idle before scaling down.
        proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
        retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
        allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.
        timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
        cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
        region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the function on.
        enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
        block_network: bool = False,  # Whether to block network access
        # Limits the number of inputs a container handles before shutting down.
        # Use `max_inputs = 1` for single-use containers.
        max_inputs: Optional[int] = None,
        include_source: Optional[bool] = None,
        # Parameters below here are experimental. Use with caution!
        _experimental_scheduler_placement: Optional[
            SchedulerPlacement
        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
        _experimental_proxy_ip: Optional[str] = None,  # IP address of proxy
        _experimental_custom_scaling_factor: Optional[float] = None,  # Custom scaling factor
        _experimental_enable_gpu_snapshot: bool = False,  # Experimentally enable GPU memory snapshots.
        # Parameters below here are deprecated. Please update your code as suggested
        keep_warm: Optional[int] = None,  # Replaced with `min_containers`
        concurrency_limit: Optional[int] = None,  # Replaced with `max_containers`
        container_idle_timeout: Optional[int] = None,  # Replaced with `scaledown_window`
        _experimental_buffer_containers: Optional[int] = None,  # Now stable API with `buffer_containers`
    ) -> Callable[[CLS_T], CLS_T]:
        """
        Decorator to register a new Modal [Cls](/docs/reference/modal.Cls) with this App.
        """
        if _warn_parentheses_missing:
            raise InvalidError("Did you forget parentheses? Suggestion: `@app.cls()`.")

        scheduler_placement = _experimental_scheduler_placement
        if region:
            if scheduler_placement:
                raise InvalidError("`region` and `_experimental_scheduler_placement` cannot be used together")
            scheduler_placement = SchedulerPlacement(region=region)

        def wrapper(user_cls: CLS_T) -> CLS_T:
            # Check if the decorated object is a class
            if not inspect.isclass(user_cls):
                raise TypeError("The @app.cls decorator must be used on a class.")

            batch_functions = _find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.BATCHED)
            if batch_functions:
                if len(batch_functions) > 1:
                    raise InvalidError(f"Modal class {user_cls.__name__} can only have one batched function.")
                if len(_find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.FUNCTION)) > 1:
                    raise InvalidError(
                        f"Modal class {user_cls.__name__} with a modal batched function cannot have other modal methods."  # noqa
                    )
                batch_function = next(iter(batch_functions.values()))
                batch_max_size = batch_function.batch_max_size
                batch_wait_ms = batch_function.batch_wait_ms
            else:
                batch_max_size = None
                batch_wait_ms = None

            if (
                _find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.ENTER_PRE_SNAPSHOT)
                and not enable_memory_snapshot
            ):
                raise InvalidError("A class must have `enable_memory_snapshot=True` to use `snap=True` on its methods.")

            info = FunctionInfo(None, serialized=serialized, user_cls=user_cls)

            cls_func = _Function.from_local(
                info,
                app=self,
                image=image or self._get_default_image(),
                secrets=[*self._secrets, *secrets],
                gpu=gpu,
                mounts=[*self._mounts, *mounts],
                network_file_systems=network_file_systems,
                allow_cross_region_volumes=allow_cross_region_volumes,
                volumes={**self._volumes, **volumes},
                cpu=cpu,
                memory=memory,
                ephemeral_disk=ephemeral_disk,
                min_containers=min_containers,
                max_containers=max_containers,
                buffer_containers=buffer_containers,
                scaledown_window=scaledown_window,
                proxy=proxy,
                retries=retries,
                allow_concurrent_inputs=allow_concurrent_inputs,
                batch_max_size=batch_max_size,
                batch_wait_ms=batch_wait_ms,
                timeout=timeout,
                cloud=cloud,
                enable_memory_snapshot=enable_memory_snapshot,
                block_network=block_network,
                max_inputs=max_inputs,
                scheduler_placement=scheduler_placement,
                include_source=include_source if include_source is not None else self._include_source_default,
                _experimental_proxy_ip=_experimental_proxy_ip,
                _experimental_custom_scaling_factor=_experimental_custom_scaling_factor,
                _experimental_enable_gpu_snapshot=_experimental_enable_gpu_snapshot,
            )

            self._add_function(cls_func, is_web_endpoint=False)

            cls: _Cls = _Cls.from_local(user_cls, self, cls_func)

            tag: str = user_cls.__name__
            self._add_class(tag, cls)
            return cls  # type: ignore  # a _Cls instance "simulates" being the user provided class

        return wrapper

    async def spawn_sandbox(
        self,
        *entrypoint_args: str,
        image: Optional[_Image] = None,  # The image to run as the container for the sandbox.
        mounts: Sequence[_Mount] = (),  # Mounts to attach to the sandbox.
        secrets: Sequence[_Secret] = (),  # Environment variables to inject into the sandbox.
        network_file_systems: dict[Union[str, PurePosixPath], _NetworkFileSystem] = {},
        timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.
        workdir: Optional[str] = None,  # Working directory of the sandbox.
        gpu: GPU_T = None,
        cloud: Optional[str] = None,
        region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the sandbox on.
        # Specify, in fractional CPU cores, how many CPU cores to request.
        # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
        # CPU throttling will prevent a container from exceeding its specified limit.
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        # Specify, in MiB, a memory request which is the minimum memory required.
        # Or, pass (request, limit) to additionally specify a hard limit in MiB.
        memory: Optional[Union[int, tuple[int, int]]] = None,
        block_network: bool = False,  # Whether to block network access
        volumes: dict[
            Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
        ] = {},  # Mount points for Modal Volumes and CloudBucketMounts
        pty_info: Optional[api_pb2.PTYInfo] = None,
        _experimental_scheduler_placement: Optional[
            SchedulerPlacement
        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
    ) -> None:
        """mdmd:hidden"""
        arglist = ", ".join(repr(s) for s in entrypoint_args)
        message = (
            "`App.spawn_sandbox` is deprecated.\n\n"
            "Sandboxes can be created using the `Sandbox` object:\n\n"
            f"```\nsb = Sandbox.create({arglist}, app=app)\n```\n\n"
            "See https://modal.com/docs/guide/sandbox for more info on working with sandboxes."
        )
        deprecation_error((2024, 7, 5), message)

    def include(self, /, other_app: "_App"):
        """Include another App's objects in this one.

        Useful for splitting up Modal Apps across different self-contained files.

        ```python
        app_a = modal.App("a")
        @app.function()
        def foo():
            ...

        app_b = modal.App("b")
        @app.function()
        def bar():
            ...

        app_a.include(app_b)

        @app_a.local_entrypoint()
        def main():
            # use function declared on the included app
            bar.remote()
        ```
        """
        for tag, function in other_app._functions.items():
            self._add_function(function, False)  # TODO(erikbern): webhook config?

        for tag, cls in other_app._classes.items():
            existing_cls = self._classes.get(tag)
            if existing_cls and existing_cls != cls:
                logger.warning(
                    f"Named app class {tag} with existing value {existing_cls} is being "
                    f"overwritten by a different class {cls}"
                )

            self._add_class(tag, cls)

    async def _logs(self, client: Optional[_Client] = None) -> AsyncGenerator[str, None]:
        """Stream logs from the app.

        This method is considered private and its interface may change - use at your own risk!
        """
        if not self._app_id:
            raise InvalidError("`app._logs` requires a running/stopped app.")

        client = client or self._client or await _Client.from_env()

        last_log_batch_entry_id: Optional[str] = None
        while True:
            request = api_pb2.AppGetLogsRequest(
                app_id=self._app_id,
                timeout=55,
                last_entry_id=last_log_batch_entry_id,
            )
            async for log_batch in client.stub.AppGetLogs.unary_stream(request):
                if log_batch.entry_id:
                    # log_batch entry_id is empty for fd="server" messages from AppGetLogs
                    last_log_batch_entry_id = log_batch.entry_id
                if log_batch.app_done:
                    return
                for log in log_batch.items:
                    if log.data:
                        yield log.data

    @classmethod
    def _get_container_app(cls) -> Optional["_App"]:
        """Returns the `App` running inside a container.

        This will return `None` outside of a Modal container."""
        return cls._container_app

    @classmethod
    def _reset_container_app(cls):
        """Only used for tests."""
        cls._container_app = None


App = synchronize_api(_App)


class _Stub(_App):
    """mdmd:hidden
    This enables using a "Stub" class instead of "App".

    For most of Modal's history, the app class was called "Stub", so this exists for
    backwards compatibility, in order to facilitate moving from "Stub" to "App".
    """

    def __new__(cls, *args, **kwargs):
        deprecation_warning(
            (2024, 4, 29),
            'The use of "Stub" has been deprecated in favor of "App".'
            " This is a pure name change with no other implications.",
        )
        return _App(*args, **kwargs)


Stub = synchronize_api(_Stub)


================================================
File: modal/call_graph.py
================================================
# Copyright Modal Labs 2022
from dataclasses import dataclass
from enum import IntEnum
from typing import Optional

from modal_proto import api_pb2


class InputStatus(IntEnum):
    """Enum representing status of a function input."""

    PENDING = 0
    SUCCESS = api_pb2.GenericResult.GENERIC_STATUS_SUCCESS
    FAILURE = api_pb2.GenericResult.GENERIC_STATUS_FAILURE
    INIT_FAILURE = api_pb2.GenericResult.GENERIC_STATUS_INIT_FAILURE
    TERMINATED = api_pb2.GenericResult.GENERIC_STATUS_TERMINATED
    TIMEOUT = api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT

    @classmethod
    def _missing_(cls, value):
        return cls.PENDING


@dataclass
class InputInfo:
    """Simple data structure storing information about a function input."""

    input_id: str
    function_call_id: str
    task_id: str
    status: InputStatus
    function_name: str
    module_name: str
    children: list["InputInfo"]


def _reconstruct_call_graph(ser_graph: api_pb2.FunctionGetCallGraphResponse) -> list[InputInfo]:
    function_calls_by_id: dict[str, api_pb2.FunctionCallCallGraphInfo] = {}
    inputs_by_id: dict[str, api_pb2.InputCallGraphInfo] = {}

    for function_call in ser_graph.function_calls:
        function_calls_by_id[function_call.function_call_id] = function_call

    for input in ser_graph.inputs:
        inputs_by_id[input.input_id] = input

    input_info_by_id: dict[str, InputInfo] = {}
    result = []

    def _reconstruct(input_id: str) -> Optional[InputInfo]:
        if input_id in input_info_by_id:
            return input_info_by_id[input_id]

        # Input info can be missing, because input retention is limited.
        if input_id not in inputs_by_id:
            return None

        input = inputs_by_id[input_id]
        function_call = function_calls_by_id[input.function_call_id]
        input_info_by_id[input_id] = InputInfo(
            input_id,
            input.function_call_id,
            input.task_id,
            InputStatus(input.status),
            function_call.function_name,
            function_call.module_name,
            [],
        )

        if function_call.parent_input_id:
            # Find parent and append to list of children.
            parent = _reconstruct(function_call.parent_input_id)
            if parent:
                parent.children.append(input_info_by_id[input_id])
        else:
            # Top-level input.
            result.append(input_info_by_id[input_id])

        return input_info_by_id[input_id]

    for input_id in inputs_by_id.keys():
        _reconstruct(input_id)

    return result


================================================
File: modal/client.py
================================================
# Copyright Modal Labs 2022
import asyncio
import os
import platform
import warnings
from collections.abc import AsyncGenerator, AsyncIterator, Collection, Mapping
from typing import (
    Any,
    ClassVar,
    Generic,
    Optional,
    TypeVar,
    Union,
)

import grpclib.client
from google.protobuf import empty_pb2
from google.protobuf.message import Message
from synchronicity.async_wrap import asynccontextmanager

from modal._utils.async_utils import synchronizer
from modal_proto import api_grpc, api_pb2, modal_api_grpc
from modal_version import __version__

from ._traceback import print_server_warnings
from ._utils import async_utils
from ._utils.async_utils import TaskContext, synchronize_api
from ._utils.grpc_utils import connect_channel, create_channel, retry_transient_errors
from .config import _check_config, _is_remote, config, logger
from .exception import AuthError, ClientClosed, ConnectionError

HEARTBEAT_INTERVAL: float = config.get("heartbeat_interval")
HEARTBEAT_TIMEOUT: float = HEARTBEAT_INTERVAL + 0.1


def _get_metadata(client_type: int, credentials: Optional[tuple[str, str]], version: str) -> dict[str, str]:
    # This implements a simplified version of platform.platform() that's still machine-readable
    uname: platform.uname_result = platform.uname()
    if uname.system == "Darwin":
        system, release = "macOS", platform.mac_ver()[0]
    else:
        system, release = uname.system, uname.release
    platform_str = "-".join(s.replace("-", "_") for s in (system, release, uname.machine))

    metadata = {
        "x-modal-client-version": version,
        "x-modal-client-type": str(client_type),
        "x-modal-python-version": platform.python_version(),
        "x-modal-node": platform.node(),
        "x-modal-platform": platform_str,
    }
    if credentials and client_type == api_pb2.CLIENT_TYPE_CLIENT:
        token_id, token_secret = credentials
        metadata.update(
            {
                "x-modal-token-id": token_id,
                "x-modal-token-secret": token_secret,
            }
        )
    return metadata


ReturnType = TypeVar("ReturnType")
_Value = Union[str, bytes]
_MetadataLike = Union[Mapping[str, _Value], Collection[tuple[str, _Value]]]
RequestType = TypeVar("RequestType", bound=Message)
ResponseType = TypeVar("ResponseType", bound=Message)


class _Client:
    _client_from_env: ClassVar[Optional["_Client"]] = None
    _client_from_env_lock: ClassVar[Optional[asyncio.Lock]] = None
    _cancellation_context: TaskContext
    _cancellation_context_event_loop: asyncio.AbstractEventLoop = None
    _stub: Optional[api_grpc.ModalClientStub]
    _snapshotted: bool

    def __init__(
        self,
        server_url: str,
        client_type: int,
        credentials: Optional[tuple[str, str]],
        version: str = __version__,
    ):
        """mdmd:hidden
        The Modal client object is not intended to be instantiated directly by users.
        """
        self.server_url = server_url
        self.client_type = client_type
        self._credentials = credentials
        self.version = version
        self._closed = False
        self._channel: Optional[grpclib.client.Channel] = None
        self._stub: Optional[modal_api_grpc.ModalClientModal] = None
        self._snapshotted = False
        self._owner_pid = None

    def is_closed(self) -> bool:
        return self._closed

    @property
    def stub(self) -> modal_api_grpc.ModalClientModal:
        """mdmd:hidden"""
        assert self._stub
        return self._stub

    async def _open(self):
        self._closed = False
        assert self._stub is None
        metadata = _get_metadata(self.client_type, self._credentials, self.version)
        self._channel = create_channel(self.server_url, metadata=metadata)
        try:
            await connect_channel(self._channel)
        except OSError as exc:
            raise ConnectionError(str(exc))
        self._cancellation_context = TaskContext(grace=0.5)  # allow running rpcs to finish in 0.5s when closing client
        self._cancellation_context_event_loop = asyncio.get_running_loop()
        await self._cancellation_context.__aenter__()
        self._grpclib_stub = api_grpc.ModalClientStub(self._channel)
        self._stub = modal_api_grpc.ModalClientModal(self._grpclib_stub, client=self)
        self._owner_pid = os.getpid()

    async def _close(self, prep_for_restore: bool = False):
        logger.debug(f"Client ({id(self)}): closing")
        self._closed = True
        await self._cancellation_context.__aexit__(None, None, None)  # wait for all rpcs to be finished/cancelled
        if self._channel is not None:
            self._channel.close()

        if prep_for_restore:
            self._snapshotted = True

        # Remove cached client.
        self.set_env_client(None)

    async def hello(self):
        """Connect to server and retrieve version information; raise appropriate error for various failures."""
        logger.debug(f"Client ({id(self)}): Starting")
        resp = await retry_transient_errors(self.stub.ClientHello, empty_pb2.Empty())
        print_server_warnings(resp.server_warnings)

    async def __aenter__(self):
        await self._open()
        return self

    async def __aexit__(self, exc_type, exc, tb):
        await self._close()

    @classmethod
    @asynccontextmanager
    async def anonymous(cls, server_url: str) -> AsyncIterator["_Client"]:
        """mdmd:hidden
        Create a connection with no credentials; to be used for token creation.
        """
        logger.debug("Client: Starting client without authentication")
        client = cls(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials=None)
        try:
            await client._open()
            yield client
        finally:
            await client._close()

    @classmethod
    async def from_env(cls, _override_config=None) -> "_Client":
        """mdmd:hidden
        Singleton that is instantiated from the Modal config and reused on subsequent calls.
        """
        _check_config()

        if _override_config:
            # Only used for testing
            c = _override_config
        else:
            c = config

        credentials: Optional[tuple[str, str]]

        if cls._client_from_env_lock is None:
            cls._client_from_env_lock = asyncio.Lock()

        async with cls._client_from_env_lock:
            if cls._client_from_env:
                return cls._client_from_env

            token_id = c["token_id"]
            token_secret = c["token_secret"]
            if _is_remote():
                if token_id or token_secret:
                    warnings.warn(
                        "Modal tokens provided by MODAL_TOKEN_ID and MODAL_TOKEN_SECRET"
                        " (or through the config file) are ignored inside containers."
                    )
                client_type = api_pb2.CLIENT_TYPE_CONTAINER
                credentials = None
            elif token_id and token_secret:
                client_type = api_pb2.CLIENT_TYPE_CLIENT
                credentials = (token_id, token_secret)
            else:
                raise AuthError(
                    "Token missing. Could not authenticate client."
                    " If you have token credentials, see modal.com/docs/reference/modal.config for setup help."
                    " If you are a new user, register an account at modal.com, then run `modal token new`."
                )

            server_url = c["server_url"]
            client = _Client(server_url, client_type, credentials)
            await client._open()
            async_utils.on_shutdown(client._close())
            cls._client_from_env = client
            return client

    @classmethod
    async def from_credentials(cls, token_id: str, token_secret: str) -> "_Client":
        """
        Constructor based on token credentials; useful for managing Modal on behalf of third-party users.

        **Usage:**

        ```python notest
        client = modal.Client.from_credentials("my_token_id", "my_token_secret")

        modal.Sandbox.create("echo", "hi", client=client, app=app)
        ```
        """
        _check_config()
        server_url = config["server_url"]
        client_type = api_pb2.CLIENT_TYPE_CLIENT
        credentials = (token_id, token_secret)
        client = _Client(server_url, client_type, credentials)
        await client._open()
        async_utils.on_shutdown(client._close())
        return client

    @classmethod
    async def verify(cls, server_url: str, credentials: tuple[str, str]) -> None:
        """mdmd:hidden
        Check whether can the client can connect to this server with these credentials; raise if not.
        """
        async with cls(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
            await client.hello()  # Will call ClientHello RPC and possibly raise AuthError or ConnectionError

    @classmethod
    def set_env_client(cls, client: Optional["_Client"]):
        """mdmd:hidden"""
        # Just used from tests.
        cls._client_from_env = client

    async def _call_safely(self, coro, readable_method: str):
        """Runs coroutine wrapped in a task that's part of the client's task context

        * Raises ClientClosed in case the client is closed while the coroutine is executed
        * Logs warning if call is made outside of the event loop that the client is running in,
          and execute without the cancellation context in that case
        """

        if self.is_closed():
            coro.close()  # prevent "was never awaited"
            raise ClientClosed(id(self))

        current_event_loop = asyncio.get_running_loop()
        if current_event_loop == self._cancellation_context_event_loop:
            # make request cancellable if we are in the same event loop as the rpc context
            # this should usually be the case!
            try:
                request_task = self._cancellation_context.create_task(coro)
                request_task.set_name(readable_method)
                return await request_task
            except asyncio.CancelledError:
                if self.is_closed():
                    raise ClientClosed(id(self)) from None
                raise  # if the task is cancelled as part of synchronizer shutdown or similar, don't raise ClientClosed
        else:
            # this should be rare - mostly used in tests where rpc requests sometimes are triggered
            # outside of a client context/synchronicity loop
            logger.warning(f"RPC request to {readable_method} made outside of task context")
            return await coro

    async def _reset_on_pid_change(self):
        if self._owner_pid and self._owner_pid != os.getpid():
            # not calling .close() since that would also interact with stale resources
            # just reset the internal state
            self._channel = None
            self._stub = None
            self._grpclib_stub = None
            self._owner_pid = None

            self.set_env_client(None)
            # TODO(elias): reset _cancellation_context in case ?
            await self._open()

    async def _get_grpclib_method(self, method_name: str) -> Any:
        # safely get grcplib method that is bound to a valid channel
        # This prevents usage of stale methods across forks of processes
        await self._reset_on_pid_change()
        return getattr(self._grpclib_stub, method_name)

    @synchronizer.nowrap
    async def _call_unary(
        self,
        method_name: str,
        request: Any,
        *,
        timeout: Optional[float] = None,
        metadata: Optional[_MetadataLike] = None,
    ) -> Any:
        grpclib_method = await self._get_grpclib_method(method_name)
        coro = grpclib_method(request, timeout=timeout, metadata=metadata)
        return await self._call_safely(coro, grpclib_method.name)

    @synchronizer.nowrap
    async def _call_stream(
        self,
        method_name: str,
        request: Any,
        *,
        metadata: Optional[_MetadataLike],
    ) -> AsyncGenerator[Any, None]:
        grpclib_method = await self._get_grpclib_method(method_name)
        stream_context = grpclib_method.open(metadata=metadata)
        stream = await self._call_safely(stream_context.__aenter__(), f"{grpclib_method.name}.open")
        try:
            await self._call_safely(stream.send_message(request, end=True), f"{grpclib_method.name}.send_message")
            while 1:
                try:
                    yield await self._call_safely(stream.__anext__(), f"{grpclib_method.name}.recv")
                except StopAsyncIteration:
                    break
        except BaseException as exc:
            did_handle_exception = await stream_context.__aexit__(type(exc), exc, exc.__traceback__)
            if not did_handle_exception:
                raise
        else:
            await stream_context.__aexit__(None, None, None)


Client = synchronize_api(_Client)


class UnaryUnaryWrapper(Generic[RequestType, ResponseType]):
    # Calls a grpclib.UnaryUnaryMethod using a specific Client instance, respecting
    # if that client is closed etc. and possibly introducing Modal-specific retry logic
    wrapped_method: grpclib.client.UnaryUnaryMethod[RequestType, ResponseType]
    client: _Client

    def __init__(self, wrapped_method: grpclib.client.UnaryUnaryMethod[RequestType, ResponseType], client: _Client):
        # we pass in the wrapped_method here to get the correct static types
        # but don't use the reference directly, see `def wrapped_method` below
        self._wrapped_full_name = wrapped_method.name
        self._wrapped_method_name = wrapped_method.name.rsplit("/", 1)[1]
        self.client = client

    @property
    def name(self) -> str:
        return self._wrapped_full_name

    async def __call__(
        self,
        req: RequestType,
        *,
        timeout: Optional[float] = None,
        metadata: Optional[_MetadataLike] = None,
    ) -> ResponseType:
        if self.client._snapshotted:
            logger.debug(f"refreshing client after snapshot for {self._wrapped_method_name}")
            self.client = await _Client.from_env()
        return await self.client._call_unary(self._wrapped_method_name, req, timeout=timeout, metadata=metadata)


class UnaryStreamWrapper(Generic[RequestType, ResponseType]):
    wrapped_method: grpclib.client.UnaryStreamMethod[RequestType, ResponseType]

    def __init__(self, wrapped_method: grpclib.client.UnaryStreamMethod[RequestType, ResponseType], client: _Client):
        self._wrapped_full_name = wrapped_method.name
        self._wrapped_method_name = wrapped_method.name.rsplit("/", 1)[1]
        self.client = client

    @property
    def name(self) -> str:
        return self._wrapped_full_name

    async def unary_stream(
        self,
        request,
        metadata: Optional[Any] = None,
    ):
        if self.client._snapshotted:
            logger.debug(f"refreshing client after snapshot for {self._wrapped_method_name}")
            self.client = await _Client.from_env()
        async for response in self.client._call_stream(self._wrapped_method_name, request, metadata=metadata):
            yield response


================================================
File: modal/cloud_bucket_mount.py
================================================
# Copyright Modal Labs 2022
from dataclasses import dataclass
from typing import Optional
from urllib.parse import urlparse

from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from .config import logger
from .secret import _Secret


@dataclass
class _CloudBucketMount:
    """Mounts a cloud bucket to your container. Currently supports AWS S3 buckets.

    S3 buckets are mounted using [AWS S3 Mountpoint](https://github.com/awslabs/mountpoint-s3).
    S3 mounts are optimized for reading large files sequentially. It does not support every file operation; consult
    [the AWS S3 Mountpoint documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md)
    for more information.

    **AWS S3 Usage**

    ```python
    import subprocess

    app = modal.App()
    secret = modal.Secret.from_name(
        "aws-secret",
        required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
        # Note: providing AWS_REGION can help when automatic detection of the bucket region fails.
    )

    @app.function(
        volumes={
            "/my-mount": modal.CloudBucketMount(
                bucket_name="s3-bucket-name",
                secret=secret,
                read_only=True
            )
        }
    )
    def f():
        subprocess.run(["ls", "/my-mount"], check=True)
    ```

    **Cloudflare R2 Usage**

    Cloudflare R2 is [S3-compatible](https://developers.cloudflare.com/r2/api/s3/api/) so its setup looks
    very similar to S3. But additionally the `bucket_endpoint_url` argument must be passed.

    ```python
    import subprocess

    app = modal.App()
    secret = modal.Secret.from_name(
        "r2-secret",
        required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
    )

    @app.function(
        volumes={
            "/my-mount": modal.CloudBucketMount(
                bucket_name="my-r2-bucket",
                bucket_endpoint_url="https://<ACCOUNT ID>.r2.cloudflarestorage.com",
                secret=secret,
                read_only=True
            )
        }
    )
    def f():
        subprocess.run(["ls", "/my-mount"], check=True)
    ```

    **Google GCS Usage**

    Google Cloud Storage (GCS) is [S3-compatible](https://cloud.google.com/storage/docs/interoperability).
    GCS Buckets also require a secret with Google-specific key names (see below) populated with
    a [HMAC key](https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create).

    ```python
    import subprocess

    app = modal.App()
    gcp_hmac_secret = modal.Secret.from_name(
        "gcp-secret",
        required_keys=["GOOGLE_ACCESS_KEY_ID", "GOOGLE_ACCESS_KEY_SECRET"]
    )

    @app.function(
        volumes={
            "/my-mount": modal.CloudBucketMount(
                bucket_name="my-gcs-bucket",
                bucket_endpoint_url="https://storage.googleapis.com",
                secret=gcp_hmac_secret,
            )
        }
    )
    def f():
        subprocess.run(["ls", "/my-mount"], check=True)
    ```
    """

    bucket_name: str
    # Endpoint URL is used to support Cloudflare R2 and Google Cloud Platform GCS.
    bucket_endpoint_url: Optional[str] = None

    key_prefix: Optional[str] = None

    # Credentials used to access a cloud bucket.
    # If the bucket is private, the secret **must** contain AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY.
    # If the bucket is publicly accessible, the secret is unnecessary and can be omitted.
    secret: Optional[_Secret] = None

    # Role ARN used for using OIDC authentication to access a cloud bucket.
    oidc_auth_role_arn: Optional[str] = None

    read_only: bool = False
    requester_pays: bool = False


def cloud_bucket_mounts_to_proto(mounts: list[tuple[str, _CloudBucketMount]]) -> list[api_pb2.CloudBucketMount]:
    """Helper function to convert `CloudBucketMount` to a list of protobufs that can be passed to the server."""
    cloud_bucket_mounts: list[api_pb2.CloudBucketMount] = []

    for path, mount in mounts:
        # crude mapping from mount arguments to type.
        if mount.bucket_endpoint_url:
            parse_result = urlparse(mount.bucket_endpoint_url)
            if parse_result.hostname.endswith("r2.cloudflarestorage.com"):
                bucket_type = api_pb2.CloudBucketMount.BucketType.R2
            elif parse_result.hostname.endswith("storage.googleapis.com"):
                bucket_type = api_pb2.CloudBucketMount.BucketType.GCP
            else:
                logger.warning(
                    "CloudBucketMount received unrecognized bucket endpoint URL. "
                    "Assuming AWS S3 configuration as fallback."
                )
                bucket_type = api_pb2.CloudBucketMount.BucketType.S3
        else:
            # just assume S3; this is backwards and forwards compatible.
            bucket_type = api_pb2.CloudBucketMount.BucketType.S3

        if mount.requester_pays and not mount.secret:
            raise ValueError("Credentials required in order to use Requester Pays.")

        if mount.key_prefix and not mount.key_prefix.endswith("/"):
            raise ValueError("key_prefix will be prefixed to all object paths, so it must end in a '/'")
        else:
            key_prefix = mount.key_prefix

        cloud_bucket_mount = api_pb2.CloudBucketMount(
            bucket_name=mount.bucket_name,
            bucket_endpoint_url=mount.bucket_endpoint_url,
            mount_path=path,
            credentials_secret_id=mount.secret.object_id if mount.secret else "",
            read_only=mount.read_only,
            bucket_type=bucket_type,
            requester_pays=mount.requester_pays,
            key_prefix=key_prefix,
            oidc_auth_role_arn=mount.oidc_auth_role_arn,
        )
        cloud_bucket_mounts.append(cloud_bucket_mount)

    return cloud_bucket_mounts


CloudBucketMount = synchronize_api(_CloudBucketMount)


================================================
File: modal/cls.py
================================================
# Copyright Modal Labs 2022
import dataclasses
import inspect
import os
import typing
from collections.abc import Collection
from typing import Any, Callable, Optional, TypeVar, Union

from google.protobuf.message import Message
from grpclib import GRPCError, Status

from modal._utils.function_utils import CLASS_PARAM_TYPE_MAP, FunctionInfo
from modal_proto import api_pb2

from ._functions import _Function, _parse_retries
from ._object import _Object
from ._partial_function import (
    _find_callables_for_obj,
    _find_partial_methods_for_user_cls,
    _PartialFunction,
    _PartialFunctionFlags,
)
from ._resolver import Resolver
from ._resources import convert_fn_config_to_resources_config
from ._serialization import check_valid_cls_constructor_arg
from ._traceback import print_server_warnings
from ._utils.async_utils import synchronize_api, synchronizer
from ._utils.deprecation import deprecation_warning, renamed_parameter, warn_on_renamed_autoscaler_settings
from ._utils.grpc_utils import retry_transient_errors
from ._utils.mount_utils import validate_volumes
from .client import _Client
from .config import config
from .exception import ExecutionError, InvalidError, NotFoundError
from .gpu import GPU_T
from .retries import Retries
from .secret import _Secret
from .volume import _Volume

T = TypeVar("T")


if typing.TYPE_CHECKING:
    import modal.app


def _use_annotation_parameters(user_cls: type) -> bool:
    has_parameters = any(is_parameter(cls_member) for cls_member in user_cls.__dict__.values())
    has_explicit_constructor = user_cls.__init__ != object.__init__
    return has_parameters and not has_explicit_constructor


def _get_class_constructor_signature(user_cls: type) -> inspect.Signature:
    if not _use_annotation_parameters(user_cls):
        return inspect.signature(user_cls)
    else:
        constructor_parameters = []
        for name, annotation_value in user_cls.__dict__.get("__annotations__", {}).items():
            if hasattr(user_cls, name):
                parameter_spec = getattr(user_cls, name)
                if is_parameter(parameter_spec):
                    maybe_default = {}
                    if not isinstance(parameter_spec.default, _NO_DEFAULT):
                        maybe_default["default"] = parameter_spec.default

                    param = inspect.Parameter(
                        name=name,
                        annotation=annotation_value,
                        kind=inspect.Parameter.POSITIONAL_OR_KEYWORD,
                        **maybe_default,
                    )
                    constructor_parameters.append(param)

        return inspect.Signature(constructor_parameters)


@dataclasses.dataclass()
class _ServiceOptions:
    secrets: typing.Collection[_Secret]
    resources: Optional[api_pb2.Resources]
    retry_policy: Optional[api_pb2.FunctionRetryPolicy]
    concurrency_limit: Optional[int]
    timeout_secs: Optional[int]
    task_idle_timeout_secs: Optional[int]
    validated_volumes: typing.Sequence[tuple[str, _Volume]]
    target_concurrent_inputs: Optional[int]


def _bind_instance_method(cls: "_Cls", service_function: _Function, method_name: str):
    """Binds an "instance service function" to a specific method using metadata for that method

    This "dummy" _Function gets no unique object_id and isn't backend-backed at all, since all
    it does it forward invocations to the underlying instance_service_function with the specified method
    """
    assert service_function._obj

    def hydrate_from_instance_service_function(new_function: _Function):
        assert service_function.is_hydrated
        assert cls.is_hydrated
        # After 0.67 is minimum required version, we should be able to use method metadata directly
        # from the service_function instead (see _Cls._hydrate_metadata), but for now we use the Cls
        # since it can take the data from the cls metadata OR function metadata depending on source
        method_metadata = cls._method_metadata[method_name]
        new_function._hydrate(service_function.object_id, service_function.client, method_metadata)

    async def _load(fun: "_Function", resolver: Resolver, existing_object_id: Optional[str]):
        # there is currently no actual loading logic executed to create each method on
        # the *parametrized* instance of a class - it uses the parameter-bound service-function
        # for the instance. This load method just makes sure to set all attributes after the
        # `service_function` has been loaded (it's in the `_deps`)
        hydrate_from_instance_service_function(fun)

    def _deps():
        unhydrated_deps = []
        # without this check, the common service_function will be reloaded by all methods
        # TODO(elias): Investigate if we can fix this multi-loader in the resolver - feels like a bug?
        if not cls.is_hydrated:
            unhydrated_deps.append(cls)
        if not service_function.is_hydrated:
            unhydrated_deps.append(service_function)
        return unhydrated_deps

    rep = f"Method({cls._name}.{method_name})"

    fun = _Function._from_loader(
        _load,
        rep,
        deps=_deps,
        hydrate_lazily=True,
    )
    if service_function.is_hydrated:
        # Eager hydration (skip load) if the instance service function is already loaded
        hydrate_from_instance_service_function(fun)

    if cls._is_local():
        partial_function = cls._method_partials[method_name]
        fun._info = FunctionInfo(
            # ugly - needed for .local()  TODO (elias): Clean up!
            partial_function.raw_f,
            user_cls=cls._user_cls,
            serialized=service_function.info.is_serialized(),
        )

    fun._obj = service_function._obj
    fun._is_method = True
    fun._app = service_function._app
    fun._spec = service_function._spec
    return fun


class _Obj:
    """An instance of a `Cls`, i.e. `Cls("foo", 42)` returns an `Obj`.

    All this class does is to return `Function` objects."""

    _cls: "_Cls"  # parent
    _functions: dict[str, _Function]
    _has_entered: bool
    _user_cls_instance: Optional[Any] = None
    _args: tuple[Any, ...]
    _kwargs: dict[str, Any]

    _instance_service_function: Optional[_Function] = None  # this gets set lazily
    _options: Optional[_ServiceOptions]

    def __init__(
        self,
        cls: "_Cls",
        user_cls: Optional[type],  # this would be None in case of lookups
        options: Optional[_ServiceOptions],
        args,
        kwargs,
    ):
        for i, arg in enumerate(args):
            check_valid_cls_constructor_arg(i + 1, arg)
        for key, kwarg in kwargs.items():
            check_valid_cls_constructor_arg(key, kwarg)
        self._cls = cls

        # Used for construction local object lazily
        self._has_entered = False
        self._user_cls = user_cls

        # used for lazy construction in case of explicit constructors
        self._args = args
        self._kwargs = kwargs
        self._options = options

    def _cached_service_function(self) -> "modal.functions._Function":
        # Returns a service function for this _Obj, serving all its methods
        # In case of methods without parameters or options, this is simply proxying to the class service function
        if not self._instance_service_function:
            assert self._cls._class_service_function
            self._instance_service_function = self._cls._class_service_function._bind_parameters(
                self, self._options, self._args, self._kwargs
            )
        return self._instance_service_function

    def _get_parameter_values(self) -> dict[str, Any]:
        # binds args and kwargs according to the class constructor signature
        # (implicit by parameters or explicit)
        # can only be called where the local definition exists
        sig = _get_class_constructor_signature(self._user_cls)
        bound_vars = sig.bind(*self._args, **self._kwargs)
        bound_vars.apply_defaults()
        return bound_vars.arguments

    def _new_user_cls_instance(self):
        if not _use_annotation_parameters(self._user_cls):
            # TODO(elias): deprecate this code path eventually
            user_cls_instance = self._user_cls(*self._args, **self._kwargs)
        else:
            # ignore constructor (assumes there is no custom constructor,
            # which is guaranteed by _use_annotation_parameters)
            # set the attributes on the class corresponding to annotations
            # with = parameter() specifications
            param_values = self._get_parameter_values()
            user_cls_instance = self._user_cls.__new__(self._user_cls)  # new instance without running __init__
            user_cls_instance.__dict__.update(param_values)

        # TODO: always use Obj instances instead of making modifications to user cls
        # TODO: OR (if simpler for now) replace all the PartialFunctions on the user cls
        #   with getattr(self, method_name)

        # user cls instances are only created locally, so we have all partial functions available
        instance_methods = {}
        for method_name in _find_partial_methods_for_user_cls(self._user_cls, _PartialFunctionFlags.FUNCTION):
            instance_methods[method_name] = getattr(self, method_name)

        user_cls_instance._modal_functions = instance_methods
        return user_cls_instance

    async def keep_warm(self, warm_pool_size: int) -> None:
        """Set the warm pool size for the class containers

        Please exercise care when using this advanced feature!
        Setting and forgetting a warm pool on functions can lead to increased costs.

        Note that all Modal methods and web endpoints of a class share the same set
        of containers and the warm_pool_size affects that common container pool.

        ```python notest
        # Usage on a parametrized function.
        Model = modal.Cls.from_name("my-app", "Model")
        Model("fine-tuned-model").keep_warm(2)
        ```
        """
        await self._cached_service_function().keep_warm(warm_pool_size)

    def _cached_user_cls_instance(self):
        """Get or construct the local object

        Used for .local() calls and getting attributes of classes"""
        if not self._user_cls_instance:
            self._user_cls_instance = self._new_user_cls_instance()  # Instantiate object

        return self._user_cls_instance

    def _enter(self):
        assert self._user_cls
        if not self._has_entered:
            user_cls_instance = self._cached_user_cls_instance()
            if hasattr(user_cls_instance, "__enter__"):
                user_cls_instance.__enter__()

            for method_flag in (
                _PartialFunctionFlags.ENTER_PRE_SNAPSHOT,
                _PartialFunctionFlags.ENTER_POST_SNAPSHOT,
            ):
                for enter_method in _find_callables_for_obj(user_cls_instance, method_flag).values():
                    enter_method()

            self._has_entered = True

    @property
    def _entered(self) -> bool:
        # needed because _aenter is nowrap
        return self._has_entered

    @_entered.setter
    def _entered(self, val: bool):
        self._has_entered = val

    @synchronizer.nowrap
    async def _aenter(self):
        if not self._entered:  # use the property to get at the impl class
            user_cls_instance = self._cached_user_cls_instance()
            if hasattr(user_cls_instance, "__aenter__"):
                await user_cls_instance.__aenter__()
            elif hasattr(user_cls_instance, "__enter__"):
                user_cls_instance.__enter__()
        self._has_entered = True

    def __getattr__(self, k):
        # This is a bit messy and branchy because:
        # * Support .remote() on both hydrated (local or remote classes) or unhydrated classes (remote classes only)
        # * Support .local() on both hydrated and unhydrated classes (assuming local access to code)
        # * Support attribute access (when local cls is available)

        # The returned _Function objects need to be lazily loaded (including loading the Cls and/or service function)
        # since we can't assume the class is already loaded when this gets called, e.g.
        # CLs.from_name(...)().my_func.remote().

        def _get_maybe_method() -> Optional["_Function"]:
            """Gets _Function object for method - either for a local or a hydrated remote class

            * If class is neither local or hydrated - raise exception (should never happen)
            * If attribute isn't a method - return None
            """
            if self._cls._is_local():
                if k not in self._cls._method_partials:
                    return None
            elif self._cls.is_hydrated:
                if k not in self._cls._method_metadata:
                    return None
            else:
                raise ExecutionError(
                    "Class is neither hydrated or local - this is probably a bug in the Modal client. Contact support"
                )

            return _bind_instance_method(self._cls, self._cached_service_function(), k)

        if self._cls.is_hydrated or self._cls._is_local():
            # Class is hydrated or local so we know which methods exist
            if maybe_method := _get_maybe_method():
                return maybe_method
            elif self._cls._is_local():
                # We have the local definition, and the attribute isn't a method
                # so we instantiate if we don't have an instance, and try to get the attribute
                user_cls_instance = self._cached_user_cls_instance()
                return getattr(user_cls_instance, k)
            else:
                # This is the case for a *hydrated* class without the local definition, i.e. a lookup
                # where the attribute isn't a registered method of the class
                raise NotFoundError(
                    f"Class has no method `{k}` and attributes (or undecorated methods) can't be accessed for"
                    f" remote classes (`Cls.from_name` instances)"
                )

        # Not hydrated Cls, and we don't have the class - typically a Cls.from_name that
        # has not yet been loaded. So use a special loader that loads it lazily:
        async def method_loader(fun, resolver: Resolver, existing_object_id):
            await resolver.load(self._cls)  # load class so we get info about methods
            method_function = _get_maybe_method()
            if method_function is None:
                raise NotFoundError(
                    f"Class has no method {k}, and attributes can't be accessed for `Cls.from_name` instances"
                )
            await resolver.load(method_function)  # get the appropriate method handle (lazy)
            fun._hydrate_from_other(method_function)

        # The reason we don't *always* use this lazy loader is because it precludes attribute access
        # on local classes.
        return _Function._from_loader(
            method_loader,
            rep=f"Method({self._cls._name}.{k})",
            deps=lambda: [],  # TODO: use cls as dep instead of loading inside method_loader?
            hydrate_lazily=True,
        )


Obj = synchronize_api(_Obj)


class _Cls(_Object, type_prefix="cs"):
    """
    Cls adds method pooling and [lifecycle hook](/docs/guide/lifecycle-functions) behavior
    to [modal.Function](/docs/reference/modal.Function).

    Generally, you will not construct a Cls directly.
    Instead, use the [`@app.cls()`](/docs/reference/modal.App#cls) decorator on the App object.
    """

    _class_service_function: Optional[_Function]  # The _Function (read "service") serving *all* methods of the class
    _options: Optional[_ServiceOptions]

    _app: Optional["modal.app._App"] = None  # not set for lookups
    _name: Optional[str]
    # Only set for hydrated classes:
    _method_metadata: Optional[dict[str, api_pb2.FunctionHandleMetadata]] = None

    # These are only set where source is locally available:
    # TODO: wrap these in a single optional/property for consistency
    _user_cls: Optional[type] = None
    _method_partials: Optional[dict[str, _PartialFunction]] = None
    _callables: dict[str, Callable[..., Any]]

    def _initialize_from_empty(self):
        self._user_cls = None
        self._class_service_function = None
        self._options = None
        self._callables = {}
        self._name = None

    def _initialize_from_other(self, other: "_Cls"):
        super()._initialize_from_other(other)
        self._user_cls = other._user_cls
        self._class_service_function = other._class_service_function
        self._method_partials = other._method_partials
        self._options = other._options
        self._callables = other._callables
        self._name = other._name
        self._method_metadata = other._method_metadata

    def _get_partial_functions(self) -> dict[str, _PartialFunction]:
        if not self._user_cls:
            raise AttributeError("You can only get the partial functions of a local Cls instance")
        return _find_partial_methods_for_user_cls(self._user_cls, _PartialFunctionFlags.all())

    def _get_app(self) -> "modal.app._App":
        assert self._app is not None
        return self._app

    def _get_user_cls(self) -> type:
        assert self._user_cls is not None
        return self._user_cls

    def _get_name(self) -> str:
        assert self._name is not None
        return self._name

    def _get_class_service_function(self) -> _Function:
        assert self._class_service_function is not None
        return self._class_service_function

    def _get_method_names(self) -> Collection[str]:
        # returns method names for a *local* class only for now (used by cli)
        return self._method_partials.keys()

    def _hydrate_metadata(self, metadata: Message):
        assert isinstance(metadata, api_pb2.ClassHandleMetadata)
        class_service_function = self._get_class_service_function()
        assert class_service_function.is_hydrated

        if class_service_function._method_handle_metadata and len(class_service_function._method_handle_metadata):
            # If we have the metadata on the class service function
            # This should be the case for any loaded class (remote or local) as of v0.67
            method_metadata = class_service_function._method_handle_metadata
        else:
            # Method metadata stored on the backend Cls object - pre 0.67 lookups
            # Can be removed when v0.67 is least supported version (all metadata is on the function)
            method_metadata = {}
            for method in metadata.methods:
                method_metadata[method.function_name] = method.function_handle_metadata
        self._method_metadata = method_metadata

    @staticmethod
    def validate_construction_mechanism(user_cls):
        """mdmd:hidden"""
        params = {k: v for k, v in user_cls.__dict__.items() if is_parameter(v)}
        has_custom_constructor = user_cls.__init__ != object.__init__
        if params and has_custom_constructor:
            raise InvalidError(
                "A class can't have both a custom __init__ constructor "
                "and dataclass-style modal.parameter() annotations"
            )

        annotations = user_cls.__dict__.get("__annotations__", {})  # compatible with older pythons
        missing_annotations = params.keys() - annotations.keys()
        if missing_annotations:
            raise InvalidError("All modal.parameter() specifications need to be type annotated")

        annotated_params = {k: t for k, t in annotations.items() if k in params}
        for k, t in annotated_params.items():
            if t not in CLASS_PARAM_TYPE_MAP:
                t_name = getattr(t, "__name__", repr(t))
                supported = ", ".join(t.__name__ for t in CLASS_PARAM_TYPE_MAP.keys())
                raise InvalidError(
                    f"{user_cls.__name__}.{k}: {t_name} is not a supported parameter type. Use one of: {supported}"
                )

    @staticmethod
    def from_local(user_cls, app: "modal.app._App", class_service_function: _Function) -> "_Cls":
        """mdmd:hidden"""
        # validate signature
        _Cls.validate_construction_mechanism(user_cls)

        method_partials: dict[str, _PartialFunction] = _find_partial_methods_for_user_cls(
            user_cls, _PartialFunctionFlags.FUNCTION
        )

        for method_name, partial_function in method_partials.items():
            if partial_function.webhook_config is not None:
                full_name = f"{user_cls.__name__}.{method_name}"
                app._web_endpoints.append(full_name)
            partial_function.wrapped = True

        # Disable the warning that lifecycle methods are not wrapped
        for partial_function in _find_partial_methods_for_user_cls(user_cls, ~_PartialFunctionFlags.FUNCTION).values():
            partial_function.wrapped = True

        # Get all callables
        callables: dict[str, Callable] = {
            k: pf.raw_f for k, pf in _find_partial_methods_for_user_cls(user_cls, _PartialFunctionFlags.all()).items()
        }

        def _deps() -> list[_Function]:
            return [class_service_function]

        async def _load(self: "_Cls", resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.ClassCreateRequest(
                app_id=resolver.app_id, existing_class_id=existing_object_id, only_class_function=True
            )
            resp = await resolver.client.stub.ClassCreate(req)
            self._hydrate(resp.class_id, resolver.client, resp.handle_metadata)

        rep = f"Cls({user_cls.__name__})"
        cls: _Cls = _Cls._from_loader(_load, rep, deps=_deps)
        cls._app = app
        cls._user_cls = user_cls
        cls._class_service_function = class_service_function
        cls._method_partials = method_partials
        cls._callables = callables
        cls._name = user_cls.__name__
        return cls

    @classmethod
    @renamed_parameter((2024, 12, 18), "tag", "name")
    def from_name(
        cls: type["_Cls"],
        app_name: str,
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
        workspace: Optional[str] = None,  # Deprecated and unused
    ) -> "_Cls":
        """Reference a Cls from a deployed App by its name.

        In contrast to `modal.Cls.lookup`, this is a lazy method
        that defers hydrating the local object with metadata from
        Modal servers until the first time it is actually used.

        ```python
        Model = modal.Cls.from_name("other-app", "Model")
        ```
        """
        _environment_name = environment_name or config.get("environment")

        if workspace is not None:
            deprecation_warning(
                (2025, 1, 27), "The `workspace` argument is no longer used and will be removed in a future release."
            )

        async def _load_remote(self: _Cls, resolver: Resolver, existing_object_id: Optional[str]):
            request = api_pb2.ClassGetRequest(
                app_name=app_name,
                object_tag=name,
                namespace=namespace,
                environment_name=_environment_name,
                lookup_published=workspace is not None,
                only_class_function=True,
            )
            try:
                response = await retry_transient_errors(resolver.client.stub.ClassGet, request)
            except GRPCError as exc:
                if exc.status == Status.NOT_FOUND:
                    raise NotFoundError(exc.message)
                elif exc.status == Status.FAILED_PRECONDITION:
                    raise InvalidError(exc.message)
                else:
                    raise

            print_server_warnings(response.server_warnings)
            await resolver.load(self._class_service_function)
            self._hydrate(response.class_id, resolver.client, response.handle_metadata)

        rep = f"Ref({app_name})"
        cls = cls._from_loader(_load_remote, rep, is_another_app=True, hydrate_lazily=True)

        class_service_name = f"{name}.*"  # special name of the base service function for the class
        cls._class_service_function = _Function._from_name(
            app_name,
            class_service_name,
            namespace=namespace,
            environment_name=_environment_name,
        )
        cls._name = name
        return cls

    @warn_on_renamed_autoscaler_settings
    def with_options(
        self: "_Cls",
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        memory: Optional[Union[int, tuple[int, int]]] = None,
        gpu: GPU_T = None,
        secrets: Collection[_Secret] = (),
        volumes: dict[Union[str, os.PathLike], _Volume] = {},
        retries: Optional[Union[int, Retries]] = None,
        max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
        scaledown_window: Optional[int] = None,  # Max amount of time a container can remain idle before scaling down.
        timeout: Optional[int] = None,
        allow_concurrent_inputs: Optional[int] = None,
        # The following parameters are deprecated
        concurrency_limit: Optional[int] = None,  # Now called `max_containers`
        container_idle_timeout: Optional[int] = None,  # Now called `scaledown_window`
    ) -> "_Cls":
        """
        **Beta:** Allows for the runtime modification of a modal.Cls's configuration.

        This is a beta feature and may be unstable.

        **Usage:**

        ```python notest
        Model = modal.Cls.from_name("my_app", "Model")
        ModelUsingGPU = Model.with_options(gpu="A100")
        ModelUsingGPU().generate.remote(42)  # will run with an A100 GPU
        ```
        """
        retry_policy = _parse_retries(retries, f"Class {self.__name__}" if self._user_cls else "")
        if gpu or cpu or memory:
            resources = convert_fn_config_to_resources_config(cpu=cpu, memory=memory, gpu=gpu, ephemeral_disk=None)
        else:
            resources = None

        async def _load_from_base(new_cls, resolver, existing_object_id):
            # this is a bit confusing, the cls will always have the same metadata
            # since it has the same *class* service function (i.e. "template")
            # But the (instance) service function for each Obj will be different
            # since it will rebind to whatever `_options` have been assigned on
            # the particular Cls parent
            if not self.is_hydrated:
                # this should only happen for Cls.from_name instances
                # other classes should already be hydrated!
                await resolver.load(self)

            new_cls._initialize_from_other(self)

        def _deps():
            return []

        cls = _Cls._from_loader(_load_from_base, rep=f"{self._name}.with_options(...)", is_another_app=True, deps=_deps)
        cls._initialize_from_other(self)
        cls._options = _ServiceOptions(
            secrets=secrets,
            resources=resources,
            retry_policy=retry_policy,
            # TODO(michael) Update the protos to use the new terminology
            concurrency_limit=max_containers,
            task_idle_timeout_secs=scaledown_window,
            timeout_secs=timeout,
            validated_volumes=validate_volumes(volumes),
            target_concurrent_inputs=allow_concurrent_inputs,
        )

        return cls

    @staticmethod
    @renamed_parameter((2024, 12, 18), "tag", "name")
    async def lookup(
        app_name: str,
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        workspace: Optional[str] = None,  # Deprecated and unused
    ) -> "_Cls":
        """Lookup a Cls from a deployed App by its name.

        DEPRECATED: This method is deprecated in favor of `modal.Cls.from_name`.

        In contrast to `modal.Cls.from_name`, this is an eager method
        that will hydrate the local object with metadata from Modal servers.

        ```python notest
        Model = modal.Cls.from_name("other-app", "Model")
        model = Model()
        model.inference(...)
        ```
        """
        deprecation_warning(
            (2025, 1, 27),
            "`modal.Cls.lookup` is deprecated and will be removed in a future release."
            " It can be replaced with `modal.Cls.from_name`."
            "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
        )
        obj = _Cls.from_name(
            app_name, name, namespace=namespace, environment_name=environment_name, workspace=workspace
        )
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client)
        await resolver.load(obj)
        return obj

    @synchronizer.no_input_translation
    def __call__(self, *args, **kwargs) -> _Obj:
        """This acts as the class constructor."""
        return _Obj(
            self,
            self._user_cls,
            self._options,
            args,
            kwargs,
        )

    def __getattr__(self, k):
        # TODO: remove this method - access to attributes on classes (not instances) should be discouraged
        if not self._is_local() or k in self._method_partials:
            # if not local (== k *could* be a method) or it is local and we know k is a method
            deprecation_warning(
                (2025, 1, 13),
                "Usage of methods directly on the class will soon be deprecated, "
                "instantiate classes before using methods, e.g.:\n"
                f"{self._name}().{k} instead of {self._name}.{k}",
                pending=True,
            )
            return getattr(self(), k)
        # non-method attribute access on local class - arguably shouldn't be used either:
        return getattr(self._user_cls, k)

    def _is_local(self) -> bool:
        return self._user_cls is not None


Cls = synchronize_api(_Cls)


class _NO_DEFAULT:
    def __repr__(self):
        return "modal.cls._NO_DEFAULT()"


_no_default = _NO_DEFAULT()


class _Parameter:
    default: Any
    init: bool

    def __init__(self, default: Any, init: bool):
        self.default = default
        self.init = init

    def __get__(self, obj, obj_type=None) -> Any:
        if obj:
            if self.default is _no_default:
                raise AttributeError("field has no default value and no specified value")
            return self.default
        return self


def is_parameter(p: Any) -> bool:
    return isinstance(p, _Parameter) and p.init


def parameter(*, default: Any = _no_default, init: bool = True) -> Any:
    """Used to specify options for modal.cls parameters, similar to dataclass.field for dataclasses
    ```
    class A:
        a: str = modal.parameter()

    ```

    If `init=False` is specified, the field is not considered a parameter for the
    Modal class and not used in the synthesized constructor. This can be used to
    optionally annotate the type of a field that's used internally, for example values
    being set by @enter lifecycle methods, without breaking type checkers, but it has
    no runtime effect on the class.
    """
    # has to return Any to be assignable to any annotation (https://github.com/microsoft/pyright/issues/5102)
    return _Parameter(default=default, init=init)


================================================
File: modal/config.py
================================================
# Copyright Modal Labs 2022
r"""Modal intentionally keeps configurability to a minimum.

The main configuration options are the API tokens: the token id and the token secret.
These can be configured in two ways:

1. By running the `modal token set` command.
   This writes the tokens to `.modal.toml` file in your home directory.
2. By setting the environment variables `MODAL_TOKEN_ID` and `MODAL_TOKEN_SECRET`.
   This takes precedence over the previous method.

.modal.toml
---------------

The `.modal.toml` file is generally stored in your home directory.
It should look like this::

```toml
[default]
token_id = "ak-12345..."
token_secret = "as-12345..."
```

You can create this file manually, or you can run the `modal token set ...`
command (see below).

Setting tokens using the CLI
----------------------------

You can set a token by running the command::

```
modal token set \
  --token-id <token id> \
  --token-secret <token secret>
```

This will write the token id and secret to `.modal.toml`.

If the token id or secret is provided as the string `-` (a single dash),
then it will be read in a secret way from stdin instead.

Other configuration options
---------------------------

Other possible configuration options are:

* `loglevel` (in the .toml file) / `MODAL_LOGLEVEL` (as an env var).
  Defaults to `WARNING`. Set this to `DEBUG` to see internal messages.
* `logs_timeout` (in the .toml file) / `MODAL_LOGS_TIMEOUT` (as an env var).
  Defaults to 10.
  Number of seconds to wait for logs to drain when closing the session,
  before giving up.
* `automount` (in the .toml file) / `MODAL_AUTOMOUNT` (as an env var).
  Defaults to True.
  By default, Modal automatically mounts modules imported in the current scope, that
  are deemed to be "local". This can be turned off by setting this to False.
* `force_build` (in the .toml file) / `MODAL_FORCE_BUILD` (as an env var).
  Defaults to False.
  When set, ignores the Image cache and builds all Image layers. Note that this
  will break the cache for all images based on the rebuilt layers, so other images
  may rebuild on subsequent runs / deploys even if the config is reverted.
* `ignore_cache` (in the .toml file) / `MODAL_IGNORE_CACHE` (as an env var).
  Defaults to False.
  When set, ignores the Image cache and builds all Image layers. Unlike `force_build`,
  this will not overwrite the cache for other images that have the same recipe.
  Subsequent runs that do not use this option will pull the *previous* Image from
  the cache, if one exists. It can be useful for testing an App's robustness to
  Image rebuilds without clobbering Images used by other Apps.
* `traceback` (in the .toml file) / `MODAL_TRACEBACK` (as an env var).
  Defaults to False. Enables printing full tracebacks on unexpected CLI
  errors, which can be useful for debugging client issues.

Meta-configuration
------------------

Some "meta-options" are set using environment variables only:

* `MODAL_CONFIG_PATH` lets you override the location of the .toml file,
  by default `~/.modal.toml`.
* `MODAL_PROFILE` lets you use multiple sections in the .toml file
  and switch between them. It defaults to "default".
"""

import logging
import os
import typing
import warnings
from textwrap import dedent
from typing import Any, Optional

from google.protobuf.empty_pb2 import Empty

from modal_proto import api_pb2

from ._utils.deprecation import deprecation_error
from ._utils.logger import configure_logger
from .exception import InvalidError

# Locate config file and read it

user_config_path: str = os.environ.get("MODAL_CONFIG_PATH") or os.path.expanduser("~/.modal.toml")


def _is_remote() -> bool:
    # We want to prevent read/write on a modal config file in the container
    # environment, both because that doesn't make sense and might cause weird
    # behavior, and because we want to keep the `toml` dependency out of the
    # container runtime.
    return os.environ.get("MODAL_IS_REMOTE") == "1"


def _read_user_config():
    config_data = {}
    if not _is_remote() and os.path.exists(user_config_path):
        # Defer toml import so we don't need it in the container runtime environment
        import toml

        try:
            with open(user_config_path) as f:
                config_data = toml.load(f)
        except Exception as exc:
            config_problem = str(exc)
        else:
            if not all(isinstance(e, dict) for e in config_data.values()):
                config_problem = "TOML file must contain table sections for each profile."
            else:
                config_problem = ""
        if config_problem:
            message = f"\nError when reading the modal configuration from `{user_config_path}`.\n\n{config_problem}"
            raise InvalidError(message)
    return config_data


_user_config = _read_user_config()


async def _lookup_workspace(server_url: str, token_id: str, token_secret: str) -> api_pb2.WorkspaceNameLookupResponse:
    from .client import _Client

    credentials = (token_id, token_secret)
    async with _Client(server_url, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        return await client.stub.WorkspaceNameLookup(Empty(), timeout=3)


def config_profiles():
    """List the available modal profiles in the .modal.toml file."""
    return _user_config.keys()


def _config_active_profile() -> str:
    for key, values in _user_config.items():
        if values.get("active", False) is True:
            return key
    else:
        return "default"


def config_set_active_profile(env: str) -> None:
    """Set the user's active modal profile by writing it to the `.modal.toml` file."""
    if env not in _user_config:
        raise KeyError(env)

    for key, values in _user_config.items():
        values.pop("active", None)

    _user_config[env]["active"] = True
    _write_user_config(_user_config)


def _check_config() -> None:
    num_profiles = len(_user_config)
    num_active = sum(v.get("active", False) for v in _user_config.values())
    if num_active > 1:
        raise InvalidError(
            "More than one Modal profile is active. "
            "Please fix with `modal profile activate` or by editing your Modal config file "
            f"({user_config_path})."
        )
    elif num_profiles > 1 and num_active == 0 and _profile == "default":
        # Eventually we plan to have num_profiles > 1 with num_active = 0 be an error
        # But we want to give users time to activate one of their profiles without disruption
        message = dedent(
            """
            Support for using an implicit 'default' profile is deprecated.
            Please use `modal profile activate` to activate one of your profiles.
            (Use `modal profile list` to see the options.)
            """
        )
        deprecation_error((2024, 2, 6), message)


_profile = os.environ.get("MODAL_PROFILE") or _config_active_profile()

# Define settings


def _to_boolean(x: object) -> bool:
    return str(x).lower() not in {"", "0", "false"}


class _Setting(typing.NamedTuple):
    default: typing.Any = None
    transform: typing.Callable[[str], typing.Any] = lambda x: x  # noqa: E731


_SETTINGS = {
    "loglevel": _Setting("WARNING", lambda s: s.upper()),
    "log_format": _Setting("STRING", lambda s: s.upper()),
    "server_url": _Setting("https://api.modal.com"),
    "token_id": _Setting(),
    "token_secret": _Setting(),
    "task_id": _Setting(),
    "serve_timeout": _Setting(transform=float),
    "sync_entrypoint": _Setting(),
    "logs_timeout": _Setting(10, float),
    "image_id": _Setting(),
    "automount": _Setting(True, transform=_to_boolean),
    "heartbeat_interval": _Setting(15, float),
    "function_runtime": _Setting(),
    "function_runtime_debug": _Setting(False, transform=_to_boolean),  # For internal debugging use.
    "runtime_perf_record": _Setting(False, transform=_to_boolean),  # For internal debugging use.
    "environment": _Setting(),
    "default_cloud": _Setting(None, transform=lambda x: x if x else None),
    "worker_id": _Setting(),  # For internal debugging use.
    "restore_state_path": _Setting("/__modal/restore-state.json"),
    "force_build": _Setting(False, transform=_to_boolean),
    "ignore_cache": _Setting(False, transform=_to_boolean),
    "traceback": _Setting(False, transform=_to_boolean),
    "image_builder_version": _Setting(),
    "strict_parameters": _Setting(False, transform=_to_boolean),  # For internal/experimental use
    "snapshot_debug": _Setting(False, transform=_to_boolean),
    "client_retries": _Setting(False, transform=_to_boolean),  # For internal testing.
    "cuda_checkpoint_path": _Setting("/__modal/.bin/cuda-checkpoint"),  # Used for snapshotting GPU memory.
}


class Config:
    """Singleton that holds configuration used by Modal internally."""

    def __init__(self):
        pass

    def get(self, key, profile=None, use_env=True):
        """Looks up a configuration value.

        Will check (in decreasing order of priority):
        1. Any environment variable of the form MODAL_FOO_BAR (when use_env is True)
        2. Settings in the user's .toml configuration file
        3. The default value of the setting
        """
        if profile is None:
            profile = _profile
        s = _SETTINGS[key]
        env_var_key = "MODAL_" + key.upper()
        if use_env and env_var_key in os.environ:
            return s.transform(os.environ[env_var_key])
        elif profile in _user_config and key in _user_config[profile]:
            return s.transform(_user_config[profile][key])
        else:
            return s.default

    def override_locally(self, key: str, value: str):
        # Override setting in this process by overriding environment variable for the setting
        #
        # Does NOT write back to settings file etc.
        try:
            self.get(key)
            os.environ["MODAL_" + key.upper()] = value
        except KeyError:
            # Override env vars not available in config, e.g. NVIDIA_VISIBLE_DEVICES.
            # This is used for restoring env vars from a memory snapshot.
            os.environ[key.upper()] = value

    def __getitem__(self, key):
        return self.get(key)

    def __repr__(self):
        return repr(self.to_dict())

    def to_dict(self):
        return {key: self.get(key) for key in sorted(_SETTINGS)}


config = Config()

# Logging

logger = logging.getLogger("modal-client")
configure_logger(logger, config["loglevel"], config["log_format"])

# Utils to write config


def _store_user_config(
    new_settings: dict[str, Any], profile: Optional[str] = None, active_profile: Optional[str] = None
):
    """Internal method, used by the CLI to set tokens."""
    if profile is None:
        profile = _profile
    user_config = _read_user_config()
    user_config.setdefault(profile, {}).update(**new_settings)
    if active_profile is not None:
        for prof_name, prof_config in user_config.items():
            if prof_name == active_profile:
                prof_config["active"] = True
            else:
                prof_config.pop("active", None)
    _write_user_config(user_config)


def _write_user_config(user_config):
    if _is_remote():
        raise InvalidError("Can't update config file in remote environment.")

    # Defer toml import so we don't need it in the container runtime environment
    import toml

    with open(user_config_path, "w") as f:
        toml.dump(user_config, f)


# Make sure all deprecation warnings are shown
# See https://docs.python.org/3/library/warnings.html#overriding-the-default-filter
warnings.filterwarnings(
    "default",
    category=DeprecationWarning,
    module="modal",
)


================================================
File: modal/container_process.py
================================================
# Copyright Modal Labs 2024
import asyncio
import platform
from typing import Generic, Optional, TypeVar

from modal_proto import api_pb2

from ._utils.async_utils import TaskContext, synchronize_api
from ._utils.deprecation import deprecation_error
from ._utils.grpc_utils import retry_transient_errors
from ._utils.shell_utils import stream_from_stdin, write_to_fd
from .client import _Client
from .exception import InteractiveTimeoutError, InvalidError
from .io_streams import _StreamReader, _StreamWriter
from .stream_type import StreamType

T = TypeVar("T", str, bytes)


class _ContainerProcess(Generic[T]):
    _process_id: Optional[str] = None
    _stdout: _StreamReader[T]
    _stderr: _StreamReader[T]
    _stdin: _StreamWriter
    _text: bool
    _by_line: bool
    _returncode: Optional[int] = None

    def __init__(
        self,
        process_id: str,
        client: _Client,
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        text: bool = True,
        by_line: bool = False,
    ) -> None:
        self._process_id = process_id
        self._client = client
        self._text = text
        self._by_line = by_line
        self._stdout = _StreamReader[T](
            api_pb2.FILE_DESCRIPTOR_STDOUT,
            process_id,
            "container_process",
            self._client,
            stream_type=stdout,
            text=text,
            by_line=by_line,
        )
        self._stderr = _StreamReader[T](
            api_pb2.FILE_DESCRIPTOR_STDERR,
            process_id,
            "container_process",
            self._client,
            stream_type=stderr,
            text=text,
            by_line=by_line,
        )
        self._stdin = _StreamWriter(process_id, "container_process", self._client)

    @property
    def stdout(self) -> _StreamReader[T]:
        """StreamReader for the container process's stdout stream."""
        return self._stdout

    @property
    def stderr(self) -> _StreamReader[T]:
        """StreamReader for the container process's stderr stream."""
        return self._stderr

    @property
    def stdin(self) -> _StreamWriter:
        """StreamWriter for the container process's stdin stream."""
        return self._stdin

    @property
    def returncode(self) -> int:
        if self._returncode is None:
            raise InvalidError(
                "You must call wait() before accessing the returncode. "
                "To poll for the status of a running process, use poll() instead."
            )
        return self._returncode

    async def poll(self) -> Optional[int]:
        """Check if the container process has finished running.

        Returns `None` if the process is still running, else returns the exit code.
        """
        if self._returncode is not None:
            return self._returncode

        req = api_pb2.ContainerExecWaitRequest(exec_id=self._process_id, timeout=0)
        resp: api_pb2.ContainerExecWaitResponse = await retry_transient_errors(self._client.stub.ContainerExecWait, req)

        if resp.completed:
            self._returncode = resp.exit_code
            return self._returncode

        return None

    async def wait(self) -> int:
        """Wait for the container process to finish running. Returns the exit code."""

        if self._returncode is not None:
            return self._returncode

        while True:
            req = api_pb2.ContainerExecWaitRequest(exec_id=self._process_id, timeout=50)
            resp: api_pb2.ContainerExecWaitResponse = await retry_transient_errors(
                self._client.stub.ContainerExecWait, req
            )
            if resp.completed:
                self._returncode = resp.exit_code
                return self._returncode

    async def attach(self, *, pty: Optional[bool] = None):
        if platform.system() == "Windows":
            print("interactive exec is not currently supported on Windows.")
            return

        if pty is not None:
            deprecation_error(
                (2024, 12, 9),
                "The `pty` argument to `modal.container_process.attach(pty=...)` is deprecated, "
                "as only PTY mode is supported. Please remove the argument.",
            )

        from rich.console import Console

        console = Console()

        connecting_status = console.status("Connecting...")
        connecting_status.start()
        on_connect = asyncio.Event()

        async def _write_to_fd_loop(stream: _StreamReader):
            # Don't skip empty messages so we can detect when the process has booted.
            async for chunk in stream._get_logs(skip_empty_messages=False):
                if chunk is None:
                    break

                if not on_connect.is_set():
                    connecting_status.stop()
                    on_connect.set()

                await write_to_fd(stream.file_descriptor, chunk)

        async def _handle_input(data: bytes, message_index: int):
            self.stdin.write(data)
            await self.stdin.drain()

        async with TaskContext() as tc:
            stdout_task = tc.create_task(_write_to_fd_loop(self.stdout))
            stderr_task = tc.create_task(_write_to_fd_loop(self.stderr))

            try:
                # time out if we can't connect to the server fast enough
                await asyncio.wait_for(on_connect.wait(), timeout=60)

                async with stream_from_stdin(_handle_input, use_raw_terminal=True):
                    await stdout_task
                    await stderr_task

                # TODO: this doesn't work right now.
                # if exit_status != 0:
                #     raise ExecutionError(f"Process exited with status code {exit_status}")

            except (asyncio.TimeoutError, TimeoutError):
                connecting_status.stop()
                stdout_task.cancel()
                stderr_task.cancel()
                raise InteractiveTimeoutError("Failed to establish connection to container. Please try again.")


ContainerProcess = synchronize_api(_ContainerProcess)


================================================
File: modal/dict.py
================================================
# Copyright Modal Labs 2022
from collections.abc import AsyncIterator
from typing import Any, Optional

from grpclib import GRPCError
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method, live_method_gen
from ._resolver import Resolver
from ._serialization import deserialize, serialize
from ._utils.async_utils import TaskContext, synchronize_api
from ._utils.deprecation import deprecation_warning, renamed_parameter
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from .client import _Client
from .config import logger
from .exception import RequestSizeError


def _serialize_dict(data):
    return [api_pb2.DictEntry(key=serialize(k), value=serialize(v)) for k, v in data.items()]


class _Dict(_Object, type_prefix="di"):
    """Distributed dictionary for storage in Modal apps.

    Keys and values can be essentially any object, so long as they can be serialized by
    `cloudpickle`, which includes other Modal objects.

    **Lifetime of a Dict and its items**

    An individual dict entry will expire 30 days after it was last added to its Dict object.
    Additionally, data are stored in memory on the Modal server and could be lost due to
    unexpected server restarts. Because of this, `Dict` is best suited for storing short-term
    state and is not recommended for durable storage.

    **Usage**

    ```python
    from modal import Dict

    my_dict = Dict.from_name("my-persisted_dict", create_if_missing=True)

    my_dict["some key"] = "some value"
    my_dict[123] = 456

    assert my_dict["some key"] == "some value"
    assert my_dict[123] == 456
    ```

    The `Dict` class offers a few methods for operations that are usually accomplished
    in Python with operators, such as `Dict.put` and `Dict.contains`. The advantage of
    these methods is that they can be safely called in an asynchronous context, whereas
    their operator-based analogues will block the event loop.

    For more examples, see the [guide](/docs/guide/dicts-and-queues#modal-dicts).
    """

    def __init__(self, data={}):
        """mdmd:hidden"""
        raise RuntimeError(
            "`Dict(...)` constructor is not allowed. Please use `Dict.from_name` or `Dict.ephemeral` instead"
        )

    @classmethod
    @asynccontextmanager
    async def ephemeral(
        cls: type["_Dict"],
        data: Optional[dict] = None,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
    ) -> AsyncIterator["_Dict"]:
        """Creates a new ephemeral dict within a context manager:

        Usage:
        ```python
        from modal import Dict

        with Dict.ephemeral() as d:
            d["foo"] = "bar"
        ```

        ```python notest
        async with Dict.ephemeral() as d:
            await d.put.aio("foo", "bar")
        ```
        """
        if client is None:
            client = await _Client.from_env()
        serialized = _serialize_dict(data if data is not None else {})
        request = api_pb2.DictGetOrCreateRequest(
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
            environment_name=_get_environment_name(environment_name),
            data=serialized,
        )
        response = await retry_transient_errors(client.stub.DictGetOrCreate, request, total_timeout=10.0)
        async with TaskContext() as tc:
            request = api_pb2.DictHeartbeatRequest(dict_id=response.dict_id)
            tc.infinite_loop(lambda: client.stub.DictHeartbeat(request), sleep=_heartbeat_sleep)
            yield cls._new_hydrated(response.dict_id, client, None, is_another_app=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    def from_name(
        name: str,
        data: Optional[dict] = None,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_Dict":
        """Reference a named Dict, creating if necessary.

        In contrast to `modal.Dict.lookup`, this is a lazy method
        that defers hydrating the local object with metadata from
        Modal servers until the first time it is actually used.

        ```python
        d = modal.Dict.from_name("my-dict", create_if_missing=True)
        d[123] = 456
        ```
        """
        check_object_name(name, "Dict")

        async def _load(self: _Dict, resolver: Resolver, existing_object_id: Optional[str]):
            serialized = _serialize_dict(data if data is not None else {})
            req = api_pb2.DictGetOrCreateRequest(
                deployment_name=name,
                namespace=namespace,
                environment_name=_get_environment_name(environment_name, resolver),
                object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
                data=serialized,
            )
            response = await resolver.client.stub.DictGetOrCreate(req)
            logger.debug(f"Created dict with id {response.dict_id}")
            self._hydrate(response.dict_id, resolver.client, None)

        return _Dict._from_loader(_load, "Dict()", is_another_app=True, hydrate_lazily=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def lookup(
        name: str,
        data: Optional[dict] = None,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_Dict":
        """Lookup a named Dict.

        DEPRECATED: This method is deprecated in favor of `modal.Dict.from_name`.

        In contrast to `modal.Dict.from_name`, this is an eager method
        that will hydrate the local object with metadata from Modal servers.

        ```python
        d = modal.Dict.from_name("my-dict")
        d["xyz"] = 123
        ```
        """
        deprecation_warning(
            (2025, 1, 27),
            "`modal.Dict.lookup` is deprecated and will be removed in a future release."
            " It can be replaced with `modal.Dict.from_name`."
            "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
        )
        obj = _Dict.from_name(
            name,
            data=data,
            namespace=namespace,
            environment_name=environment_name,
            create_if_missing=create_if_missing,
        )
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client)
        await resolver.load(obj)
        return obj

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def delete(
        name: str,
        *,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
    ):
        obj = await _Dict.from_name(name, environment_name=environment_name).hydrate(client)
        req = api_pb2.DictDeleteRequest(dict_id=obj.object_id)
        await retry_transient_errors(obj._client.stub.DictDelete, req)

    @live_method
    async def clear(self) -> None:
        """Remove all items from the Dict."""
        req = api_pb2.DictClearRequest(dict_id=self.object_id)
        await retry_transient_errors(self._client.stub.DictClear, req)

    @live_method
    async def get(self, key: Any, default: Optional[Any] = None) -> Any:
        """Get the value associated with a key.

        Returns `default` if key does not exist.
        """
        req = api_pb2.DictGetRequest(dict_id=self.object_id, key=serialize(key))
        resp = await retry_transient_errors(self._client.stub.DictGet, req)
        if not resp.found:
            return default
        return deserialize(resp.value, self._client)

    @live_method
    async def contains(self, key: Any) -> bool:
        """Return if a key is present."""
        req = api_pb2.DictContainsRequest(dict_id=self.object_id, key=serialize(key))
        resp = await retry_transient_errors(self._client.stub.DictContains, req)
        return resp.found

    @live_method
    async def len(self) -> int:
        """Return the length of the dictionary, including any expired keys."""
        req = api_pb2.DictLenRequest(dict_id=self.object_id)
        resp = await retry_transient_errors(self._client.stub.DictLen, req)
        return resp.len

    @live_method
    async def __getitem__(self, key: Any) -> Any:
        """Get the value associated with a key.

        Note: this function will block the event loop when called in an async context.
        """
        NOT_FOUND = object()
        value = await self.get(key, NOT_FOUND)
        if value is NOT_FOUND:
            raise KeyError(f"{key} not in dict {self.object_id}")

        return value

    @live_method
    async def update(self, **kwargs) -> None:
        """Update the dictionary with additional items."""
        serialized = _serialize_dict(kwargs)
        req = api_pb2.DictUpdateRequest(dict_id=self.object_id, updates=serialized)
        try:
            await retry_transient_errors(self._client.stub.DictUpdate, req)
        except GRPCError as exc:
            if "status = '413'" in exc.message:
                raise RequestSizeError("Dict.update request is too large") from exc
            else:
                raise exc

    @live_method
    async def put(self, key: Any, value: Any) -> None:
        """Add a specific key-value pair to the dictionary."""
        updates = {key: value}
        serialized = _serialize_dict(updates)
        req = api_pb2.DictUpdateRequest(dict_id=self.object_id, updates=serialized)
        try:
            await retry_transient_errors(self._client.stub.DictUpdate, req)
        except GRPCError as exc:
            if "status = '413'" in exc.message:
                raise RequestSizeError("Dict.put request is too large") from exc
            else:
                raise exc

    @live_method
    async def __setitem__(self, key: Any, value: Any) -> None:
        """Set a specific key-value pair to the dictionary.

        Note: this function will block the event loop when called in an async context.
        """
        return await self.put(key, value)

    @live_method
    async def pop(self, key: Any) -> Any:
        """Remove a key from the dictionary, returning the value if it exists."""
        req = api_pb2.DictPopRequest(dict_id=self.object_id, key=serialize(key))
        resp = await retry_transient_errors(self._client.stub.DictPop, req)
        if not resp.found:
            raise KeyError(f"{key} not in dict {self.object_id}")
        return deserialize(resp.value, self._client)

    @live_method
    async def __delitem__(self, key: Any) -> Any:
        """Delete a key from the dictionary.

        Note: this function will block the event loop when called in an async context.
        """
        return await self.pop(key)

    @live_method
    async def __contains__(self, key: Any) -> bool:
        """Return if a key is present.

        Note: this function will block the event loop when called in an async context.
        """
        return await self.contains(key)

    @live_method_gen
    async def keys(self) -> AsyncIterator[Any]:
        """Return an iterator over the keys in this dictionary.

        Note that (unlike with Python dicts) the return value is a simple iterator,
        and results are unordered.
        """
        req = api_pb2.DictContentsRequest(dict_id=self.object_id, keys=True)
        async for resp in self._client.stub.DictContents.unary_stream(req):
            yield deserialize(resp.key, self._client)

    @live_method_gen
    async def values(self) -> AsyncIterator[Any]:
        """Return an iterator over the values in this dictionary.

        Note that (unlike with Python dicts) the return value is a simple iterator,
        and results are unordered.
        """
        req = api_pb2.DictContentsRequest(dict_id=self.object_id, values=True)
        async for resp in self._client.stub.DictContents.unary_stream(req):
            yield deserialize(resp.value, self._client)

    @live_method_gen
    async def items(self) -> AsyncIterator[tuple[Any, Any]]:
        """Return an iterator over the (key, value) tuples in this dictionary.

        Note that (unlike with Python dicts) the return value is a simple iterator,
        and results are unordered.
        """
        req = api_pb2.DictContentsRequest(dict_id=self.object_id, keys=True, values=True)
        async for resp in self._client.stub.DictContents.unary_stream(req):
            yield (deserialize(resp.key, self._client), deserialize(resp.value, self._client))


Dict = synchronize_api(_Dict)


================================================
File: modal/environments.py
================================================
# Copyright Modal Labs 2023
from dataclasses import dataclass
from typing import Optional

from google.protobuf.empty_pb2 import Empty
from google.protobuf.message import Message
from google.protobuf.wrappers_pb2 import StringValue

from modal_proto import api_pb2

from ._object import _Object
from ._resolver import Resolver
from ._utils.async_utils import synchronize_api, synchronizer
from ._utils.deprecation import deprecation_warning, renamed_parameter
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from .client import _Client
from .config import config, logger


@dataclass(frozen=True)
class EnvironmentSettings:
    image_builder_version: str  # Ideally would be typed with ImageBuilderVersion literal
    webhook_suffix: str


class _Environment(_Object, type_prefix="en"):
    _settings: EnvironmentSettings

    def __init__(self):
        """mdmd:hidden"""
        raise RuntimeError("`Environment(...)` constructor is not allowed. Please use `Environment.from_name` instead.")

    # TODO(michael) Keeping this private for now until we decide what else should be in it
    # And what the rules should be about updates / mutability
    # @property
    # def settings(self) -> EnvironmentSettings:
    #     return self._settings

    def _hydrate_metadata(self, metadata: Message):
        # Overridden concrete implementation of base class method
        assert metadata and isinstance(metadata, api_pb2.EnvironmentMetadata)
        # TODO(michael) should probably expose the `name` from the metadata
        # as the way to discover the name of the "default" environment

        # Is there a simpler way to go Message -> Dataclass?
        self._settings = EnvironmentSettings(
            image_builder_version=metadata.settings.image_builder_version,
            webhook_suffix=metadata.settings.webhook_suffix,
        )

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    def from_name(
        name: str,
        create_if_missing: bool = False,
    ):
        if name:
            # Allow null names for the case where we want to look up the "default" environment,
            # which is defined by the server. It feels messy to have "from_name" without a name, though?
            # We're adding this mostly for internal use right now. We could consider an environment-only
            # alternate constructor, like `Environment.get_default`, rather than exposing "unnamed"
            # environments as part of public API when we make this class more useful.
            check_object_name(name, "Environment")

        async def _load(self: _Environment, resolver: Resolver, existing_object_id: Optional[str]):
            request = api_pb2.EnvironmentGetOrCreateRequest(
                deployment_name=name,
                object_creation_type=(
                    api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING
                    if create_if_missing
                    else api_pb2.OBJECT_CREATION_TYPE_UNSPECIFIED
                ),
            )
            response = await retry_transient_errors(resolver.client.stub.EnvironmentGetOrCreate, request)
            logger.debug(f"Created environment with id {response.environment_id}")
            self._hydrate(response.environment_id, resolver.client, response.metadata)

        # TODO environment name (and id?) in the repr? (We should make reprs consistently more useful)
        return _Environment._from_loader(_load, "Environment()", is_another_app=True, hydrate_lazily=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def lookup(
        name: str,
        client: Optional[_Client] = None,
        create_if_missing: bool = False,
    ):
        deprecation_warning(
            (2025, 1, 27),
            "`modal.Environment.lookup` is deprecated and will be removed in a future release."
            " It can be replaced with `modal.Environment.from_name`."
            "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
        )
        obj = _Environment.from_name(name, create_if_missing=create_if_missing)
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client)
        await resolver.load(obj)
        return obj


Environment = synchronize_api(_Environment)


# Needs to be after definition; synchronicity interferes with forward references?
ENVIRONMENT_CACHE: dict[str, _Environment] = {}


async def _get_environment_cached(name: str, client: _Client) -> _Environment:
    if name in ENVIRONMENT_CACHE:
        return ENVIRONMENT_CACHE[name]
    environment = await _Environment.from_name(name).hydrate(client)
    ENVIRONMENT_CACHE[name] = environment
    return environment


@synchronizer.create_blocking
async def delete_environment(name: str, client: Optional[_Client] = None):
    if client is None:
        client = await _Client.from_env()
    await client.stub.EnvironmentDelete(api_pb2.EnvironmentDeleteRequest(name=name))


@synchronizer.create_blocking
async def update_environment(
    current_name: str,
    *,
    new_name: Optional[str] = None,
    new_web_suffix: Optional[str] = None,
    client: Optional[_Client] = None,
):
    new_name_pb2 = None
    new_web_suffix_pb2 = None
    if new_name is not None:
        if len(new_name) < 1:
            raise ValueError("The new environment name cannot be empty")

        new_name_pb2 = StringValue(value=new_name)

    if new_web_suffix is not None:
        new_web_suffix_pb2 = StringValue(value=new_web_suffix)

    update_payload = api_pb2.EnvironmentUpdateRequest(
        current_name=current_name, name=new_name_pb2, web_suffix=new_web_suffix_pb2
    )
    if client is None:
        client = await _Client.from_env()
    await client.stub.EnvironmentUpdate(update_payload)


@synchronizer.create_blocking
async def create_environment(name: str, client: Optional[_Client] = None):
    if client is None:
        client = await _Client.from_env()
    await client.stub.EnvironmentCreate(api_pb2.EnvironmentCreateRequest(name=name))


@synchronizer.create_blocking
async def list_environments(client: Optional[_Client] = None) -> list[api_pb2.EnvironmentListItem]:
    if client is None:
        client = await _Client.from_env()
    resp = await client.stub.EnvironmentList(Empty())
    return list(resp.items)


def ensure_env(environment_name: Optional[str] = None) -> str:
    """Override config environment with environment from environment_name

    This is necessary since a cli command that runs Modal code, without explicit
    environment specification wouldn't pick up the environment specified in a
    command line flag otherwise, e.g. when doing `modal run --env=foo`
    """
    if environment_name is not None:
        config.override_locally("environment", environment_name)

    return config.get("environment")


================================================
File: modal/exception.py
================================================
# Copyright Modal Labs 2022
import random
import signal


class Error(Exception):
    """
    Base class for all Modal errors. See [`modal.exception`](/docs/reference/modal.exception) for the specialized
    error classes.

    **Usage**

    ```python notest
    import modal

    try:
        ...
    except modal.Error:
        # Catch any exception raised by Modal's systems.
        print("Responding to error...")
    ```
    """


class RemoteError(Error):
    """Raised when an error occurs on the Modal server."""


class TimeoutError(Error):
    """Base class for Modal timeouts."""


class SandboxTimeoutError(TimeoutError):
    """Raised when a Sandbox exceeds its execution duration limit and times out."""


class SandboxTerminatedError(Error):
    """Raised when a Sandbox is terminated for an internal reason."""


class FunctionTimeoutError(TimeoutError):
    """Raised when a Function exceeds its execution duration limit and times out."""


class MountUploadTimeoutError(TimeoutError):
    """Raised when a Mount upload times out."""


class VolumeUploadTimeoutError(TimeoutError):
    """Raised when a Volume upload times out."""


class InteractiveTimeoutError(TimeoutError):
    """Raised when interactive frontends time out while trying to connect to a container."""


class OutputExpiredError(TimeoutError):
    """Raised when the Output exceeds expiration and times out."""


class AuthError(Error):
    """Raised when a client has missing or invalid authentication."""


class ConnectionError(Error):
    """Raised when an issue occurs while connecting to the Modal servers."""


class InvalidError(Error):
    """Raised when user does something invalid."""


class VersionError(Error):
    """Raised when the current client version of Modal is unsupported."""


class NotFoundError(Error):
    """Raised when a requested resource was not found."""


class ExecutionError(Error):
    """Raised when something unexpected happened during runtime."""


class DeserializationError(Error):
    """Raised to provide more context when an error is encountered during deserialization."""


class SerializationError(Error):
    """Raised to provide more context when an error is encountered during serialization."""


class RequestSizeError(Error):
    """Raised when an operation produces a gRPC request that is rejected by the server for being too large."""


class DeprecationError(UserWarning):
    """UserWarning category emitted when a deprecated Modal feature or API is used."""

    # Overloading it to evade the default filter, which excludes __main__.


class PendingDeprecationError(UserWarning):
    """Soon to be deprecated feature. Only used intermittently because of multi-repo concerns."""


class ServerWarning(UserWarning):
    """Warning originating from the Modal server and re-issued in client code."""


class InternalFailure(Error):
    """
    Retriable internal error.
    """


class _CliUserExecutionError(Exception):
    """mdmd:hidden
    Private wrapper for exceptions during when importing or running stubs from the CLI.

    This intentionally does not inherit from `modal.exception.Error` because it
    is a private type that should never bubble up to users. Exceptions raised in
    the CLI at this stage will have tracebacks printed.
    """

    def __init__(self, user_source: str):
        # `user_source` should be the filepath for the user code that is the source of the exception.
        # This is used by our exception handler to show the traceback starting from that point.
        self.user_source = user_source


def _simulate_preemption_interrupt(signum, frame):
    signal.alarm(30)  # simulate a SIGKILL after 30s
    raise KeyboardInterrupt("Simulated preemption interrupt from modal-client!")


def simulate_preemption(wait_seconds: int, jitter_seconds: int = 0):
    """
    Utility for simulating a preemption interrupt after `wait_seconds` seconds.
    The first interrupt is the SIGINT signal. After 30 seconds, a second
    interrupt will trigger.

    This second interrupt simulates SIGKILL, and should not be caught.
    Optionally add between zero and `jitter_seconds` seconds of additional waiting before first interrupt.

    **Usage:**

    ```python notest
    import time
    from modal.exception import simulate_preemption

    simulate_preemption(3)

    try:
        time.sleep(4)
    except KeyboardInterrupt:
        print("got preempted") # Handle interrupt
        raise
    ```

    See https://modal.com/docs/guide/preemption for more details on preemption
    handling.
    """
    signal.signal(signal.SIGALRM, _simulate_preemption_interrupt)
    jitter = random.randrange(0, jitter_seconds) if jitter_seconds else 0
    signal.alarm(wait_seconds + jitter)


class InputCancellation(BaseException):
    """Raised when the current input is cancelled by the task

    Intentionally a BaseException instead of an Exception, so it won't get
    caught by unspecified user exception clauses that might be used for retries and
    other control flow.
    """


class ModuleNotMountable(Exception):
    pass


class ClientClosed(Error):
    pass


class FilesystemExecutionError(Error):
    """Raised when an unknown error is thrown during a container filesystem operation."""


================================================
File: modal/experimental.py
================================================
# Copyright Modal Labs 2022
from dataclasses import dataclass
from typing import Any, Callable, Optional

from modal_proto import api_pb2

from ._clustered_functions import ClusterInfo, get_cluster_info as _get_cluster_info
from ._functions import _Function
from ._object import _get_environment_name
from ._partial_function import _PartialFunction, _PartialFunctionFlags
from ._runtime.container_io_manager import _ContainerIOManager
from ._utils.async_utils import synchronizer
from .client import _Client
from .exception import InvalidError


def stop_fetching_inputs():
    """Don't fetch any more inputs from the server, after the current one.
    The container will exit gracefully after the current input is processed."""
    _ContainerIOManager.stop_fetching_inputs()


def get_local_input_concurrency():
    """Get the container's local input concurrency.
    If recently reduced to particular value, it can return a larger number than
    set due to in-progress inputs."""
    return _ContainerIOManager.get_input_concurrency()


def set_local_input_concurrency(concurrency: int):
    """Set the container's local input concurrency. Dynamic concurrency will be disabled.
    When setting to a smaller value, this method will not interrupt in-progress inputs.
    """
    _ContainerIOManager.set_input_concurrency(concurrency)


def clustered(size: int, broadcast: bool = True):
    """Provision clusters of colocated and networked containers for the Function.

    Parameters:
    size: int
        Number of containers spun up to handle each input.
    broadcast: bool = True
        If True, inputs will be sent simultaneously to each container. Otherwise,
        inputs will be sent only to the rank-0 container, which is responsible for
        delegating to the workers.
    """

    assert broadcast, "broadcast=False has not been implemented yet!"

    if size <= 0:
        raise ValueError("cluster size must be greater than 0")

    def wrapper(raw_f: Callable[..., Any]) -> _PartialFunction:
        if isinstance(raw_f, _Function):
            raw_f = raw_f.get_raw_f()
            raise InvalidError(
                f"Applying decorators for {raw_f} in the wrong order!\nUsage:\n\n"
                "@app.function()\n@modal.clustered()\ndef clustered_function():\n    ..."
            )
        return _PartialFunction(
            raw_f, _PartialFunctionFlags.FUNCTION | _PartialFunctionFlags.CLUSTERED, cluster_size=size
        )

    return wrapper


def get_cluster_info() -> ClusterInfo:
    return _get_cluster_info()


@dataclass
class AppInfo:
    app_id: str
    name: str
    containers: int


@synchronizer.create_blocking
async def list_deployed_apps(environment_name: str = "", client: Optional[_Client] = None) -> list[AppInfo]:
    """List deployed Apps along with the number of containers currently running."""
    # This function exists to provide backwards compatibility for some users who had been
    # calling into the private function that previously backed the `modal app list` CLI command.
    # We plan to add more Python API for exposing this sort of information, but we haven't
    # settled on a design we're happy with yet. In the meantime, this function will continue
    # to support existing codebases. It's likely that the final API will be different
    # (e.g. more oriented around the App object). This function should be gracefully deprecated
    # one the new API is released.
    client = client or await _Client.from_env()

    resp: api_pb2.AppListResponse = await client.stub.AppList(
        api_pb2.AppListRequest(environment_name=_get_environment_name(environment_name))
    )

    app_infos = []
    for app_stats in resp.apps:
        if app_stats.state == api_pb2.APP_STATE_DEPLOYED:
            app_infos.append(
                AppInfo(
                    app_id=app_stats.app_id,
                    name=app_stats.description,
                    containers=app_stats.n_running_tasks,
                )
            )
    return app_infos


================================================
File: modal/file_io.py
================================================
# Copyright Modal Labs 2024
import asyncio
import enum
import io
from dataclasses import dataclass
from typing import TYPE_CHECKING, AsyncIterator, Generic, Optional, Sequence, TypeVar, Union, cast

if TYPE_CHECKING:
    import _typeshed

import json

from grpclib.exceptions import GRPCError, StreamTerminatedError

from modal._utils.async_utils import TaskContext
from modal._utils.grpc_utils import retry_transient_errors
from modal.exception import ClientClosed
from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES
from .client import _Client
from .exception import FilesystemExecutionError, InvalidError

WRITE_CHUNK_SIZE = 16 * 1024 * 1024  # 16 MiB
WRITE_FILE_SIZE_LIMIT = 1024 * 1024 * 1024  # 1 GiB
READ_FILE_SIZE_LIMIT = 100 * 1024 * 1024  # 100 MiB

ERROR_MAPPING = {
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_UNSPECIFIED: FilesystemExecutionError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_PERM: PermissionError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NOENT: FileNotFoundError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_IO: IOError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NXIO: IOError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NOMEM: MemoryError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_ACCES: PermissionError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_EXIST: FileExistsError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NOTDIR: NotADirectoryError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_ISDIR: IsADirectoryError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_INVAL: OSError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_MFILE: OSError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_FBIG: OSError,
    api_pb2.SystemErrorCode.SYSTEM_ERROR_CODE_NOSPC: OSError,
}

T = TypeVar("T", str, bytes)


async def _delete_bytes(file: "_FileIO", start: Optional[int] = None, end: Optional[int] = None) -> None:
    """Delete a range of bytes from the file.

    `start` and `end` are byte offsets. `start` is inclusive, `end` is exclusive.
    If either is None, the start or end of the file is used, respectively.
    """
    assert file._file_descriptor is not None
    file._check_closed()
    if start is not None and end is not None:
        if start >= end:
            raise ValueError("start must be less than end")
    resp = await retry_transient_errors(
        file._client.stub.ContainerFilesystemExec,
        api_pb2.ContainerFilesystemExecRequest(
            file_delete_bytes_request=api_pb2.ContainerFileDeleteBytesRequest(
                file_descriptor=file._file_descriptor,
                start_inclusive=start,
                end_exclusive=end,
            ),
            task_id=file._task_id,
        ),
    )
    await file._wait(resp.exec_id)


async def _replace_bytes(file: "_FileIO", data: bytes, start: Optional[int] = None, end: Optional[int] = None) -> None:
    """Replace a range of bytes in the file with new data. The length of the data does not
    have to be the same as the length of the range being replaced.

    `start` and `end` are byte offsets. `start` is inclusive, `end` is exclusive.
    If either is None, the start or end of the file is used, respectively.
    """
    assert file._file_descriptor is not None
    file._check_closed()
    if start is not None and end is not None:
        if start >= end:
            raise InvalidError("start must be less than end")
    if len(data) > WRITE_CHUNK_SIZE:
        raise InvalidError("Write request payload exceeds 16 MiB limit")
    resp = await retry_transient_errors(
        file._client.stub.ContainerFilesystemExec,
        api_pb2.ContainerFilesystemExecRequest(
            file_write_replace_bytes_request=api_pb2.ContainerFileWriteReplaceBytesRequest(
                file_descriptor=file._file_descriptor,
                data=data,
                start_inclusive=start,
                end_exclusive=end,
            ),
            task_id=file._task_id,
        ),
    )
    await file._wait(resp.exec_id)


class FileWatchEventType(enum.Enum):
    Unknown = "Unknown"
    Access = "Access"
    Create = "Create"
    Modify = "Modify"
    Remove = "Remove"


@dataclass
class FileWatchEvent:
    paths: list[str]
    type: FileWatchEventType


# The FileIO class is designed to mimic Python's io.FileIO
# See https://github.com/python/cpython/blob/main/Lib/_pyio.py#L1459
class _FileIO(Generic[T]):
    """FileIO handle, used in the Sandbox filesystem API.

    The API is designed to mimic Python's io.FileIO.

    **Usage**

    ```python
    import modal

    app = modal.App.lookup("my-app", create_if_missing=True)

    sb = modal.Sandbox.create(app=app)
    f = sb.open("/tmp/foo.txt", "w")
    f.write("hello")
    f.close()
    ```
    """

    _binary = False
    _readable = False
    _writable = False
    _appended = False
    _closed = True

    _task_id: str = ""
    _file_descriptor: str = ""
    _client: _Client
    _watch_output_buffer: list[Optional[bytes]] = []

    def __init__(self, client: _Client, task_id: str) -> None:
        self._client = client
        self._task_id = task_id

    def _validate_mode(self, mode: str) -> None:
        if not any(char in mode for char in "rwax"):
            raise ValueError(f"Invalid file mode: {mode}")

        self._readable = "r" in mode or "+" in mode
        self._writable = "w" in mode or "a" in mode or "x" in mode or "+" in mode
        self._appended = "a" in mode
        self._binary = "b" in mode

        valid_chars = set("rwaxb+")
        if any(char not in valid_chars for char in mode):
            raise ValueError(f"Invalid file mode: {mode}")

        mode_count = sum(1 for c in mode if c in "rwax")
        if mode_count > 1:
            raise ValueError("must have exactly one of create/read/write/append mode")

        seen_chars = set()
        for char in mode:
            if char in seen_chars:
                raise ValueError(f"Invalid file mode: {mode}")
            seen_chars.add(char)

    def _handle_error(self, error: api_pb2.SystemErrorMessage) -> None:
        error_class = ERROR_MAPPING.get(error.error_code, FilesystemExecutionError)
        raise error_class(error.error_message)

    async def _consume_output(self, exec_id: str) -> AsyncIterator[Optional[bytes]]:
        req = api_pb2.ContainerFilesystemExecGetOutputRequest(
            exec_id=exec_id,
            timeout=55,
        )
        async for batch in self._client.stub.ContainerFilesystemExecGetOutput.unary_stream(req):
            if batch.eof:
                yield None
                break
            if batch.HasField("error"):
                self._handle_error(batch.error)
            for message in batch.output:
                yield message

    async def _consume_watch_output(self, exec_id: str) -> None:
        completed = False
        retries_remaining = 10
        while not completed:
            try:
                iterator = self._consume_output(exec_id)
                async for message in iterator:
                    self._watch_output_buffer.append(message)
                    if message is None:
                        completed = True
                        break

            except (GRPCError, StreamTerminatedError, ClientClosed) as exc:
                if retries_remaining > 0:
                    retries_remaining -= 1
                    if isinstance(exc, GRPCError):
                        if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                            await asyncio.sleep(1.0)
                            continue
                    elif isinstance(exc, StreamTerminatedError):
                        continue
                    elif isinstance(exc, ClientClosed):
                        # If the client was closed, the user has triggered a cleanup.
                        break
                raise exc

    async def _parse_watch_output(self, event: bytes) -> Optional[FileWatchEvent]:
        try:
            event_json = json.loads(event.decode())
            return FileWatchEvent(type=FileWatchEventType(event_json["event_type"]), paths=event_json["paths"])
        except (json.JSONDecodeError, KeyError, ValueError):
            # skip invalid events
            return None

    async def _wait(self, exec_id: str) -> bytes:
        # The logic here is similar to how output is read from `exec`
        output = b""
        completed = False
        retries_remaining = 10
        while not completed:
            try:
                async for data in self._consume_output(exec_id):
                    if data is None:
                        completed = True
                        break
                    output += data
            except (GRPCError, StreamTerminatedError) as exc:
                if retries_remaining > 0:
                    retries_remaining -= 1
                    if isinstance(exc, GRPCError):
                        if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                            await asyncio.sleep(1.0)
                            continue
                    elif isinstance(exc, StreamTerminatedError):
                        continue
                raise
        return output

    def _validate_type(self, data: Union[bytes, str]) -> None:
        if self._binary and isinstance(data, str):
            raise TypeError("Expected bytes when in binary mode")
        if not self._binary and isinstance(data, bytes):
            raise TypeError("Expected str when in text mode")

    async def _open_file(self, path: str, mode: str) -> None:
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_open_request=api_pb2.ContainerFileOpenRequest(path=path, mode=mode),
                task_id=self._task_id,
            ),
        )
        if not resp.HasField("file_descriptor"):
            raise FilesystemExecutionError("Failed to open file")
        self._file_descriptor = resp.file_descriptor
        await self._wait(resp.exec_id)

    @classmethod
    async def create(
        cls, path: str, mode: Union["_typeshed.OpenTextMode", "_typeshed.OpenBinaryMode"], client: _Client, task_id: str
    ) -> "_FileIO":
        """Create a new FileIO handle."""
        self = _FileIO(client, task_id)
        self._validate_mode(mode)
        await self._open_file(path, mode)
        self._closed = False
        return self

    async def _make_read_request(self, n: Optional[int]) -> bytes:
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_read_request=api_pb2.ContainerFileReadRequest(file_descriptor=self._file_descriptor, n=n),
                task_id=self._task_id,
            ),
        )
        return await self._wait(resp.exec_id)

    async def read(self, n: Optional[int] = None) -> T:
        """Read n bytes from the current position, or the entire remaining file if n is None."""
        self._check_closed()
        self._check_readable()
        if n is not None and n > READ_FILE_SIZE_LIMIT:
            raise ValueError("Read request payload exceeds 100 MiB limit")
        output = await self._make_read_request(n)
        if self._binary:
            return cast(T, output)
        return cast(T, output.decode("utf-8"))

    async def readline(self) -> T:
        """Read a single line from the current position."""
        self._check_closed()
        self._check_readable()
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_read_line_request=api_pb2.ContainerFileReadLineRequest(file_descriptor=self._file_descriptor),
                task_id=self._task_id,
            ),
        )
        output = await self._wait(resp.exec_id)
        if self._binary:
            return cast(T, output)
        return cast(T, output.decode("utf-8"))

    async def readlines(self) -> Sequence[T]:
        """Read all lines from the current position."""
        self._check_closed()
        self._check_readable()
        output = await self._make_read_request(None)
        if self._binary:
            lines_bytes = output.split(b"\n")
            return_bytes = [line + b"\n" for line in lines_bytes[:-1]] + ([lines_bytes[-1]] if lines_bytes[-1] else [])
            return cast(Sequence[T], return_bytes)
        else:
            lines = output.decode("utf-8").split("\n")
            return_strs = [line + "\n" for line in lines[:-1]] + ([lines[-1]] if lines[-1] else [])
            return cast(Sequence[T], return_strs)

    async def write(self, data: Union[bytes, str]) -> None:
        """Write data to the current position.

        Writes may not appear until the entire buffer is flushed, which
        can be done manually with `flush()` or automatically when the file is
        closed.
        """
        self._check_closed()
        self._check_writable()
        self._validate_type(data)
        if isinstance(data, str):
            data = data.encode("utf-8")
        if len(data) > WRITE_FILE_SIZE_LIMIT:
            raise ValueError("Write request payload exceeds 1 GiB limit")
        for i in range(0, len(data), WRITE_CHUNK_SIZE):
            chunk = data[i : i + WRITE_CHUNK_SIZE]
            resp = await retry_transient_errors(
                self._client.stub.ContainerFilesystemExec,
                api_pb2.ContainerFilesystemExecRequest(
                    file_write_request=api_pb2.ContainerFileWriteRequest(
                        file_descriptor=self._file_descriptor,
                        data=chunk,
                    ),
                    task_id=self._task_id,
                ),
            )
            await self._wait(resp.exec_id)

    async def flush(self) -> None:
        """Flush the buffer to disk."""
        self._check_closed()
        self._check_writable()
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_flush_request=api_pb2.ContainerFileFlushRequest(file_descriptor=self._file_descriptor),
                task_id=self._task_id,
            ),
        )
        await self._wait(resp.exec_id)

    def _get_whence(self, whence: int):
        if whence == 0:
            return api_pb2.SeekWhence.SEEK_SET
        elif whence == 1:
            return api_pb2.SeekWhence.SEEK_CUR
        elif whence == 2:
            return api_pb2.SeekWhence.SEEK_END
        else:
            raise ValueError(f"Invalid whence value: {whence}")

    async def seek(self, offset: int, whence: int = 0) -> None:
        """Move to a new position in the file.

        `whence` defaults to 0 (absolute file positioning); other values are 1
        (relative to the current position) and 2 (relative to the file's end).
        """
        self._check_closed()
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_seek_request=api_pb2.ContainerFileSeekRequest(
                    file_descriptor=self._file_descriptor,
                    offset=offset,
                    whence=self._get_whence(whence),
                ),
                task_id=self._task_id,
            ),
        )
        await self._wait(resp.exec_id)

    @classmethod
    async def ls(cls, path: str, client: _Client, task_id: str) -> list[str]:
        """List the contents of the provided directory."""
        self = _FileIO(client, task_id)
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_ls_request=api_pb2.ContainerFileLsRequest(path=path),
                task_id=task_id,
            ),
        )
        output = await self._wait(resp.exec_id)
        try:
            return json.loads(output.decode("utf-8"))["paths"]
        except json.JSONDecodeError:
            raise FilesystemExecutionError("failed to parse list output")

    @classmethod
    async def mkdir(cls, path: str, client: _Client, task_id: str, parents: bool = False) -> None:
        """Create a new directory."""
        self = _FileIO(client, task_id)
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_mkdir_request=api_pb2.ContainerFileMkdirRequest(path=path, make_parents=parents),
                task_id=self._task_id,
            ),
        )
        await self._wait(resp.exec_id)

    @classmethod
    async def rm(cls, path: str, client: _Client, task_id: str, recursive: bool = False) -> None:
        """Remove a file or directory in the Sandbox."""
        self = _FileIO(client, task_id)
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_rm_request=api_pb2.ContainerFileRmRequest(path=path, recursive=recursive),
                task_id=self._task_id,
            ),
        )
        await self._wait(resp.exec_id)

    @classmethod
    async def watch(
        cls,
        path: str,
        client: _Client,
        task_id: str,
        filter: Optional[list[FileWatchEventType]] = None,
        recursive: bool = False,
        timeout: Optional[int] = None,
    ) -> AsyncIterator[FileWatchEvent]:
        self = _FileIO(client, task_id)
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_watch_request=api_pb2.ContainerFileWatchRequest(
                    path=path,
                    recursive=recursive,
                    timeout_secs=timeout,
                ),
                task_id=self._task_id,
            ),
        )
        async with TaskContext() as tc:
            tc.create_task(self._consume_watch_output(resp.exec_id))

            buffer = b""
            while True:
                if len(self._watch_output_buffer) > 0:
                    item = self._watch_output_buffer.pop(0)
                    if item is None:
                        break
                    buffer += item
                    # a single event may be split across multiple messages
                    # the end of an event is marked by two newlines
                    if buffer.endswith(b"\n\n"):
                        try:
                            event_json = json.loads(buffer.strip().decode())
                            event = FileWatchEvent(
                                type=FileWatchEventType(event_json["event_type"]),
                                paths=event_json["paths"],
                            )
                            if not filter or event.type in filter:
                                yield event
                        except (json.JSONDecodeError, KeyError, ValueError):
                            # skip invalid events
                            pass
                        buffer = b""
                else:
                    await asyncio.sleep(0.1)

    async def _close(self) -> None:
        # Buffer is flushed by the runner on close
        resp = await retry_transient_errors(
            self._client.stub.ContainerFilesystemExec,
            api_pb2.ContainerFilesystemExecRequest(
                file_close_request=api_pb2.ContainerFileCloseRequest(file_descriptor=self._file_descriptor),
                task_id=self._task_id,
            ),
        )
        self._closed = True
        await self._wait(resp.exec_id)

    async def close(self) -> None:
        """Flush the buffer and close the file."""
        await self._close()

    # also validated in the runner, but checked in the client to catch errors early
    def _check_writable(self) -> None:
        if not self._writable:
            raise io.UnsupportedOperation("not writeable")

    # also validated in the runner, but checked in the client to catch errors early
    def _check_readable(self) -> None:
        if not self._readable:
            raise io.UnsupportedOperation("not readable")

    # also validated in the runner, but checked in the client to catch errors early
    def _check_closed(self) -> None:
        if self._closed:
            raise ValueError("I/O operation on closed file")

    async def __aenter__(self) -> "_FileIO":
        return self

    async def __aexit__(self, exc_type, exc_value, traceback) -> None:
        await self._close()


delete_bytes = synchronize_api(_delete_bytes)
replace_bytes = synchronize_api(_replace_bytes)
FileIO = synchronize_api(_FileIO)


================================================
File: modal/file_pattern_matcher.py
================================================
# Copyright Modal Labs 2024
"""Pattern matching library ported from https://github.com/moby/patternmatcher.

This is the same pattern-matching logic used by Docker, except it is written in
Python rather than Go. Also, the original Go library has a couple deprecated
functions that we don't implement in this port.

The main way to use this library is by constructing a `FilePatternMatcher` object,
then asking it whether file paths match any of its patterns.
"""

import os
from abc import abstractmethod
from pathlib import Path
from typing import Callable, Optional, Sequence, Union

from ._utils.pattern_utils import Pattern


class _AbstractPatternMatcher:
    _custom_repr: Optional[str] = None

    def __invert__(self) -> "_AbstractPatternMatcher":
        """Invert the filter. Returns a function that returns True if the path does not match any of the patterns.

        Usage:
        ```python
        from pathlib import Path
        from modal import FilePatternMatcher

        inverted_matcher = ~FilePatternMatcher("**/*.py")

        assert not inverted_matcher(Path("foo.py"))
        ```
        """
        return _CustomPatternMatcher(lambda path: not self(path))

    def _with_repr(self, custom_repr) -> "_AbstractPatternMatcher":
        # use to give an instance of a matcher a custom name - useful for visualizing default values in signatures
        self._custom_repr = custom_repr
        return self

    def __repr__(self) -> str:
        if self._custom_repr:
            return self._custom_repr

        return super().__repr__()

    @abstractmethod
    def __call__(self, path: Path) -> bool: ...


class _CustomPatternMatcher(_AbstractPatternMatcher):
    def __init__(self, predicate: Callable[[Path], bool]):
        self._predicate = predicate

    def __call__(self, path: Path) -> bool:
        return self._predicate(path)


class FilePatternMatcher(_AbstractPatternMatcher):
    """
    Allows matching file Path objects against a list of patterns.

    **Usage:**
    ```python
    from pathlib import Path
    from modal import FilePatternMatcher

    matcher = FilePatternMatcher("*.py")

    assert matcher(Path("foo.py"))

    # You can also negate the matcher.
    negated_matcher = ~matcher

    assert not negated_matcher(Path("foo.py"))
    ```
    """

    patterns: list[Pattern]
    _delayed_init: Callable[[], None] = None

    def _set_patterns(self, patterns: Sequence[str]) -> None:
        self.patterns = []
        for pattern in list(patterns):
            pattern = pattern.strip()
            if not pattern:
                continue
            pattern = os.path.normpath(pattern)
            new_pattern = Pattern()
            if pattern[0] == "!":
                if len(pattern) == 1:
                    raise ValueError('Illegal exclusion pattern: "!"')
                new_pattern.exclusion = True
                pattern = pattern[1:]
            # In Python, we can proceed without explicit syntax checking
            new_pattern.cleaned_pattern = pattern
            new_pattern.dirs = pattern.split(os.path.sep)
            self.patterns.append(new_pattern)

    def __init__(self, *pattern: str) -> None:
        """Initialize a new FilePatternMatcher instance.

        Args:
            pattern (str): One or more pattern strings.

        Raises:
            ValueError: If an illegal exclusion pattern is provided.
        """
        self._set_patterns(pattern)

    @classmethod
    def from_file(cls, file_path: Union[str, Path]) -> "FilePatternMatcher":
        """Initialize a new FilePatternMatcher instance from a file.

        The patterns in the file will be read lazily when the matcher is first used.

        Args:
            file_path (Path): The path to the file containing patterns.

        **Usage:**
        ```python
        from modal import FilePatternMatcher

        matcher = FilePatternMatcher.from_file("/path/to/ignorefile")
        ```

        """
        uninitialized = cls.__new__(cls)

        def _delayed_init():
            uninitialized._set_patterns(Path(file_path).read_text("utf8").splitlines())
            uninitialized._delayed_init = None

        uninitialized._delayed_init = _delayed_init
        return uninitialized

    def _matches(self, file_path: str) -> bool:
        """Check if the file path or any of its parent directories match the patterns.

        This is equivalent to `MatchesOrParentMatches()` in the original Go
        library. The reason is that `Matches()` in the original library is
        deprecated due to buggy behavior.
        """
        matched = False
        file_path = os.path.normpath(file_path)
        if file_path == ".":
            # Don't let them exclude everything; kind of silly.
            return False
        parent_path = os.path.dirname(file_path)
        if parent_path == "":
            parent_path = "."
        parent_path_dirs = parent_path.split(os.path.sep)

        for pattern in self.patterns:
            # Skip evaluation based on current match status and pattern exclusion
            if pattern.exclusion != matched:
                continue

            match = pattern.match(file_path)

            if not match and parent_path != ".":
                # Check if the pattern matches any of the parent directories
                for i in range(len(parent_path_dirs)):
                    dir_path = os.path.sep.join(parent_path_dirs[: i + 1])
                    if pattern.match(dir_path):
                        match = True
                        break

            if match:
                matched = not pattern.exclusion

        return matched

    def __call__(self, file_path: Path) -> bool:
        if self._delayed_init:
            self._delayed_init()
        return self._matches(str(file_path))


# _with_repr allows us to use this matcher as a default value in a function signature
#  and get a nice repr in the docs and auto-generated type stubs:
NON_PYTHON_FILES = (~FilePatternMatcher("**/*.py"))._with_repr(f"{__name__}.NON_PYTHON_FILES")
_NOTHING = (~FilePatternMatcher())._with_repr(f"{__name__}._NOTHING")  # match everything = ignore nothing


def _ignore_fn(ignore: Union[Sequence[str], Callable[[Path], bool]]) -> Callable[[Path], bool]:
    # if a callable is passed, return it
    # otherwise, treat input as a sequence of patterns and return a callable pattern matcher for those
    if callable(ignore):
        return ignore

    return FilePatternMatcher(*ignore)


================================================
File: modal/functions.py
================================================
# Copyright Modal Labs 2025
from ._functions import _Function, _FunctionCall, _gather
from ._utils.async_utils import synchronize_api

Function = synchronize_api(_Function, target_module=__name__)
FunctionCall = synchronize_api(_FunctionCall, target_module=__name__)
gather = synchronize_api(_gather, target_module=__name__)


================================================
File: modal/gpu.py
================================================
# Copyright Modal Labs 2022
from typing import Union

from modal_proto import api_pb2

from ._utils.deprecation import deprecation_warning
from .exception import InvalidError


class _GPUConfig:
    gpu_type: str
    count: int

    def __init__(self, gpu_type: str, count: int):
        name = self.__class__.__name__
        str_value = gpu_type
        if count > 1:
            str_value += f":{count}"
        deprecation_warning((2025, 2, 7), f'`gpu={name}(...)` is deprecated. Use `gpu="{str_value}"` instead.')
        self.gpu_type = gpu_type
        self.count = count

    def _to_proto(self) -> api_pb2.GPUConfig:
        """Convert this GPU config to an internal protobuf representation."""
        return api_pb2.GPUConfig(
            gpu_type=self.gpu_type,
            count=self.count,
        )


class T4(_GPUConfig):
    """
    [NVIDIA T4 Tensor Core](https://www.nvidia.com/en-us/data-center/tesla-t4/) GPU class.

    A low-cost data center GPU based on the Turing architecture, providing 16GB of GPU memory.
    """

    def __init__(
        self,
        count: int = 1,  # Number of GPUs per container. Defaults to 1.
    ):
        super().__init__("T4", count)

    def __repr__(self):
        return f"GPU(T4, count={self.count})"


class L4(_GPUConfig):
    """
    [NVIDIA L4 Tensor Core](https://www.nvidia.com/en-us/data-center/l4/) GPU class.

    A mid-tier data center GPU based on the Ada Lovelace architecture, providing 24GB of GPU memory.
    Includes RTX (ray tracing) support.
    """

    def __init__(
        self,
        count: int = 1,  # Number of GPUs per container. Defaults to 1.
    ):
        super().__init__("L4", count)

    def __repr__(self):
        return f"GPU(L4, count={self.count})"


class A100(_GPUConfig):
    """
    [NVIDIA A100 Tensor Core](https://www.nvidia.com/en-us/data-center/a100/) GPU class.

    The flagship data center GPU of the Ampere architecture. Available in 40GB and 80GB GPU memory configurations.
    """

    def __init__(
        self,
        *,
        count: int = 1,  # Number of GPUs per container. Defaults to 1.
        size: Union[str, None] = None,  # Select GB configuration of GPU device: "40GB" or "80GB". Defaults to "40GB".
    ):
        if size == "40GB" or not size:
            super().__init__("A100-40GB", count)
        elif size == "80GB":
            super().__init__("A100-80GB", count)
        else:
            raise ValueError(f"size='{size}' is invalid. A100s can only have memory values of 40GB or 80GB.")

    def __repr__(self):
        return f"GPU({self.gpu_type}, count={self.count})"


class A10G(_GPUConfig):
    """
    [NVIDIA A10G Tensor Core](https://www.nvidia.com/en-us/data-center/products/a10-gpu/) GPU class.

    A mid-tier data center GPU based on the Ampere architecture, providing 24 GB of memory.
    A10G GPUs deliver up to 3.3x better ML training performance, 3x better ML inference performance,
    and 3x better graphics performance, in comparison to NVIDIA T4 GPUs.
    """

    def __init__(
        self,
        *,
        # Number of GPUs per container. Defaults to 1.
        # Useful if you have very large models that don't fit on a single GPU.
        count: int = 1,
    ):
        super().__init__("A10G", count)

    def __repr__(self):
        return f"GPU(A10G, count={self.count})"


class H100(_GPUConfig):
    """
    [NVIDIA H100 Tensor Core](https://www.nvidia.com/en-us/data-center/h100/) GPU class.

    The flagship data center GPU of the Hopper architecture.
    Enhanced support for FP8 precision and a Transformer Engine that provides up to 4X faster training
    over the prior generation for GPT-3 (175B) models.
    """

    def __init__(
        self,
        *,
        # Number of GPUs per container. Defaults to 1.
        # Useful if you have very large models that don't fit on a single GPU.
        count: int = 1,
    ):
        super().__init__("H100", count)

    def __repr__(self):
        return f"GPU(H100, count={self.count})"


class L40S(_GPUConfig):
    """
    [NVIDIA L40S](https://www.nvidia.com/en-us/data-center/l40s/) GPU class.

    The L40S is a data center GPU for the Ada Lovelace architecture. It has 48 GB of on-chip
    GDDR6 RAM and enhanced support for FP8 precision.
    """

    def __init__(
        self,
        *,
        # Number of GPUs per container. Defaults to 1.
        # Useful if you have very large models that don't fit on a single GPU.
        count: int = 1,
    ):
        super().__init__("L40S", count)

    def __repr__(self):
        return f"GPU(L40S, count={self.count})"


class Any(_GPUConfig):
    """Selects any one of the GPU classes available within Modal, according to availability."""

    def __init__(self, *, count: int = 1):
        super().__init__("ANY", count)

    def __repr__(self):
        return f"GPU(Any, count={self.count})"


__doc__ = """
**GPU configuration shortcodes**

You can pass a wide range of `str` values for the `gpu` parameter of
[`@app.function`](/docs/reference/modal.App#function).

For instance:
- `gpu="H100"` will attach 1 H100 GPU to each container
- `gpu="L40S"` will attach 1 L40S GPU to each container
- `gpu="T4:4"` will attach 4 T4 GPUs to each container

You can see a list of Modal GPU options in the
[GPU docs](https://modal.com/docs/guide/gpu).

**Example**

```python
@app.function(gpu="A100-80GB:4")
def my_gpu_function():
    ... # This will have 4 A100-80GB with each container
```

**Deprecation notes**

An older deprecated way to configure GPU is also still supported,
but will be removed in future versions of Modal. Examples:

- `gpu=modal.gpu.H100()` will attach 1 H100 GPU to each container
- `gpu=modal.gpu.T4(count=4)` will attach 4 T4 GPUs to each container
- `gpu=modal.gpu.A100()` will attach 1 A100-40GB GPUs to each container
- `gpu=modal.gpu.A100(size="80GB")` will attach 1 A100-80GB GPUs to each container
"""

GPU_T = Union[None, str, _GPUConfig]


def parse_gpu_config(value: GPU_T) -> api_pb2.GPUConfig:
    if isinstance(value, _GPUConfig):
        return value._to_proto()
    elif isinstance(value, str):
        count = 1
        if ":" in value:
            value, count_str = value.split(":", 1)
            try:
                count = int(count_str)
            except ValueError:
                raise InvalidError(f"Invalid GPU count: {count_str}. Value must be an integer.")
        gpu_type = value.upper()
        return api_pb2.GPUConfig(
            gpu_type=gpu_type,
            count=count,
        )
    elif value is None:
        return api_pb2.GPUConfig()
    else:
        raise InvalidError(
            f"Invalid GPU config: {value}. Value must be a string or `None` (or a deprecated `modal.gpu` object)"
        )


================================================
File: modal/io_streams.py
================================================
# Copyright Modal Labs 2022
import asyncio
from collections.abc import AsyncGenerator, AsyncIterator
from typing import (
    TYPE_CHECKING,
    Generic,
    Literal,
    Optional,
    TypeVar,
    Union,
    cast,
)

from grpclib import Status
from grpclib.exceptions import GRPCError, StreamTerminatedError

from modal.exception import ClientClosed, InvalidError
from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from ._utils.grpc_utils import RETRYABLE_GRPC_STATUS_CODES, retry_transient_errors
from .client import _Client
from .stream_type import StreamType

if TYPE_CHECKING:
    pass


async def _sandbox_logs_iterator(
    sandbox_id: str, file_descriptor: "api_pb2.FileDescriptor.ValueType", last_entry_id: str, client: _Client
) -> AsyncGenerator[tuple[Optional[bytes], str], None]:
    req = api_pb2.SandboxGetLogsRequest(
        sandbox_id=sandbox_id,
        file_descriptor=file_descriptor,
        timeout=55,
        last_entry_id=last_entry_id,
    )
    async for log_batch in client.stub.SandboxGetLogs.unary_stream(req):
        last_entry_id = log_batch.entry_id

        for message in log_batch.items:
            yield (message.data.encode("utf-8"), last_entry_id)
        if log_batch.eof:
            yield (None, last_entry_id)
            break


async def _container_process_logs_iterator(
    process_id: str, file_descriptor: "api_pb2.FileDescriptor.ValueType", client: _Client
) -> AsyncGenerator[Optional[bytes], None]:
    req = api_pb2.ContainerExecGetOutputRequest(
        exec_id=process_id, timeout=55, file_descriptor=file_descriptor, get_raw_bytes=True
    )
    async for batch in client.stub.ContainerExecGetOutput.unary_stream(req):
        if batch.HasField("exit_code"):
            yield None
            break
        for item in batch.items:
            yield item.message_bytes


T = TypeVar("T", str, bytes)


class _StreamReader(Generic[T]):
    """Retrieve logs from a stream (`stdout` or `stderr`).

    As an asynchronous iterable, the object supports the `for` and `async for`
    statements. Just loop over the object to read in chunks.

    **Usage**

    ```python
    from modal import Sandbox

    sandbox = Sandbox.create(
        "bash",
        "-c",
        "for i in $(seq 1 10); do echo foo; sleep 0.1; done",
        app=app,
    )
    for message in sandbox.stdout:
        print(f"Message: {message}")
    ```
    """

    _stream: Optional[AsyncGenerator[Optional[bytes], None]]

    def __init__(
        self,
        file_descriptor: "api_pb2.FileDescriptor.ValueType",
        object_id: str,
        object_type: Literal["sandbox", "container_process"],
        client: _Client,
        stream_type: StreamType = StreamType.PIPE,
        text: bool = True,
        by_line: bool = False,
    ) -> None:
        """mdmd:hidden"""
        self._file_descriptor = file_descriptor
        self._object_type = object_type
        self._object_id = object_id
        self._client = client
        self._stream = None
        self._last_entry_id: str = ""
        self._line_buffer = b""

        # Sandbox logs are streamed to the client as strings, so StreamReaders reading
        # them must have text mode enabled.
        if object_type == "sandbox" and not text:
            raise ValueError("Sandbox streams must have text mode enabled.")

        # line-buffering is only supported when text=True
        if by_line and not text:
            raise ValueError("line-buffering is only supported when text=True")

        self._text = text
        self._by_line = by_line

        # Whether the reader received an EOF. Once EOF is True, it returns
        # an empty string for any subsequent reads (including async for)
        self.eof = False

        if not isinstance(stream_type, StreamType):
            raise TypeError(f"stream_type must be of type StreamType, got {type(stream_type)}")

        # We only support piping sandbox logs because they're meant to be durable logs stored
        # on the user's application.
        if object_type == "sandbox" and stream_type != StreamType.PIPE:
            raise ValueError("Sandbox streams must be piped.")
        self._stream_type = stream_type

        if self._object_type == "container_process":
            # Container process streams need to be consumed as they are produced,
            # otherwise the process will block. Use a buffer to store the stream
            # until the client consumes it.
            self._container_process_buffer: list[Optional[bytes]] = []
            self._consume_container_process_task = asyncio.create_task(self._consume_container_process_stream())

    @property
    def file_descriptor(self) -> int:
        """Possible values are `1` for stdout and `2` for stderr."""
        return self._file_descriptor

    async def read(self) -> T:
        """Fetch the entire contents of the stream until EOF.

        **Usage**

        ```python
        from modal import Sandbox

        sandbox = Sandbox.create("echo", "hello", app=app)
        sandbox.wait()

        print(sandbox.stdout.read())
        ```
        """
        data_str = ""
        data_bytes = b""
        async for message in self._get_logs():
            if message is None:
                break
            if self._text:
                data_str += message.decode("utf-8")
            else:
                data_bytes += message

        if self._text:
            return cast(T, data_str)
        else:
            return cast(T, data_bytes)

    async def _consume_container_process_stream(self):
        """Consume the container process stream and store messages in the buffer."""
        if self._stream_type == StreamType.DEVNULL:
            return

        completed = False
        retries_remaining = 10
        while not completed:
            try:
                iterator = _container_process_logs_iterator(self._object_id, self._file_descriptor, self._client)

                async for message in iterator:
                    if self._stream_type == StreamType.STDOUT and message:
                        print(message.decode("utf-8"), end="")
                    elif self._stream_type == StreamType.PIPE:
                        self._container_process_buffer.append(message)
                    if message is None:
                        completed = True
                        break

            except (GRPCError, StreamTerminatedError, ClientClosed) as exc:
                if retries_remaining > 0:
                    retries_remaining -= 1
                    if isinstance(exc, GRPCError):
                        if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                            await asyncio.sleep(1.0)
                            continue
                    elif isinstance(exc, StreamTerminatedError):
                        continue
                    elif isinstance(exc, ClientClosed):
                        # If the client was closed, the user has triggered a cleanup.
                        break
                raise exc

    async def _stream_container_process(self) -> AsyncGenerator[tuple[Optional[bytes], str], None]:
        """Streams the container process buffer to the reader."""
        entry_id = 0
        if self._last_entry_id:
            entry_id = int(self._last_entry_id) + 1

        while True:
            if entry_id >= len(self._container_process_buffer):
                await asyncio.sleep(0.1)
                continue

            item = self._container_process_buffer[entry_id]

            yield (item, str(entry_id))
            if item is None:
                break

            entry_id += 1

    async def _get_logs(self, skip_empty_messages: bool = True) -> AsyncGenerator[Optional[bytes], None]:
        """Streams sandbox or process logs from the server to the reader.

        Logs returned by this method may contain partial or multiple lines at a time.

        When the stream receives an EOF, it yields None. Once an EOF is received,
        subsequent invocations will not yield logs.
        """
        if self._stream_type != StreamType.PIPE:
            raise InvalidError("Logs can only be retrieved using the PIPE stream type.")

        if self.eof:
            yield None
            return

        completed = False

        retries_remaining = 10
        while not completed:
            try:
                if self._object_type == "sandbox":
                    iterator = _sandbox_logs_iterator(
                        self._object_id, self._file_descriptor, self._last_entry_id, self._client
                    )
                else:
                    iterator = self._stream_container_process()

                async for message, entry_id in iterator:
                    self._last_entry_id = entry_id
                    # Empty messages are sent when the process boots up. Don't yield them unless
                    # we're using the empty message to signal process liveness.
                    if skip_empty_messages and message == b"":
                        continue

                    yield message
                    if message is None:
                        completed = True
                        self.eof = True

            except (GRPCError, StreamTerminatedError) as exc:
                if retries_remaining > 0:
                    retries_remaining -= 1
                    if isinstance(exc, GRPCError):
                        if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                            await asyncio.sleep(1.0)
                            continue
                    elif isinstance(exc, StreamTerminatedError):
                        continue
                raise

    async def _get_logs_by_line(self) -> AsyncGenerator[Optional[bytes], None]:
        """Process logs from the server and yield complete lines only."""
        async for message in self._get_logs():
            if message is None:
                if self._line_buffer:
                    yield self._line_buffer
                    self._line_buffer = b""
                yield None
            else:
                assert isinstance(message, bytes)
                self._line_buffer += message
                while b"\n" in self._line_buffer:
                    line, self._line_buffer = self._line_buffer.split(b"\n", 1)
                    yield line + b"\n"

    def __aiter__(self) -> AsyncIterator[T]:
        """mdmd:hidden"""
        if not self._stream:
            if self._by_line:
                self._stream = self._get_logs_by_line()
            else:
                self._stream = self._get_logs()
        return self

    async def __anext__(self) -> T:
        """mdmd:hidden"""
        assert self._stream is not None

        value = await self._stream.__anext__()

        # The stream yields None if it receives an EOF batch.
        if value is None:
            raise StopAsyncIteration

        if self._text:
            return cast(T, value.decode("utf-8"))
        else:
            return cast(T, value)

    async def aclose(self):
        """mdmd:hidden"""
        if self._stream:
            await self._stream.aclose()


MAX_BUFFER_SIZE = 2 * 1024 * 1024


class _StreamWriter:
    """Provides an interface to buffer and write logs to a sandbox or container process stream (`stdin`)."""

    def __init__(self, object_id: str, object_type: Literal["sandbox", "container_process"], client: _Client) -> None:
        """mdmd:hidden"""
        self._index = 1
        self._object_id = object_id
        self._object_type = object_type
        self._client = client
        self._is_closed = False
        self._buffer = bytearray()

    def _get_next_index(self) -> int:
        index = self._index
        self._index += 1
        return index

    def write(self, data: Union[bytes, bytearray, memoryview, str]) -> None:
        """Write data to the stream but does not send it immediately.

        This is non-blocking and queues the data to an internal buffer. Must be
        used along with the `drain()` method, which flushes the buffer.

        **Usage**

        ```python
        from modal import Sandbox

        sandbox = Sandbox.create(
            "bash",
            "-c",
            "while read line; do echo $line; done",
            app=app,
        )
        sandbox.stdin.write(b"foo\\n")
        sandbox.stdin.write(b"bar\\n")
        sandbox.stdin.write_eof()

        sandbox.stdin.drain()
        sandbox.wait()
        ```
        """
        if self._is_closed:
            raise ValueError("Stdin is closed. Cannot write to it.")
        if isinstance(data, (bytes, bytearray, memoryview, str)):
            if isinstance(data, str):
                data = data.encode("utf-8")
            if len(self._buffer) + len(data) > MAX_BUFFER_SIZE:
                raise BufferError("Buffer size exceed limit. Call drain to clear the buffer.")
            self._buffer.extend(data)
        else:
            raise TypeError(f"data argument must be a bytes-like object, not {type(data).__name__}")

    def write_eof(self) -> None:
        """Close the write end of the stream after the buffered data is drained.

        If the process was blocked on input, it will become unblocked after
        `write_eof()`. This method needs to be used along with the `drain()`
        method, which flushes the EOF to the process.
        """
        self._is_closed = True

    async def drain(self) -> None:
        """Flush the write buffer and send data to the running process.

        This is a flow control method that blocks until data is sent. It returns
        when it is appropriate to continue writing data to the stream.

        **Usage**

        ```python notest
        writer.write(data)
        writer.drain()
        ```

        Async usage:
        ```python notest
        writer.write(data)  # not a blocking operation
        await writer.drain.aio()
        ```
        """
        data = bytes(self._buffer)
        self._buffer.clear()
        index = self._get_next_index()

        try:
            if self._object_type == "sandbox":
                await retry_transient_errors(
                    self._client.stub.SandboxStdinWrite,
                    api_pb2.SandboxStdinWriteRequest(
                        sandbox_id=self._object_id, index=index, eof=self._is_closed, input=data
                    ),
                )
            else:
                await retry_transient_errors(
                    self._client.stub.ContainerExecPutInput,
                    api_pb2.ContainerExecPutInputRequest(
                        exec_id=self._object_id,
                        input=api_pb2.RuntimeInputMessage(message=data, message_index=index, eof=self._is_closed),
                    ),
                )
        except GRPCError as exc:
            if exc.status == Status.FAILED_PRECONDITION:
                raise ValueError(exc.message)
            else:
                raise exc


StreamReader = synchronize_api(_StreamReader)
StreamWriter = synchronize_api(_StreamWriter)


================================================
File: modal/mount.py
================================================
# Copyright Modal Labs 2022
import abc
import asyncio
import concurrent.futures
import dataclasses
import os
import site
import sys
import sysconfig
import time
import typing
from collections.abc import AsyncGenerator
from pathlib import Path, PurePosixPath
from typing import Callable, Optional, Sequence, Union

from google.protobuf.message import Message

import modal.exception
import modal.file_pattern_matcher
from modal_proto import api_pb2
from modal_version import __version__

from ._object import _get_environment_name, _Object
from ._resolver import Resolver
from ._utils.async_utils import aclosing, async_map, synchronize_api
from ._utils.blob_utils import FileUploadSpec, blob_upload_file, get_file_upload_spec_from_path
from ._utils.deprecation import deprecation_warning, renamed_parameter
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from ._utils.package_utils import get_module_mount_info
from .client import _Client
from .config import config, logger
from .exception import InvalidError, ModuleNotMountable
from .file_pattern_matcher import FilePatternMatcher

ROOT_DIR: PurePosixPath = PurePosixPath("/root")
MOUNT_PUT_FILE_CLIENT_TIMEOUT = 10 * 60  # 10 min max for transferring files

# Supported releases and versions for python-build-standalone.
#
# These can be updated safely, but changes will trigger a rebuild for all images
# that rely on `add_python()` in their constructor.
PYTHON_STANDALONE_VERSIONS: dict[str, tuple[str, str]] = {
    "3.9": ("20230826", "3.9.18"),
    "3.10": ("20230826", "3.10.13"),
    "3.11": ("20230826", "3.11.5"),
    "3.12": ("20240107", "3.12.1"),
    "3.13": ("20241008", "3.13.0"),
}

MOUNT_DEPRECATION_MESSAGE_PATTERN = """modal.Mount usage will soon be deprecated.

Use {replacement} instead, which is functionally and performance-wise equivalent.

See https://modal.com/docs/guide/modal-1-0-migration for more details.
"""


def client_mount_name() -> str:
    """Get the deployed name of the client package mount."""
    return f"modal-client-mount-{__version__}"


def python_standalone_mount_name(version: str) -> str:
    """Get the deployed name of the python-build-standalone mount."""
    if "-" in version:  # default to glibc
        version, libc = version.split("-")
    else:
        libc = "gnu"
    if version not in PYTHON_STANDALONE_VERSIONS:
        raise modal.exception.InvalidError(
            f"Unsupported standalone python version: {version!r}, supported values are "
            f"{list(PYTHON_STANDALONE_VERSIONS)}"
        )
    if libc != "gnu":
        raise modal.exception.InvalidError(f"Unsupported libc identifier: {libc}")
    release, full_version = PYTHON_STANDALONE_VERSIONS[version]
    return f"python-build-standalone.{release}.{full_version}-{libc}"


class _MountEntry(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def description(self) -> str: ...

    @abc.abstractmethod
    def get_files_to_upload(self) -> typing.Iterator[tuple[Path, str]]: ...

    @abc.abstractmethod
    def watch_entry(self) -> tuple[Path, Path]: ...

    @abc.abstractmethod
    def top_level_paths(self) -> list[tuple[Path, PurePosixPath]]: ...


def _select_files(entries: list[_MountEntry]) -> list[tuple[Path, PurePosixPath]]:
    # TODO: make this async
    all_files: set[tuple[Path, PurePosixPath]] = set()
    for entry in entries:
        all_files |= set(entry.get_files_to_upload())
    return list(all_files)


@dataclasses.dataclass
class _MountFile(_MountEntry):
    local_file: Path
    remote_path: PurePosixPath

    def description(self) -> str:
        return str(self.local_file)

    def get_files_to_upload(self):
        local_file = self.local_file.resolve()
        if not local_file.exists():
            raise FileNotFoundError(local_file)

        rel_filename = self.remote_path
        yield local_file, rel_filename

    def watch_entry(self):
        safe_path = self.local_file.expanduser().absolute()
        return safe_path.parent, safe_path

    def top_level_paths(self) -> list[tuple[Path, PurePosixPath]]:
        return [(self.local_file, self.remote_path)]


@dataclasses.dataclass
class _MountDir(_MountEntry):
    local_dir: Path
    remote_path: PurePosixPath
    ignore: Callable[[Path], bool]
    recursive: bool

    def description(self):
        return str(self.local_dir.expanduser().absolute())

    def get_files_to_upload(self):
        # we can't use .resolve() eagerly here since that could end up "renaming" symlinked files
        # see test_mount_directory_with_symlinked_file
        local_dir = self.local_dir.expanduser().absolute()

        if not local_dir.exists():
            raise FileNotFoundError(local_dir)

        if not local_dir.is_dir():
            raise NotADirectoryError(local_dir)

        if self.recursive:
            gen = (os.path.join(root, name) for root, dirs, files in os.walk(local_dir) for name in files)
        else:
            gen = (dir_entry.path for dir_entry in os.scandir(local_dir) if dir_entry.is_file())

        for local_filename in gen:
            local_path = Path(local_filename)
            if not self.ignore(local_path):
                local_relpath = local_path.expanduser().absolute().relative_to(local_dir)
                mount_path = self.remote_path / local_relpath.as_posix()
                yield local_path.resolve(), mount_path

    def watch_entry(self):
        return self.local_dir.resolve().expanduser(), None

    def top_level_paths(self) -> list[tuple[Path, PurePosixPath]]:
        return [(self.local_dir, self.remote_path)]


def module_mount_condition(module_base: Path):
    SKIP_BYTECODE = True  # hard coded for now
    SKIP_DOT_PREFIXED = True

    def condition(f: str):
        path = Path(f)
        if SKIP_BYTECODE and path.suffix == ".pyc":
            return False

        # Check parent dir names to see if file should be included,
        # but ignore dir names above root of mounted module:
        # /a/.venv/site-packages/mymod/foo.py should be included by default
        # /a/my_mod/.config/foo.py should *not* be included by default
        while path != module_base and path != path.parent:
            if SKIP_BYTECODE and path.name == "__pycache__":
                return False

            if SKIP_DOT_PREFIXED and path.name.startswith("."):
                return False

            path = path.parent

        return True

    return condition


def module_mount_ignore_condition(module_base: Path):
    return lambda f: not module_mount_condition(module_base)(str(f))


@dataclasses.dataclass
class _MountedPythonModule(_MountEntry):
    # the purpose of this is to keep printable information about which Python package
    # was mounted. Functionality wise it's the same as mounting a dir or a file with
    # the Module

    module_name: str
    remote_dir: Union[PurePosixPath, str] = ROOT_DIR.as_posix()  # cast needed here for type stub generation...
    ignore: Optional[Callable[[Path], bool]] = None

    def description(self) -> str:
        return f"PythonPackage:{self.module_name}"

    def _proxy_entries(self) -> list[_MountEntry]:
        mount_infos = get_module_mount_info(self.module_name)
        entries = []
        for mount_info in mount_infos:
            is_package, base_path = mount_info
            if is_package:
                remote_dir = PurePosixPath(self.remote_dir, *self.module_name.split("."))
                entries.append(
                    _MountDir(
                        base_path,
                        remote_path=remote_dir,
                        ignore=self.ignore or module_mount_ignore_condition(base_path),
                        recursive=True,
                    )
                )
            else:
                path_segments = self.module_name.split(".")[:-1]
                remote_path = PurePosixPath(self.remote_dir, *path_segments, Path(base_path).name)
                entries.append(
                    _MountFile(
                        local_file=Path(base_path),
                        remote_path=remote_path,
                    )
                )
        return entries

    def get_files_to_upload(self) -> typing.Iterator[tuple[Path, str]]:
        for entry in self._proxy_entries():
            yield from entry.get_files_to_upload()

    def watch_entry(self) -> tuple[Path, Path]:
        for entry in self._proxy_entries():
            # TODO: fix watch for mounts of multi-path packages
            return entry.watch_entry()

    def top_level_paths(self) -> list[tuple[Path, PurePosixPath]]:
        paths = []
        for sub in self._proxy_entries():
            paths.extend(sub.top_level_paths())
        return paths


class NonLocalMountError(Exception):
    # used internally to signal an error when trying to access entries on a non-local mount definition
    pass


class _Mount(_Object, type_prefix="mo"):
    """
    **Deprecated**: Mounts should not be used explicitly anymore, use `Image.add_local_*` commands instead.

    Create a mount for a local directory or file that can be attached
    to one or more Modal functions.

    **Usage**

    ```python notest
    import modal
    import os
    app = modal.App()

    @app.function(mounts=[modal.Mount.from_local_dir("~/foo", remote_path="/root/foo")])
    def f():
        # `/root/foo` has the contents of `~/foo`.
        print(os.listdir("/root/foo/"))
    ```

    Modal syncs the contents of the local directory every time the app runs, but uses the hash of
    the file's contents to skip uploading files that have been uploaded before.
    """

    _entries: Optional[list[_MountEntry]] = None
    _deployment_name: Optional[str] = None
    _namespace: Optional[int] = None
    _environment_name: Optional[str] = None
    _content_checksum_sha256_hex: Optional[str] = None

    @staticmethod
    def _new(entries: list[_MountEntry] = []) -> "_Mount":
        rep = f"Mount({entries})"

        async def mount_content_deduplication_key():
            try:
                included_files = await asyncio.get_event_loop().run_in_executor(None, _select_files, entries)
            except NonLocalMountError:
                return None
            return (_Mount._type_prefix, "local", frozenset(included_files))

        obj = _Mount._from_loader(_Mount._load_mount, rep, deduplication_key=mount_content_deduplication_key)
        obj._entries = entries
        obj._is_local = True
        return obj

    def _extend(self, entry: _MountEntry) -> "_Mount":
        return _Mount._new(self._entries + [entry])

    @property
    def entries(self):
        """mdmd:hidden"""
        if self._entries is None:
            raise NonLocalMountError()
        return self._entries

    def _hydrate_metadata(self, handle_metadata: Optional[Message]):
        assert isinstance(handle_metadata, api_pb2.MountHandleMetadata)
        self._content_checksum_sha256_hex = handle_metadata.content_checksum_sha256_hex

    def _top_level_paths(self) -> list[tuple[Path, PurePosixPath]]:
        # Returns [(local_absolute_path, remote_path), ...] for all top level entries in the Mount
        # Used to determine if a package mount is installed in a sys directory or not
        res: list[tuple[Path, PurePosixPath]] = []
        for entry in self.entries:
            res.extend(entry.top_level_paths())
        return res

    def is_local(self) -> bool:
        """mdmd:hidden"""
        # TODO(erikbern): since any remote ref bypasses the constructor,
        # we can't rely on it to be set. Let's clean this up later.
        return getattr(self, "_is_local", False)

    @staticmethod
    def _add_local_dir(
        local_path: Path,
        remote_path: PurePosixPath,
        ignore: Callable[[Path], bool] = modal.file_pattern_matcher._NOTHING,
    ):
        return _Mount._new()._extend(
            _MountDir(
                local_dir=local_path,
                ignore=ignore,
                remote_path=remote_path,
                recursive=True,
            ),
        )

    def add_local_dir(
        self,
        local_path: Union[str, Path],
        *,
        # Where the directory is placed within in the mount
        remote_path: Union[str, PurePosixPath, None] = None,
        # Predicate filter function for file selection, which should accept a filepath and return `True` for inclusion.
        # Defaults to including all files.
        condition: Optional[Callable[[str], bool]] = None,
        # add files from subdirectories as well
        recursive: bool = True,
    ) -> "_Mount":
        """
        Add a local directory to the `Mount` object.
        """
        local_path = Path(local_path)
        if remote_path is None:
            remote_path = local_path.name
        remote_path = PurePosixPath("/", remote_path)
        if condition is None:

            def include_all(path):
                return True

            condition = include_all

        def converted_condition(path: Path) -> bool:
            return not condition(str(path))

        return self._extend(
            _MountDir(
                local_dir=local_path,
                ignore=converted_condition,
                remote_path=remote_path,
                recursive=recursive,
            ),
        )

    @staticmethod
    def from_local_dir(
        local_path: Union[str, Path],
        *,
        # Where the directory is placed within in the mount
        remote_path: Union[str, PurePosixPath, None] = None,
        # Predicate filter function for file selection, which should accept a filepath and return `True` for inclusion.
        # Defaults to including all files.
        condition: Optional[Callable[[str], bool]] = None,
        # add files from subdirectories as well
        recursive: bool = True,
    ) -> "_Mount":
        """
        **Deprecated:** Use image.add_local_dir() instead

        Create a `Mount` from a local directory.

        **Usage**

        ```python notest
        assets = modal.Mount.from_local_dir(
            "~/assets",
            condition=lambda pth: not ".venv" in pth,
            remote_path="/assets",
        )
        ```
        """
        deprecation_warning(
            (2025, 1, 8), MOUNT_DEPRECATION_MESSAGE_PATTERN.format(replacement="image.add_local_dir"), pending=True
        )
        return _Mount._from_local_dir(local_path, remote_path=remote_path, condition=condition, recursive=recursive)

    @staticmethod
    def _from_local_dir(
        local_path: Union[str, Path],
        *,
        # Where the directory is placed within in the mount
        remote_path: Union[str, PurePosixPath, None] = None,
        # Predicate filter function for file selection, which should accept a filepath and return `True` for inclusion.
        # Defaults to including all files.
        condition: Optional[Callable[[str], bool]] = None,
        # add files from subdirectories as well
        recursive: bool = True,
    ) -> "_Mount":
        return _Mount._new().add_local_dir(
            local_path, remote_path=remote_path, condition=condition, recursive=recursive
        )

    def add_local_file(
        self,
        local_path: Union[str, Path],
        remote_path: Union[str, PurePosixPath, None] = None,
    ) -> "_Mount":
        """
        Add a local file to the `Mount` object.
        """
        local_path = Path(local_path)
        if remote_path is None:
            remote_path = local_path.name
        remote_path = PurePosixPath("/", remote_path)
        return self._extend(
            _MountFile(
                local_file=local_path,
                remote_path=PurePosixPath(remote_path),
            ),
        )

    @staticmethod
    def from_local_file(local_path: Union[str, Path], remote_path: Union[str, PurePosixPath, None] = None) -> "_Mount":
        """
        **Deprecated**: Use image.add_local_file() instead

        Create a `Mount` mounting a single local file.

        **Usage**

        ```python notest
        # Mount the DBT profile in user's home directory into container.
        dbt_profiles = modal.Mount.from_local_file(
            local_path="~/profiles.yml",
            remote_path="/root/dbt_profile/profiles.yml",
        )
        ```
        """
        deprecation_warning(
            (2025, 1, 8), MOUNT_DEPRECATION_MESSAGE_PATTERN.format(replacement="image.add_local_file"), pending=True
        )
        return _Mount._from_local_file(local_path, remote_path)

    @staticmethod
    def _from_local_file(local_path: Union[str, Path], remote_path: Union[str, PurePosixPath, None] = None) -> "_Mount":
        return _Mount._new().add_local_file(local_path, remote_path=remote_path)

    @staticmethod
    def _description(entries: list[_MountEntry]) -> str:
        local_contents = [e.description() for e in entries]
        return ", ".join(local_contents)

    @staticmethod
    async def _get_files(entries: list[_MountEntry]) -> AsyncGenerator[FileUploadSpec, None]:
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as exe:
            all_files = await loop.run_in_executor(exe, _select_files, entries)

            futs = []
            for local_filename, remote_filename in all_files:
                logger.debug(f"Mounting {local_filename} as {remote_filename}")
                futs.append(loop.run_in_executor(exe, get_file_upload_spec_from_path, local_filename, remote_filename))

            logger.debug(f"Computing checksums for {len(futs)} files using {exe._max_workers} worker threads")
            for fut in asyncio.as_completed(futs):
                try:
                    yield await fut
                except FileNotFoundError as exc:
                    # Can happen with temporary files (e.g. emacs will write temp files and delete them quickly)
                    logger.info(f"Ignoring file not found: {exc}")

    async def _load_mount(
        self: "_Mount",
        resolver: Resolver,
        existing_object_id: Optional[str],
    ):
        t0 = time.monotonic()

        # Asynchronously list and checksum files with a thread pool, then upload them concurrently.
        n_seen, n_finished = 0, 0
        total_uploads, total_bytes = 0, 0
        accounted_hashes: set[str] = set()
        message_label = _Mount._description(self._entries)
        blob_upload_concurrency = asyncio.Semaphore(16)  # Limit uploads of large files.
        status_row = resolver.add_status_row()

        async def _put_file(file_spec: FileUploadSpec) -> api_pb2.MountFile:
            nonlocal n_seen, n_finished, total_uploads, total_bytes
            n_seen += 1
            status_row.message(f"Creating mount {message_label}: Uploaded {n_finished}/{n_seen} files")

            remote_filename = file_spec.mount_filename
            mount_file = api_pb2.MountFile(
                filename=remote_filename,
                sha256_hex=file_spec.sha256_hex,
                mode=file_spec.mode,
            )

            if file_spec.sha256_hex in accounted_hashes:
                n_finished += 1
                return mount_file

            request = api_pb2.MountPutFileRequest(sha256_hex=file_spec.sha256_hex)
            accounted_hashes.add(file_spec.sha256_hex)
            response = await retry_transient_errors(resolver.client.stub.MountPutFile, request, base_delay=1)

            if response.exists:
                n_finished += 1
                return mount_file

            total_uploads += 1
            total_bytes += file_spec.size

            if file_spec.use_blob:
                logger.debug(f"Creating blob file for {file_spec.source_description} ({file_spec.size} bytes)")
                async with blob_upload_concurrency:
                    with file_spec.source() as fp:
                        blob_id = await blob_upload_file(
                            fp, resolver.client.stub, sha256_hex=file_spec.sha256_hex, md5_hex=file_spec.md5_hex
                        )
                logger.debug(f"Uploading blob file {file_spec.source_description} as {remote_filename}")
                request2 = api_pb2.MountPutFileRequest(data_blob_id=blob_id, sha256_hex=file_spec.sha256_hex)
            else:
                logger.debug(
                    f"Uploading file {file_spec.source_description} to {remote_filename} ({file_spec.size} bytes)"
                )
                request2 = api_pb2.MountPutFileRequest(data=file_spec.content, sha256_hex=file_spec.sha256_hex)

            start_time = time.monotonic()
            while time.monotonic() - start_time < MOUNT_PUT_FILE_CLIENT_TIMEOUT:
                response = await retry_transient_errors(resolver.client.stub.MountPutFile, request2, base_delay=1)
                if response.exists:
                    n_finished += 1
                    return mount_file

            raise modal.exception.MountUploadTimeoutError(f"Mounting of {file_spec.source_description} timed out")

        # Upload files, or check if they already exist.
        n_concurrent_uploads = 512
        files: list[api_pb2.MountFile] = []
        async with aclosing(
            async_map(_Mount._get_files(self._entries), _put_file, concurrency=n_concurrent_uploads)
        ) as stream:
            async for file in stream:
                files.append(file)

        if not files:
            logger.warning(f"Mount of '{message_label}' is empty.")

        # Build the mount.
        status_row.message(f"Creating mount {message_label}: Finalizing index of {len(files)} files")
        if self._deployment_name:
            req = api_pb2.MountGetOrCreateRequest(
                deployment_name=self._deployment_name,
                namespace=self._namespace,
                environment_name=self._environment_name,
                object_creation_type=api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS,
                files=files,
            )
        elif resolver.app_id is not None:
            req = api_pb2.MountGetOrCreateRequest(
                object_creation_type=api_pb2.OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP,
                files=files,
                app_id=resolver.app_id,
            )
        else:
            req = api_pb2.MountGetOrCreateRequest(
                object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
                files=files,
                environment_name=resolver.environment_name,
            )

        resp = await retry_transient_errors(resolver.client.stub.MountGetOrCreate, req, base_delay=1)
        status_row.finish(f"Created mount {message_label}")

        logger.debug(f"Uploaded {total_uploads} new files and {total_bytes} bytes in {time.monotonic() - t0}s")
        self._hydrate(resp.mount_id, resolver.client, resp.handle_metadata)

    @staticmethod
    def from_local_python_packages(
        *module_names: str,
        remote_dir: Union[str, PurePosixPath] = ROOT_DIR.as_posix(),
        # Predicate filter function for file selection, which should accept a filepath and return `True` for inclusion.
        # Defaults to including all files.
        condition: Optional[Callable[[str], bool]] = None,
        ignore: Optional[Union[Sequence[str], Callable[[Path], bool]]] = None,
    ) -> "_Mount":
        """
        **Deprecated**: Use image.add_local_python_source instead

        Returns a `modal.Mount` that makes local modules listed in `module_names` available inside the container.
        This works by mounting the local path of each module's package to a directory inside the container
        that's on `PYTHONPATH`.

        **Usage**

        ```python notest
        import modal
        import my_local_module

        app = modal.App()

        @app.function(mounts=[
            modal.Mount.from_local_python_packages("my_local_module", "my_other_module"),
        ])
        def f():
            my_local_module.do_stuff()
        ```
        """
        deprecation_warning(
            (2025, 1, 8),
            MOUNT_DEPRECATION_MESSAGE_PATTERN.format(replacement="image.add_local_python_source"),
            pending=True,
        )
        return _Mount._from_local_python_packages(
            *module_names, remote_dir=remote_dir, condition=condition, ignore=ignore
        )

    @staticmethod
    def _from_local_python_packages(
        *module_names: str,
        remote_dir: Union[str, PurePosixPath] = ROOT_DIR.as_posix(),
        # Predicate filter function for file selection, which should accept a filepath and return `True` for inclusion.
        # Defaults to including all files.
        condition: Optional[Callable[[str], bool]] = None,
        ignore: Optional[Union[Sequence[str], Callable[[Path], bool]]] = None,
    ) -> "_Mount":
        # Don't re-run inside container.

        if condition is not None:
            if ignore is not None:
                raise InvalidError("Cannot specify both `ignore` and `condition`")

            def converted_condition(path: Path) -> bool:
                return not condition(str(path))

            ignore = converted_condition
        elif isinstance(ignore, list):
            ignore = FilePatternMatcher(*ignore)

        mount = _Mount._new()
        from ._runtime.execution_context import is_local

        if not is_local():
            return mount  # empty/non-mountable mount in case it's used from within a container
        for module_name in module_names:
            mount = mount._extend(_MountedPythonModule(module_name, remote_dir, ignore))
        return mount

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    def from_name(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
    ) -> "_Mount":
        """mdmd:hidden"""

        async def _load(provider: _Mount, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.MountGetOrCreateRequest(
                deployment_name=name,
                namespace=namespace,
                environment_name=_get_environment_name(environment_name, resolver),
            )
            response = await resolver.client.stub.MountGetOrCreate(req)
            provider._hydrate(response.mount_id, resolver.client, response.handle_metadata)

        return _Mount._from_loader(_load, "Mount()", hydrate_lazily=True)

    @classmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def lookup(
        cls: type["_Mount"],
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
    ) -> "_Mount":
        """mdmd:hidden"""
        deprecation_warning(
            (2025, 1, 27),
            "`modal.Mount.lookup` is deprecated and will be removed in a future release."
            " It can be replaced with `modal.Mount.from_name`."
            "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
        )
        obj = _Mount.from_name(name, namespace=namespace, environment_name=environment_name)
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client)
        await resolver.load(obj)
        return obj

    async def _deploy(
        self: "_Mount",
        deployment_name: Optional[str] = None,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
        client: Optional[_Client] = None,
    ) -> None:
        check_object_name(deployment_name, "Mount")
        environment_name = _get_environment_name(environment_name, resolver=None)
        self._deployment_name = deployment_name
        self._namespace = namespace
        self._environment_name = environment_name
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client, environment_name=environment_name)
        await resolver.load(self)

    def _get_metadata(self) -> api_pb2.MountHandleMetadata:
        if self._content_checksum_sha256_hex is None:
            raise ValueError("Trying to access checksum of unhydrated mount")

        return api_pb2.MountHandleMetadata(content_checksum_sha256_hex=self._content_checksum_sha256_hex)


Mount = synchronize_api(_Mount)


def _create_client_mount():
    # TODO(erikbern): make this a static method on the Mount class
    import synchronicity

    import modal

    # Get the base_path because it also contains `modal_proto`.
    modal_parent_dir, _ = os.path.split(modal.__path__[0])
    client_mount = _Mount._new()

    for pkg_name in MODAL_PACKAGES:
        package_base_path = Path(modal_parent_dir) / pkg_name
        client_mount = client_mount.add_local_dir(
            package_base_path,
            remote_path=f"/pkg/{pkg_name}",
            condition=module_mount_condition(package_base_path),
            recursive=True,
        )

    # Mount synchronicity, so version changes don't trigger image rebuilds for users.
    synchronicity_base_path = Path(synchronicity.__path__[0])
    client_mount = client_mount.add_local_dir(
        synchronicity_base_path,
        remote_path="/pkg/synchronicity",
        condition=module_mount_condition(synchronicity_base_path),
        recursive=True,
    )
    return client_mount


create_client_mount = synchronize_api(_create_client_mount)


def _get_client_mount():
    # TODO(erikbern): make this a static method on the Mount class
    if config["sync_entrypoint"]:
        return _create_client_mount()
    else:
        return _Mount.from_name(client_mount_name(), namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL)


SYS_PREFIXES = {
    Path(p)
    for p in (
        sys.prefix,
        sys.base_prefix,
        sys.exec_prefix,
        sys.base_exec_prefix,
        *sysconfig.get_paths().values(),
        *site.getsitepackages(),
        site.getusersitepackages(),
    )
}


SYS_PREFIXES |= {p.resolve() for p in SYS_PREFIXES}

MODAL_PACKAGES = ["modal", "modal_proto", "modal_version"]


def _is_modal_path(remote_path: PurePosixPath):
    path_prefix = remote_path.parts[:3]
    remote_python_paths = [("/", "root"), ("/", "pkg")]
    for base in remote_python_paths:
        is_modal_path = path_prefix in [base + (mod,) for mod in MODAL_PACKAGES] or path_prefix == base + (
            "synchronicity",
        )
        if is_modal_path:
            return True
    return False


def get_sys_modules_mounts() -> dict[str, _Mount]:
    """mdmd:hidden

    Auto-mount local modules that have been imported in global scope.
    This may or may not include the "entrypoint" of the function as well, depending on how modal is invoked
    Note: sys.modules may change during the iteration
    """
    auto_mounts = {}
    top_level_modules = []
    skip_prefixes = set()
    for name, module in sorted(sys.modules.items(), key=lambda kv: len(kv[0])):
        parent = name.rsplit(".")[0]
        if parent and parent in skip_prefixes:
            skip_prefixes.add(name)
            continue
        skip_prefixes.add(name)
        top_level_modules.append((name, module))

    for module_name, module in top_level_modules:
        if module_name.startswith("__"):
            # skip "built in" modules like __main__ and __mp_main__
            # the running function's main file should be included anyway
            continue

        try:
            # at this point we don't know if the sys.modules module should be mounted or not
            potential_mount = _Mount._from_local_python_packages(module_name)
            mount_paths = potential_mount._top_level_paths()
        except ModuleNotMountable:
            # this typically happens if the module is a built-in, has binary components or doesn't exist
            continue

        for local_path, remote_path in mount_paths:
            if any(local_path.is_relative_to(p) for p in SYS_PREFIXES) or _is_modal_path(remote_path):
                # skip any module that has paths in SYS_PREFIXES, or would overwrite the modal Package in the container
                break
        else:
            auto_mounts[module_name] = potential_mount

    return auto_mounts


================================================
File: modal/network_file_system.py
================================================
# Copyright Modal Labs 2023
import functools
import os
import time
from collections.abc import AsyncIterator
from pathlib import Path, PurePosixPath
from typing import Any, BinaryIO, Callable, Optional, Union

from grpclib import GRPCError, Status
from synchronicity.async_wrap import asynccontextmanager

import modal
from modal_proto import api_pb2

from ._object import (
    EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
    _get_environment_name,
    _Object,
    live_method,
    live_method_gen,
)
from ._resolver import Resolver
from ._utils.async_utils import TaskContext, aclosing, async_map, sync_or_async_iter, synchronize_api
from ._utils.blob_utils import LARGE_FILE_LIMIT, blob_iter, blob_upload_file
from ._utils.deprecation import deprecation_warning, renamed_parameter
from ._utils.grpc_utils import retry_transient_errors
from ._utils.hash_utils import get_sha256_hex
from ._utils.name_utils import check_object_name
from .client import _Client
from .exception import InvalidError
from .volume import FileEntry

NETWORK_FILE_SYSTEM_PUT_FILE_CLIENT_TIMEOUT = (
    10 * 60
)  # 10 min max for transferring files (does not include upload time to s3)


def network_file_system_mount_protos(
    validated_network_file_systems: list[tuple[str, "_NetworkFileSystem"]],
    allow_cross_region_volumes: bool,
) -> list[api_pb2.SharedVolumeMount]:
    network_file_system_mounts = []
    # Relies on dicts being ordered (true as of Python 3.6).
    for path, volume in validated_network_file_systems:
        network_file_system_mounts.append(
            api_pb2.SharedVolumeMount(
                mount_path=path,
                shared_volume_id=volume.object_id,
                allow_cross_region=allow_cross_region_volumes,
            )
        )
    return network_file_system_mounts


class _NetworkFileSystem(_Object, type_prefix="sv"):
    """A shared, writable file system accessible by one or more Modal functions.

    By attaching this file system as a mount to one or more functions, they can
    share and persist data with each other.

    **Usage**

    ```python
    import modal

    nfs = modal.NetworkFileSystem.from_name("my-nfs", create_if_missing=True)
    app = modal.App()

    @app.function(network_file_systems={"/root/foo": nfs})
    def f():
        pass

    @app.function(network_file_systems={"/root/goo": nfs})
    def g():
        pass
    ```

    Also see the CLI methods for accessing network file systems:

    ```
    modal nfs --help
    ```

    A `NetworkFileSystem` can also be useful for some local scripting scenarios, e.g.:

    ```python notest
    nfs = modal.NetworkFileSystem.from_name("my-network-file-system")
    for chunk in nfs.read_file("my_db_dump.csv"):
        ...
    ```
    """

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    def from_name(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_NetworkFileSystem":
        """Reference a NetworkFileSystem by its name, creating if necessary.

        In contrast to `modal.NetworkFileSystem.lookup`, this is a lazy method
        that defers hydrating the local object with metadata from Modal servers
        until the first time it is actually used.

        ```python notest
        nfs = NetworkFileSystem.from_name("my-nfs", create_if_missing=True)

        @app.function(network_file_systems={"/data": nfs})
        def f():
            pass
        ```
        """
        check_object_name(name, "NetworkFileSystem")

        async def _load(self: _NetworkFileSystem, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.SharedVolumeGetOrCreateRequest(
                deployment_name=name,
                namespace=namespace,
                environment_name=_get_environment_name(environment_name, resolver),
                object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
            )
            try:
                response = await resolver.client.stub.SharedVolumeGetOrCreate(req)
                self._hydrate(response.shared_volume_id, resolver.client, None)
            except GRPCError as exc:
                if exc.status == Status.NOT_FOUND and exc.message == "App has wrong entity vo":
                    raise InvalidError(
                        f"Attempted to mount: `{name}` as a NetworkFileSystem " + "which already exists as a Volume"
                    )
                raise

        return _NetworkFileSystem._from_loader(_load, "NetworkFileSystem()", hydrate_lazily=True)

    @classmethod
    @asynccontextmanager
    async def ephemeral(
        cls: type["_NetworkFileSystem"],
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
    ) -> AsyncIterator["_NetworkFileSystem"]:
        """Creates a new ephemeral network filesystem within a context manager:

        Usage:
        ```python
        with modal.NetworkFileSystem.ephemeral() as nfs:
            assert nfs.listdir("/") == []
        ```

        ```python notest
        async with modal.NetworkFileSystem.ephemeral() as nfs:
            assert await nfs.listdir("/") == []
        ```
        """
        if client is None:
            client = await _Client.from_env()
        request = api_pb2.SharedVolumeGetOrCreateRequest(
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
            environment_name=_get_environment_name(environment_name),
        )
        response = await client.stub.SharedVolumeGetOrCreate(request)
        async with TaskContext() as tc:
            request = api_pb2.SharedVolumeHeartbeatRequest(shared_volume_id=response.shared_volume_id)
            tc.infinite_loop(lambda: client.stub.SharedVolumeHeartbeat(request), sleep=_heartbeat_sleep)
            yield cls._new_hydrated(response.shared_volume_id, client, None, is_another_app=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def lookup(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_NetworkFileSystem":
        """Lookup a named NetworkFileSystem.

        DEPRECATED: This method is deprecated in favor of `modal.NetworkFileSystem.from_name`.

        In contrast to `modal.NetworkFileSystem.from_name`, this is an eager method
        that will hydrate the local object with metadata from Modal servers.

        ```python notest
        nfs = modal.NetworkFileSystem.lookup("my-nfs")
        print(nfs.listdir("/"))
        ```
        """
        deprecation_warning(
            (2025, 1, 27),
            "`modal.NetworkFileSystem.lookup` is deprecated and will be removed in a future release."
            " It can be replaced with `modal.NetworkFileSystem.from_name`."
            "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
        )
        obj = _NetworkFileSystem.from_name(
            name, namespace=namespace, environment_name=environment_name, create_if_missing=create_if_missing
        )
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client)
        await resolver.load(obj)
        return obj

    @staticmethod
    async def create_deployed(
        deployment_name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
    ) -> str:
        """mdmd:hidden"""
        check_object_name(deployment_name, "NetworkFileSystem")
        if client is None:
            client = await _Client.from_env()
        request = api_pb2.SharedVolumeGetOrCreateRequest(
            deployment_name=deployment_name,
            namespace=namespace,
            environment_name=_get_environment_name(environment_name),
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS,
        )
        resp = await retry_transient_errors(client.stub.SharedVolumeGetOrCreate, request)
        return resp.shared_volume_id

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def delete(name: str, client: Optional[_Client] = None, environment_name: Optional[str] = None):
        obj = await _NetworkFileSystem.from_name(name, environment_name=environment_name).hydrate(client)
        req = api_pb2.SharedVolumeDeleteRequest(shared_volume_id=obj.object_id)
        await retry_transient_errors(obj._client.stub.SharedVolumeDelete, req)

    @live_method
    async def write_file(self, remote_path: str, fp: BinaryIO, progress_cb: Optional[Callable[..., Any]] = None) -> int:
        """Write from a file object to a path on the network file system, atomically.

        Will create any needed parent directories automatically.

        If remote_path ends with `/` it's assumed to be a directory and the
        file will be uploaded with its current name to that directory.
        """
        progress_cb = progress_cb or (lambda *_, **__: None)

        sha_hash = get_sha256_hex(fp)
        fp.seek(0, os.SEEK_END)
        data_size = fp.tell()
        fp.seek(0)
        if data_size > LARGE_FILE_LIMIT:
            progress_task_id = progress_cb(name=remote_path, size=data_size)
            blob_id = await blob_upload_file(
                fp,
                self._client.stub,
                progress_report_cb=functools.partial(progress_cb, progress_task_id),
                sha256_hex=sha_hash,
            )
            req = api_pb2.SharedVolumePutFileRequest(
                shared_volume_id=self.object_id,
                path=remote_path,
                data_blob_id=blob_id,
                sha256_hex=sha_hash,
                resumable=True,
            )
        else:
            data = fp.read()
            req = api_pb2.SharedVolumePutFileRequest(
                shared_volume_id=self.object_id, path=remote_path, data=data, resumable=True
            )

        t0 = time.monotonic()
        while time.monotonic() - t0 < NETWORK_FILE_SYSTEM_PUT_FILE_CLIENT_TIMEOUT:
            response = await retry_transient_errors(self._client.stub.SharedVolumePutFile, req)
            if response.exists:
                break
        else:
            raise modal.exception.TimeoutError(f"Uploading of {remote_path} timed out")

        return data_size  # might be better if this is returned from the server

    @live_method_gen
    async def read_file(self, path: str) -> AsyncIterator[bytes]:
        """Read a file from the network file system"""
        req = api_pb2.SharedVolumeGetFileRequest(shared_volume_id=self.object_id, path=path)
        try:
            response = await retry_transient_errors(self._client.stub.SharedVolumeGetFile, req)
        except GRPCError as exc:
            raise FileNotFoundError(exc.message) if exc.status == Status.NOT_FOUND else exc
        if response.WhichOneof("data_oneof") == "data":
            yield response.data
        else:
            async for data in blob_iter(response.data_blob_id, self._client.stub):
                yield data

    @live_method_gen
    async def iterdir(self, path: str) -> AsyncIterator[FileEntry]:
        """Iterate over all files in a directory in the network file system.

        * Passing a directory path lists all files in the directory (names are relative to the directory)
        * Passing a file path returns a list containing only that file's listing description
        * Passing a glob path (including at least one * or ** sequence) returns all files matching
        that glob path (using absolute paths)
        """
        req = api_pb2.SharedVolumeListFilesRequest(shared_volume_id=self.object_id, path=path)
        async for batch in self._client.stub.SharedVolumeListFilesStream.unary_stream(req):
            for entry in batch.entries:
                yield FileEntry._from_proto(entry)

    @live_method
    async def add_local_file(
        self,
        local_path: Union[Path, str],
        remote_path: Optional[Union[str, PurePosixPath, None]] = None,
        progress_cb: Optional[Callable[..., Any]] = None,
    ):
        local_path = Path(local_path)
        if remote_path is None:
            remote_path = PurePosixPath("/", local_path.name).as_posix()
        else:
            remote_path = PurePosixPath(remote_path).as_posix()

        with local_path.open("rb") as local_file:
            return await self.write_file(remote_path, local_file, progress_cb=progress_cb)

    @live_method
    async def add_local_dir(
        self,
        local_path: Union[Path, str],
        remote_path: Optional[Union[str, PurePosixPath, None]] = None,
        progress_cb: Optional[Callable[..., Any]] = None,
    ):
        _local_path = Path(local_path)
        if remote_path is None:
            remote_path = PurePosixPath("/", _local_path.name).as_posix()
        else:
            remote_path = PurePosixPath(remote_path).as_posix()

        assert _local_path.is_dir()

        def gen_transfers():
            for subpath in _local_path.rglob("*"):
                if subpath.is_dir():
                    continue
                relpath_str = subpath.relative_to(_local_path).as_posix()
                yield subpath, PurePosixPath(remote_path, relpath_str)

        async def _add_local_file(paths: tuple[Path, PurePosixPath]) -> int:
            return await self.add_local_file(paths[0], paths[1], progress_cb)

        async with aclosing(async_map(sync_or_async_iter(gen_transfers()), _add_local_file, concurrency=20)) as stream:
            async for _ in stream:  # consume/execute the map
                pass

    @live_method
    async def listdir(self, path: str) -> list[FileEntry]:
        """List all files in a directory in the network file system.

        * Passing a directory path lists all files in the directory (names are relative to the directory)
        * Passing a file path returns a list containing only that file's listing description
        * Passing a glob path (including at least one * or ** sequence) returns all files matching
        that glob path (using absolute paths)
        """
        return [entry async for entry in self.iterdir(path)]

    @live_method
    async def remove_file(self, path: str, recursive=False):
        """Remove a file in a network file system."""
        req = api_pb2.SharedVolumeRemoveFileRequest(shared_volume_id=self.object_id, path=path, recursive=recursive)
        await retry_transient_errors(self._client.stub.SharedVolumeRemoveFile, req)


NetworkFileSystem = synchronize_api(_NetworkFileSystem)


================================================
File: modal/object.py
================================================
# Copyright Modal Labs 2025
from ._object import _Object
from ._utils.async_utils import synchronize_api

Object = synchronize_api(_Object, target_module=__name__)


================================================
File: modal/output.py
================================================
# Copyright Modal Labs 2024
"""Interface to Modal's OutputManager functionality.

These functions live here so that Modal library code can import them without
transitively importing Rich, as we do in global scope in _output.py. This allows
us to avoid importing Rich for client code that runs in the container environment.

"""

import contextlib
from collections.abc import Generator
from typing import TYPE_CHECKING, Optional

if TYPE_CHECKING:
    from ._output import OutputManager


OUTPUT_ENABLED = False


@contextlib.contextmanager
def enable_output(show_progress: bool = True) -> Generator[None, None, None]:
    """Context manager that enable output when using the Python SDK.

    This will print to stdout and stderr things such as
    1. Logs from running functions
    2. Status of creating objects
    3. Map progress

    Example:
    ```python
    app = modal.App()
    with modal.enable_output():
        with app.run():
            ...
    ```
    """
    from ._output import OutputManager

    # Toggle the output flag from within this function so that we can
    # call _get_output_manager from within the library and only import
    # the _output module if output is explicitly enabled. That prevents
    # us from trying to import rich inside a container environment where
    # it might not be installed. This is sort of hacky and I would prefer
    # a more thorough refactor where the OutputManager is fully useable
    # without rich installed, but that's a larger project.
    global OUTPUT_ENABLED

    try:
        with OutputManager.enable_output(show_progress):
            OUTPUT_ENABLED = True
            yield
    finally:
        OUTPUT_ENABLED = False


def _get_output_manager() -> Optional["OutputManager"]:
    """Interface to the OutputManager that returns None when output is not enabled."""
    if OUTPUT_ENABLED:
        from ._output import OutputManager

        return OutputManager.get()
    else:
        return None


================================================
File: modal/parallel_map.py
================================================
# Copyright Modal Labs 2024
import asyncio
import time
import typing
from dataclasses import dataclass
from typing import Any, Callable, Optional

from grpclib import GRPCError, Status

from modal._runtime.execution_context import current_input_id
from modal._utils.async_utils import (
    AsyncOrSyncIterable,
    aclosing,
    async_map_ordered,
    async_merge,
    async_zip,
    queue_batch_iterator,
    sync_or_async_iter,
    synchronize_api,
    synchronizer,
    warn_if_generator_is_not_consumed,
)
from modal._utils.blob_utils import BLOB_MAX_PARALLELISM
from modal._utils.function_utils import (
    ATTEMPT_TIMEOUT_GRACE_PERIOD,
    OUTPUTS_TIMEOUT,
    _create_input,
    _process_result,
)
from modal._utils.grpc_utils import retry_transient_errors
from modal.config import logger
from modal_proto import api_pb2

if typing.TYPE_CHECKING:
    import modal.client


class _SynchronizedQueue:
    """mdmd:hidden"""

    # small wrapper around asyncio.Queue to make it cross-thread compatible through synchronicity
    async def init(self):
        # in Python 3.8 the asyncio.Queue is bound to the event loop on creation
        # so it needs to be created in a synchronicity-wrapped init method
        self.q = asyncio.Queue()

    @synchronizer.no_io_translation
    async def put(self, item):
        await self.q.put(item)

    @synchronizer.no_io_translation
    async def get(self):
        return await self.q.get()


SynchronizedQueue = synchronize_api(_SynchronizedQueue)


@dataclass
class _OutputValue:
    # box class for distinguishing None results from non-existing/None markers
    value: Any


MAP_INVOCATION_CHUNK_SIZE = 49

if typing.TYPE_CHECKING:
    import modal.functions


async def _map_invocation(
    function: "modal.functions._Function",
    raw_input_queue: _SynchronizedQueue,
    client: "modal.client._Client",
    order_outputs: bool,
    return_exceptions: bool,
    count_update_callback: Optional[Callable[[int, int], None]],
):
    assert client.stub
    request = api_pb2.FunctionMapRequest(
        function_id=function.object_id,
        parent_input_id=current_input_id() or "",
        function_call_type=api_pb2.FUNCTION_CALL_TYPE_MAP,
        return_exceptions=return_exceptions,
    )
    response = await retry_transient_errors(client.stub.FunctionMap, request)

    function_call_id = response.function_call_id

    have_all_inputs = False
    num_inputs = 0
    num_outputs = 0

    def count_update():
        if count_update_callback is not None:
            count_update_callback(num_outputs, num_inputs)

    pending_outputs: dict[str, int] = {}  # Map input_id -> next expected gen_index value
    completed_outputs: set[str] = set()  # Set of input_ids whose outputs are complete (expecting no more values)

    input_queue: asyncio.Queue = asyncio.Queue()

    async def create_input(argskwargs):
        nonlocal num_inputs
        idx = num_inputs
        num_inputs += 1
        (args, kwargs) = argskwargs
        return await _create_input(args, kwargs, client, idx=idx, method_name=function._use_method_name)

    async def input_iter():
        while 1:
            raw_input = await raw_input_queue.get()
            if raw_input is None:  # end of input sentinel
                break
            yield raw_input  # args, kwargs

    async def drain_input_generator():
        # Parallelize uploading blobs
        async with aclosing(
            async_map_ordered(input_iter(), create_input, concurrency=BLOB_MAX_PARALLELISM)
        ) as streamer:
            async for item in streamer:
                await input_queue.put(item)

        # close queue iterator
        await input_queue.put(None)
        yield

    async def pump_inputs():
        assert client.stub
        nonlocal have_all_inputs, num_inputs
        async for items in queue_batch_iterator(input_queue, MAP_INVOCATION_CHUNK_SIZE):
            request = api_pb2.FunctionPutInputsRequest(
                function_id=function.object_id, inputs=items, function_call_id=function_call_id
            )
            logger.debug(
                f"Pushing {len(items)} inputs to server. Num queued inputs awaiting push is {input_queue.qsize()}."
            )
            while True:
                try:
                    resp = await retry_transient_errors(
                        client.stub.FunctionPutInputs,
                        request,
                        # with 8 retries we log the warning below about every 30 secondswhich isn't too spammy.
                        max_retries=8,
                        max_delay=15,
                        additional_status_codes=[Status.RESOURCE_EXHAUSTED],
                    )
                    break
                except GRPCError as err:
                    if err.status != Status.RESOURCE_EXHAUSTED:
                        raise err
                    logger.warning(
                        f"Warning: map progress for function {function._function_name} is limited."
                        " Common bottlenecks include slow iteration over results, or function backlogs."
                    )

            count_update()
            for item in resp.inputs:
                pending_outputs.setdefault(item.input_id, 0)
            logger.debug(
                f"Successfully pushed {len(items)} inputs to server. "
                f"Num queued inputs awaiting push is {input_queue.qsize()}."
            )

        have_all_inputs = True
        yield

    async def get_all_outputs():
        assert client.stub
        nonlocal num_inputs, num_outputs, have_all_inputs
        last_entry_id = "0-0"
        while not have_all_inputs or len(pending_outputs) > len(completed_outputs):
            request = api_pb2.FunctionGetOutputsRequest(
                function_call_id=function_call_id,
                timeout=OUTPUTS_TIMEOUT,
                last_entry_id=last_entry_id,
                clear_on_success=False,
                requested_at=time.time(),
            )
            response = await retry_transient_errors(
                client.stub.FunctionGetOutputs,
                request,
                max_retries=20,
                attempt_timeout=OUTPUTS_TIMEOUT + ATTEMPT_TIMEOUT_GRACE_PERIOD,
            )

            if len(response.outputs) == 0:
                continue

            last_entry_id = response.last_entry_id
            for item in response.outputs:
                pending_outputs.setdefault(item.input_id, 0)
                if item.input_id in completed_outputs:
                    # If this input is already completed, it means the output has already been
                    # processed and was received again due to a duplicate.
                    continue
                completed_outputs.add(item.input_id)
                num_outputs += 1
                yield item

    async def get_all_outputs_and_clean_up():
        assert client.stub
        try:
            async with aclosing(get_all_outputs()) as output_items:
                async for item in output_items:
                    yield item
        finally:
            # "ack" that we have all outputs we are interested in and let backend clear results
            request = api_pb2.FunctionGetOutputsRequest(
                function_call_id=function_call_id,
                timeout=0,
                last_entry_id="0-0",
                clear_on_success=True,
                requested_at=time.time(),
            )
            await retry_transient_errors(client.stub.FunctionGetOutputs, request)

    async def fetch_output(item: api_pb2.FunctionGetOutputsItem) -> tuple[int, Any]:
        try:
            output = await _process_result(item.result, item.data_format, client.stub, client)
        except Exception as e:
            if return_exceptions:
                output = e
            else:
                raise e
        return (item.idx, output)

    async def poll_outputs():
        # map to store out-of-order outputs received
        received_outputs = {}
        output_idx = 0

        async with aclosing(
            async_map_ordered(get_all_outputs_and_clean_up(), fetch_output, concurrency=BLOB_MAX_PARALLELISM)
        ) as streamer:
            async for idx, output in streamer:
                count_update()
                if not order_outputs:
                    yield _OutputValue(output)
                else:
                    # hold on to outputs for function maps, so we can reorder them correctly.
                    received_outputs[idx] = output
                    while output_idx in received_outputs:
                        output = received_outputs.pop(output_idx)
                        yield _OutputValue(output)
                        output_idx += 1

        assert len(received_outputs) == 0

    async with aclosing(async_merge(drain_input_generator(), pump_inputs(), poll_outputs())) as streamer:
        async for response in streamer:
            if response is not None:
                yield response.value


@warn_if_generator_is_not_consumed(function_name="Function.map")
def _map_sync(
    self,
    *input_iterators: typing.Iterable[Any],  # one input iterator per argument in the mapped-over function/generator
    kwargs={},  # any extra keyword arguments for the function
    order_outputs: bool = True,  # return outputs in order
    return_exceptions: bool = False,  # propagate exceptions (False) or aggregate them in the results list (True)
) -> AsyncOrSyncIterable:
    """Parallel map over a set of inputs.

    Takes one iterator argument per argument in the function being mapped over.

    Example:
    ```python
    @app.function()
    def my_func(a):
        return a ** 2


    @app.local_entrypoint()
    def main():
        assert list(my_func.map([1, 2, 3, 4])) == [1, 4, 9, 16]
    ```

    If applied to a `stub.function`, `map()` returns one result per input and the output order
    is guaranteed to be the same as the input order. Set `order_outputs=False` to return results
    in the order that they are completed instead.

    `return_exceptions` can be used to treat exceptions as successful results:

    ```python
    @app.function()
    def my_func(a):
        if a == 2:
            raise Exception("ohno")
        return a ** 2


    @app.local_entrypoint()
    def main():
        # [0, 1, UserCodeException(Exception('ohno'))]
        print(list(my_func.map(range(3), return_exceptions=True)))
    ```
    """

    return AsyncOrSyncIterable(
        _map_async(
            self, *input_iterators, kwargs=kwargs, order_outputs=order_outputs, return_exceptions=return_exceptions
        ),
        nested_async_message=(
            "You can't iter(Function.map()) or Function.for_each() from an async function. "
            "Use async for ... Function.map.aio() or Function.for_each.aio() instead."
        ),
    )


@warn_if_generator_is_not_consumed(function_name="Function.map.aio")
async def _map_async(
    self,
    *input_iterators: typing.Union[
        typing.Iterable[Any], typing.AsyncIterable[Any]
    ],  # one input iterator per argument in the mapped-over function/generator
    kwargs={},  # any extra keyword arguments for the function
    order_outputs: bool = True,  # return outputs in order
    return_exceptions: bool = False,  # propagate exceptions (False) or aggregate them in the results list (True)
) -> typing.AsyncGenerator[Any, None]:
    """mdmd:hidden
    This runs in an event loop on the main thread

    It concurrently feeds new input to the input queue and yields available outputs
    to the caller.
    Note that since the iterator(s) can block, it's a bit opaque how often the event
    loop decides to get a new input vs how often it will emit a new output.
    We could make this explicit as an improvement or even let users decide what they
    prefer: throughput (prioritize queueing inputs) or latency (prioritize yielding results)
    """
    raw_input_queue: Any = SynchronizedQueue()  # type: ignore
    raw_input_queue.init()

    async def feed_queue():
        # This runs in a main thread event loop, so it doesn't block the synchronizer loop
        async with aclosing(async_zip(*[sync_or_async_iter(it) for it in input_iterators])) as streamer:
            async for args in streamer:
                await raw_input_queue.put.aio((args, kwargs))
        await raw_input_queue.put.aio(None)  # end-of-input sentinel

    feed_input_task = asyncio.create_task(feed_queue())

    try:
        # note that `map()` and `map.aio()` are not synchronicity-wrapped, since
        # they accept executable code in the form of
        # iterators that we don't want to run inside the synchronicity thread.
        # Instead, we delegate to `._map()` with a safer Queue as input
        async with aclosing(self._map.aio(raw_input_queue, order_outputs, return_exceptions)) as map_output_stream:
            async for output in map_output_stream:
                yield output
    finally:
        feed_input_task.cancel()  # should only be needed in case of exceptions


def _for_each_sync(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
    """Execute function for all inputs, ignoring outputs.

    Convenient alias for `.map()` in cases where the function just needs to be called.
    as the caller doesn't have to consume the generator to process the inputs.
    """
    # TODO(erikbern): it would be better if this is more like a map_spawn that immediately exits
    # rather than iterating over the result
    for _ in self.map(*input_iterators, kwargs=kwargs, order_outputs=False, return_exceptions=ignore_exceptions):
        pass


async def _for_each_async(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
    async for _ in self.map.aio(  # type: ignore
        *input_iterators, kwargs=kwargs, order_outputs=False, return_exceptions=ignore_exceptions
    ):
        pass


@warn_if_generator_is_not_consumed(function_name="Function.starmap")
async def _starmap_async(
    self,
    input_iterator: typing.Union[typing.Iterable[typing.Sequence[Any]], typing.AsyncIterable[typing.Sequence[Any]]],
    kwargs={},
    order_outputs: bool = True,
    return_exceptions: bool = False,
) -> typing.AsyncIterable[Any]:
    raw_input_queue: Any = SynchronizedQueue()  # type: ignore
    raw_input_queue.init()

    async def feed_queue():
        # This runs in a main thread event loop, so it doesn't block the synchronizer loop
        async with aclosing(sync_or_async_iter(input_iterator)) as streamer:
            async for args in streamer:
                await raw_input_queue.put.aio((args, kwargs))
        await raw_input_queue.put.aio(None)  # end-of-input sentinel

    feed_input_task = asyncio.create_task(feed_queue())
    try:
        async for output in self._map.aio(raw_input_queue, order_outputs, return_exceptions):  # type: ignore[reportFunctionMemberAccess]
            yield output
    finally:
        feed_input_task.cancel()  # should only be needed in case of exceptions


@warn_if_generator_is_not_consumed(function_name="Function.starmap.aio")
def _starmap_sync(
    self,
    input_iterator: typing.Iterable[typing.Sequence[Any]],
    kwargs={},
    order_outputs: bool = True,
    return_exceptions: bool = False,
) -> AsyncOrSyncIterable:
    """Like `map`, but spreads arguments over multiple function arguments.

    Assumes every input is a sequence (e.g. a tuple).

    Example:
    ```python
    @app.function()
    def my_func(a, b):
        return a + b


    @app.local_entrypoint()
    def main():
        assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]
    ```
    """
    return AsyncOrSyncIterable(
        _starmap_async(
            self, input_iterator, kwargs=kwargs, order_outputs=order_outputs, return_exceptions=return_exceptions
        ),
        nested_async_message=(
            "You can't run Function.map() or Function.for_each() from an async function. "
            "Use Function.map.aio()/Function.for_each.aio() instead."
        ),
    )


================================================
File: modal/partial_function.py
================================================
# Copyright Modal Labs 2025
from modal._utils.async_utils import synchronize_api

from ._partial_function import (
    _asgi_app,
    _batched,
    _build,
    _enter,
    _exit,
    _method,
    _PartialFunction,
    _web_endpoint,
    _web_server,
    _wsgi_app,
)

# The only reason these are wrapped is to get translated type stubs, they
# don't actually run any async code as of 2025-02-04:
PartialFunction = synchronize_api(_PartialFunction, target_module=__name__)
method = synchronize_api(_method, target_module=__name__)
web_endpoint = synchronize_api(_web_endpoint, target_module=__name__)
asgi_app = synchronize_api(_asgi_app, target_module=__name__)
wsgi_app = synchronize_api(_wsgi_app, target_module=__name__)
web_server = synchronize_api(_web_server, target_module=__name__)
build = synchronize_api(_build, target_module=__name__)
enter = synchronize_api(_enter, target_module=__name__)
exit = synchronize_api(_exit, target_module=__name__)
batched = synchronize_api(_batched, target_module=__name__)


================================================
File: modal/proxy.py
================================================
# Copyright Modal Labs 2024
from typing import Optional

from modal_proto import api_pb2

from ._object import _get_environment_name, _Object
from ._resolver import Resolver
from ._utils.async_utils import synchronize_api


class _Proxy(_Object, type_prefix="pr"):
    """Proxy objects give your Modal containers a static outbound IP address.

    This can be used for connecting to a remote address with network whitelist, for example
    a database. See [the guide](/docs/guide/proxy-ips) for more information.
    """

    @staticmethod
    def from_name(
        name: str,
        environment_name: Optional[str] = None,
    ) -> "_Proxy":
        """Reference a Proxy by its name.

        In contrast to most other Modal objects, new Proxy objects must be
        provisioned via the Dashboard and cannot be created on the fly from code.

        """

        async def _load(self: _Proxy, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.ProxyGetRequest(
                name=name,
                environment_name=_get_environment_name(environment_name, resolver),
            )
            response: api_pb2.ProxyGetResponse = await resolver.client.stub.ProxyGet(req)
            self._hydrate(response.proxy.proxy_id, resolver.client, None)

        return _Proxy._from_loader(_load, "Proxy()", is_another_app=True)


Proxy = synchronize_api(_Proxy, target_module=__name__)


================================================
File: modal/queue.py
================================================
# Copyright Modal Labs 2022
import queue  # The system library
import time
import warnings
from collections.abc import AsyncGenerator, AsyncIterator
from typing import Any, Optional

from grpclib import GRPCError, Status
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method, live_method_gen
from ._resolver import Resolver
from ._serialization import deserialize, serialize
from ._utils.async_utils import TaskContext, synchronize_api, warn_if_generator_is_not_consumed
from ._utils.deprecation import deprecation_warning, renamed_parameter
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from .client import _Client
from .exception import InvalidError, RequestSizeError


class _Queue(_Object, type_prefix="qu"):
    """Distributed, FIFO queue for data flow in Modal apps.

    The queue can contain any object serializable by `cloudpickle`, including Modal objects.

    By default, the `Queue` object acts as a single FIFO queue which supports puts and gets (blocking and non-blocking).

    **Usage**

    ```python
    from modal import Queue

    # Create an ephemeral queue which is anonymous and garbage collected
    with Queue.ephemeral() as my_queue:
        # Putting values
        my_queue.put("some value")
        my_queue.put(123)

        # Getting values
        assert my_queue.get() == "some value"
        assert my_queue.get() == 123

        # Using partitions
        my_queue.put(0)
        my_queue.put(1, partition="foo")
        my_queue.put(2, partition="bar")

        # Default and "foo" partition are ignored by the get operation.
        assert my_queue.get(partition="bar") == 2

        # Set custom 10s expiration time on "foo" partition.
        my_queue.put(3, partition="foo", partition_ttl=10)

        # (beta feature) Iterate through items in place (read immutably)
        my_queue.put(1)
        assert [v for v in my_queue.iterate()] == [0, 1]

    # You can also create persistent queues that can be used across apps
    queue = Queue.from_name("my-persisted-queue", create_if_missing=True)
    queue.put(42)
    assert queue.get() == 42
    ```

    For more examples, see the [guide](/docs/guide/dicts-and-queues#modal-queues).

    **Queue partitions (beta)**

    Specifying partition keys gives access to other independent FIFO partitions within the same `Queue` object.
    Across any two partitions, puts and gets are completely independent.
    For example, a put in one partition does not affect a get in any other partition.

    When no partition key is specified (by default), puts and gets will operate on a default partition.
    This default partition is also isolated from all other partitions.
    Please see the Usage section below for an example using partitions.

    **Lifetime of a queue and its partitions**

    By default, each partition is cleared 24 hours after the last `put` operation.
    A lower TTL can be specified by the `partition_ttl` argument in the `put` or `put_many` methods.
    Each partition's expiry is handled independently.

    As such, `Queue`s are best used for communication between active functions and not relied on for persistent storage.

    On app completion or after stopping an app any associated `Queue` objects are cleaned up.
    All its partitions will be cleared.

    **Limits**

    A single `Queue` can contain up to 100,000 partitions, each with up to 5,000 items. Each item can be up to 256 KiB.

    Partition keys must be non-empty and must not exceed 64 bytes.
    """

    def __init__(self):
        """mdmd:hidden"""
        raise RuntimeError("Queue() is not allowed. Please use `Queue.from_name(...)` or `Queue.ephemeral()` instead.")

    @staticmethod
    def validate_partition_key(partition: Optional[str]) -> bytes:
        if partition is not None:
            partition_key = partition.encode("utf-8")
            if len(partition_key) == 0 or len(partition_key) > 64:
                raise InvalidError("Queue partition key must be between 1 and 64 characters.")
        else:
            partition_key = b""

        return partition_key

    @classmethod
    @asynccontextmanager
    async def ephemeral(
        cls: type["_Queue"],
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
    ) -> AsyncIterator["_Queue"]:
        """Creates a new ephemeral queue within a context manager:

        Usage:
        ```python
        from modal import Queue

        with Queue.ephemeral() as q:
            q.put(123)
        ```

        ```python notest
        async with Queue.ephemeral() as q:
            await q.put.aio(123)
        ```
        """
        if client is None:
            client = await _Client.from_env()
        request = api_pb2.QueueGetOrCreateRequest(
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
            environment_name=_get_environment_name(environment_name),
        )
        response = await client.stub.QueueGetOrCreate(request)
        async with TaskContext() as tc:
            request = api_pb2.QueueHeartbeatRequest(queue_id=response.queue_id)
            tc.infinite_loop(lambda: client.stub.QueueHeartbeat(request), sleep=_heartbeat_sleep)
            yield cls._new_hydrated(response.queue_id, client, None, is_another_app=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    def from_name(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_Queue":
        """Reference a named Queue, creating if necessary.

        In contrast to `modal.Queue.lookup`, this is a lazy method
        the defers hydrating the local object with metadata from
        Modal servers until the first time it is actually used.

        ```python
        q = modal.Queue.from_name("my-queue", create_if_missing=True)
        q.put(123)
        ```
        """
        check_object_name(name, "Queue")

        async def _load(self: _Queue, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.QueueGetOrCreateRequest(
                deployment_name=name,
                namespace=namespace,
                environment_name=_get_environment_name(environment_name, resolver),
                object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
            )
            response = await resolver.client.stub.QueueGetOrCreate(req)
            self._hydrate(response.queue_id, resolver.client, None)

        return _Queue._from_loader(_load, "Queue()", is_another_app=True, hydrate_lazily=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def lookup(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
    ) -> "_Queue":
        """Lookup a named Queue.

        DEPRECATED: This method is deprecated in favor of `modal.Queue.from_name`.

        In contrast to `modal.Queue.from_name`, this is an eager method
        that will hydrate the local object with metadata from Modal servers.

        ```python notest
        q = modal.Queue.lookup("my-queue")
        q.put(123)
        ```
        """
        deprecation_warning(
            (2025, 1, 27),
            "`modal.Queue.lookup` is deprecated and will be removed in a future release."
            " It can be replaced with `modal.Queue.from_name`."
            "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
        )
        obj = _Queue.from_name(
            name, namespace=namespace, environment_name=environment_name, create_if_missing=create_if_missing
        )
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client)
        await resolver.load(obj)
        return obj

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def delete(name: str, *, client: Optional[_Client] = None, environment_name: Optional[str] = None):
        obj = await _Queue.from_name(name, environment_name=environment_name).hydrate(client)
        req = api_pb2.QueueDeleteRequest(queue_id=obj.object_id)
        await retry_transient_errors(obj._client.stub.QueueDelete, req)

    async def _get_nonblocking(self, partition: Optional[str], n_values: int) -> list[Any]:
        request = api_pb2.QueueGetRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            timeout=0,
            n_values=n_values,
        )

        response = await retry_transient_errors(self._client.stub.QueueGet, request)
        if response.values:
            return [deserialize(value, self._client) for value in response.values]
        else:
            return []

    async def _get_blocking(self, partition: Optional[str], timeout: Optional[float], n_values: int) -> list[Any]:
        if timeout is not None:
            deadline = time.time() + timeout
        else:
            deadline = None

        while True:
            request_timeout = 50.0  # We prevent longer ones in order to keep the connection alive

            if deadline is not None:
                request_timeout = min(request_timeout, deadline - time.time())

            request = api_pb2.QueueGetRequest(
                queue_id=self.object_id,
                partition_key=self.validate_partition_key(partition),
                timeout=request_timeout,
                n_values=n_values,
            )

            response = await retry_transient_errors(self._client.stub.QueueGet, request)

            if response.values:
                return [deserialize(value, self._client) for value in response.values]

            if deadline is not None and time.time() > deadline:
                break

        raise queue.Empty()

    @live_method
    async def clear(self, *, partition: Optional[str] = None, all: bool = False) -> None:
        """Clear the contents of a single partition or all partitions."""
        if partition and all:
            raise InvalidError("Partition must be null when requesting to clear all.")
        request = api_pb2.QueueClearRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            all_partitions=all,
        )
        await retry_transient_errors(self._client.stub.QueueClear, request)

    @live_method
    async def get(
        self, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
    ) -> Optional[Any]:
        """Remove and return the next object in the queue.

        If `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for
        an object, or until `timeout` if specified. Raises a native `queue.Empty` exception
        if the `timeout` is reached.

        If `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is
        ignored in this case.
        """

        if block:
            values = await self._get_blocking(partition, timeout, 1)
        else:
            if timeout is not None:
                warnings.warn("Timeout is ignored for non-blocking get.")
            values = await self._get_nonblocking(partition, 1)

        if values:
            return values[0]
        else:
            return None

    @live_method
    async def get_many(
        self, n_values: int, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
    ) -> list[Any]:
        """Remove and return up to `n_values` objects from the queue.

        If there are fewer than `n_values` items in the queue, return all of them.

        If `block` is `True` (the default) and the queue is empty, `get` will wait indefinitely for
        at least 1 object to be present, or until `timeout` if specified. Raises the stdlib's `queue.Empty`
        exception if the `timeout` is reached.

        If `block` is `False`, `get` returns `None` immediately if the queue is empty. The `timeout` is
        ignored in this case.
        """

        if block:
            return await self._get_blocking(partition, timeout, n_values)
        else:
            if timeout is not None:
                warnings.warn("Timeout is ignored for non-blocking get.")
            return await self._get_nonblocking(partition, n_values)

    @live_method
    async def put(
        self,
        v: Any,
        block: bool = True,
        timeout: Optional[float] = None,
        *,
        partition: Optional[str] = None,
        partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
    ) -> None:
        """Add an object to the end of the queue.

        If `block` is `True` and the queue is full, this method will retry indefinitely or
        until `timeout` if specified. Raises the stdlib's `queue.Full` exception if the `timeout` is reached.
        If blocking it is not recommended to omit the `timeout`, as the operation could wait indefinitely.

        If `block` is `False`, this method raises `queue.Full` immediately if the queue is full. The `timeout` is
        ignored in this case."""
        await self.put_many([v], block, timeout, partition=partition, partition_ttl=partition_ttl)

    @live_method
    async def put_many(
        self,
        vs: list[Any],
        block: bool = True,
        timeout: Optional[float] = None,
        *,
        partition: Optional[str] = None,
        partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
    ) -> None:
        """Add several objects to the end of the queue.

        If `block` is `True` and the queue is full, this method will retry indefinitely or
        until `timeout` if specified. Raises the stdlib's `queue.Full` exception if the `timeout` is reached.
        If blocking it is not recommended to omit the `timeout`, as the operation could wait indefinitely.

        If `block` is `False`, this method raises `queue.Full` immediately if the queue is full. The `timeout` is
        ignored in this case.
        """
        if block:
            await self._put_many_blocking(partition, partition_ttl, vs, timeout)
        else:
            if timeout is not None:
                warnings.warn("`timeout` argument is ignored for non-blocking put.")
            await self._put_many_nonblocking(partition, partition_ttl, vs)

    async def _put_many_blocking(
        self, partition: Optional[str], partition_ttl: int, vs: list[Any], timeout: Optional[float] = None
    ):
        vs_encoded = [serialize(v) for v in vs]

        request = api_pb2.QueuePutRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            values=vs_encoded,
            partition_ttl_seconds=partition_ttl,
        )
        try:
            await retry_transient_errors(
                self._client.stub.QueuePut,
                request,
                # A full queue will return this status.
                additional_status_codes=[Status.RESOURCE_EXHAUSTED],
                max_delay=30.0,
                max_retries=None,
                total_timeout=timeout,
            )
        except GRPCError as exc:
            if exc.status == Status.RESOURCE_EXHAUSTED:
                raise queue.Full(str(exc))
            elif "status = '413'" in exc.message:
                method = "put_many" if len(vs) > 1 else "put"
                raise RequestSizeError(f"Queue.{method} request is too large") from exc
            else:
                raise exc

    async def _put_many_nonblocking(self, partition: Optional[str], partition_ttl: int, vs: list[Any]):
        vs_encoded = [serialize(v) for v in vs]
        request = api_pb2.QueuePutRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            values=vs_encoded,
            partition_ttl_seconds=partition_ttl,
        )
        try:
            await retry_transient_errors(self._client.stub.QueuePut, request)
        except GRPCError as exc:
            if exc.status == Status.RESOURCE_EXHAUSTED:
                raise queue.Full(exc.message)
            elif "status = '413'" in exc.message:
                method = "put_many" if len(vs) > 1 else "put"
                raise RequestSizeError(f"Queue.{method} request is too large") from exc
            else:
                raise exc

    @live_method
    async def len(self, *, partition: Optional[str] = None, total: bool = False) -> int:
        """Return the number of objects in the queue partition."""
        if partition and total:
            raise InvalidError("Partition must be null when requesting total length.")
        request = api_pb2.QueueLenRequest(
            queue_id=self.object_id,
            partition_key=self.validate_partition_key(partition),
            total=total,
        )
        response = await retry_transient_errors(self._client.stub.QueueLen, request)
        return response.len

    @warn_if_generator_is_not_consumed()
    @live_method_gen
    async def iterate(
        self, *, partition: Optional[str] = None, item_poll_timeout: float = 0.0
    ) -> AsyncGenerator[Any, None]:
        """(Beta feature) Iterate through items in the queue without mutation.

        Specify `item_poll_timeout` to control how long the iterator should wait for the next time before giving up.
        """
        last_entry_id: Optional[str] = None
        validated_partition_key = self.validate_partition_key(partition)
        fetch_deadline = time.time() + item_poll_timeout

        MAX_POLL_DURATION = 30.0
        while True:
            poll_duration = max(0.0, min(MAX_POLL_DURATION, fetch_deadline - time.time()))
            request = api_pb2.QueueNextItemsRequest(
                queue_id=self.object_id,
                partition_key=validated_partition_key,
                last_entry_id=last_entry_id,
                item_poll_timeout=poll_duration,
            )

            response: api_pb2.QueueNextItemsResponse = await retry_transient_errors(
                self._client.stub.QueueNextItems, request
            )
            if response.items:
                for item in response.items:
                    yield deserialize(item.value, self._client)
                    last_entry_id = item.entry_id
                fetch_deadline = time.time() + item_poll_timeout
            elif time.time() > fetch_deadline:
                break


Queue = synchronize_api(_Queue)


================================================
File: modal/retries.py
================================================
# Copyright Modal Labs 2022
import asyncio
from datetime import timedelta

from modal_proto import api_pb2

from .exception import InvalidError

MIN_INPUT_RETRY_DELAY_MS = 1000
MAX_INPUT_RETRY_DELAY_MS = 24 * 60 * 60 * 1000


class Retries:
    """Adds a retry policy to a Modal function.

    **Usage**

    ```python
    import modal
    app = modal.App()

    # Basic configuration.
    # This sets a policy of max 4 retries with 1-second delay between failures.
    @app.function(retries=4)
    def f():
        pass


    # Fixed-interval retries with 3-second delay between failures.
    @app.function(
        retries=modal.Retries(
            max_retries=2,
            backoff_coefficient=1.0,
            initial_delay=3.0,
        )
    )
    def g():
        pass


    # Exponential backoff, with retry delay doubling after each failure.
    @app.function(
        retries=modal.Retries(
            max_retries=4,
            backoff_coefficient=2.0,
            initial_delay=1.0,
        )
    )
    def h():
        pass
    ```
    """

    def __init__(
        self,
        *,
        # The maximum number of retries that can be made in the presence of failures.
        max_retries: int,
        # Coefficent controlling how much the retry delay increases each retry attempt.
        # A backoff coefficient of 1.0 creates fixed-delay where the delay period always equals the initial delay.
        backoff_coefficient: float = 2.0,
        # Number of seconds that must elapse before the first retry occurs.
        initial_delay: float = 1.0,
        # Maximum length of retry delay in seconds, preventing the delay from growing infinitely.
        max_delay: float = 60.0,
    ):
        """
        Construct a new retries policy, supporting exponential and fixed-interval delays via a backoff coefficient.
        """
        if max_retries < 0:
            raise InvalidError(f"Invalid retries number: {max_retries}. Function retries must be non-negative.")

        if max_retries > 10:
            raise InvalidError(f"Invalid retries number: {max_retries}. Retries must be between 0 and 10.")

        if max_delay < 1.0:
            raise InvalidError(f"Invalid max_delay: {max_delay}. max_delay must be at least 1 second.")

        # TODO(Jonathon): Right now we can only support a maximum delay of 60 seconds
        # b/c tasks can finish as early as after MIN_CONTAINER_IDLE_TIMEOUT seconds
        if max_delay > 60:
            raise InvalidError(f"Invalid max_delay argument: {max_delay}. Must be between 1-60 seconds.")

        if initial_delay < 0.0:
            raise InvalidError(f"Invalid initial_delay argument: {initial_delay}. Delay must be positive.")

        # initial_delay should be bounded by max_delay, but this is an extra defensive check.
        if initial_delay > 60:
            raise InvalidError(f"Invalid initial_delay argument: {initial_delay}. Must be between 0-60 seconds.")

        if not 1.0 <= backoff_coefficient <= 10.0:
            raise InvalidError(
                f"Invalid backoff_coefficient: {backoff_coefficient}. "
                "Coefficient must be between 1.0 (fixed-interval backoff) and 10.0"
            )

        self.max_retries = max_retries
        self.backoff_coefficient = backoff_coefficient
        self.initial_delay = timedelta(seconds=initial_delay)
        self.max_delay = timedelta(seconds=max_delay)

    def _to_proto(self) -> api_pb2.FunctionRetryPolicy:
        """Convert this retries policy to an internal protobuf representation."""
        return api_pb2.FunctionRetryPolicy(
            retries=self.max_retries,
            backoff_coefficient=self.backoff_coefficient,
            initial_delay_ms=self.initial_delay // timedelta(milliseconds=1),
            max_delay_ms=self.max_delay // timedelta(milliseconds=1),
        )


class RetryManager:
    """
    Helper class to apply the specified retry policy.
    """

    def __init__(self, retry_policy: api_pb2.FunctionRetryPolicy):
        self.retry_policy = retry_policy
        self.attempt_count = 0

    async def raise_or_sleep(self, exc: Exception):
        """
        Raises an exception if the maximum retry count has been reached, otherwise sleeps for calculated delay.
        """
        self.attempt_count += 1
        if self.attempt_count > self.retry_policy.retries:
            raise exc
        delay_ms = self._retry_delay_ms(self.attempt_count, self.retry_policy)
        await asyncio.sleep(delay_ms / 1000)

    @staticmethod
    def _retry_delay_ms(attempt_count: int, retry_policy: api_pb2.FunctionRetryPolicy) -> float:
        """
        Computes the amount of time to sleep before retrying based on the backend_coefficient and initial_delay_ms args.
        """
        if attempt_count < 1:
            raise ValueError(f"Cannot compute retry delay. attempt_count must be at least 1, but was {attempt_count}")
        delay_ms = retry_policy.initial_delay_ms * (retry_policy.backoff_coefficient ** (attempt_count - 1))
        if delay_ms < MIN_INPUT_RETRY_DELAY_MS:
            return MIN_INPUT_RETRY_DELAY_MS
        if delay_ms > MAX_INPUT_RETRY_DELAY_MS:
            return MAX_INPUT_RETRY_DELAY_MS
        return delay_ms


================================================
File: modal/runner.py
================================================
# Copyright Modal Labs 2022
import asyncio
import dataclasses
import os
import time
import typing
import warnings
from collections.abc import AsyncGenerator
from multiprocessing.synchronize import Event
from typing import TYPE_CHECKING, Any, Optional, TypeVar

from grpclib import GRPCError, Status
from synchronicity.async_wrap import asynccontextmanager

import modal_proto.api_pb2
from modal_proto import api_pb2

from ._functions import _Function
from ._object import _get_environment_name, _Object
from ._pty import get_pty_info
from ._resolver import Resolver
from ._runtime.execution_context import is_local
from ._traceback import print_server_warnings, traceback_contains_remote_call
from ._utils.async_utils import TaskContext, gather_cancel_on_exc, synchronize_api
from ._utils.deprecation import deprecation_error
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name, is_valid_tag
from .client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, _Client
from .cls import _Cls
from .config import config, logger
from .environments import _get_environment_cached
from .exception import InteractiveTimeoutError, InvalidError, RemoteError, _CliUserExecutionError
from .output import _get_output_manager, enable_output
from .running_app import RunningApp, running_app_from_layout
from .sandbox import _Sandbox
from .secret import _Secret
from .stream_type import StreamType

if TYPE_CHECKING:
    from .app import _App
else:
    _App = TypeVar("_App")


V = TypeVar("V")


async def _heartbeat(client: _Client, app_id: str) -> None:
    request = api_pb2.AppHeartbeatRequest(app_id=app_id)
    # TODO(erikbern): we should capture exceptions here
    # * if request fails: destroy the client
    # * if server says the app is gone: print a helpful warning about detaching
    await retry_transient_errors(client.stub.AppHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT)


async def _init_local_app_existing(client: _Client, existing_app_id: str, environment_name: str) -> RunningApp:
    # Get all the objects first
    obj_req = api_pb2.AppGetLayoutRequest(app_id=existing_app_id)
    obj_resp, _ = await gather_cancel_on_exc(
        retry_transient_errors(client.stub.AppGetLayout, obj_req),
        # Cache the environment associated with the app now as we will use it later
        _get_environment_cached(environment_name, client),
    )
    app_page_url = f"https://modal.com/apps/{existing_app_id}"  # TODO (elias): this should come from the backend
    return running_app_from_layout(
        existing_app_id,
        obj_resp.app_layout,
        app_page_url=app_page_url,
    )


async def _init_local_app_new(
    client: _Client,
    description: str,
    app_state: int,  # ValueType
    environment_name: str = "",
    interactive: bool = False,
) -> RunningApp:
    app_req = api_pb2.AppCreateRequest(
        description=description,
        environment_name=environment_name,
        app_state=app_state,  # type: ignore
    )
    app_resp, _ = await gather_cancel_on_exc(  # TODO: use TaskGroup?
        retry_transient_errors(client.stub.AppCreate, app_req),
        # Cache the environment associated with the app now as we will use it later
        _get_environment_cached(environment_name, client),
    )
    logger.debug(f"Created new app with id {app_resp.app_id}")
    return RunningApp(
        app_resp.app_id,
        app_page_url=app_resp.app_page_url,
        app_logs_url=app_resp.app_logs_url,
        interactive=interactive,
    )


async def _init_local_app_from_name(
    client: _Client,
    name: str,
    namespace: Any,
    environment_name: str = "",
) -> RunningApp:
    # Look up any existing deployment
    app_req = api_pb2.AppGetByDeploymentNameRequest(
        name=name,
        namespace=namespace,
        environment_name=environment_name,
    )
    app_resp = await retry_transient_errors(client.stub.AppGetByDeploymentName, app_req)
    existing_app_id = app_resp.app_id or None

    # Grab the app
    if existing_app_id is not None:
        return await _init_local_app_existing(client, existing_app_id, environment_name)
    else:
        return await _init_local_app_new(
            client, name, api_pb2.APP_STATE_INITIALIZING, environment_name=environment_name
        )


async def _create_all_objects(
    client: _Client,
    running_app: RunningApp,
    functions: dict[str, _Function],
    classes: dict[str, _Cls],
    environment_name: str,
) -> None:
    """Create objects that have been defined but not created on the server."""
    indexed_objects: dict[str, _Object] = {**functions, **classes}
    resolver = Resolver(
        client,
        environment_name=environment_name,
        app_id=running_app.app_id,
    )
    with resolver.display():
        # Get current objects, and reset all objects
        tag_to_object_id = {**running_app.function_ids, **running_app.class_ids}
        running_app.function_ids = {}
        running_app.class_ids = {}

        # Assign all objects
        for tag, obj in indexed_objects.items():
            # Reset object_id in case the app runs twice
            # TODO(erikbern): clean up the interface
            obj._unhydrate()

        # Preload all functions to make sure they have ids assigned before they are loaded.
        # This is important to make sure any enclosed function handle references in serialized
        # functions have ids assigned to them when the function is serialized.
        # Note: when handles/objs are merged, all objects will need to get ids pre-assigned
        # like this in order to be referrable within serialized functions
        async def _preload(tag, obj):
            existing_object_id = tag_to_object_id.get(tag)
            # Note: preload only currently implemented for Functions, returns None otherwise
            # this is to ensure that directly referenced functions from the global scope has
            # ids associated with them when they are serialized into other functions
            await resolver.preload(obj, existing_object_id)
            if obj.is_hydrated:
                tag_to_object_id[tag] = obj.object_id

        await TaskContext.gather(*(_preload(tag, obj) for tag, obj in indexed_objects.items()))

        async def _load(tag, obj):
            existing_object_id = tag_to_object_id.get(tag)
            await resolver.load(obj, existing_object_id)
            if _Function._is_id_type(obj.object_id):
                running_app.function_ids[tag] = obj.object_id
            elif _Cls._is_id_type(obj.object_id):
                running_app.class_ids[tag] = obj.object_id
            else:
                raise RuntimeError(f"Unexpected object {obj.object_id}")

        await TaskContext.gather(*(_load(tag, obj) for tag, obj in indexed_objects.items()))


async def _publish_app(
    client: _Client,
    running_app: RunningApp,
    app_state: int,  # api_pb2.AppState.value
    functions: dict[str, _Function],
    classes: dict[str, _Cls],
    name: str = "",  # Only relevant for deployments
    tag: str = "",  # Only relevant for deployments
) -> tuple[str, list[api_pb2.Warning]]:
    """Wrapper for AppPublish RPC."""

    definition_ids = {obj.object_id: obj._get_metadata().definition_id for obj in functions.values()}  # type: ignore

    request = api_pb2.AppPublishRequest(
        app_id=running_app.app_id,
        name=name,
        deployment_tag=tag,
        app_state=app_state,  # type: ignore  : should be a api_pb2.AppState.value
        function_ids=running_app.function_ids,
        class_ids=running_app.class_ids,
        definition_ids=definition_ids,
    )
    try:
        response = await retry_transient_errors(client.stub.AppPublish, request)
    except GRPCError as exc:
        if exc.status == Status.INVALID_ARGUMENT or exc.status == Status.FAILED_PRECONDITION:
            raise InvalidError(exc.message)
        raise

    print_server_warnings(response.server_warnings)
    return response.url, response.server_warnings


async def _disconnect(
    client: _Client,
    app_id: str,
    reason: "modal_proto.api_pb2.AppDisconnectReason.ValueType",
    exc_str: str = "",
) -> None:
    """Tell the server the client has disconnected for this app. Terminates all running tasks
    for ephemeral apps."""

    if exc_str:
        exc_str = exc_str[:1000]  # Truncate to 1000 chars

    logger.debug("Sending app disconnect/stop request")
    req_disconnect = api_pb2.AppClientDisconnectRequest(app_id=app_id, reason=reason, exception=exc_str)
    await retry_transient_errors(client.stub.AppClientDisconnect, req_disconnect)
    logger.debug("App disconnected")


async def _status_based_disconnect(client: _Client, app_id: str, exc_info: Optional[BaseException] = None):
    """Disconnect local session of a running app, sending relevant metadata

    exc_info: Exception if an exception caused the disconnect
    """
    if isinstance(exc_info, (KeyboardInterrupt, asyncio.CancelledError)):
        reason = api_pb2.APP_DISCONNECT_REASON_KEYBOARD_INTERRUPT
    elif exc_info is not None:
        if traceback_contains_remote_call(exc_info.__traceback__):
            reason = api_pb2.APP_DISCONNECT_REASON_REMOTE_EXCEPTION
        else:
            reason = api_pb2.APP_DISCONNECT_REASON_LOCAL_EXCEPTION
    else:
        reason = api_pb2.APP_DISCONNECT_REASON_ENTRYPOINT_COMPLETED
    if isinstance(exc_info, _CliUserExecutionError):
        exc_str = repr(exc_info.__cause__)
    elif exc_info:
        exc_str = repr(exc_info)
    else:
        exc_str = ""

    await _disconnect(client, app_id, reason, exc_str)


@asynccontextmanager
async def _run_app(
    app: _App,
    *,
    client: Optional[_Client] = None,
    detach: bool = False,
    environment_name: Optional[str] = None,
    interactive: bool = False,
) -> AsyncGenerator[_App, None]:
    """mdmd:hidden"""
    if environment_name is None:
        environment_name = typing.cast(str, config.get("environment"))

    if not is_local():
        raise InvalidError(
            "Can not run an app from within a container."
            " Are you calling app.run() directly?"
            " Consider using the `modal run` shell command."
        )
    if app._running_app:
        raise InvalidError(
            "App is already running and can't be started again.\n"
            "You should not use `app.run` or `run_app` within a Modal `local_entrypoint`"
        )

    if app.description is None:
        import __main__

        if "__file__" in dir(__main__):
            app.set_description(os.path.basename(__main__.__file__))
        else:
            # Interactive mode does not have __file__.
            # https://docs.python.org/3/library/__main__.html#import-main
            app.set_description(__main__.__name__)

    if client is None:
        client = await _Client.from_env()

    app_state = api_pb2.APP_STATE_DETACHED if detach else api_pb2.APP_STATE_EPHEMERAL

    output_mgr = _get_output_manager()
    if interactive and output_mgr is None:
        warnings.warn(
            "Interactive mode is disabled because no output manager is active. "
            "Use 'with modal.enable_output():' to enable interactive mode and see logs.",
            stacklevel=2,
        )
        interactive = False

    running_app: RunningApp = await _init_local_app_new(
        client,
        app.description or "",
        environment_name=environment_name or "",
        app_state=app_state,
        interactive=interactive,
    )

    logs_timeout = config["logs_timeout"]
    async with app._set_local_app(client, running_app), TaskContext(grace=logs_timeout) as tc:
        # Start heartbeats loop to keep the client alive
        # we don't log heartbeat exceptions in detached mode
        # as losing the local connection will not affect the running app
        def heartbeat():
            return _heartbeat(client, running_app.app_id)

        tc.infinite_loop(heartbeat, sleep=HEARTBEAT_INTERVAL, log_exception=not detach)
        logs_loop: Optional[asyncio.Task] = None

        if output_mgr is not None:
            # Defer import so this module is rich-safe
            # TODO(michael): The get_app_logs_loop function is itself rich-safe aside from accepting an OutputManager
            # as an argument, so with some refactoring we could avoid the need for this deferred import.
            from modal._output import get_app_logs_loop

            with output_mgr.make_live(output_mgr.step_progress("Initializing...")):
                initialized_msg = (
                    f"Initialized. [grey70]View run at [underline]{running_app.app_page_url}[/underline][/grey70]"
                )
                output_mgr.print(output_mgr.step_completed(initialized_msg))
                output_mgr.update_app_page_url(running_app.app_page_url or "ERROR:NO_APP_PAGE")

            # Start logs loop

            logs_loop = tc.create_task(
                get_app_logs_loop(client, output_mgr, app_id=running_app.app_id, app_logs_url=running_app.app_logs_url)
            )

        try:
            # Create all members
            await _create_all_objects(client, running_app, app._functions, app._classes, environment_name)

            # Publish the app
            await _publish_app(client, running_app, app_state, app._functions, app._classes)
        except asyncio.CancelledError as e:
            # this typically happens on sigint/ctrl-C during setup (the KeyboardInterrupt happens in the main thread)
            if output_mgr := _get_output_manager():
                output_mgr.print("Aborting app initialization...\n")

            await _status_based_disconnect(client, running_app.app_id, e)
            raise
        except BaseException as e:
            await _status_based_disconnect(client, running_app.app_id, e)
            raise

        try:
            # Show logs from dynamically created images.
            # TODO: better way to do this
            if output_mgr := _get_output_manager():
                output_mgr.enable_image_logs()

            # Yield to context
            if output_mgr := _get_output_manager():
                with output_mgr.show_status_spinner():
                    yield app
            else:
                yield app
            # successful completion!
            await _status_based_disconnect(client, running_app.app_id, exc_info=None)
        except KeyboardInterrupt as e:
            # this happens only if sigint comes in during the yield block above
            if detach:
                if output_mgr := _get_output_manager():
                    output_mgr.print(output_mgr.step_completed("Shutting down Modal client."))
                    output_mgr.print(
                        "The detached app keeps running. You can track its progress at: "
                        f"[magenta]{running_app.app_page_url}[/magenta]"
                        ""
                    )
                if logs_loop:
                    logs_loop.cancel()
                await _status_based_disconnect(client, running_app.app_id, e)
            else:
                if output_mgr := _get_output_manager():
                    output_mgr.print(
                        "Disconnecting from Modal - This will terminate your Modal app in a few seconds.\n"
                    )
                await _status_based_disconnect(client, running_app.app_id, e)
                if logs_loop:
                    try:
                        await asyncio.wait_for(logs_loop, timeout=logs_timeout)
                    except asyncio.TimeoutError:
                        logger.warning("Timed out waiting for final app logs.")

                if output_mgr:
                    output_mgr.print(
                        output_mgr.step_completed(
                            "App aborted. "
                            f"[grey70]View run at [underline]{running_app.app_page_url}[/underline][/grey70]"
                        )
                    )
            return
        except BaseException as e:
            logger.info("Exception during app run")
            await _status_based_disconnect(client, running_app.app_id, e)
            raise

        # wait for logs gracefully, even though the task context would do the same
        # this allows us to log a more specific warning in case the app doesn't
        # provide all logs before exit
        if logs_loop:
            try:
                await asyncio.wait_for(logs_loop, timeout=logs_timeout)
            except asyncio.TimeoutError:
                logger.warning("Timed out waiting for final app logs.")

    if output_mgr := _get_output_manager():
        output_mgr.print(
            output_mgr.step_completed(
                f"App completed. [grey70]View run at [underline]{running_app.app_page_url}[/underline][/grey70]"
            )
        )


async def _serve_update(
    app: _App,
    existing_app_id: str,
    is_ready: Event,
    environment_name: str,
) -> None:
    """mdmd:hidden"""
    # Used by child process to reinitialize a served app
    client = await _Client.from_env()
    try:
        running_app: RunningApp = await _init_local_app_existing(client, existing_app_id, environment_name)

        # Create objects
        await _create_all_objects(
            client,
            running_app,
            app._functions,
            app._classes,
            environment_name,
        )

        # Publish the updated app
        await _publish_app(client, running_app, api_pb2.APP_STATE_UNSPECIFIED, app._functions, app._classes)

        # Communicate to the parent process
        is_ready.set()
    except asyncio.exceptions.CancelledError:
        # Stopped by parent process
        pass


@dataclasses.dataclass(frozen=True)
class DeployResult:
    """Dataclass representing the result of deploying an app."""

    app_id: str
    app_page_url: str
    app_logs_url: str
    warnings: list[str]


async def _deploy_app(
    app: _App,
    name: Optional[str] = None,
    namespace: Any = api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
    tag: str = "",
) -> DeployResult:
    """Deploy an app and export its objects persistently.

    Typically, using the command-line tool `modal deploy <module or script>`
    should be used, instead of this method.

    **Usage:**

    ```python
    if __name__ == "__main__":
        deploy_app(app)
    ```

    Deployment has two primary purposes:

    * Persists all of the objects in the app, allowing them to live past the
      current app run. For schedules this enables headless "cron"-like
      functionality where scheduled functions continue to be invoked after
      the client has disconnected.
    * Allows for certain kinds of these objects, _deployment objects_, to be
      referred to and used by other apps.
    """
    if environment_name is None:
        environment_name = typing.cast(str, config.get("environment"))

    name = name or app.name
    if not name:
        raise InvalidError(
            "You need to either supply an explicit deployment name to the deploy command, "
            "or have a name set on the app.\n"
            "\n"
            "Examples:\n"
            'app.deploy("some_name")\n\n'
            "or\n"
            'app = App("some-name")'
        )
    else:
        check_object_name(name, "App")

    if tag and not is_valid_tag(tag):
        raise InvalidError(
            f"Deployment tag {tag!r} is invalid."
            "\n\nTags may only contain alphanumeric characters, dashes, periods, and underscores, "
            "and must be 50 characters or less"
        )

    if client is None:
        client = await _Client.from_env()

    t0 = time.time()

    running_app: RunningApp = await _init_local_app_from_name(
        client, name, namespace, environment_name=environment_name
    )

    async with TaskContext(0) as tc:
        # Start heartbeats loop to keep the client alive
        def heartbeat():
            return _heartbeat(client, running_app.app_id)

        tc.infinite_loop(heartbeat, sleep=HEARTBEAT_INTERVAL)

        try:
            # Create all members
            await _create_all_objects(
                client,
                running_app,
                app._functions,
                app._classes,
                environment_name=environment_name,
            )

            app_url, warnings = await _publish_app(
                client, running_app, api_pb2.APP_STATE_DEPLOYED, app._functions, app._classes, name, tag
            )
        except Exception as e:
            # Note that AppClientDisconnect only stops the app if it's still initializing, and is a no-op otherwise.
            await _disconnect(client, running_app.app_id, reason=api_pb2.APP_DISCONNECT_REASON_DEPLOYMENT_EXCEPTION)
            raise e

    if output_mgr := _get_output_manager():
        t = time.time() - t0
        output_mgr.print(output_mgr.step_completed(f"App deployed in {t:.3f}s! 🎉"))
        output_mgr.print(f"\nView Deployment: [magenta]{app_url}[/magenta]")
    return DeployResult(
        app_id=running_app.app_id,
        app_page_url=running_app.app_page_url,
        app_logs_url=running_app.app_logs_url,  # type: ignore
        warnings=[warning.message for warning in warnings],
    )


async def _interactive_shell(
    _app: _App, cmds: list[str], environment_name: str = "", pty: bool = True, **kwargs: Any
) -> None:
    """Run an interactive shell (like `bash`) within the image for this app.

    This is useful for online debugging and interactive exploration of the
    contents of this image. If `cmd` is optionally provided, it will be run
    instead of the default shell inside this image.

    **Example**

    ```python
    import modal

    app = modal.App(image=modal.Image.debian_slim().apt_install("vim"))
    ```

    You can now run this using

    ```
    modal shell script.py --cmd /bin/bash
    ```

    When calling programmatically, `kwargs` are passed to `Sandbox.create()`.
    """

    client = await _Client.from_env()
    async with _run_app(_app, client=client, environment_name=environment_name):
        sandbox_cmds = cmds if len(cmds) > 0 else ["/bin/bash"]
        sandbox_env = {
            "MODAL_TOKEN_ID": config["token_id"],
            "MODAL_TOKEN_SECRET": config["token_secret"],
            "MODAL_ENVIRONMENT": _get_environment_name(),
        }
        secrets = kwargs.pop("secrets", []) + [_Secret.from_dict(sandbox_env)]
        with enable_output():  # show any image build logs
            sandbox = await _Sandbox.create(
                "sleep",
                "100000",
                app=_app,
                secrets=secrets,
                **kwargs,
            )

        try:
            if pty:
                container_process = await sandbox.exec(
                    *sandbox_cmds, pty_info=get_pty_info(shell=True) if pty else None
                )
                await container_process.attach()
            else:
                container_process = await sandbox.exec(
                    *sandbox_cmds, stdout=StreamType.STDOUT, stderr=StreamType.STDOUT
                )
                await container_process.wait()
        except InteractiveTimeoutError:
            # Check on status of Sandbox. It may have crashed, causing connection failure.
            req = api_pb2.SandboxWaitRequest(sandbox_id=sandbox._object_id, timeout=0)
            resp = await retry_transient_errors(sandbox._client.stub.SandboxWait, req)
            if resp.result.exception:
                raise RemoteError(resp.result.exception)
            else:
                raise


def _run_stub(*args: Any, **kwargs: Any):
    """mdmd:hidden
    `run_stub` has been renamed to `run_app` and is deprecated. Please update your code.
    """
    deprecation_error(
        (2024, 5, 1), "`run_stub` has been renamed to `run_app` and is deprecated. Please update your code."
    )


def _deploy_stub(*args: Any, **kwargs: Any):
    """mdmd:hidden"""
    message = "`deploy_stub` has been renamed to `deploy_app`. Please update your code."
    deprecation_error((2024, 5, 1), message)


run_app = synchronize_api(_run_app)
serve_update = synchronize_api(_serve_update)
deploy_app = synchronize_api(_deploy_app)
interactive_shell = synchronize_api(_interactive_shell)
run_stub = synchronize_api(_run_stub)
deploy_stub = synchronize_api(_deploy_stub)


================================================
File: modal/running_app.py
================================================
# Copyright Modal Labs 2024
from dataclasses import dataclass, field
from typing import Optional

from google.protobuf.message import Message

from modal._utils.grpc_utils import get_proto_oneof
from modal_proto import api_pb2


@dataclass
class RunningApp:
    app_id: str
    app_page_url: Optional[str] = None
    app_logs_url: Optional[str] = None
    function_ids: dict[str, str] = field(default_factory=dict)
    class_ids: dict[str, str] = field(default_factory=dict)
    object_handle_metadata: dict[str, Optional[Message]] = field(default_factory=dict)
    interactive: bool = False


def running_app_from_layout(
    app_id: str,
    app_layout: api_pb2.AppLayout,
    app_page_url: Optional[str] = None,
) -> RunningApp:
    object_handle_metadata = {}
    for obj in app_layout.objects:
        handle_metadata: Optional[Message] = get_proto_oneof(obj, "handle_metadata_oneof")
        object_handle_metadata[obj.object_id] = handle_metadata

    return RunningApp(
        app_id,
        function_ids=dict(app_layout.function_ids),
        class_ids=dict(app_layout.class_ids),
        object_handle_metadata=object_handle_metadata,
        app_page_url=app_page_url,
    )


================================================
File: modal/sandbox.py
================================================
# Copyright Modal Labs 2022
import asyncio
import os
from collections.abc import AsyncGenerator, Sequence
from typing import TYPE_CHECKING, AsyncIterator, Literal, Optional, Union, overload

if TYPE_CHECKING:
    import _typeshed

from google.protobuf.message import Message
from grpclib import GRPCError, Status

from modal._tunnel import Tunnel
from modal.cloud_bucket_mount import _CloudBucketMount, cloud_bucket_mounts_to_proto
from modal.volume import _Volume
from modal_proto import api_pb2

from ._object import _get_environment_name, _Object
from ._resolver import Resolver
from ._resources import convert_fn_config_to_resources_config
from ._utils.async_utils import TaskContext, synchronize_api
from ._utils.deprecation import deprecation_error
from ._utils.grpc_utils import retry_transient_errors
from ._utils.mount_utils import validate_network_file_systems, validate_volumes
from .client import _Client
from .config import config
from .container_process import _ContainerProcess
from .exception import ExecutionError, InvalidError, SandboxTerminatedError, SandboxTimeoutError
from .file_io import FileWatchEvent, FileWatchEventType, _FileIO
from .gpu import GPU_T
from .image import _Image
from .io_streams import StreamReader, StreamWriter, _StreamReader, _StreamWriter
from .mount import _Mount
from .network_file_system import _NetworkFileSystem, network_file_system_mount_protos
from .proxy import _Proxy
from .scheduler_placement import SchedulerPlacement
from .secret import _Secret
from .snapshot import _SandboxSnapshot
from .stream_type import StreamType

_default_image: _Image = _Image.debian_slim()


# The maximum number of bytes that can be passed to an exec on Linux.
# Though this is technically a 'server side' limit, it is unlikely to change.
# getconf ARG_MAX will show this value on a host.
ARG_MAX_BYTES = 2_097_152  # 2MiB

if TYPE_CHECKING:
    import modal.app


def _validate_exec_args(entrypoint_args: Sequence[str]) -> None:
    # Entrypoint args must be strings.
    if not all(isinstance(arg, str) for arg in entrypoint_args):
        raise InvalidError("All entrypoint arguments must be strings")
    # Avoid "[Errno 7] Argument list too long" errors.
    total_arg_len = sum(len(arg) for arg in entrypoint_args)
    if total_arg_len > ARG_MAX_BYTES:
        raise InvalidError(
            f"Total length of entrypoint arguments must be less than {ARG_MAX_BYTES} bytes (ARG_MAX). "
            f"Got {total_arg_len} bytes."
        )


class _Sandbox(_Object, type_prefix="sb"):
    """A `Sandbox` object lets you interact with a running sandbox. This API is similar to Python's
    [asyncio.subprocess.Process](https://docs.python.org/3/library/asyncio-subprocess.html#asyncio.subprocess.Process).

    Refer to the [guide](/docs/guide/sandbox) on how to spawn and use sandboxes.
    """

    _result: Optional[api_pb2.GenericResult]
    _stdout: _StreamReader[str]
    _stderr: _StreamReader[str]
    _stdin: _StreamWriter
    _task_id: Optional[str] = None
    _tunnels: Optional[dict[int, Tunnel]] = None
    _enable_snapshot: bool = False

    @staticmethod
    def _new(
        entrypoint_args: Sequence[str],
        image: _Image,
        mounts: Sequence[_Mount],
        secrets: Sequence[_Secret],
        timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        gpu: GPU_T = None,
        cloud: Optional[str] = None,
        region: Optional[Union[str, Sequence[str]]] = None,
        cpu: Optional[float] = None,
        memory: Optional[Union[int, tuple[int, int]]] = None,
        network_file_systems: dict[Union[str, os.PathLike], _NetworkFileSystem] = {},
        block_network: bool = False,
        cidr_allowlist: Optional[Sequence[str]] = None,
        volumes: dict[Union[str, os.PathLike], Union[_Volume, _CloudBucketMount]] = {},
        pty_info: Optional[api_pb2.PTYInfo] = None,
        encrypted_ports: Sequence[int] = [],
        unencrypted_ports: Sequence[int] = [],
        proxy: Optional[_Proxy] = None,
        _experimental_scheduler_placement: Optional[SchedulerPlacement] = None,
        enable_snapshot: bool = False,
    ) -> "_Sandbox":
        """mdmd:hidden"""

        if len(entrypoint_args) == 0:
            raise InvalidError("entrypoint_args must not be empty")

        validated_network_file_systems = validate_network_file_systems(network_file_systems)

        scheduler_placement: Optional[SchedulerPlacement] = _experimental_scheduler_placement
        if region:
            if scheduler_placement:
                raise InvalidError("`region` and `_experimental_scheduler_placement` cannot be used together")
            scheduler_placement = SchedulerPlacement(region=region)

        if isinstance(gpu, list):
            raise InvalidError(
                "Sandboxes do not support configuring a list of GPUs. "
                "Specify a single GPU configuration, e.g. gpu='a10g'"
            )

        if workdir is not None and not workdir.startswith("/"):
            raise InvalidError(f"workdir must be an absolute path, got: {workdir}")

        # Validate volumes
        validated_volumes = validate_volumes(volumes)
        cloud_bucket_mounts = [(k, v) for k, v in validated_volumes if isinstance(v, _CloudBucketMount)]
        validated_volumes = [(k, v) for k, v in validated_volumes if isinstance(v, _Volume)]

        def _deps() -> list[_Object]:
            deps: list[_Object] = [image] + list(mounts) + list(secrets)
            for _, vol in validated_network_file_systems:
                deps.append(vol)
            for _, vol in validated_volumes:
                deps.append(vol)
            for _, cloud_bucket_mount in cloud_bucket_mounts:
                if cloud_bucket_mount.secret:
                    deps.append(cloud_bucket_mount.secret)
            if proxy:
                deps.append(proxy)
            return deps

        async def _load(self: _Sandbox, resolver: Resolver, _existing_object_id: Optional[str]):
            # Relies on dicts being ordered (true as of Python 3.6).
            volume_mounts = [
                api_pb2.VolumeMount(
                    mount_path=path,
                    volume_id=volume.object_id,
                    allow_background_commits=True,
                )
                for path, volume in validated_volumes
            ]

            open_ports = [api_pb2.PortSpec(port=port, unencrypted=False) for port in encrypted_ports]
            open_ports.extend([api_pb2.PortSpec(port=port, unencrypted=True) for port in unencrypted_ports])

            if block_network:
                # If the network is blocked, cidr_allowlist is invalid as we don't allow any network access.
                if cidr_allowlist is not None:
                    raise InvalidError("`cidr_allowlist` cannot be used when `block_network` is enabled")
                network_access = api_pb2.NetworkAccess(
                    network_access_type=api_pb2.NetworkAccess.NetworkAccessType.BLOCKED,
                )
            elif cidr_allowlist is None:
                # If the allowlist is empty, we allow all network access.
                network_access = api_pb2.NetworkAccess(
                    network_access_type=api_pb2.NetworkAccess.NetworkAccessType.OPEN,
                )
            else:
                network_access = api_pb2.NetworkAccess(
                    network_access_type=api_pb2.NetworkAccess.NetworkAccessType.ALLOWLIST,
                    allowed_cidrs=cidr_allowlist,
                )

            ephemeral_disk = None  # Ephemeral disk requests not supported on Sandboxes.
            definition = api_pb2.Sandbox(
                entrypoint_args=entrypoint_args,
                image_id=image.object_id,
                mount_ids=[mount.object_id for mount in mounts] + [mount.object_id for mount in image._mount_layers],
                secret_ids=[secret.object_id for secret in secrets],
                timeout_secs=timeout,
                workdir=workdir,
                resources=convert_fn_config_to_resources_config(
                    cpu=cpu, memory=memory, gpu=gpu, ephemeral_disk=ephemeral_disk
                ),
                cloud_provider_str=cloud if cloud else None,  # Supersedes cloud_provider
                nfs_mounts=network_file_system_mount_protos(validated_network_file_systems, False),
                runtime_debug=config.get("function_runtime_debug"),
                cloud_bucket_mounts=cloud_bucket_mounts_to_proto(cloud_bucket_mounts),
                volume_mounts=volume_mounts,
                pty_info=pty_info,
                scheduler_placement=scheduler_placement.proto if scheduler_placement else None,
                worker_id=config.get("worker_id"),
                open_ports=api_pb2.PortSpecs(ports=open_ports),
                network_access=network_access,
                proxy_id=(proxy.object_id if proxy else None),
                enable_snapshot=enable_snapshot,
            )

            # Note - `resolver.app_id` will be `None` for app-less sandboxes
            create_req = api_pb2.SandboxCreateRequest(
                app_id=resolver.app_id, definition=definition, environment_name=resolver.environment_name
            )
            create_resp = await retry_transient_errors(resolver.client.stub.SandboxCreate, create_req)

            sandbox_id = create_resp.sandbox_id
            self._hydrate(sandbox_id, resolver.client, None)

        return _Sandbox._from_loader(_load, "Sandbox()", deps=_deps)

    @staticmethod
    async def create(
        *entrypoint_args: str,
        app: Optional["modal.app._App"] = None,  # Optionally associate the sandbox with an app
        environment_name: Optional[str] = None,  # Optionally override the default environment
        image: Optional[_Image] = None,  # The image to run as the container for the sandbox.
        mounts: Sequence[_Mount] = (),  # Mounts to attach to the sandbox.
        secrets: Sequence[_Secret] = (),  # Environment variables to inject into the sandbox.
        network_file_systems: dict[Union[str, os.PathLike], _NetworkFileSystem] = {},
        timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.
        workdir: Optional[str] = None,  # Working directory of the sandbox.
        gpu: GPU_T = None,
        cloud: Optional[str] = None,
        region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the sandbox on.
        # Specify, in fractional CPU cores, how many CPU cores to request.
        # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
        # CPU throttling will prevent a container from exceeding its specified limit.
        cpu: Optional[Union[float, tuple[float, float]]] = None,
        # Specify, in MiB, a memory request which is the minimum memory required.
        # Or, pass (request, limit) to additionally specify a hard limit in MiB.
        memory: Optional[Union[int, tuple[int, int]]] = None,
        block_network: bool = False,  # Whether to block network access
        # List of CIDRs the sandbox is allowed to access. If None, all CIDRs are allowed.
        cidr_allowlist: Optional[Sequence[str]] = None,
        volumes: dict[
            Union[str, os.PathLike], Union[_Volume, _CloudBucketMount]
        ] = {},  # Mount points for Modal Volumes and CloudBucketMounts
        pty_info: Optional[api_pb2.PTYInfo] = None,
        # List of ports to tunnel into the sandbox. Encrypted ports are tunneled with TLS.
        encrypted_ports: Sequence[int] = [],
        # List of ports to tunnel into the sandbox without encryption.
        unencrypted_ports: Sequence[int] = [],
        # Reference to a Modal Proxy to use in front of this Sandbox.
        proxy: Optional[_Proxy] = None,
        # Enable memory snapshots.
        _experimental_enable_snapshot: bool = False,
        _experimental_scheduler_placement: Optional[
            SchedulerPlacement
        ] = None,  # Experimental controls over fine-grained scheduling (alpha).
        client: Optional[_Client] = None,
    ) -> "_Sandbox":
        from .app import _App

        environment_name = _get_environment_name(environment_name)

        # If there are no entrypoint args, we'll sleep forever so that the sandbox will stay
        # alive long enough for the user to interact with it.
        if len(entrypoint_args) == 0:
            max_sleep_time = 60 * 60 * 24 * 2  # 2 days is plenty since workers roll every 24h
            entrypoint_args = ("sleep", str(max_sleep_time))

        _validate_exec_args(entrypoint_args)

        # TODO(erikbern): Get rid of the `_new` method and create an already-hydrated object
        obj = _Sandbox._new(
            entrypoint_args,
            image=image or _default_image,
            mounts=mounts,
            secrets=secrets,
            timeout=timeout,
            workdir=workdir,
            gpu=gpu,
            cloud=cloud,
            region=region,
            cpu=cpu,
            memory=memory,
            network_file_systems=network_file_systems,
            block_network=block_network,
            cidr_allowlist=cidr_allowlist,
            volumes=volumes,
            pty_info=pty_info,
            encrypted_ports=encrypted_ports,
            unencrypted_ports=unencrypted_ports,
            proxy=proxy,
            _experimental_scheduler_placement=_experimental_scheduler_placement,
            enable_snapshot=_experimental_enable_snapshot,
        )
        obj._enable_snapshot = _experimental_enable_snapshot

        app_id: Optional[str] = None
        app_client: Optional[_Client] = None

        if app is not None:
            if app.app_id is None:
                raise ValueError(
                    "App has not been initialized yet. To create an App lazily, use `App.lookup`: \n"
                    "app = modal.App.lookup('my-app', create_if_missing=True)\n"
                    "modal.Sandbox.create('echo', 'hi', app=app)\n"
                    "In order to initialize an existing `App` object, refer to our docs: https://modal.com/docs/guide/apps"
                )

            app_id = app.app_id
            app_client = app._client
        elif (container_app := _App._get_container_app()) is not None:
            app_id = container_app.app_id
            app_client = container_app._client
        else:
            arglist = ", ".join(repr(s) for s in entrypoint_args)
            deprecation_error(
                (2024, 9, 14),
                "Creating a `Sandbox` without an `App` is deprecated.\n\n"
                "You may pass in an `App` object, or reference one by name with `App.lookup`:\n\n"
                "```\n"
                "app = modal.App.lookup('sandbox-app', create_if_missing=True)\n"
                f"sb = modal.Sandbox.create({arglist}, app=app)\n"
                "```",
            )

        client = client or app_client or await _Client.from_env()

        resolver = Resolver(client, environment_name=environment_name, app_id=app_id)
        await resolver.load(obj)
        return obj

    def _hydrate_metadata(self, handle_metadata: Optional[Message]):
        self._stdout: _StreamReader[str] = StreamReader[str](
            api_pb2.FILE_DESCRIPTOR_STDOUT, self.object_id, "sandbox", self._client, by_line=True
        )
        self._stderr: _StreamReader[str] = StreamReader[str](
            api_pb2.FILE_DESCRIPTOR_STDERR, self.object_id, "sandbox", self._client, by_line=True
        )
        self._stdin = StreamWriter(self.object_id, "sandbox", self._client)
        self._result = None

    @staticmethod
    async def from_id(sandbox_id: str, client: Optional[_Client] = None) -> "_Sandbox":
        """Construct a Sandbox from an id and look up the Sandbox result.

        The ID of a Sandbox object can be accessed using `.object_id`.
        """
        if client is None:
            client = await _Client.from_env()

        req = api_pb2.SandboxWaitRequest(sandbox_id=sandbox_id, timeout=0)
        resp = await retry_transient_errors(client.stub.SandboxWait, req)

        obj = _Sandbox._new_hydrated(sandbox_id, client, None)

        if resp.result.status:
            obj._result = resp.result

        return obj

    async def set_tags(self, tags: dict[str, str], *, client: Optional[_Client] = None):
        """Set tags (key-value pairs) on the Sandbox. Tags can be used to filter results in `Sandbox.list`."""
        environment_name = _get_environment_name()
        if client is None:
            client = await _Client.from_env()

        tags_list = [api_pb2.SandboxTag(tag_name=name, tag_value=value) for name, value in tags.items()]

        req = api_pb2.SandboxTagsSetRequest(
            environment_name=environment_name,
            sandbox_id=self.object_id,
            tags=tags_list,
        )
        try:
            await retry_transient_errors(client.stub.SandboxTagsSet, req)
        except GRPCError as exc:
            raise InvalidError(exc.message) if exc.status == Status.INVALID_ARGUMENT else exc

    async def snapshot_filesystem(self, timeout: int = 55) -> _Image:
        """Snapshot the filesystem of the Sandbox.

        Returns an [`Image`](https://modal.com/docs/reference/modal.Image) object which
        can be used to spawn a new Sandbox with the same filesystem.
        """
        await self._get_task_id()  # Ensure the sandbox has started
        req = api_pb2.SandboxSnapshotFsRequest(sandbox_id=self.object_id, timeout=timeout)
        resp = await retry_transient_errors(self._client.stub.SandboxSnapshotFs, req)

        if resp.result.status != api_pb2.GenericResult.GENERIC_STATUS_SUCCESS:
            raise ExecutionError(resp.result.exception)

        image_id = resp.image_id
        metadata = resp.image_metadata

        async def _load(self: _Image, resolver: Resolver, existing_object_id: Optional[str]):
            # no need to hydrate again since we do it eagerly below
            pass

        rep = "Image()"
        image = _Image._from_loader(_load, rep, hydrate_lazily=True)
        image._hydrate(image_id, self._client, metadata)  # hydrating eagerly since we have all of the data

        return image

    # Live handle methods

    async def wait(self, raise_on_termination: bool = True):
        """Wait for the Sandbox to finish running."""

        while True:
            req = api_pb2.SandboxWaitRequest(sandbox_id=self.object_id, timeout=50)
            resp = await retry_transient_errors(self._client.stub.SandboxWait, req)
            if resp.result.status:
                self._result = resp.result

                if resp.result.status == api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT:
                    raise SandboxTimeoutError()
                elif resp.result.status == api_pb2.GenericResult.GENERIC_STATUS_TERMINATED and raise_on_termination:
                    raise SandboxTerminatedError()
                break

    async def tunnels(self, timeout: int = 50) -> dict[int, Tunnel]:
        """Get tunnel metadata for the sandbox.

        Raises `SandboxTimeoutError` if the tunnels are not available after the timeout.

        Returns a dictionary of `Tunnel` objects which are keyed by the container port.

        NOTE: Previous to client v0.64.152, this returned a list of `TunnelData` objects.
        """

        if self._tunnels:
            return self._tunnels

        req = api_pb2.SandboxGetTunnelsRequest(sandbox_id=self.object_id, timeout=timeout)
        resp = await retry_transient_errors(self._client.stub.SandboxGetTunnels, req)

        # If we couldn't get the tunnels in time, report the timeout.
        if resp.result.status == api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT:
            raise SandboxTimeoutError()

        # Otherwise, we got the tunnels and can report the result.
        self._tunnels = {
            t.container_port: Tunnel(t.host, t.port, t.unencrypted_host, t.unencrypted_port) for t in resp.tunnels
        }

        return self._tunnels

    async def terminate(self):
        """Terminate Sandbox execution.

        This is a no-op if the Sandbox has already finished running."""

        await retry_transient_errors(
            self._client.stub.SandboxTerminate, api_pb2.SandboxTerminateRequest(sandbox_id=self.object_id)
        )
        await self.wait(raise_on_termination=False)

    async def poll(self) -> Optional[int]:
        """Check if the Sandbox has finished running.

        Returns `None` if the Sandbox is still running, else returns the exit code.
        """

        req = api_pb2.SandboxWaitRequest(sandbox_id=self.object_id, timeout=0)
        resp = await retry_transient_errors(self._client.stub.SandboxWait, req)

        if resp.result.status:
            self._result = resp.result

        return self.returncode

    async def _get_task_id(self):
        while not self._task_id:
            resp = await self._client.stub.SandboxGetTaskId(api_pb2.SandboxGetTaskIdRequest(sandbox_id=self.object_id))
            self._task_id = resp.task_id
            if not self._task_id:
                await asyncio.sleep(0.5)
        return self._task_id

    @overload
    async def exec(
        self,
        *cmds: str,
        pty_info: Optional[api_pb2.PTYInfo] = None,
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        secrets: Sequence[_Secret] = (),
        text: Literal[True] = True,
        bufsize: Literal[-1, 1] = -1,
        _pty_info: Optional[api_pb2.PTYInfo] = None,
    ) -> _ContainerProcess[str]: ...

    @overload
    async def exec(
        self,
        *cmds: str,
        pty_info: Optional[api_pb2.PTYInfo] = None,
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        secrets: Sequence[_Secret] = (),
        text: Literal[False] = False,
        bufsize: Literal[-1, 1] = -1,
        _pty_info: Optional[api_pb2.PTYInfo] = None,
    ) -> _ContainerProcess[bytes]: ...

    async def exec(
        self,
        *cmds: str,
        pty_info: Optional[api_pb2.PTYInfo] = None,  # Deprecated: internal use only
        stdout: StreamType = StreamType.PIPE,
        stderr: StreamType = StreamType.PIPE,
        timeout: Optional[int] = None,
        workdir: Optional[str] = None,
        secrets: Sequence[_Secret] = (),
        # Encode output as text.
        text: bool = True,
        # Control line-buffered output.
        # -1 means unbuffered, 1 means line-buffered (only available if `text=True`).
        bufsize: Literal[-1, 1] = -1,
        # Internal option to set terminal size and metadata
        _pty_info: Optional[api_pb2.PTYInfo] = None,
    ):
        """Execute a command in the Sandbox and return a ContainerProcess handle.

        See the [`ContainerProcess`](/docs/reference/modal.container_process#modalcontainer_processcontainerprocess)
        docs for more information.

        **Usage**

        ```python
        app = modal.App.lookup("my-app", create_if_missing=True)

        sandbox = modal.Sandbox.create("sleep", "infinity", app=app)

        process = sandbox.exec("bash", "-c", "for i in $(seq 1 10); do echo foo $i; sleep 0.5; done")

        for line in process.stdout:
            print(line)
        ```
        """

        if workdir is not None and not workdir.startswith("/"):
            raise InvalidError(f"workdir must be an absolute path, got: {workdir}")
        _validate_exec_args(cmds)

        # Force secret resolution so we can pass the secret IDs to the backend.
        secret_coros = [secret.hydrate(client=self._client) for secret in secrets]
        await TaskContext.gather(*secret_coros)

        task_id = await self._get_task_id()
        req = api_pb2.ContainerExecRequest(
            task_id=task_id,
            command=cmds,
            pty_info=_pty_info or pty_info,
            runtime_debug=config.get("function_runtime_debug"),
            timeout_secs=timeout or 0,
            workdir=workdir,
            secret_ids=[secret.object_id for secret in secrets],
        )
        resp = await retry_transient_errors(self._client.stub.ContainerExec, req)
        by_line = bufsize == 1
        return _ContainerProcess(resp.exec_id, self._client, stdout=stdout, stderr=stderr, text=text, by_line=by_line)

    async def _experimental_snapshot(self) -> _SandboxSnapshot:
        await self._get_task_id()
        snap_req = api_pb2.SandboxSnapshotRequest(sandbox_id=self.object_id)
        snap_resp = await retry_transient_errors(self._client.stub.SandboxSnapshot, snap_req)

        snapshot_id = snap_resp.snapshot_id

        # wait for the snapshot to succeed. this is implemented as a second idempotent rpc
        # because the snapshot itself may take a while to complete.
        wait_req = api_pb2.SandboxSnapshotWaitRequest(snapshot_id=snapshot_id, timeout=55.0)
        wait_resp = await retry_transient_errors(self._client.stub.SandboxSnapshotWait, wait_req)
        if wait_resp.result.status != api_pb2.GenericResult.GENERIC_STATUS_SUCCESS:
            raise ExecutionError(wait_resp.result.exception)

        async def _load(self: _SandboxSnapshot, resolver: Resolver, existing_object_id: Optional[str]):
            # we eagerly hydrate the sandbox snapshot below
            pass

        rep = "SandboxSnapshot()"
        obj = _SandboxSnapshot._from_loader(_load, rep, hydrate_lazily=True)
        obj._hydrate(snapshot_id, self._client, None)

        return obj

    @staticmethod
    async def _experimental_from_snapshot(snapshot: _SandboxSnapshot, client: Optional[_Client] = None):
        client = client or await _Client.from_env()

        restore_req = api_pb2.SandboxRestoreRequest(snapshot_id=snapshot.object_id)
        restore_resp: api_pb2.SandboxRestoreResponse = await retry_transient_errors(
            client.stub.SandboxRestore, restore_req
        )
        sandbox = await _Sandbox.from_id(restore_resp.sandbox_id, client)

        task_id_req = api_pb2.SandboxGetTaskIdRequest(
            sandbox_id=restore_resp.sandbox_id, wait_until_ready=True, timeout=55.0
        )
        resp = await retry_transient_errors(client.stub.SandboxGetTaskId, task_id_req)
        if resp.task_result.status not in [
            api_pb2.GenericResult.GENERIC_STATUS_UNSPECIFIED,
            api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
        ]:
            raise ExecutionError(resp.task_result.exception)
        return sandbox

    @overload
    async def open(
        self,
        path: str,
        mode: "_typeshed.OpenTextMode",
    ) -> _FileIO[str]: ...

    @overload
    async def open(
        self,
        path: str,
        mode: "_typeshed.OpenBinaryMode",
    ) -> _FileIO[bytes]: ...

    async def open(
        self,
        path: str,
        mode: Union["_typeshed.OpenTextMode", "_typeshed.OpenBinaryMode"] = "r",
    ):
        """Open a file in the Sandbox and return a FileIO handle.

        See the [`FileIO`](/docs/reference/modal.file_io#modalfile_iofileio) docs for more information.

        **Usage**

        ```python notest
        sb = modal.Sandbox.create(app=sb_app)
        f = sb.open("/test.txt", "w")
        f.write("hello")
        f.close()
        ```
        """
        task_id = await self._get_task_id()
        return await _FileIO.create(path, mode, self._client, task_id)

    async def ls(self, path: str) -> list[str]:
        """List the contents of a directory in the Sandbox."""
        task_id = await self._get_task_id()
        return await _FileIO.ls(path, self._client, task_id)

    async def mkdir(self, path: str, parents: bool = False) -> None:
        """Create a new directory in the Sandbox."""
        task_id = await self._get_task_id()
        return await _FileIO.mkdir(path, self._client, task_id, parents)

    async def rm(self, path: str, recursive: bool = False) -> None:
        """Remove a file or directory in the Sandbox."""
        task_id = await self._get_task_id()
        return await _FileIO.rm(path, self._client, task_id, recursive)

    async def watch(
        self,
        path: str,
        filter: Optional[list[FileWatchEventType]] = None,
        recursive: Optional[bool] = None,
        timeout: Optional[int] = None,
    ) -> AsyncIterator[FileWatchEvent]:
        task_id = await self._get_task_id()
        async for event in _FileIO.watch(path, self._client, task_id, filter, recursive, timeout):
            yield event

    @property
    def stdout(self) -> _StreamReader[str]:
        """
        [`StreamReader`](/docs/reference/modal.io_streams#modalio_streamsstreamreader) for
        the sandbox's stdout stream.
        """

        return self._stdout

    @property
    def stderr(self) -> _StreamReader[str]:
        """[`StreamReader`](/docs/reference/modal.io_streams#modalio_streamsstreamreader) for
        the sandbox's stderr stream.
        """

        return self._stderr

    @property
    def stdin(self) -> _StreamWriter:
        """
        [`StreamWriter`](/docs/reference/modal.io_streams#modalio_streamsstreamwriter) for
        the sandbox's stdin stream.
        """

        return self._stdin

    @property
    def returncode(self) -> Optional[int]:
        """Return code of the sandbox process if it has finished running, else `None`."""

        if self._result is None:
            return None
        # Statuses are converted to exitcodes so we can conform to subprocess API.
        # TODO: perhaps there should be a separate property that returns an enum directly?
        elif self._result.status == api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT:
            return 124
        elif self._result.status == api_pb2.GenericResult.GENERIC_STATUS_TERMINATED:
            return 137
        else:
            return self._result.exitcode

    @staticmethod
    async def list(
        *, app_id: Optional[str] = None, tags: Optional[dict[str, str]] = None, client: Optional[_Client] = None
    ) -> AsyncGenerator["_Sandbox", None]:
        """List all sandboxes for the current environment or app ID (if specified). If tags are specified, only
        sandboxes that have at least those tags are returned. Returns an iterator over `Sandbox` objects."""
        before_timestamp = None
        environment_name = _get_environment_name()
        if client is None:
            client = await _Client.from_env()

        tags_list = [api_pb2.SandboxTag(tag_name=name, tag_value=value) for name, value in tags.items()] if tags else []

        while True:
            req = api_pb2.SandboxListRequest(
                app_id=app_id,
                before_timestamp=before_timestamp,
                environment_name=environment_name,
                include_finished=False,
                tags=tags_list,
            )

            # Fetches a batch of sandboxes.
            try:
                resp = await retry_transient_errors(client.stub.SandboxList, req)
            except GRPCError as exc:
                raise InvalidError(exc.message) if exc.status == Status.INVALID_ARGUMENT else exc

            if not resp.sandboxes:
                return

            for sandbox_info in resp.sandboxes:
                obj = _Sandbox._new_hydrated(sandbox_info.id, client, None)
                obj._result = sandbox_info.task_info.result
                yield obj

            # Fetch the next batch starting from the end of the current one.
            before_timestamp = resp.sandboxes[-1].created_at


Sandbox = synchronize_api(_Sandbox)


def __getattr__(name):
    if name == "LogsReader":
        deprecation_error(
            (2024, 8, 12),
            "`modal.sandbox.LogsReader` is deprecated. Please import `modal.io_streams.StreamReader` instead.",
        )
        from .io_streams import StreamReader

        return StreamReader
    elif name == "StreamWriter":
        deprecation_error(
            (2024, 8, 12),
            "`modal.sandbox.StreamWriter` is deprecated. Please import `modal.io_streams.StreamWriter` instead.",
        )
        from .io_streams import StreamWriter

        return StreamWriter
    raise AttributeError(f"module {__name__} has no attribute {name}")


================================================
File: modal/schedule.py
================================================
# Copyright Modal Labs 2022
from modal_proto import api_pb2


class Schedule:
    """Schedules represent a time frame to repeatedly run a Modal function."""

    def __init__(self, proto_message):
        self.proto_message = proto_message


class Cron(Schedule):
    """Cron jobs are a type of schedule, specified using the
    [Unix cron tab](https://crontab.guru/) syntax.

    The alternative schedule type is the [`modal.Period`](/docs/reference/modal.Period).

    **Usage**

    ```python
    import modal
    app = modal.App()


    @app.function(schedule=modal.Cron("* * * * *"))
    def f():
        print("This function will run every minute")
    ```

    We can specify different schedules with cron strings, for example:

    ```python
    modal.Cron("5 4 * * *")  # run at 4:05am every night
    modal.Cron("0 9 * * 4")  # runs every Thursday 9am
    ```

    """

    def __init__(self, cron_string: str) -> None:
        """Construct a schedule that runs according to a cron expression string."""
        cron = api_pb2.Schedule.Cron(cron_string=cron_string)
        super().__init__(api_pb2.Schedule(cron=cron))


class Period(Schedule):
    """Create a schedule that runs every given time interval.

    **Usage**

    ```python
    import modal
    app = modal.App()

    @app.function(schedule=modal.Period(days=1))
    def f():
        print("This function will run every day")

    modal.Period(hours=4)          # runs every 4 hours
    modal.Period(minutes=15)       # runs every 15 minutes
    modal.Period(seconds=math.pi)  # runs every 3.141592653589793 seconds
    ```

    Only `seconds` can be a float. All other arguments are integers.

    Note that `days=1` will trigger the function the same time every day.
    This does not have the same behavior as `seconds=84000` since days have
    different lengths due to daylight savings and leap seconds. Similarly,
    using `months=1` will trigger the function on the same day each month.

    This behaves similar to the
    [dateutil](https://dateutil.readthedocs.io/en/latest/relativedelta.html)
    package.
    """

    def __init__(
        self,
        years: int = 0,
        months: int = 0,
        weeks: int = 0,
        days: int = 0,
        hours: int = 0,
        minutes: int = 0,
        seconds: float = 0,
    ) -> None:
        period = api_pb2.Schedule.Period(
            years=years,
            months=months,
            weeks=weeks,
            days=days,
            hours=hours,
            minutes=minutes,
            seconds=seconds,
        )
        super().__init__(api_pb2.Schedule(period=period))


================================================
File: modal/scheduler_placement.py
================================================
# Copyright Modal Labs 2024
from collections.abc import Sequence
from typing import Optional, Union

from modal_proto import api_pb2


class SchedulerPlacement:
    """mdmd:hidden This is an experimental feature."""

    proto: api_pb2.SchedulerPlacement

    def __init__(
        self,
        region: Optional[Union[str, Sequence[str]]] = None,
        zone: Optional[str] = None,
        spot: Optional[bool] = None,
        instance_type: Optional[Union[str, Sequence[str]]] = None,
    ):
        """mdmd:hidden"""
        _lifecycle: Optional[str] = None
        if spot is not None:
            _lifecycle = "spot" if spot else "on-demand"

        regions = []
        if region:
            if isinstance(region, str):
                regions = [region]
            else:
                regions = list(region)

        instance_types = []
        if instance_type:
            if isinstance(instance_type, str):
                instance_types = [instance_type]
            else:
                instance_types = list(instance_type)

        self.proto = api_pb2.SchedulerPlacement(
            regions=regions,
            _zone=zone,
            _lifecycle=_lifecycle,
            _instance_types=instance_types,
        )


================================================
File: modal/secret.py
================================================
# Copyright Modal Labs 2022
import os
from typing import Optional, Union

from grpclib import GRPCError, Status

from modal_proto import api_pb2

from ._object import _get_environment_name, _Object
from ._resolver import Resolver
from ._runtime.execution_context import is_local
from ._utils.async_utils import synchronize_api
from ._utils.deprecation import deprecation_warning, renamed_parameter
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from .client import _Client
from .exception import InvalidError, NotFoundError

ENV_DICT_WRONG_TYPE_ERR = "the env_dict argument to Secret has to be a dict[str, Union[str, None]]"


class _Secret(_Object, type_prefix="st"):
    """Secrets provide a dictionary of environment variables for images.

    Secrets are a secure way to add credentials and other sensitive information
    to the containers your functions run in. You can create and edit secrets on
    [the dashboard](/secrets), or programmatically from Python code.

    See [the secrets guide page](/docs/guide/secrets) for more information.
    """

    @staticmethod
    def from_dict(
        env_dict: dict[
            str, Union[str, None]
        ] = {},  # dict of entries to be inserted as environment variables in functions using the secret
    ):
        """Create a secret from a str-str dictionary. Values can also be `None`, which is ignored.

        Usage:
        ```python
        @app.function(secrets=[modal.Secret.from_dict({"FOO": "bar"})])
        def run():
            print(os.environ["FOO"])
        ```
        """
        if not isinstance(env_dict, dict):
            raise InvalidError(ENV_DICT_WRONG_TYPE_ERR)

        env_dict_filtered: dict[str, str] = {k: v for k, v in env_dict.items() if v is not None}
        if not all(isinstance(k, str) for k in env_dict_filtered.keys()):
            raise InvalidError(ENV_DICT_WRONG_TYPE_ERR)
        if not all(isinstance(v, str) for v in env_dict_filtered.values()):
            raise InvalidError(ENV_DICT_WRONG_TYPE_ERR)

        async def _load(self: _Secret, resolver: Resolver, existing_object_id: Optional[str]):
            if resolver.app_id is not None:
                object_creation_type = api_pb2.OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP
            else:
                object_creation_type = api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL

            req = api_pb2.SecretGetOrCreateRequest(
                object_creation_type=object_creation_type,
                env_dict=env_dict_filtered,
                app_id=resolver.app_id,
                environment_name=resolver.environment_name,
            )
            try:
                resp = await resolver.client.stub.SecretGetOrCreate(req)
            except GRPCError as exc:
                if exc.status == Status.INVALID_ARGUMENT:
                    raise InvalidError(exc.message)
                if exc.status == Status.FAILED_PRECONDITION:
                    raise InvalidError(exc.message)
                raise
            self._hydrate(resp.secret_id, resolver.client, None)

        rep = f"Secret.from_dict([{', '.join(env_dict.keys())}])"
        return _Secret._from_loader(_load, rep, hydrate_lazily=True)

    @staticmethod
    def from_local_environ(
        env_keys: list[str],  # list of local env vars to be included for remote execution
    ):
        """Create secrets from local environment variables automatically."""

        if is_local():
            try:
                return _Secret.from_dict({k: os.environ[k] for k in env_keys})
            except KeyError as exc:
                missing_key = exc.args[0]
                raise InvalidError(
                    f"Could not find local environment variable '{missing_key}' for Secret.from_local_environ"
                )

        return _Secret.from_dict({})

    @staticmethod
    def from_dotenv(path=None, *, filename=".env"):
        """Create secrets from a .env file automatically.

        If no argument is provided, it will use the current working directory as the starting
        point for finding a `.env` file. Note that it does not use the location of the module
        calling `Secret.from_dotenv`.

        If called with an argument, it will use that as a starting point for finding `.env` files.
        In particular, you can call it like this:
        ```python
        @app.function(secrets=[modal.Secret.from_dotenv(__file__)])
        def run():
            print(os.environ["USERNAME"])  # Assumes USERNAME is defined in your .env file
        ```

        This will use the location of the script calling `modal.Secret.from_dotenv` as a
        starting point for finding the `.env` file.

        A file named `.env` is expected by default, but this can be overridden with the `filename`
        keyword argument:

        ```python
        @app.function(secrets=[modal.Secret.from_dotenv(filename=".env-dev")])
        def run():
            ...
        ```
        """

        async def _load(self: _Secret, resolver: Resolver, existing_object_id: Optional[str]):
            try:
                from dotenv import dotenv_values, find_dotenv
                from dotenv.main import _walk_to_root
            except ImportError:
                raise ImportError(
                    "Need the `dotenv` package installed. You can install it by running `pip install python-dotenv`."
                )

            if path is not None:
                # This basically implements the logic in find_dotenv
                for dirname in _walk_to_root(path):
                    check_path = os.path.join(dirname, filename)
                    if os.path.isfile(check_path):
                        dotenv_path = check_path
                        break
                else:
                    dotenv_path = ""
            else:
                # TODO(erikbern): dotenv tries to locate .env files based on location of the file in the stack frame.
                # Since the modal code "intermediates" this, a .env file in user's local directory won't be picked up.
                # To simplify this, we just support the cwd and don't do any automatic path inference.
                dotenv_path = find_dotenv(filename, usecwd=True)

            env_dict = dotenv_values(dotenv_path)

            req = api_pb2.SecretGetOrCreateRequest(
                object_creation_type=api_pb2.OBJECT_CREATION_TYPE_ANONYMOUS_OWNED_BY_APP,
                env_dict=env_dict,
                app_id=resolver.app_id,
            )
            resp = await resolver.client.stub.SecretGetOrCreate(req)

            self._hydrate(resp.secret_id, resolver.client, None)

        return _Secret._from_loader(_load, "Secret.from_dotenv()", hydrate_lazily=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    def from_name(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
        required_keys: list[
            str
        ] = [],  # Optionally, a list of required environment variables (will be asserted server-side)
    ) -> "_Secret":
        """Reference a Secret by its name.

        In contrast to most other Modal objects, named Secrets must be provisioned
        from the Dashboard. See other methods for alternate ways of creating a new
        Secret from code.

        ```python
        secret = modal.Secret.from_name("my-secret")

        @app.function(secrets=[secret])
        def run():
           ...
        ```
        """

        async def _load(self: _Secret, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.SecretGetOrCreateRequest(
                deployment_name=name,
                namespace=namespace,
                environment_name=_get_environment_name(environment_name, resolver),
                required_keys=required_keys,
            )
            try:
                response = await resolver.client.stub.SecretGetOrCreate(req)
            except GRPCError as exc:
                if exc.status == Status.NOT_FOUND:
                    raise NotFoundError(exc.message)
                else:
                    raise
            self._hydrate(response.secret_id, resolver.client, None)

        return _Secret._from_loader(_load, "Secret()", hydrate_lazily=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def lookup(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        required_keys: list[str] = [],
    ) -> "_Secret":
        """mdmd:hidden"""
        deprecation_warning(
            (2025, 1, 27),
            "`modal.Secret.lookup` is deprecated and will be removed in a future release."
            " It can be replaced with `modal.Secret.from_name`."
            "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
        )
        obj = _Secret.from_name(
            name, namespace=namespace, environment_name=environment_name, required_keys=required_keys
        )
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client)
        await resolver.load(obj)
        return obj

    @staticmethod
    async def create_deployed(
        deployment_name: str,
        env_dict: dict[str, str],
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        overwrite: bool = False,
    ) -> str:
        """mdmd:hidden"""
        check_object_name(deployment_name, "Secret")
        if client is None:
            client = await _Client.from_env()
        if overwrite:
            object_creation_type = api_pb2.OBJECT_CREATION_TYPE_CREATE_OVERWRITE_IF_EXISTS
        else:
            object_creation_type = api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS
        request = api_pb2.SecretGetOrCreateRequest(
            deployment_name=deployment_name,
            namespace=namespace,
            environment_name=_get_environment_name(environment_name),
            object_creation_type=object_creation_type,
            env_dict=env_dict,
        )
        resp = await retry_transient_errors(client.stub.SecretGetOrCreate, request)
        return resp.secret_id


Secret = synchronize_api(_Secret)


================================================
File: modal/serving.py
================================================
# Copyright Modal Labs 2023
import multiprocessing
import platform
from collections.abc import AsyncGenerator
from multiprocessing.context import SpawnProcess
from multiprocessing.synchronize import Event
from typing import TYPE_CHECKING, Optional, TypeVar

from synchronicity.async_wrap import asynccontextmanager

from modal._output import OutputManager

from ._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
from ._utils.deprecation import deprecation_error
from ._utils.logger import logger
from ._watcher import watch
from .cli.import_refs import ImportRef, import_app_from_ref
from .client import _Client
from .config import config
from .output import _get_output_manager, enable_output
from .runner import _run_app, serve_update

if TYPE_CHECKING:
    from .app import _App
else:
    _App = TypeVar("_App")


def _run_serve(
    import_ref: ImportRef, existing_app_id: str, is_ready: Event, environment_name: str, show_progress: bool
):
    # subprocess entrypoint
    _app = import_app_from_ref(import_ref, base_cmd="modal serve")
    blocking_app = synchronizer._translate_out(_app)

    with enable_output(show_progress=show_progress):
        serve_update(blocking_app, existing_app_id, is_ready, environment_name)


async def _restart_serve(
    import_ref: ImportRef, *, existing_app_id: str, environment_name: str, timeout: float = 5.0
) -> SpawnProcess:
    ctx = multiprocessing.get_context("spawn")  # Needed to reload the interpreter
    is_ready = ctx.Event()
    output_mgr = OutputManager.get()
    show_progress = output_mgr is not None
    p = ctx.Process(target=_run_serve, args=(import_ref, existing_app_id, is_ready, environment_name, show_progress))
    p.start()
    await asyncify(is_ready.wait)(timeout)
    # TODO(erikbern): we don't fail if the above times out, but that's somewhat intentional, since
    # the child process might build a huge image or similar
    return p


async def _terminate(proc: Optional[SpawnProcess], timeout: float = 5.0):
    if proc is None:
        return
    try:
        proc.terminate()
        await asyncify(proc.join)(timeout)
        if proc.exitcode is not None:
            if output_mgr := _get_output_manager():
                output_mgr.print(f"Serve process {proc.pid} terminated")
        else:
            if output_mgr := _get_output_manager():
                output_mgr.print(f"[red]Serve process {proc.pid} didn't terminate after {timeout}s, killing it[/red]")
            proc.kill()
    except ProcessLookupError:
        pass  # Child process already finished


async def _run_watch_loop(
    import_ref: ImportRef,
    *,
    app_id: str,
    watcher: AsyncGenerator[set[str], None],
    environment_name: str,
):
    unsupported_msg = None
    if platform.system() == "Windows":
        unsupported_msg = "Live-reload skipped. This feature is currently unsupported on Windows"
        " This can hopefully be fixed in a future version of Modal."

    if unsupported_msg:
        if output_mgr := _get_output_manager():
            async for _ in watcher:
                output_mgr.print(unsupported_msg)
    else:
        curr_proc = None
        try:
            async for trigger_files in watcher:
                logger.debug(f"The following files triggered an app update: {', '.join(trigger_files)}")
                await _terminate(curr_proc)
                curr_proc = await _restart_serve(import_ref, existing_app_id=app_id, environment_name=environment_name)
        finally:
            await _terminate(curr_proc)


@asynccontextmanager
async def _serve_app(
    app: "_App",
    import_ref: ImportRef,
    *,
    _watcher: Optional[AsyncGenerator[set[str], None]] = None,  # for testing
    environment_name: Optional[str] = None,
) -> AsyncGenerator["_App", None]:
    if environment_name is None:
        environment_name = config.get("environment")

    client = await _Client.from_env()

    async with _run_app(app, client=client, environment_name=environment_name):
        if _watcher is not None:
            watcher = _watcher  # Only used by tests
        else:
            mounts_to_watch = app._get_watch_mounts()
            watcher = watch(mounts_to_watch)
        async with TaskContext(grace=0.1) as tc:
            tc.create_task(
                _run_watch_loop(import_ref, app_id=app.app_id, watcher=watcher, environment_name=environment_name)
            )
            yield app


def _serve_stub(*args, **kwargs):
    deprecation_error((2024, 5, 1), "`serve_stub` is deprecated. Please use `serve_app` instead.")


serve_app = synchronize_api(_serve_app)
serve_stub = synchronize_api(_serve_stub)


================================================
File: modal/snapshot.py
================================================
# Copyright Modal Labs 2024
from typing import Optional

from modal_proto import api_pb2

from ._object import _Object
from ._resolver import Resolver
from ._utils.async_utils import synchronize_api
from ._utils.grpc_utils import retry_transient_errors
from .client import _Client


class _SandboxSnapshot(_Object, type_prefix="sn"):
    """A `SandboxSnapshot` object lets you interact with a stored Sandbox snapshot that was created by calling
    .snapshot() on a Sandbox instance. This includes both the filesystem and memory state of the original Sandbox at the
    time the snapshot was taken.
    """

    @staticmethod
    async def from_id(sandbox_snapshot_id: str, client: Optional[_Client] = None):
        if client is None:
            client = await _Client.from_env()

        async def _load(self: _SandboxSnapshot, resolver: Resolver, existing_object_id: Optional[str]):
            await retry_transient_errors(
                client.stub.SandboxSnapshotGet, api_pb2.SandboxSnapshotGetRequest(snapshot_id=sandbox_snapshot_id)
            )

        rep = "SandboxSnapshot()"
        obj = _SandboxSnapshot._from_loader(_load, rep)
        obj._hydrate(sandbox_snapshot_id, client, None)

        return obj


SandboxSnapshot = synchronize_api(_SandboxSnapshot)


================================================
File: modal/stream_type.py
================================================
# Copyright Modal Labs 2022
import subprocess
from enum import Enum


class StreamType(Enum):
    # Discard all logs from the stream.
    DEVNULL = subprocess.DEVNULL
    # Store logs in a pipe to be read by the client.
    PIPE = subprocess.PIPE
    # Print logs to stdout immediately.
    STDOUT = subprocess.STDOUT

    def __repr__(self):
        return f"{self.__module__}.{self.__class__.__name__}.{self.name}"


================================================
File: modal/token_flow.py
================================================
# Copyright Modal Labs 2023
import itertools
import os
import webbrowser
from collections.abc import AsyncGenerator
from typing import Optional

import aiohttp.web
from rich.console import Console
from synchronicity.async_wrap import asynccontextmanager

from modal_proto import api_pb2

from ._utils.async_utils import synchronize_api
from ._utils.http_utils import run_temporary_http_server
from .client import _Client
from .config import _lookup_workspace, _store_user_config, config, config_profiles, user_config_path
from .exception import AuthError


class _TokenFlow:
    def __init__(self, client: _Client):
        self.stub = client.stub

    @asynccontextmanager
    async def start(
        self, utm_source: Optional[str] = None, next_url: Optional[str] = None
    ) -> AsyncGenerator[tuple[str, str, str], None]:
        """mdmd:hidden"""
        # Run a temporary http server returning the token id on /
        # This helps us add direct validation later
        # TODO(erikbern): handle failure launching server

        async def slash(request):
            headers = {"Access-Control-Allow-Origin": "*"}
            return aiohttp.web.Response(text=self.token_flow_id, headers=headers)

        app = aiohttp.web.Application()
        app.add_routes([aiohttp.web.get("/", slash)])
        async with run_temporary_http_server(app) as url:
            req = api_pb2.TokenFlowCreateRequest(
                utm_source=utm_source,
                next_url=next_url,
                localhost_port=int(url.split(":")[-1]),
            )
            resp = await self.stub.TokenFlowCreate(req)
            self.token_flow_id = resp.token_flow_id
            self.wait_secret = resp.wait_secret
            yield (resp.token_flow_id, resp.web_url, resp.code)

    async def finish(
        self, timeout: float = 40.0, grpc_extra_timeout: float = 5.0
    ) -> Optional[api_pb2.TokenFlowWaitResponse]:
        """mdmd:hidden"""
        # Wait for token flow to finish
        req = api_pb2.TokenFlowWaitRequest(
            token_flow_id=self.token_flow_id, timeout=timeout, wait_secret=self.wait_secret
        )
        resp = await self.stub.TokenFlowWait(req, timeout=(timeout + grpc_extra_timeout))
        if not resp.timeout:
            return resp
        else:
            return None


TokenFlow = synchronize_api(_TokenFlow)


async def _new_token(
    *,
    profile: Optional[str] = None,
    activate: bool = True,
    verify: bool = True,
    source: Optional[str] = None,
    next_url: Optional[str] = None,
):
    server_url = config.get("server_url", profile=profile)

    console = Console()

    result: Optional[api_pb2.TokenFlowWaitResponse] = None
    async with _Client.anonymous(server_url) as client:
        token_flow = _TokenFlow(client)

        async with token_flow.start(source, next_url) as (_, web_url, code):
            with console.status("Waiting for authentication in the web browser", spinner="dots"):
                # Open the web url in the browser
                if _open_url(web_url):
                    console.print(
                        "The web browser should have opened for you to authenticate and get an API token.\n"
                        "If it didn't, please copy this URL into your web browser manually:\n"
                    )
                else:
                    console.print(
                        "[red]Was not able to launch web browser[/red]\n"
                        "Please go to this URL manually and complete the flow:\n"
                    )
                console.print(f"[link={web_url}]{web_url}[/link]\n")
                if code:
                    console.print(f"Enter this code: [yellow]{code}[/yellow]\n")

            with console.status("Waiting for token flow to complete...", spinner="dots") as status:
                for attempt in itertools.count():
                    result = await token_flow.finish()
                    if result is not None:
                        break
                    status.update(f"Waiting for token flow to complete... (attempt {attempt + 2})")

        console.print("[green]Web authentication finished successfully![/green]")

    assert result is not None

    if result.workspace_username:
        console.print(
            f"[green]Token is connected to the [magenta]{result.workspace_username}[/magenta] workspace.[/green]"
        )

    await _set_token(result.token_id, result.token_secret, profile=profile, activate=activate, verify=verify)


async def _set_token(
    token_id: str,
    token_secret: str,
    *,
    profile: Optional[str] = None,
    activate: bool = True,
    verify: bool = True,
):
    # TODO add server_url as a parameter for verification?
    server_url = config.get("server_url", profile=profile)
    console = Console()
    if verify:
        console.print(f"Verifying token against [blue]{server_url}[/blue]")
        await _Client.verify(server_url, (token_id, token_secret))
        console.print("[green]Token verified successfully![/green]")

    if profile is None:
        if "MODAL_PROFILE" in os.environ:
            profile = os.environ["MODAL_PROFILE"]
        else:
            try:
                workspace = await _lookup_workspace(server_url, token_id, token_secret)
            except AuthError as exc:
                if not verify:
                    # Improve the error message for verification failure with --no-verify to reduce surprise
                    msg = "No profile name given, but could not authenticate client to look up workspace name."
                    raise AuthError(msg) from exc
                raise exc
            profile = workspace.username

    config_data = {"token_id": token_id, "token_secret": token_secret}
    # Activate the profile when requested or if no other profiles currently exist
    active_profile = profile if (activate or not config_profiles()) else None
    with console.status("Storing token", spinner="dots"):
        _store_user_config(config_data, profile=profile, active_profile=active_profile)
    console.print(
        f"[green]Token written to [magenta]{user_config_path}[/magenta] in profile "
        f"[magenta]{profile}[/magenta].[/green]"
    )


def _open_url(url: str) -> bool:
    """Opens url in web browser, making sure we use a modern one (not Lynx etc)"""
    if "PYTEST_CURRENT_TEST" in os.environ:
        return False
    try:
        browser = webbrowser.get()
        # zpresto defines `BROWSER=open` by default on macOS, which causes `webbrowser` to return `GenericBrowser`.
        if isinstance(browser, webbrowser.GenericBrowser) and browser.name != "open":
            return False
        else:
            return browser.open_new_tab(url)
    except webbrowser.Error:
        return False


================================================
File: modal/volume.py
================================================
# Copyright Modal Labs 2023
import asyncio
import concurrent.futures
import enum
import functools
import os
import platform
import re
import time
import typing
from collections.abc import AsyncGenerator, AsyncIterator, Generator, Sequence
from dataclasses import dataclass
from pathlib import Path, PurePosixPath
from typing import (
    IO,
    Any,
    BinaryIO,
    Callable,
    Optional,
    Union,
)

from grpclib import GRPCError, Status
from synchronicity.async_wrap import asynccontextmanager

import modal_proto.api_pb2
from modal.exception import VolumeUploadTimeoutError
from modal_proto import api_pb2

from ._object import EPHEMERAL_OBJECT_HEARTBEAT_SLEEP, _get_environment_name, _Object, live_method, live_method_gen
from ._resolver import Resolver
from ._utils.async_utils import TaskContext, aclosing, async_map, asyncnullcontext, synchronize_api
from ._utils.blob_utils import (
    FileUploadSpec,
    blob_iter,
    blob_upload_file,
    get_file_upload_spec_from_fileobj,
    get_file_upload_spec_from_path,
)
from ._utils.deprecation import deprecation_error, deprecation_warning, renamed_parameter
from ._utils.grpc_utils import retry_transient_errors
from ._utils.name_utils import check_object_name
from .client import _Client
from .config import logger

# Max duration for uploading to volumes files
# As a guide, files >40GiB will take >10 minutes to upload.
VOLUME_PUT_FILE_CLIENT_TIMEOUT = 60 * 60


class FileEntryType(enum.IntEnum):
    """Type of a file entry listed from a Modal volume."""

    UNSPECIFIED = 0
    FILE = 1
    DIRECTORY = 2
    SYMLINK = 3


@dataclass(frozen=True)
class FileEntry:
    """A file or directory entry listed from a Modal volume."""

    path: str
    type: FileEntryType
    mtime: int
    size: int

    @classmethod
    def _from_proto(cls, proto: api_pb2.FileEntry) -> "FileEntry":
        return cls(
            path=proto.path,
            type=FileEntryType(proto.type),
            mtime=proto.mtime,
            size=proto.size,
        )


class _Volume(_Object, type_prefix="vo"):
    """A writeable volume that can be used to share files between one or more Modal functions.

    The contents of a volume is exposed as a filesystem. You can use it to share data between different functions, or
    to persist durable state across several instances of the same function.

    Unlike a networked filesystem, you need to explicitly reload the volume to see changes made since it was mounted.
    Similarly, you need to explicitly commit any changes you make to the volume for the changes to become visible
    outside the current container.

    Concurrent modification is supported, but concurrent modifications of the same files should be avoided! Last write
    wins in case of concurrent modification of the same file - any data the last writer didn't have when committing
    changes will be lost!

    As a result, volumes are typically not a good fit for use cases where you need to make concurrent modifications to
    the same file (nor is distributed file locking supported).

    Volumes can only be reloaded if there are no open files for the volume - attempting to reload with open files
    will result in an error.

    **Usage**

    ```python
    import modal

    app = modal.App()
    volume = modal.Volume.from_name("my-persisted-volume", create_if_missing=True)

    @app.function(volumes={"/root/foo": volume})
    def f():
        with open("/root/foo/bar.txt", "w") as f:
            f.write("hello")
        volume.commit()  # Persist changes

    @app.function(volumes={"/root/foo": volume})
    def g():
        volume.reload()  # Fetch latest changes
        with open("/root/foo/bar.txt", "r") as f:
            print(f.read())
    ```
    """

    _lock: Optional[asyncio.Lock] = None

    async def _get_lock(self):
        # To (mostly*) prevent multiple concurrent operations on the same volume, which can cause problems under
        # some unlikely circumstances.
        # *: You can bypass this by creating multiple handles to the same volume, e.g. via lookup. But this
        # covers the typical case = good enough.

        # Note: this function runs no async code but is marked as async to ensure it's
        # being run inside the synchronicity event loop and binds the lock to the
        # correct event loop on Python 3.9 which eagerly assigns event loops on
        # constructions of locks
        if self._lock is None:
            self._lock = asyncio.Lock()
        return self._lock

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    def from_name(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
        version: "typing.Optional[modal_proto.api_pb2.VolumeFsVersion.ValueType]" = None,
    ) -> "_Volume":
        """Reference a Volume by name, creating if necessary.

        In contrast to `modal.Volume.lookup`, this is a lazy method
        that defers hydrating the local object with metadata from
        Modal servers until the first time is is actually used.

        ```python
        vol = modal.Volume.from_name("my-volume", create_if_missing=True)

        app = modal.App()

        # Volume refers to the same object, even across instances of `app`.
        @app.function(volumes={"/data": vol})
        def f():
            pass
        ```
        """
        check_object_name(name, "Volume")

        async def _load(self: _Volume, resolver: Resolver, existing_object_id: Optional[str]):
            req = api_pb2.VolumeGetOrCreateRequest(
                deployment_name=name,
                namespace=namespace,
                environment_name=_get_environment_name(environment_name, resolver),
                object_creation_type=(api_pb2.OBJECT_CREATION_TYPE_CREATE_IF_MISSING if create_if_missing else None),
                version=version,
            )
            response = await resolver.client.stub.VolumeGetOrCreate(req)
            self._hydrate(response.volume_id, resolver.client, None)

        return _Volume._from_loader(_load, "Volume()", hydrate_lazily=True)

    @classmethod
    @asynccontextmanager
    async def ephemeral(
        cls: type["_Volume"],
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        version: "typing.Optional[modal_proto.api_pb2.VolumeFsVersion.ValueType]" = None,
        _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
    ) -> AsyncGenerator["_Volume", None]:
        """Creates a new ephemeral volume within a context manager:

        Usage:
        ```python
        import modal
        with modal.Volume.ephemeral() as vol:
            assert vol.listdir("/") == []
        ```

        ```python notest
        async with modal.Volume.ephemeral() as vol:
            assert await vol.listdir("/") == []
        ```
        """
        if client is None:
            client = await _Client.from_env()
        request = api_pb2.VolumeGetOrCreateRequest(
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_EPHEMERAL,
            environment_name=_get_environment_name(environment_name),
            version=version,
        )
        response = await client.stub.VolumeGetOrCreate(request)
        async with TaskContext() as tc:
            request = api_pb2.VolumeHeartbeatRequest(volume_id=response.volume_id)
            tc.infinite_loop(lambda: client.stub.VolumeHeartbeat(request), sleep=_heartbeat_sleep)
            yield cls._new_hydrated(response.volume_id, client, None, is_another_app=True)

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def lookup(
        name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        create_if_missing: bool = False,
        version: "typing.Optional[modal_proto.api_pb2.VolumeFsVersion.ValueType]" = None,
    ) -> "_Volume":
        """Lookup a named Volume.

        DEPRECATED: This method is deprecated in favor of `modal.Volume.from_name`.

        In contrast to `modal.Volume.from_name`, this is an eager method
        that will hydrate the local object with metadata from Modal servers.

        ```python notest
        vol = modal.Volume.from_name("my-volume")
        print(vol.listdir("/"))
        ```
        """
        deprecation_warning(
            (2025, 1, 27),
            "`modal.Volume.lookup` is deprecated and will be removed in a future release."
            " It can be replaced with `modal.Volume.from_name`."
            "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more information.",
        )
        obj = _Volume.from_name(
            name,
            namespace=namespace,
            environment_name=environment_name,
            create_if_missing=create_if_missing,
            version=version,
        )
        if client is None:
            client = await _Client.from_env()
        resolver = Resolver(client=client)
        await resolver.load(obj)
        return obj

    @staticmethod
    async def create_deployed(
        deployment_name: str,
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
        version: "typing.Optional[modal_proto.api_pb2.VolumeFsVersion.ValueType]" = None,
    ) -> str:
        """mdmd:hidden"""
        check_object_name(deployment_name, "Volume")
        if client is None:
            client = await _Client.from_env()
        request = api_pb2.VolumeGetOrCreateRequest(
            deployment_name=deployment_name,
            namespace=namespace,
            environment_name=_get_environment_name(environment_name),
            object_creation_type=api_pb2.OBJECT_CREATION_TYPE_CREATE_FAIL_IF_EXISTS,
            version=version,
        )
        resp = await retry_transient_errors(client.stub.VolumeGetOrCreate, request)
        return resp.volume_id

    @live_method
    async def _do_reload(self, lock=True):
        async with (await self._get_lock()) if lock else asyncnullcontext():
            req = api_pb2.VolumeReloadRequest(volume_id=self.object_id)
            _ = await retry_transient_errors(self._client.stub.VolumeReload, req)

    @live_method
    async def commit(self):
        """Commit changes to the volume.

        If successful, the changes made are now persisted in durable storage and available to other containers accessing
        the volume.
        """
        async with await self._get_lock():
            req = api_pb2.VolumeCommitRequest(volume_id=self.object_id)
            try:
                # TODO(gongy): only apply indefinite retries on 504 status.
                resp = await retry_transient_errors(self._client.stub.VolumeCommit, req, max_retries=90)
                if not resp.skip_reload:
                    # Reload changes on successful commit.
                    await self._do_reload(lock=False)
            except GRPCError as exc:
                raise RuntimeError(exc.message) if exc.status in (Status.FAILED_PRECONDITION, Status.NOT_FOUND) else exc

    @live_method
    async def reload(self):
        """Make latest committed state of volume available in the running container.

        Any uncommitted changes to the volume, such as new or modified files, may implicitly be committed when
        reloading.

        Reloading will fail if there are open files for the volume.
        """
        try:
            await self._do_reload()
        except GRPCError as exc:
            # TODO(staffan): This is brittle and janky, as it relies on specific paths and error messages which can
            #  change server-side at any time. Consider returning the open files directly in the error emitted from the
            #  server.
            if exc.message == "there are open files preventing the operation":
                # Attempt to identify what open files are problematic and include information about the first (to avoid
                # really verbose errors) open file in the error message to help troubleshooting.
                # This is best-effort and not necessarily bulletproof, as the view of open files inside the container
                # might differ from that outside - but it will at least catch common errors.
                vol_path = f"/__modal/volumes/{self.object_id}"
                annotation = _open_files_error_annotation(vol_path)
                if annotation:
                    raise RuntimeError(f"{exc.message}: {annotation}")

            raise RuntimeError(exc.message) if exc.status in (Status.FAILED_PRECONDITION, Status.NOT_FOUND) else exc

    @live_method_gen
    async def iterdir(self, path: str, *, recursive: bool = True) -> AsyncIterator[FileEntry]:
        """Iterate over all files in a directory in the volume.

        Passing a directory path lists all files in the directory. For a file path, return only that
        file's description. If `recursive` is set to True, list all files and folders under the path
        recursively.
        """
        from modal_version import major_number, minor_number

        # This allows us to remove the server shim after 0.62 is no longer supported.
        deprecation = deprecation_warning if (major_number, minor_number) <= (0, 62) else deprecation_error
        if path.endswith("**"):
            msg = (
                "Glob patterns in `volume get` and `Volume.listdir()` are deprecated. "
                "Please pass recursive=True instead. For the CLI, just remove the glob suffix."
            )
            deprecation(
                (2024, 4, 23),
                msg,
            )
        elif path.endswith("*"):
            deprecation(
                (2024, 4, 23),
                (
                    "Glob patterns in `volume get` and `Volume.listdir()` are deprecated. "
                    "Please remove the glob `*` suffix."
                ),
            )

        req = api_pb2.VolumeListFilesRequest(volume_id=self.object_id, path=path, recursive=recursive)
        async for batch in self._client.stub.VolumeListFiles.unary_stream(req):
            for entry in batch.entries:
                yield FileEntry._from_proto(entry)

    @live_method
    async def listdir(self, path: str, *, recursive: bool = False) -> list[FileEntry]:
        """List all files under a path prefix in the modal.Volume.

        Passing a directory path lists all files in the directory. For a file path, return only that
        file's description. If `recursive` is set to True, list all files and folders under the path
        recursively.
        """
        return [entry async for entry in self.iterdir(path, recursive=recursive)]

    @live_method_gen
    async def read_file(self, path: str) -> AsyncIterator[bytes]:
        """
        Read a file from the modal.Volume.

        **Example:**

        ```python notest
        vol = modal.Volume.from_name("my-modal-volume")
        data = b""
        for chunk in vol.read_file("1mb.csv"):
            data += chunk
        print(len(data))  # == 1024 * 1024
        ```
        """
        req = api_pb2.VolumeGetFileRequest(volume_id=self.object_id, path=path)
        try:
            response = await retry_transient_errors(self._client.stub.VolumeGetFile, req)
        except GRPCError as exc:
            raise FileNotFoundError(exc.message) if exc.status == Status.NOT_FOUND else exc
        # TODO(Jonathon): use ranged requests.
        if response.WhichOneof("data_oneof") == "data":
            yield response.data
            return
        else:
            async for data in blob_iter(response.data_blob_id, self._client.stub):
                yield data

    @live_method
    async def read_file_into_fileobj(self, path: str, fileobj: IO[bytes]) -> int:
        """mdmd:hidden

        Read volume file into file-like IO object.
        In the future, this will replace the current generator implementation of the `read_file` method.
        """

        chunk_size_bytes = 8 * 1024 * 1024
        start = 0
        req = api_pb2.VolumeGetFileRequest(volume_id=self.object_id, path=path, start=start, len=chunk_size_bytes)
        try:
            response = await retry_transient_errors(self._client.stub.VolumeGetFile, req)
        except GRPCError as exc:
            raise FileNotFoundError(exc.message) if exc.status == Status.NOT_FOUND else exc
        if response.WhichOneof("data_oneof") != "data":
            raise RuntimeError("expected to receive 'data' in response")

        n = fileobj.write(response.data)
        if n != len(response.data):
            raise OSError(f"failed to write {len(response.data)} bytes to output. Wrote {n}.")
        elif n == response.size:
            return response.size
        elif n > response.size:
            raise RuntimeError(f"length of returned data exceeds reported filesize: {n} > {response.size}")
        # else: there's more data to read. continue reading with further ranged GET requests.
        file_size = response.size
        written = n

        while True:
            req = api_pb2.VolumeGetFileRequest(volume_id=self.object_id, path=path, start=written, len=chunk_size_bytes)
            response = await retry_transient_errors(self._client.stub.VolumeGetFile, req)
            if response.WhichOneof("data_oneof") != "data":
                raise RuntimeError("expected to receive 'data' in response")
            if len(response.data) > chunk_size_bytes:
                raise RuntimeError(f"received more data than requested: {len(response.data)} > {chunk_size_bytes}")
            elif (written + len(response.data)) > file_size:
                raise RuntimeError(f"received data exceeds filesize of {chunk_size_bytes}")

            n = fileobj.write(response.data)
            if n != len(response.data):
                raise OSError(f"failed to write {len(response.data)} bytes to output. Wrote {n}.")
            written += n
            if written == file_size:
                break

        return written

    @live_method
    async def remove_file(self, path: str, recursive: bool = False) -> None:
        """Remove a file or directory from a volume."""
        req = api_pb2.VolumeRemoveFileRequest(volume_id=self.object_id, path=path, recursive=recursive)
        await retry_transient_errors(self._client.stub.VolumeRemoveFile, req)

    @live_method
    async def copy_files(self, src_paths: Sequence[str], dst_path: str) -> None:
        """
        Copy files within the volume from src_paths to dst_path.
        The semantics of the copy operation follow those of the UNIX cp command.

        The `src_paths` parameter is a list. If you want to copy a single file, you should pass a list with a
        single element.

        `src_paths` and `dst_path` should refer to the desired location *inside* the volume. You do not need to prepend
        the volume mount path.

        **Usage**

        ```python notest
        vol = modal.Volume.from_name("my-modal-volume")

        vol.copy_files(["bar/example.txt"], "bar2")  # Copy files to another directory
        vol.copy_files(["bar/example.txt"], "bar/example2.txt")  # Rename a file by copying
        ```

        Note that if the volume is already mounted on the Modal function, you should use normal filesystem operations
        like `os.rename()` and then `commit()` the volume. The `copy_files()` method is useful when you don't have
        the volume mounted as a filesystem, e.g. when running a script on your local computer.
        """
        request = api_pb2.VolumeCopyFilesRequest(volume_id=self.object_id, src_paths=src_paths, dst_path=dst_path)
        await retry_transient_errors(self._client.stub.VolumeCopyFiles, request, base_delay=1)

    @live_method
    async def batch_upload(self, force: bool = False) -> "_VolumeUploadContextManager":
        """
        Initiate a batched upload to a volume.

        To allow overwriting existing files, set `force` to `True` (you cannot overwrite existing directories with
        uploaded files regardless).

        **Example:**

        ```python notest
        vol = modal.Volume.from_name("my-modal-volume")

        with vol.batch_upload() as batch:
            batch.put_file("local-path.txt", "/remote-path.txt")
            batch.put_directory("/local/directory/", "/remote/directory")
            batch.put_file(io.BytesIO(b"some data"), "/foobar")
        ```
        """
        return _VolumeUploadContextManager(self.object_id, self._client, force=force)

    @live_method
    async def _instance_delete(self):
        await retry_transient_errors(
            self._client.stub.VolumeDelete, api_pb2.VolumeDeleteRequest(volume_id=self.object_id)
        )

    @staticmethod
    @renamed_parameter((2024, 12, 18), "label", "name")
    async def delete(name: str, client: Optional[_Client] = None, environment_name: Optional[str] = None):
        obj = await _Volume.from_name(name, environment_name=environment_name).hydrate(client)
        req = api_pb2.VolumeDeleteRequest(volume_id=obj.object_id)
        await retry_transient_errors(obj._client.stub.VolumeDelete, req)

    @staticmethod
    async def rename(
        old_name: str,
        new_name: str,
        *,
        client: Optional[_Client] = None,
        environment_name: Optional[str] = None,
    ):
        obj = await _Volume.from_name(old_name, environment_name=environment_name).hydrate(client)
        req = api_pb2.VolumeRenameRequest(volume_id=obj.object_id, name=new_name)
        await retry_transient_errors(obj._client.stub.VolumeRename, req)


class _VolumeUploadContextManager:
    """Context manager for batch-uploading files to a Volume."""

    _volume_id: str
    _client: _Client
    _force: bool
    progress_cb: Callable[..., Any]
    _upload_generators: list[Generator[Callable[[], FileUploadSpec], None, None]]

    def __init__(
        self, volume_id: str, client: _Client, progress_cb: Optional[Callable[..., Any]] = None, force: bool = False
    ):
        """mdmd:hidden"""
        self._volume_id = volume_id
        self._client = client
        self._upload_generators = []
        self._progress_cb = progress_cb or (lambda *_, **__: None)
        self._force = force

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if not exc_val:
            # Flatten all the uploads yielded by the upload generators in the batch
            def gen_upload_providers():
                for gen in self._upload_generators:
                    yield from gen

            async def gen_file_upload_specs() -> AsyncGenerator[FileUploadSpec, None]:
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as exe:
                    # TODO: avoid eagerly expanding
                    futs = [loop.run_in_executor(exe, f) for f in gen_upload_providers()]
                    logger.debug(f"Computing checksums for {len(futs)} files using {exe._max_workers} workers")
                    for fut in asyncio.as_completed(futs):
                        yield await fut

            # Compute checksums & Upload files
            files: list[api_pb2.MountFile] = []
            async with aclosing(async_map(gen_file_upload_specs(), self._upload_file, concurrency=20)) as stream:
                async for item in stream:
                    files.append(item)

            self._progress_cb(complete=True)

            request = api_pb2.VolumePutFilesRequest(
                volume_id=self._volume_id,
                files=files,
                disallow_overwrite_existing_files=not self._force,
            )
            try:
                await retry_transient_errors(self._client.stub.VolumePutFiles, request, base_delay=1)
            except GRPCError as exc:
                raise FileExistsError(exc.message) if exc.status == Status.ALREADY_EXISTS else exc

    def put_file(
        self,
        local_file: Union[Path, str, BinaryIO],
        remote_path: Union[PurePosixPath, str],
        mode: Optional[int] = None,
    ):
        """Upload a file from a local file or file-like object.

        Will create any needed parent directories automatically.

        If `local_file` is a file-like object it must remain readable for the lifetime of the batch.
        """
        remote_path = PurePosixPath(remote_path).as_posix()
        if remote_path.endswith("/"):
            raise ValueError(f"remote_path ({remote_path}) must refer to a file - cannot end with /")

        def gen():
            if isinstance(local_file, str) or isinstance(local_file, Path):
                yield lambda: get_file_upload_spec_from_path(local_file, PurePosixPath(remote_path), mode)
            else:
                yield lambda: get_file_upload_spec_from_fileobj(local_file, PurePosixPath(remote_path), mode or 0o644)

        self._upload_generators.append(gen())

    def put_directory(
        self,
        local_path: Union[Path, str],
        remote_path: Union[PurePosixPath, str],
        recursive: bool = True,
    ):
        """
        Upload all files in a local directory.

        Will create any needed parent directories automatically.
        """
        local_path = Path(local_path)
        assert local_path.is_dir()
        remote_path = PurePosixPath(remote_path)

        def create_file_spec_provider(subpath):
            relpath_str = subpath.relative_to(local_path)
            return lambda: get_file_upload_spec_from_path(subpath, remote_path / relpath_str)

        def gen():
            glob = local_path.rglob("*") if recursive else local_path.glob("*")
            for subpath in glob:
                # Skip directories and unsupported file types (e.g. block devices)
                if subpath.is_file():
                    yield create_file_spec_provider(subpath)

        self._upload_generators.append(gen())

    async def _upload_file(self, file_spec: FileUploadSpec) -> api_pb2.MountFile:
        remote_filename = file_spec.mount_filename
        progress_task_id = self._progress_cb(name=remote_filename, size=file_spec.size)
        request = api_pb2.MountPutFileRequest(sha256_hex=file_spec.sha256_hex)
        response = await retry_transient_errors(self._client.stub.MountPutFile, request, base_delay=1)

        start_time = time.monotonic()
        if not response.exists:
            if file_spec.use_blob:
                logger.debug(f"Creating blob file for {file_spec.source_description} ({file_spec.size} bytes)")
                with file_spec.source() as fp:
                    blob_id = await blob_upload_file(
                        fp,
                        self._client.stub,
                        functools.partial(self._progress_cb, progress_task_id),
                        sha256_hex=file_spec.sha256_hex,
                        md5_hex=file_spec.md5_hex,
                    )
                logger.debug(f"Uploading blob file {file_spec.source_description} as {remote_filename}")
                request2 = api_pb2.MountPutFileRequest(data_blob_id=blob_id, sha256_hex=file_spec.sha256_hex)
            else:
                logger.debug(
                    f"Uploading file {file_spec.source_description} to {remote_filename} ({file_spec.size} bytes)"
                )
                request2 = api_pb2.MountPutFileRequest(data=file_spec.content, sha256_hex=file_spec.sha256_hex)
                self._progress_cb(task_id=progress_task_id, complete=True)

            while (time.monotonic() - start_time) < VOLUME_PUT_FILE_CLIENT_TIMEOUT:
                response = await retry_transient_errors(self._client.stub.MountPutFile, request2, base_delay=1)
                if response.exists:
                    break

            if not response.exists:
                raise VolumeUploadTimeoutError(f"Uploading of {file_spec.source_description} timed out")
        else:
            self._progress_cb(task_id=progress_task_id, complete=True)
        return api_pb2.MountFile(
            filename=remote_filename,
            sha256_hex=file_spec.sha256_hex,
            mode=file_spec.mode,
        )


Volume = synchronize_api(_Volume)
VolumeUploadContextManager = synchronize_api(_VolumeUploadContextManager)


def _open_files_error_annotation(mount_path: str) -> Optional[str]:
    if platform.system() != "Linux":
        return None

    self_pid = os.readlink("/proc/self")

    def find_open_file_for_pid(pid: str) -> Optional[str]:
        # /proc/{pid}/cmdline is null separated
        with open(f"/proc/{pid}/cmdline", "rb") as f:
            raw = f.read()
            parts = raw.split(b"\0")
            cmdline = " ".join([part.decode() for part in parts]).rstrip(" ")

        cwd = PurePosixPath(os.readlink(f"/proc/{pid}/cwd"))
        if cwd.is_relative_to(mount_path):
            if pid == self_pid:
                return "cwd is inside volume"
            else:
                return f"cwd of '{cmdline}' is inside volume"

        for fd in os.listdir(f"/proc/{pid}/fd"):
            try:
                path = PurePosixPath(os.readlink(f"/proc/{pid}/fd/{fd}"))
                try:
                    rel_path = path.relative_to(mount_path)
                    if pid == self_pid:
                        return f"path {rel_path} is open"
                    else:
                        return f"path {rel_path} is open from '{cmdline}'"
                except ValueError:
                    pass

            except FileNotFoundError:
                # File was closed
                pass
        return None

    pid_re = re.compile("^[1-9][0-9]*$")
    for dirent in os.listdir("/proc/"):
        if pid_re.match(dirent):
            try:
                annotation = find_open_file_for_pid(dirent)
                if annotation:
                    return annotation
            except (FileNotFoundError, PermissionError):
                pass

    return None


================================================
File: modal/_runtime/__init__.py
================================================
# Copyright Modal Labs 2024


================================================
File: modal/_runtime/asgi.py
================================================
# Copyright Modal Labs 2022

# Note: this module isn't imported unless it's needed.
# This is because aiohttp is a pretty big dependency that adds significant latency when imported

import asyncio
from collections.abc import AsyncGenerator
from typing import Any, Callable, NoReturn, Optional, cast

import aiohttp

from modal._utils.async_utils import TaskContext
from modal._utils.blob_utils import MAX_OBJECT_SIZE_BYTES
from modal._utils.package_utils import parse_major_minor_version
from modal.config import logger
from modal.exception import ExecutionError, InvalidError
from modal.experimental import stop_fetching_inputs

from .execution_context import current_function_call_id

FIRST_MESSAGE_TIMEOUT_SECONDS = 5.0


class LifespanManager:
    _startup: asyncio.Future
    _shutdown: asyncio.Future
    _queue: asyncio.Queue
    _has_run_init: bool = False
    _lifespan_supported: bool = False

    def __init__(self, asgi_app, state):
        self.asgi_app = asgi_app
        self.state = state

    async def ensure_init(self):
        # making this async even though
        # no async code since it has to run inside
        # the event loop to tie the
        # objects to the correct loop in python 3.9
        if not self._has_run_init:
            self._queue = asyncio.Queue()
            self._startup = asyncio.Future()
            self._shutdown = asyncio.Future()
            self._has_run_init = True

    async def background_task(self):
        await self.ensure_init()

        async def receive():
            self._lifespan_supported = True
            return await self._queue.get()

        async def send(message):
            if message["type"] == "lifespan.startup.complete":
                self._startup.set_result(None)
            elif message["type"] == "lifespan.startup.failed":
                self._startup.set_exception(ExecutionError("ASGI lifespan startup failed"))
            elif message["type"] == "lifespan.shutdown.complete":
                self._shutdown.set_result(None)
            elif message["type"] == "lifespan.shutdown.failed":
                self._shutdown.set_exception(ExecutionError("ASGI lifespan shutdown failed"))
            else:
                raise ExecutionError(f"Unexpected message type: {message['type']}")

        try:
            await self.asgi_app({"type": "lifespan", "state": self.state}, receive, send)
        except Exception as e:
            if not self._lifespan_supported:
                logger.info(f"ASGI lifespan task exited before receiving any messages with exception:\n{e}")
                if not self._startup.done():
                    self._startup.set_result(None)
                if not self._shutdown.done():
                    self._shutdown.set_result(None)
                return

            logger.error(f"Error in ASGI lifespan task: {e}", exc_info=True)
            if not self._startup.done():
                self._startup.set_exception(ExecutionError("ASGI lifespan task exited startup"))
            if not self._shutdown.done():
                self._shutdown.set_exception(ExecutionError("ASGI lifespan task exited shutdown"))
        else:
            logger.info("ASGI Lifespan protocol is probably not supported by this library")
            if not self._startup.done():
                self._startup.set_result(None)
            if not self._shutdown.done():
                self._shutdown.set_result(None)

    async def lifespan_startup(self):
        await self.ensure_init()
        self._queue.put_nowait({"type": "lifespan.startup"})
        await self._startup

    async def lifespan_shutdown(self):
        await self.ensure_init()
        self._queue.put_nowait({"type": "lifespan.shutdown"})
        await self._shutdown


def asgi_app_wrapper(asgi_app, container_io_manager) -> tuple[Callable[..., AsyncGenerator], LifespanManager]:
    state: dict[str, Any] = {}  # used for lifespan state

    async def fn(scope):
        if "state" in scope:
            # we don't expect users to set state in ASGI scope
            # this should be handled internally by the LifespanManager
            raise ExecutionError("Unpexected state in ASGI scope")
        scope["state"] = state
        function_call_id = current_function_call_id()
        assert function_call_id, "internal error: function_call_id not set in asgi_app() scope"

        messages_from_app: asyncio.Queue[dict[str, Any]] = asyncio.Queue(1)
        messages_to_app: asyncio.Queue[dict[str, Any]] = asyncio.Queue(1)

        async def disconnect_app():
            if scope["type"] == "http":
                await messages_to_app.put({"type": "http.disconnect"})
            elif scope["type"] == "websocket":
                await messages_to_app.put({"type": "websocket.disconnect"})

        async def handle_first_input_timeout():
            if scope["type"] == "http":
                await messages_from_app.put({"type": "http.response.start", "status": 502})
                await messages_from_app.put(
                    {
                        "type": "http.response.body",
                        "body": b"Missing request, possibly due to expiry or cancellation",
                    }
                )
            elif scope["type"] == "websocket":
                await messages_from_app.put(
                    {
                        "type": "websocket.close",
                        "code": 1011,
                        "reason": "Missing request, possibly due to expiry or cancellation",
                    }
                )
            await disconnect_app()

        async def fetch_data_in():
            # Cancel an ASGI app call if the initial message is not received within a short timeout.
            #
            # This initial message, "http.request" or "websocket.connect", should be sent
            # immediately after starting the ASGI app's function call. If it is not received, that
            # indicates a request cancellation or other abnormal circumstance.
            message_gen = container_io_manager.get_data_in.aio(function_call_id)
            first_message_task = asyncio.create_task(message_gen.__anext__())

            try:
                # we are intentionally shielding + manually cancelling first_message_task, since cancellations
                # can otherwise get ignored in case the cancellation and an awaited future resolve gets
                # triggered in the same sequence before handing back control to the event loop.
                first_message = await asyncio.shield(
                    asyncio.wait_for(first_message_task, FIRST_MESSAGE_TIMEOUT_SECONDS)
                )
            except asyncio.CancelledError:
                if not first_message_task.done():
                    # see comment above about manual cancellation
                    first_message_task.cancel()
                raise
            except (asyncio.TimeoutError, StopAsyncIteration):
                # About `StopAsyncIteration` above: The generator shouldn't typically exit,
                # but if it does, we handle it like a timeout in that case.
                await handle_first_input_timeout()
                return
            except Exception:
                logger.exception("Internal error in asgi_app_wrapper")
                await disconnect_app()
                return

            await messages_to_app.put(first_message)
            async for message in message_gen:
                await messages_to_app.put(message)

        async def send(msg):
            # Automatically split body chunks that are greater than the output size limit, to
            # prevent them from being uploaded to S3.
            if msg["type"] == "http.response.body":
                body_chunk_size = MAX_OBJECT_SIZE_BYTES - 1024  # reserve 1 KiB for framing
                body_chunk_limit = 20 * body_chunk_size
                s3_chunk_size = 50 * body_chunk_size

                size = len(msg.get("body", b""))
                if size <= body_chunk_limit:
                    chunk_size = body_chunk_size
                else:
                    # If the body is _very large_, we should still split it up to avoid sending all
                    # of the data in a huge chunk in S3.
                    chunk_size = s3_chunk_size

                if size > chunk_size:
                    indices = list(range(0, size, chunk_size))
                    for i in indices[:-1]:
                        chunk = msg["body"][i : i + chunk_size]
                        await messages_from_app.put({"type": "http.response.body", "body": chunk, "more_body": True})
                    msg["body"] = msg["body"][indices[-1] :]

            await messages_from_app.put(msg)

        # Run the ASGI app, while draining the send message queue at the same time,
        # and yielding results.
        async with TaskContext() as tc:
            tc.create_task(fetch_data_in())

            async def receive():
                return await messages_to_app.get()

            app_task = tc.create_task(asgi_app(scope, receive, send))
            pop_task = None
            while True:
                pop_task = tc.create_task(messages_from_app.get())

                try:
                    done, pending = await asyncio.wait([pop_task, app_task], return_when=asyncio.FIRST_COMPLETED)
                except asyncio.CancelledError:
                    break

                if pop_task in done:
                    yield pop_task.result()
                else:
                    # clean up the popping task, or we will leak unresolved tasks every loop iteration
                    pop_task.cancel()

                if app_task in done:
                    while not messages_from_app.empty():
                        yield messages_from_app.get_nowait()
                    app_task.result()  # consume/raise exceptions if there are any!
                    break

    return fn, LifespanManager(asgi_app, state)


def wsgi_app_wrapper(wsgi_app, container_io_manager):
    from modal._vendor.a2wsgi_wsgi import WSGIMiddleware

    asgi_app = WSGIMiddleware(wsgi_app, workers=10000, send_queue_size=1)  # unlimited workers
    return asgi_app_wrapper(asgi_app, container_io_manager)


def webhook_asgi_app(fn: Callable[..., Any], method: str, docs: bool):
    """Return a FastAPI app wrapping a function handler."""
    try:
        from fastapi import FastAPI
        from fastapi.middleware.cors import CORSMiddleware
    except ImportError as exc:
        message = (
            "Modal web_endpoint functions require FastAPI to be installed in the modal.Image."
            ' Please update your Image definition code, e.g. with `.pip_install("fastapi[standard]")`.'
        )
        raise InvalidError(message) from exc

    app = FastAPI(openapi_url="/openapi.json" if docs else None)  # disabling openapi spec disables all docs
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    app.add_api_route("/", fn, methods=[method])
    return app


def get_ip_address(ifname: bytes):
    """Get the IP address associated with a network interface in Linux."""
    import fcntl
    import socket
    import struct

    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    return socket.inet_ntoa(
        fcntl.ioctl(
            s.fileno(),
            0x8915,  # SIOCGIFADDR
            struct.pack("256s", ifname[:15]),
        )[20:24]
    )


def wait_for_web_server(host: str, port: int, *, timeout: float) -> None:
    """Wait until a web server port starts accepting TCP connections."""
    import socket
    import time

    start_time = time.monotonic()
    while True:
        try:
            with socket.create_connection((host, port), timeout=timeout):
                break
        except OSError as ex:
            time.sleep(0.01)
            if time.monotonic() - start_time >= timeout:
                raise TimeoutError(
                    f"Waited too long for port {port} to start accepting connections. "
                    "Make sure the web server is bound to 0.0.0.0 (rather than localhost or 127.0.0.1), "
                    "or adjust `startup_timeout`."
                ) from ex


def _add_forwarded_for_header(scope):
    # we strip X-Forwarded-For headers from the scope
    # but we can add it back from the ASGI scope
    # https://asgi.readthedocs.io/en/latest/specs/www.html#http-connection-scope

    # X-Forwarded-For headers is a comma separated list of IP addresses
    # there may be multiple X-Forwarded-For headers
    # we want to prepend the client IP to the first one
    # but only if it doesn't exist in one of the headers already

    first_x_forwarded_for_idx = None
    if "headers" in scope and "client" in scope:
        client_host = scope["client"][0]

        for idx, header in enumerate(scope["headers"]):
            if header[0] == b"X-Forwarded-For":
                if first_x_forwarded_for_idx is None:
                    first_x_forwarded_for_idx = idx
                values = header[1].decode().split(", ")

                if client_host in values:
                    # we already have the client IP in this header
                    # return early
                    return scope

        if first_x_forwarded_for_idx is not None:
            # we have X-Forwarded-For headers but they don't have the client IP
            # we need to prepend the client IP to the first one
            values = [client_host] + scope["headers"][first_x_forwarded_for_idx][1].decode().split(", ")
            scope["headers"][first_x_forwarded_for_idx] = (b"X-Forwarded-For", ", ".join(values).encode())
        else:
            # we don't have X-Forwarded-For headers, we need to add one
            scope["headers"].append((b"X-Forwarded-For", client_host.encode()))

    return scope


async def _proxy_http_request(session: aiohttp.ClientSession, scope, receive, send) -> None:
    proxy_response: aiohttp.ClientResponse

    scope = _add_forwarded_for_header(scope)

    async def request_generator() -> AsyncGenerator[bytes, None]:
        while True:
            message = await receive()
            if message["type"] == "http.request":
                body = message.get("body", b"")
                if body:
                    yield body
                if not message.get("more_body", False):
                    break
            elif message["type"] == "http.disconnect":
                raise ConnectionAbortedError("Disconnect message received")
            else:
                raise ExecutionError(f"Unexpected message type: {message['type']}")

    path = scope["path"]
    if scope.get("query_string"):
        path += "?" + scope["query_string"].decode()

    try:
        proxy_response = await session.request(
            method=scope["method"],
            url=path,
            headers=[(k.decode(), v.decode()) for k, v in scope["headers"]],
            data=None if scope["method"] in aiohttp.ClientRequest.GET_METHODS else request_generator(),
            allow_redirects=False,
        )
    except ConnectionAbortedError:
        return
    except aiohttp.ClientConnectionError as e:  # some versions of aiohttp wrap the error
        if isinstance(e.__cause__, ConnectionAbortedError):
            return
        raise

    async def send_response() -> None:
        msg = {
            "type": "http.response.start",
            "status": proxy_response.status,
            "headers": [(k.encode(), v.encode()) for k, v in proxy_response.headers.items()],
        }
        await send(msg)
        async for data in proxy_response.content.iter_any():
            msg = {"type": "http.response.body", "body": data, "more_body": True}
            await send(msg)
        await send({"type": "http.response.body"})

    async def listen_for_disconnect() -> NoReturn:
        while True:
            message = await receive()
            if (
                message["type"] == "http.disconnect"
                and proxy_response.connection is not None
                and proxy_response.connection.transport is not None
            ):
                proxy_response.connection.transport.abort()

    async with TaskContext() as tc:
        send_response_task = tc.create_task(send_response())
        disconnect_task = tc.create_task(listen_for_disconnect())
        await asyncio.wait([send_response_task, disconnect_task], return_when=asyncio.FIRST_COMPLETED)


async def _proxy_websocket_request(session: aiohttp.ClientSession, scope, receive, send) -> None:
    first_message = await receive()  # Consume the initial "websocket.connect" message.
    if first_message["type"] == "websocket.disconnect":
        return
    elif first_message["type"] != "websocket.connect":
        raise ExecutionError(f"Unexpected message type: {first_message['type']}")

    path = scope["path"]
    if scope.get("query_string"):
        path += "?" + scope["query_string"].decode()

    async with session.ws_connect(
        url=path,
        headers=[(k.decode(), v.decode()) for k, v in scope["headers"]],  # type: ignore
        protocols=scope.get("subprotocols", []),
    ) as upstream_ws:

        async def client_to_upstream():
            while True:
                client_message = await receive()
                if client_message["type"] == "websocket.disconnect":
                    await upstream_ws.close(code=client_message.get("code", 1005))
                    break
                elif client_message["type"] == "websocket.receive":
                    if client_message.get("text") is not None:
                        await upstream_ws.send_str(client_message["text"])
                    elif client_message.get("bytes") is not None:
                        await upstream_ws.send_bytes(client_message["bytes"])
                else:
                    raise ExecutionError(f"Unexpected message type: {client_message['type']}")

        async def upstream_to_client():
            msg: dict[str, Any] = {
                "type": "websocket.accept",
                "subprotocol": upstream_ws.protocol,
            }
            await send(msg)

            while True:
                upstream_message = await upstream_ws.receive()
                if upstream_message.type == aiohttp.WSMsgType.closed:
                    msg = {"type": "websocket.close"}
                    if upstream_message.data is not None:
                        msg["code"] = cast(aiohttp.WSCloseCode, upstream_message.data).value
                        msg["reason"] = upstream_message.extra
                    await send(msg)
                    break
                elif upstream_message.type == aiohttp.WSMsgType.text:
                    await send({"type": "websocket.send", "text": upstream_message.data})
                elif upstream_message.type == aiohttp.WSMsgType.binary:
                    await send({"type": "websocket.send", "bytes": upstream_message.data})
                else:
                    pass  # Ignore all other upstream WebSocket message types.

        async with TaskContext() as tc:
            client_to_upstream_task = tc.create_task(client_to_upstream())
            upstream_to_client_task = tc.create_task(upstream_to_client())
            await asyncio.wait([client_to_upstream_task, upstream_to_client_task], return_when=asyncio.FIRST_COMPLETED)


async def _proxy_lifespan_request(base_url, scope, receive, send) -> None:
    session: Optional[aiohttp.ClientSession] = None
    while True:
        message = await receive()
        if message["type"] == "lifespan.startup":
            if session is None:
                session = aiohttp.ClientSession(
                    base_url,
                    cookie_jar=aiohttp.DummyCookieJar(),
                    timeout=aiohttp.ClientTimeout(total=3600),
                    auto_decompress=False,
                    read_bufsize=1024 * 1024,  # 1 MiB
                    connector=aiohttp.TCPConnector(
                        limit=1000
                    ),  # 100 is the default max, but 1000 is the max for `allow_concurrent_inputs`.
                    # Note: these values will need to be kept in sync.
                    **(
                        # These options were introduced in aiohttp 3.9, and we can remove the
                        # conditional after deprecating image builder version 2023.12.
                        dict(  # type: ignore
                            max_line_size=64 * 1024,  # 64 KiB
                            max_field_size=64 * 1024,  # 64 KiB
                        )
                        if parse_major_minor_version(aiohttp.__version__) >= (3, 9)
                        else {}
                    ),
                )
                scope["state"]["session"] = session
            await send({"type": "lifespan.startup.complete"})
        elif message["type"] == "lifespan.shutdown":
            if session is not None:
                await session.close()
            await send({"type": "lifespan.shutdown.complete"})
            break
        else:
            raise ExecutionError(f"Unexpected message type: {message['type']}")


def web_server_proxy(host: str, port: int):
    """Return an ASGI app that proxies requests to a web server running on the same host."""
    if not 0 < port < 65536:
        raise InvalidError(f"Invalid port number: {port}")

    base_url = f"http://{host}:{port}"

    async def web_server_proxy_app(scope, receive, send):
        try:
            if scope["type"] == "lifespan":
                await _proxy_lifespan_request(base_url, scope, receive, send)
            elif scope["type"] == "http":
                await _proxy_http_request(scope["state"]["session"], scope, receive, send)
            elif scope["type"] == "websocket":
                await _proxy_websocket_request(scope["state"]["session"], scope, receive, send)
            else:
                raise NotImplementedError(f"Scope {scope} is not understood")

        except aiohttp.ClientConnectorError as exc:
            # If the server is not running or not reachable, we should stop fetching new inputs.
            logger.warning(f"Terminating runner due to @web_server connection issue: {exc}")
            stop_fetching_inputs()

    return web_server_proxy_app


================================================
File: modal/_runtime/container_io_manager.py
================================================
# Copyright Modal Labs 2024
import asyncio
import importlib.metadata
import inspect
import json
import math
import os
import sys
import time
import traceback
from collections.abc import AsyncGenerator, AsyncIterator
from contextlib import AsyncExitStack
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    ClassVar,
    Optional,
)

from google.protobuf.empty_pb2 import Empty
from grpclib import Status
from synchronicity.async_wrap import asynccontextmanager

import modal_proto.api_pb2
from modal._runtime import gpu_memory_snapshot
from modal._serialization import deserialize, serialize, serialize_data_format
from modal._traceback import extract_traceback, print_exception
from modal._utils.async_utils import TaskContext, asyncify, synchronize_api, synchronizer
from modal._utils.blob_utils import MAX_OBJECT_SIZE_BYTES, blob_download, blob_upload
from modal._utils.function_utils import _stream_function_call_data
from modal._utils.grpc_utils import retry_transient_errors
from modal._utils.package_utils import parse_major_minor_version
from modal.client import HEARTBEAT_INTERVAL, HEARTBEAT_TIMEOUT, _Client
from modal.config import config, logger
from modal.exception import ClientClosed, InputCancellation, InvalidError, SerializationError
from modal_proto import api_pb2

if TYPE_CHECKING:
    import modal._runtime.asgi
    import modal._runtime.user_code_imports


DYNAMIC_CONCURRENCY_INTERVAL_SECS = 3
DYNAMIC_CONCURRENCY_TIMEOUT_SECS = 10
MAX_OUTPUT_BATCH_SIZE: int = 49

RTT_S: float = 0.5  # conservative estimate of RTT in seconds.


class UserException(Exception):
    """Used to shut down the task gracefully."""


class Sentinel:
    """Used to get type-stubs to work with this object."""


class IOContext:
    """Context object for managing input, function calls, and function executions
    in a batched or single input context.
    """

    input_ids: list[str]
    function_call_ids: list[str]
    finalized_function: "modal._runtime.user_code_imports.FinalizedFunction"

    _cancel_issued: bool = False
    _cancel_callback: Optional[Callable[[], None]] = None

    def __init__(
        self,
        input_ids: list[str],
        function_call_ids: list[str],
        finalized_function: "modal._runtime.user_code_imports.FinalizedFunction",
        function_inputs: list[api_pb2.FunctionInput],
        is_batched: bool,
        client: _Client,
    ):
        self.input_ids = input_ids
        self.function_call_ids = function_call_ids
        self.finalized_function = finalized_function
        self._function_inputs = function_inputs
        self._is_batched = is_batched
        self._client = client

    @classmethod
    async def create(
        cls,
        client: _Client,
        finalized_functions: dict[str, "modal._runtime.user_code_imports.FinalizedFunction"],
        inputs: list[tuple[str, str, api_pb2.FunctionInput]],
        is_batched: bool,
    ) -> "IOContext":
        assert len(inputs) >= 1 if is_batched else len(inputs) == 1
        input_ids, function_call_ids, function_inputs = zip(*inputs)

        async def _populate_input_blobs(client: _Client, input: api_pb2.FunctionInput) -> api_pb2.FunctionInput:
            # If we got a pointer to a blob, download it from S3.
            if input.WhichOneof("args_oneof") == "args_blob_id":
                args = await blob_download(input.args_blob_id, client.stub)
                # Mutating
                input.ClearField("args_blob_id")
                input.args = args

            return input

        function_inputs = await asyncio.gather(*[_populate_input_blobs(client, input) for input in function_inputs])
        # check every input in batch executes the same function
        method_name = function_inputs[0].method_name
        assert all(method_name == input.method_name for input in function_inputs)
        finalized_function = finalized_functions[method_name]
        return cls(input_ids, function_call_ids, finalized_function, function_inputs, is_batched, client)

    def set_cancel_callback(self, cb: Callable[[], None]):
        self._cancel_callback = cb

    def cancel(self):
        # Ensure we only issue the cancellation once.
        if self._cancel_issued:
            return

        if self._cancel_callback:
            logger.warning(f"Received a cancellation signal while processing input {self.input_ids}")
            self._cancel_issued = True
            self._cancel_callback()
        else:
            # TODO (elias): This should not normally happen but there is a small chance of a race
            #  between creating a new task for an input and attaching the cancellation callback
            logger.warning("Unexpected: Could not cancel input")

    def _args_and_kwargs(self) -> tuple[tuple[Any, ...], dict[str, list[Any]]]:
        # deserializing here instead of the constructor
        # to make sure we handle user exceptions properly
        # and don't retry
        deserialized_args = [
            deserialize(input.args, self._client) if input.args else ((), {}) for input in self._function_inputs
        ]
        if not self._is_batched:
            return deserialized_args[0]

        func_name = self.finalized_function.callable.__name__

        param_names = []
        for param in inspect.signature(self.finalized_function.callable).parameters.values():
            param_names.append(param.name)

        # aggregate args and kwargs of all inputs into a kwarg dict
        kwargs_by_inputs: list[dict[str, Any]] = [{} for _ in range(len(self.input_ids))]

        for i, (args, kwargs) in enumerate(deserialized_args):
            # check that all batched inputs should have the same number of args and kwargs
            if (num_params := len(args) + len(kwargs)) != len(param_names):
                raise InvalidError(
                    f"Modal batched function {func_name} takes {len(param_names)} positional arguments, but one invocation in the batch has {num_params}."  # noqa
                )

            for j, arg in enumerate(args):
                kwargs_by_inputs[i][param_names[j]] = arg
            for k, v in kwargs.items():
                if k not in param_names:
                    raise InvalidError(
                        f"Modal batched function {func_name} got unexpected keyword argument {k} in one invocation in the batch."  # noqa
                    )
                if k in kwargs_by_inputs[i]:
                    raise InvalidError(
                        f"Modal batched function {func_name} got multiple values for argument {k} in one invocation in the batch."  # noqa
                    )
                kwargs_by_inputs[i][k] = v

        formatted_kwargs = {
            param_name: [kwargs[param_name] for kwargs in kwargs_by_inputs] for param_name in param_names
        }
        return (), formatted_kwargs

    def call_finalized_function(self) -> Any:
        logger.debug(f"Starting input {self.input_ids}")
        args, kwargs = self._args_and_kwargs()
        res = self.finalized_function.callable(*args, **kwargs)
        logger.debug(f"Finished input {self.input_ids}")
        return res

    def validate_output_data(self, data: Any) -> list[Any]:
        if not self._is_batched:
            return [data]

        function_name = self.finalized_function.callable.__name__
        if not isinstance(data, list):
            raise InvalidError(f"Output of batched function {function_name} must be a list.")
        if len(data) != len(self.input_ids):
            raise InvalidError(
                f"Output of batched function {function_name} must be a list of equal length as its inputs."
            )
        return data


class InputSlots:
    """A semaphore that allows dynamically adjusting the concurrency."""

    active: int
    value: int
    waiter: Optional[asyncio.Future]
    closed: bool

    def __init__(self, value: int) -> None:
        self.active = 0
        self.value = value
        self.waiter = None
        self.closed = False

    async def acquire(self) -> None:
        if self.active < self.value:
            self.active += 1
        elif self.waiter is None:
            self.waiter = asyncio.get_running_loop().create_future()
            await self.waiter
        else:
            raise RuntimeError("Concurrent waiters are not supported.")

    def _wake_waiter(self) -> None:
        if self.active < self.value and self.waiter is not None:
            if not self.waiter.cancelled():  # could have been cancelled during interpreter shutdown
                self.waiter.set_result(None)
            self.waiter = None
            self.active += 1

    def release(self) -> None:
        self.active -= 1
        self._wake_waiter()

    def set_value(self, value: int) -> None:
        if self.closed:
            return
        self.value = value
        self._wake_waiter()

    async def close(self) -> None:
        self.closed = True
        for _ in range(self.value):
            await self.acquire()


class _ContainerIOManager:
    """Synchronizes all RPC calls and network operations for a running container.

    TODO: maybe we shouldn't synchronize the whole class.
    Then we could potentially move a bunch of the global functions onto it.
    """

    task_id: str
    function_id: str
    app_id: str
    function_def: api_pb2.Function
    checkpoint_id: Optional[str]

    calls_completed: int
    total_user_time: float
    current_input_id: Optional[str]
    current_inputs: dict[str, IOContext]  # input_id -> IOContext
    current_input_started_at: Optional[float]

    _target_concurrency: int
    _max_concurrency: int
    _concurrency_loop: Optional[asyncio.Task]
    _input_slots: InputSlots

    _environment_name: str
    _heartbeat_loop: Optional[asyncio.Task]
    _heartbeat_condition: Optional[asyncio.Condition]
    _waiting_for_memory_snapshot: bool

    _is_interactivity_enabled: bool
    _fetching_inputs: bool

    _client: _Client

    _GENERATOR_STOP_SENTINEL: ClassVar[Sentinel] = Sentinel()
    _singleton: ClassVar[Optional["_ContainerIOManager"]] = None

    def _init(self, container_args: api_pb2.ContainerArguments, client: _Client):
        self.task_id = container_args.task_id
        self.function_id = container_args.function_id
        self.app_id = container_args.app_id
        self.function_def = container_args.function_def
        self.checkpoint_id = container_args.checkpoint_id or None

        self.calls_completed = 0
        self.total_user_time = 0.0
        self.current_input_id = None
        self.current_inputs = {}
        self.current_input_started_at = None

        if container_args.function_def.pty_info.pty_type == api_pb2.PTYInfo.PTY_TYPE_SHELL:
            target_concurrency = 1
            max_concurrency = 1
        else:
            target_concurrency = container_args.function_def.target_concurrent_inputs or 1
            max_concurrency = container_args.function_def.max_concurrent_inputs or target_concurrency

        self._target_concurrency = target_concurrency
        self._max_concurrency = max_concurrency
        self._concurrency_loop = None
        self._stop_concurrency_loop = False
        self._input_slots = InputSlots(target_concurrency)

        self._environment_name = container_args.environment_name
        self._heartbeat_loop = None
        self._heartbeat_condition = None
        self._waiting_for_memory_snapshot = False

        self._is_interactivity_enabled = False
        self._fetching_inputs = True

        self._client = client
        assert isinstance(self._client, _Client)

    @property
    def heartbeat_condition(self) -> asyncio.Condition:
        # ensures that heartbeat condition isn't assigned to an event loop until it's used for the first time
        # (On Python 3.9 and below it would be assigned to the current thread's event loop on creation)
        if self._heartbeat_condition is None:
            self._heartbeat_condition = asyncio.Condition()
        return self._heartbeat_condition

    def __new__(cls, container_args: api_pb2.ContainerArguments, client: _Client) -> "_ContainerIOManager":
        cls._singleton = super().__new__(cls)
        cls._singleton._init(container_args, client)
        return cls._singleton

    @classmethod
    def _reset_singleton(cls):
        """Only used for tests."""
        cls._singleton = None

    async def hello(self):
        await self._client.stub.ContainerHello(Empty())

    async def _run_heartbeat_loop(self):
        while 1:
            t0 = time.monotonic()
            try:
                if await self._heartbeat_handle_cancellations():
                    # got a cancellation event, fine to start another heartbeat immediately
                    # since the cancellation queue should be empty on the worker server
                    # however, we wait at least 1s to prevent short-circuiting the heartbeat loop
                    # in case there is ever a bug. This means it will take at least 1s between
                    # two subsequent cancellations on the same task at the moment
                    await asyncio.sleep(1.0)
                    continue
            except ClientClosed:
                logger.info("Stopping heartbeat loop due to client shutdown")
                break
            except Exception as exc:
                # don't stop heartbeat loop if there are transient exceptions!
                time_elapsed = time.monotonic() - t0
                error = exc
                logger.warning(f"Heartbeat attempt failed ({time_elapsed=}, {error=})")

            heartbeat_duration = time.monotonic() - t0
            time_until_next_hearbeat = max(0.0, HEARTBEAT_INTERVAL - heartbeat_duration)
            await asyncio.sleep(time_until_next_hearbeat)

    async def _heartbeat_handle_cancellations(self) -> bool:
        # Return True if a cancellation event was received, in that case
        # we shouldn't wait too long for another heartbeat
        async with self.heartbeat_condition:
            # Continuously wait until `waiting_for_memory_snapshot` is false.
            # TODO(matt): Verify that a `while` is necessary over an `if`. Spurious
            # wakeups could allow execution to continue despite `_waiting_for_memory_snapshot`
            # being true.
            while self._waiting_for_memory_snapshot:
                await self.heartbeat_condition.wait()

            request = api_pb2.ContainerHeartbeatRequest(canceled_inputs_return_outputs_v2=True)
            response = await retry_transient_errors(
                self._client.stub.ContainerHeartbeat, request, attempt_timeout=HEARTBEAT_TIMEOUT
            )

        if response.HasField("cancel_input_event"):
            # response.cancel_input_event.terminate_containers is never set, the server gets the worker to handle it.
            input_ids_to_cancel = response.cancel_input_event.input_ids
            if input_ids_to_cancel:
                for input_id in input_ids_to_cancel:
                    if input_id in self.current_inputs:
                        self.current_inputs[input_id].cancel()
            return True
        return False

    @asynccontextmanager
    async def heartbeats(self, wait_for_mem_snap: bool) -> AsyncGenerator[None, None]:
        async with TaskContext() as tc:
            self._heartbeat_loop = t = tc.create_task(self._run_heartbeat_loop())
            t.set_name("heartbeat loop")
            self._waiting_for_memory_snapshot = wait_for_mem_snap
            try:
                yield
            finally:
                t.cancel()

    def stop_heartbeat(self):
        if self._heartbeat_loop:
            self._heartbeat_loop.cancel()

    @asynccontextmanager
    async def dynamic_concurrency_manager(self) -> AsyncGenerator[None, None]:
        async with TaskContext() as tc:
            self._concurrency_loop = t = tc.create_task(self._dynamic_concurrency_loop())
            t.set_name("dynamic concurrency loop")
            try:
                yield
            finally:
                t.cancel()

    async def _dynamic_concurrency_loop(self):
        logger.debug(f"Starting dynamic concurrency loop for task {self.task_id}")
        while not self._stop_concurrency_loop:
            try:
                request = api_pb2.FunctionGetDynamicConcurrencyRequest(
                    function_id=self.function_id,
                    target_concurrency=self._target_concurrency,
                    max_concurrency=self._max_concurrency,
                )
                resp = await retry_transient_errors(
                    self._client.stub.FunctionGetDynamicConcurrency,
                    request,
                    attempt_timeout=DYNAMIC_CONCURRENCY_TIMEOUT_SECS,
                )
                if resp.concurrency != self._input_slots.value and not self._stop_concurrency_loop:
                    logger.debug(f"Dynamic concurrency set from {self._input_slots.value} to {resp.concurrency}")
                    self._input_slots.set_value(resp.concurrency)

            except Exception as exc:
                logger.debug(f"Failed to get dynamic concurrency for task {self.task_id}, {exc}")

            await asyncio.sleep(DYNAMIC_CONCURRENCY_INTERVAL_SECS)

    async def get_serialized_function(self) -> tuple[Optional[Any], Optional[Callable[..., Any]]]:
        # Fetch the serialized function definition
        request = api_pb2.FunctionGetSerializedRequest(function_id=self.function_id)
        response = await self._client.stub.FunctionGetSerialized(request)
        if response.function_serialized:
            fun = self.deserialize(response.function_serialized)
        else:
            fun = None

        if response.class_serialized:
            cls = self.deserialize(response.class_serialized)
        else:
            cls = None

        return cls, fun

    def serialize(self, obj: Any) -> bytes:
        return serialize(obj)

    def deserialize(self, data: bytes) -> Any:
        return deserialize(data, self._client)

    @synchronizer.no_io_translation
    def serialize_data_format(self, obj: Any, data_format: int) -> bytes:
        return serialize_data_format(obj, data_format)

    async def format_blob_data(self, data: bytes) -> dict[str, Any]:
        return (
            {"data_blob_id": await blob_upload(data, self._client.stub)}
            if len(data) > MAX_OBJECT_SIZE_BYTES
            else {"data": data}
        )

    async def get_data_in(self, function_call_id: str) -> AsyncIterator[Any]:
        """Read from the `data_in` stream of a function call."""
        async for data in _stream_function_call_data(self._client, function_call_id, "data_in"):
            yield data

    async def put_data_out(
        self,
        function_call_id: str,
        start_index: int,
        data_format: int,
        serialized_messages: list[Any],
    ) -> None:
        """Put data onto the `data_out` stream of a function call.

        This is used for generator outputs, which includes web endpoint responses. Note that this
        was introduced as a performance optimization in client version 0.57, so older clients will
        still use the previous Postgres-backed system based on `FunctionPutOutputs()`.
        """
        data_chunks: list[api_pb2.DataChunk] = []
        for i, message_bytes in enumerate(serialized_messages):
            chunk = api_pb2.DataChunk(data_format=data_format, index=start_index + i)  # type: ignore
            if len(message_bytes) > MAX_OBJECT_SIZE_BYTES:
                chunk.data_blob_id = await blob_upload(message_bytes, self._client.stub)
            else:
                chunk.data = message_bytes
            data_chunks.append(chunk)

        req = api_pb2.FunctionCallPutDataRequest(function_call_id=function_call_id, data_chunks=data_chunks)
        await retry_transient_errors(self._client.stub.FunctionCallPutDataOut, req)

    async def generator_output_task(self, function_call_id: str, data_format: int, message_rx: asyncio.Queue) -> None:
        """Task that feeds generator outputs into a function call's `data_out` stream."""
        index = 1
        received_sentinel = False
        while not received_sentinel:
            message = await message_rx.get()
            if message is self._GENERATOR_STOP_SENTINEL:
                break
            # ASGI 'http.response.start' and 'http.response.body' msgs are observed to be separated by 1ms.
            # If we don't sleep here for 1ms we end up with an extra call to .put_data_out().
            if index == 1:
                await asyncio.sleep(0.001)
            serialized_messages = [serialize_data_format(message, data_format)]
            total_size = len(serialized_messages[0]) + 512
            while total_size < 16 * 1024 * 1024:  # 16 MiB, maximum size in a single message
                try:
                    message = message_rx.get_nowait()
                except asyncio.QueueEmpty:
                    break
                if message is self._GENERATOR_STOP_SENTINEL:
                    received_sentinel = True
                    break
                else:
                    serialized_messages.append(serialize_data_format(message, data_format))
                    total_size += len(serialized_messages[-1]) + 512  # 512 bytes for estimated framing overhead
            await self.put_data_out(function_call_id, index, data_format, serialized_messages)
            index += len(serialized_messages)

    async def _queue_create(self, size: int) -> asyncio.Queue:
        """Create a queue, on the synchronicity event loop (needed on Python 3.8 and 3.9)."""
        return asyncio.Queue(size)

    async def _queue_put(self, queue: asyncio.Queue, value: Any) -> None:
        """Put a value onto a queue, using the synchronicity event loop."""
        await queue.put(value)

    def get_average_call_time(self) -> float:
        if self.calls_completed == 0:
            return 0

        return self.total_user_time / self.calls_completed

    def get_max_inputs_to_fetch(self):
        if self.calls_completed == 0:
            return 1

        return math.ceil(RTT_S / max(self.get_average_call_time(), 1e-6))

    @synchronizer.no_io_translation
    async def _generate_inputs(
        self,
        batch_max_size: int,
        batch_wait_ms: int,
    ) -> AsyncIterator[list[tuple[str, str, api_pb2.FunctionInput]]]:
        request = api_pb2.FunctionGetInputsRequest(function_id=self.function_id)
        iteration = 0
        while self._fetching_inputs:
            await self._input_slots.acquire()

            request.average_call_time = self.get_average_call_time()
            request.max_values = self.get_max_inputs_to_fetch()  # Deprecated; remove.
            request.input_concurrency = self.get_input_concurrency()
            request.batch_max_size, request.batch_linger_ms = batch_max_size, batch_wait_ms

            yielded = False
            try:
                # If number of active inputs is at max queue size, this will block.
                iteration += 1
                response: api_pb2.FunctionGetInputsResponse = await retry_transient_errors(
                    self._client.stub.FunctionGetInputs, request
                )

                if response.rate_limit_sleep_duration:
                    logger.info(
                        "Task exceeded rate limit, sleeping for %.2fs before trying again."
                        % response.rate_limit_sleep_duration
                    )
                    await asyncio.sleep(response.rate_limit_sleep_duration)
                elif response.inputs:
                    # for input cancellations and concurrency logic we currently assume
                    # that there is no input buffering in the container
                    assert 0 < len(response.inputs) <= max(1, request.batch_max_size)
                    inputs = []
                    final_input_received = False
                    for item in response.inputs:
                        if item.kill_switch:
                            logger.debug(f"Task {self.task_id} input kill signal input.")
                            return

                        inputs.append((item.input_id, item.function_call_id, item.input))
                        if item.input.final_input:
                            if request.batch_max_size > 0:
                                logger.debug(f"Task {self.task_id} Final input not expected in batch input stream")
                            final_input_received = True
                            break

                    # If yielded, allow input slots to be released via exit_context
                    yield inputs
                    yielded = True

                    # We only support max_inputs = 1 at the moment
                    if final_input_received or self.function_def.max_inputs == 1:
                        return
            finally:
                if not yielded:
                    self._input_slots.release()

    @synchronizer.no_io_translation
    async def run_inputs_outputs(
        self,
        finalized_functions: dict[str, "modal._runtime.user_code_imports.FinalizedFunction"],
        batch_max_size: int = 0,
        batch_wait_ms: int = 0,
    ) -> AsyncIterator[IOContext]:
        # Ensure we do not fetch new inputs when container is too busy.
        # Before trying to fetch an input, acquire an input slot:
        # - if no input is fetched, release the input slot.
        # - or, when the output for the fetched input is sent, release the input slot.
        dynamic_concurrency_manager = (
            self.dynamic_concurrency_manager() if self._max_concurrency > self._target_concurrency else AsyncExitStack()
        )
        async with dynamic_concurrency_manager:
            async for inputs in self._generate_inputs(batch_max_size, batch_wait_ms):
                io_context = await IOContext.create(self._client, finalized_functions, inputs, batch_max_size > 0)
                for input_id in io_context.input_ids:
                    self.current_inputs[input_id] = io_context

                self.current_input_id, self.current_input_started_at = io_context.input_ids[0], time.time()
                yield io_context
                self.current_input_id, self.current_input_started_at = (None, None)

            # collect all active input slots, meaning all inputs have wrapped up.
            await self._input_slots.close()

    @synchronizer.no_io_translation
    async def _push_outputs(
        self,
        io_context: IOContext,
        started_at: float,
        data_format: "modal_proto.api_pb2.DataFormat.ValueType",
        results: list[api_pb2.GenericResult],
    ) -> None:
        output_created_at = time.time()
        outputs = [
            api_pb2.FunctionPutOutputsItem(
                input_id=input_id,
                input_started_at=started_at,
                output_created_at=output_created_at,
                result=result,
                data_format=data_format,
            )
            for input_id, result in zip(io_context.input_ids, results)
        ]
        await retry_transient_errors(
            self._client.stub.FunctionPutOutputs,
            api_pb2.FunctionPutOutputsRequest(outputs=outputs),
            additional_status_codes=[Status.RESOURCE_EXHAUSTED],
            max_retries=None,  # Retry indefinitely, trying every 1s.
        )

    def serialize_exception(self, exc: BaseException) -> bytes:
        try:
            return self.serialize(exc)
        except Exception as serialization_exc:
            # We can't always serialize exceptions.
            err = f"Failed to serialize exception {exc} of type {type(exc)}: {serialization_exc}"
            logger.info(err)
            return self.serialize(SerializationError(err))

    def serialize_traceback(self, exc: BaseException) -> tuple[Optional[bytes], Optional[bytes]]:
        serialized_tb, tb_line_cache = None, None

        try:
            tb_dict, line_cache = extract_traceback(exc, self.task_id)
            serialized_tb = self.serialize(tb_dict)
            tb_line_cache = self.serialize(line_cache)
        except Exception:
            logger.info("Failed to serialize exception traceback.")

        return serialized_tb, tb_line_cache

    @asynccontextmanager
    async def handle_user_exception(self) -> AsyncGenerator[None, None]:
        """Sets the task as failed in a way where it's not retried.

        Used for handling exceptions from container lifecycle methods at the moment, which should
        trigger a task failure state.
        """
        try:
            yield
        except KeyboardInterrupt:
            # Send no task result in case we get sigint:ed by the runner
            # The status of the input should have been handled externally already in that case
            raise
        except BaseException as exc:
            if isinstance(exc, ImportError):
                # Catches errors raised by global scope imports
                check_fastapi_pydantic_compatibility(exc)

            # Since this is on a different thread, sys.exc_info() can't find the exception in the stack.
            print_exception(type(exc), exc, exc.__traceback__)

            serialized_tb, tb_line_cache = self.serialize_traceback(exc)

            result = api_pb2.GenericResult(
                status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
                data=self.serialize_exception(exc),
                exception=repr(exc),
                traceback="".join(traceback.format_exception(type(exc), exc, exc.__traceback__)),
                serialized_tb=serialized_tb or b"",
                tb_line_cache=tb_line_cache or b"",
            )

            req = api_pb2.TaskResultRequest(result=result)
            await retry_transient_errors(self._client.stub.TaskResult, req)

            # Shut down the task gracefully
            raise UserException()

    @asynccontextmanager
    async def handle_input_exception(
        self,
        io_context: IOContext,
        started_at: float,
    ) -> AsyncGenerator[None, None]:
        """Handle an exception while processing a function input."""
        try:
            yield
        except (KeyboardInterrupt, GeneratorExit):
            # We need to explicitly reraise these BaseExceptions to not handle them in the catch-all:
            # 1. KeyboardInterrupt can end up here even though this runs on non-main thread, since the
            #    code block yielded to could be sending back a main thread exception
            # 2. GeneratorExit - raised if this (async) generator is garbage collected while waiting
            #    for the yield. Typically on event loop shutdown
            raise
        except (InputCancellation, asyncio.CancelledError):
            # Create terminated outputs for these inputs to signal that the cancellations have been completed.
            results = [
                api_pb2.GenericResult(status=api_pb2.GenericResult.GENERIC_STATUS_TERMINATED)
                for _ in io_context.input_ids
            ]
            await self._push_outputs(
                io_context=io_context,
                started_at=started_at,
                data_format=api_pb2.DATA_FORMAT_PICKLE,
                results=results,
            )
            self.exit_context(started_at, io_context.input_ids)
            logger.warning(f"Successfully canceled input {io_context.input_ids}")
            return
        except BaseException as exc:
            if isinstance(exc, ImportError):
                # Catches errors raised by imports from within function body
                check_fastapi_pydantic_compatibility(exc)

            # print exception so it's logged
            print_exception(*sys.exc_info())

            serialized_tb, tb_line_cache = self.serialize_traceback(exc)

            # Note: we're not serializing the traceback since it contains
            # local references that means we can't unpickle it. We *are*
            # serializing the exception, which may have some issues (there
            # was an earlier note about it that it might not be possible
            # to unpickle it in some cases). Let's watch out for issues.

            repr_exc = repr(exc)
            if len(repr_exc) >= MAX_OBJECT_SIZE_BYTES:
                # We prevent large exception messages to avoid
                # unhandled exceptions causing inf loops
                # and just send backa trimmed version
                trimmed_bytes = len(repr_exc) - MAX_OBJECT_SIZE_BYTES - 1000
                repr_exc = repr_exc[: MAX_OBJECT_SIZE_BYTES - 1000]
                repr_exc = f"{repr_exc}...\nTrimmed {trimmed_bytes} bytes from original exception"

            data: bytes = self.serialize_exception(exc) or b""
            data_result_part = await self.format_blob_data(data)
            results = [
                api_pb2.GenericResult(
                    status=api_pb2.GenericResult.GENERIC_STATUS_FAILURE,
                    exception=repr_exc,
                    traceback=traceback.format_exc(),
                    serialized_tb=serialized_tb or b"",
                    tb_line_cache=tb_line_cache or b"",
                    **data_result_part,
                )
                for _ in io_context.input_ids
            ]
            await self._push_outputs(
                io_context=io_context,
                started_at=started_at,
                data_format=api_pb2.DATA_FORMAT_PICKLE,
                results=results,
            )
            self.exit_context(started_at, io_context.input_ids)

    def exit_context(self, started_at, input_ids: list[str]):
        self.total_user_time += time.time() - started_at
        self.calls_completed += 1

        for input_id in input_ids:
            self.current_inputs.pop(input_id)

        self._input_slots.release()

    @synchronizer.no_io_translation
    async def push_outputs(
        self,
        io_context: IOContext,
        started_at: float,
        data: Any,
        data_format: "modal_proto.api_pb2.DataFormat.ValueType",
    ) -> None:
        data = io_context.validate_output_data(data)
        formatted_data = await asyncio.gather(
            *[self.format_blob_data(self.serialize_data_format(d, data_format)) for d in data]
        )
        results = [
            api_pb2.GenericResult(
                status=api_pb2.GenericResult.GENERIC_STATUS_SUCCESS,
                **d,
            )
            for d in formatted_data
        ]
        await self._push_outputs(
            io_context=io_context,
            started_at=started_at,
            data_format=data_format,
            results=results,
        )
        self.exit_context(started_at, io_context.input_ids)

    async def memory_restore(self) -> None:
        # Busy-wait for restore. `/__modal/restore-state.json` is created
        # by the worker process with updates to the container config.
        restored_path = Path(config.get("restore_state_path"))
        start = time.perf_counter()
        while not restored_path.exists():
            logger.debug(f"Waiting for restore (elapsed={time.perf_counter() - start:.3f}s)")
            await asyncio.sleep(0.01)
            continue

        logger.debug("Container: restored")

        # Look for state file and create new client with updated credentials.
        # State data is serialized with key-value pairs, example: {"task_id": "tk-000"}
        with restored_path.open("r") as file:
            restored_state = json.load(file)

        # Start a debugger if the worker tells us to
        if int(restored_state.get("snapshot_debug", 0)):
            logger.debug("Entering snapshot debugger")
            breakpoint()

        # Local ContainerIOManager state.
        for key in ["task_id", "function_id"]:
            if value := restored_state.get(key):
                logger.debug(f"Updating ContainerIOManager.{key} = {value}")
                setattr(self, key, restored_state[key])

        # Env vars and global state.
        for key, value in restored_state.items():
            # Empty string indicates that value does not need to be updated.
            if value != "":
                config.override_locally(key, value)

        # Restore GPU memory.
        if self.function_def._experimental_enable_gpu_snapshot and self.function_def.resources.gpu_config.gpu_type:
            logger.debug("GPU memory snapshot enabled. Attempting to restore GPU memory.")
            gpu_process_state = gpu_memory_snapshot.get_state()
            if gpu_process_state != gpu_memory_snapshot.CudaCheckpointState.CHECKPOINTED:
                raise ValueError(
                    "Cannot restore GPU state if GPU isn't in a 'checkpointed' state. "
                    f"Current GPU state: {gpu_process_state}"
                )
            gpu_memory_snapshot.toggle()

        # Restore input to default state.
        self.current_input_id = None
        self.current_inputs = {}
        self.current_input_started_at = None
        self._client = await _Client.from_env()

    async def memory_snapshot(self) -> None:
        """Message server indicating that function is ready to be checkpointed."""
        if self.checkpoint_id:
            logger.debug(f"Checkpoint ID: {self.checkpoint_id} (Memory Snapshot ID)")
        else:
            raise ValueError("No checkpoint ID provided for memory snapshot")

        # Pause heartbeats since they keep the client connection open which causes the snapshotter to crash
        async with self.heartbeat_condition:
            # Snapshot GPU memory.
            if self.function_def._experimental_enable_gpu_snapshot and self.function_def.resources.gpu_config.gpu_type:
                logger.debug("GPU memory snapshot enabled. Attempting to snapshot GPU memory.")
                gpu_process_state = gpu_memory_snapshot.get_state()
                if gpu_process_state != gpu_memory_snapshot.CudaCheckpointState.RUNNING:
                    raise ValueError(
                        f"Cannot snapshot GPU state if it isn't running. Current GPU state: {gpu_process_state}"
                    )

                gpu_memory_snapshot.toggle()
                gpu_memory_snapshot.wait_for_state(gpu_memory_snapshot.CudaCheckpointState.CHECKPOINTED)

            # Notify the heartbeat loop that the snapshot phase has begun in order to
            # prevent it from sending heartbeat RPCs
            self._waiting_for_memory_snapshot = True
            self.heartbeat_condition.notify_all()

            await self._client.stub.ContainerCheckpoint(
                api_pb2.ContainerCheckpointRequest(checkpoint_id=self.checkpoint_id)
            )

            await self._client._close(prep_for_restore=True)

            logger.debug("Memory snapshot request sent. Connection closed.")
            await self.memory_restore()
            # Turn heartbeats back on. This is safe since the snapshot RPC
            # and the restore phase has finished.
            self._waiting_for_memory_snapshot = False
            self.heartbeat_condition.notify_all()

    async def volume_commit(self, volume_ids: list[str]) -> None:
        """
        Perform volume commit for given `volume_ids`.
        Only used on container exit to persist uncommitted changes on behalf of user.
        """
        if not volume_ids:
            return
        await asyncify(os.sync)()
        results = await asyncio.gather(
            *[
                retry_transient_errors(
                    self._client.stub.VolumeCommit,
                    api_pb2.VolumeCommitRequest(volume_id=v_id),
                    max_retries=9,
                    base_delay=0.25,
                    max_delay=256,
                    delay_factor=2,
                )
                for v_id in volume_ids
            ],
            return_exceptions=True,
        )
        for volume_id, res in zip(volume_ids, results):
            if isinstance(res, Exception):
                logger.error(f"modal.Volume background commit failed for {volume_id}. Exception: {res}")
            else:
                logger.debug(f"modal.Volume background commit success for {volume_id}.")

    async def interact(self, from_breakpoint: bool = False):
        if self._is_interactivity_enabled:
            # Currently, interactivity is enabled forever
            return
        self._is_interactivity_enabled = True

        if not self.function_def.pty_info.pty_type:
            trigger = "breakpoint()" if from_breakpoint else "modal.interact()"
            raise InvalidError(f"Cannot use {trigger} without running Modal in interactive mode.")

        try:
            await self._client.stub.FunctionStartPtyShell(Empty())
        except Exception as e:
            logger.error("Failed to start PTY shell.")
            raise e

    @property
    def target_concurrency(self) -> int:
        return self._target_concurrency

    @property
    def max_concurrency(self) -> int:
        return self._max_concurrency

    @classmethod
    def get_input_concurrency(cls) -> int:
        """
        Returns the number of usable input slots.

        If concurrency is reduced, active slots can exceed allotted slots. Returns the larger value
        in this case.
        """

        io_manager = cls._singleton
        assert io_manager
        return max(io_manager._input_slots.active, io_manager._input_slots.value)

    @classmethod
    def set_input_concurrency(cls, concurrency: int):
        """
        Edit the number of input slots.

        This disables the background loop which automatically adjusts concurrency
        within [target_concurrency, max_concurrency].
        """
        io_manager = cls._singleton
        assert io_manager
        io_manager._stop_concurrency_loop = True
        concurrency = min(concurrency, io_manager._max_concurrency)
        io_manager._input_slots.set_value(concurrency)

    @classmethod
    def stop_fetching_inputs(cls):
        assert cls._singleton
        cls._singleton._fetching_inputs = False


ContainerIOManager = synchronize_api(_ContainerIOManager)


def check_fastapi_pydantic_compatibility(exc: ImportError) -> None:
    """Add a helpful note to an exception that is likely caused by a pydantic<>fastapi version incompatibility.

    We need this becasue the legacy set of container requirements (image_builder_version=2023.12) contains a
    version of fastapi that is not forwards-compatible with pydantic 2.0+, and users commonly run into issues
    building an image that specifies a more recent version only for pydantic.
    """
    note = (
        "Please ensure that your Image contains compatible versions of fastapi and pydantic."
        " If using pydantic>=2.0, you must also install fastapi>=0.100."
    )
    name = exc.name or ""
    if name.startswith("pydantic"):
        try:
            fastapi_version = parse_major_minor_version(importlib.metadata.version("fastapi"))
            pydantic_version = parse_major_minor_version(importlib.metadata.version("pydantic"))
            if pydantic_version >= (2, 0) and fastapi_version < (0, 100):
                if sys.version_info < (3, 11):
                    # https://peps.python.org/pep-0678/
                    exc.__notes__ = [note]  # type: ignore
                else:
                    exc.add_note(note)
        except Exception:
            # Since we're just trying to add a helpful message, don't fail here
            pass


================================================
File: modal/_runtime/execution_context.py
================================================
# Copyright Modal Labs 2024
from contextvars import ContextVar
from typing import Callable, Optional

from modal._utils.async_utils import synchronize_api
from modal.exception import InvalidError

from .container_io_manager import _ContainerIOManager


def is_local() -> bool:
    """Returns if we are currently on the machine launching/deploying a Modal app

    Returns `True` when executed locally on the user's machine.
    Returns `False` when executed from a Modal container in the cloud.
    """
    return not _ContainerIOManager._singleton


async def _interact() -> None:
    """Enable interactivity with user input inside a Modal container.

    See the [interactivity guide](https://modal.com/docs/guide/developing-debugging#interactivity)
    for more information on how to use this function.
    """
    container_io_manager = _ContainerIOManager._singleton
    if not container_io_manager:
        raise InvalidError("Interactivity only works inside a Modal container.")
    else:
        await container_io_manager.interact()


interact = synchronize_api(_interact)


def current_input_id() -> Optional[str]:
    """Returns the input ID for the current input.

    Can only be called from Modal function (i.e. in a container context).

    ```python
    from modal import current_input_id

    @app.function()
    def process_stuff():
        print(f"Starting to process {current_input_id()}")
    ```
    """
    try:
        return _current_input_id.get()
    except LookupError:
        return None


def current_function_call_id() -> Optional[str]:
    """Returns the function call ID for the current input.

    Can only be called from Modal function (i.e. in a container context).

    ```python
    from modal import current_function_call_id

    @app.function()
    def process_stuff():
        print(f"Starting to process input from {current_function_call_id()}")
    ```
    """
    try:
        return _current_function_call_id.get()
    except LookupError:
        return None


def _set_current_context_ids(input_ids: list[str], function_call_ids: list[str]) -> Callable[[], None]:
    assert len(input_ids) == len(function_call_ids) and len(input_ids) > 0
    input_id = input_ids[0]
    function_call_id = function_call_ids[0]
    input_token = _current_input_id.set(input_id)
    function_call_token = _current_function_call_id.set(function_call_id)

    def _reset_current_context_ids():
        _current_input_id.reset(input_token)
        _current_function_call_id.reset(function_call_token)

    return _reset_current_context_ids


_current_input_id: ContextVar = ContextVar("_current_input_id")
_current_function_call_id: ContextVar = ContextVar("_current_function_call_id")


================================================
File: modal/_runtime/gpu_memory_snapshot.py
================================================
# Copyright Modal Labs 2022
#
# This module provides a simple interface for creating GPU memory snapshots,
# provising a convenient interface to `cuda-checkpoint` [1]. This is intended
# to be used in conjunction with memory snapshots.
#
# [1] https://github.com/NVIDIA/cuda-checkpoint

import os
import subprocess
import time
from enum import Enum

from modal.config import config, logger

CUDA_CHECKPOINT_PATH: str = config.get("cuda_checkpoint_path")


class CudaCheckpointState(Enum):
    """State representation from the CUDA API: https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TYPES.html#group__CUDA__TYPES_1gc96cdda177a2b8c296144567cbea4f23"""

    RUNNING = "running"
    LOCKED = "locked"
    CHECKPOINTED = "checkpointed"
    FAILED = "failed"


class CudaCheckpointException(Exception):
    pass


def toggle():
    """Toggle CUDA checkpoint state for current process, moving GPU memory to the
    CPU and back depending on the current process state when called."""
    pid = get_own_pid()
    logger.debug(f"Toggling CUDA checkpoint state for PID {pid}")

    try:
        subprocess.run(
            [
                CUDA_CHECKPOINT_PATH,
                "--toggle",
                "--pid",
                str(pid),
            ],
            check=True,
            capture_output=True,
            text=True,
        )
        logger.debug("Successfully toggled CUDA checkpoint state")

    except subprocess.CalledProcessError as e:
        logger.debug(f"Failed to toggle CUDA checkpoint state: {e.stderr}")
        raise CudaCheckpointException(e.stderr)


def get_state() -> CudaCheckpointState:
    """Get current CUDA checkpoint state for this process."""
    pid = get_own_pid()

    try:
        result = subprocess.run(
            [CUDA_CHECKPOINT_PATH, "--get-state", "--pid", str(pid)], check=True, capture_output=True, text=True
        )

        # Parse output to get state
        state_str = result.stdout.strip().lower()
        return CudaCheckpointState(state_str)

    except subprocess.CalledProcessError as e:
        logger.debug(f"Failed to get CUDA checkpoint state: {e.stderr}")
        raise CudaCheckpointException(e.stderr)


def wait_for_state(target_state: CudaCheckpointState, timeout_secs: float = 5.0):
    """Wait for CUDA checkpoint to reach a specific state."""
    logger.debug(f"Waiting for CUDA checkpoint state {target_state.value}")
    start_time = time.monotonic()

    while True:
        current_state = get_state()

        if current_state == target_state:
            logger.debug(f"Target state {target_state.value} reached")
            break

        if current_state == CudaCheckpointState.FAILED:
            raise CudaCheckpointException(f"CUDA process state is {current_state}")

        elapsed = time.monotonic() - start_time
        if elapsed >= timeout_secs:
            raise CudaCheckpointException(f"Timeout after {elapsed:.2f}s waiting for state {target_state.value}")

        time.sleep(0.1)


def get_own_pid():
    """Returns the Process ID (PID) of the current Python process
    using only the standard library.
    """
    return os.getpid()


================================================
File: modal/_runtime/telemetry.py
================================================
# Copyright Modal Labs 2024

import importlib.abc
import json
import queue
import socket
import sys
import threading
import time
import uuid
from importlib.util import find_spec, module_from_spec
from struct import pack

from modal.config import logger

MODULE_LOAD_START = "module_load_start"
MODULE_LOAD_END = "module_load_end"

MESSAGE_HEADER_FORMAT = "<I"
MESSAGE_HEADER_LEN = 4


class InterceptedModuleLoader(importlib.abc.Loader):
    def __init__(self, name, loader, interceptor):
        self.name = name
        self.loader = loader
        self.interceptor = interceptor

    def exec_module(self, module):
        if self.loader is None:
            return
        try:
            self.loader.exec_module(module)
        finally:
            self.interceptor.load_end(self.name)

    def create_module(self, spec):
        spec.loader = self.loader
        module = module_from_spec(spec)
        spec.loader = self
        return module

    def get_data(self, path: str) -> bytes:
        """
        Implementation is required to support pkgutil.get_data.

        > If the package cannot be located or loaded, or it uses a loader which does
        > not support get_data, then None is returned.

        ref: https://docs.python.org/3/library/pkgutil.html#pkgutil.get_data
        """
        return self.loader.get_data(path)

    def get_resource_reader(self, fullname: str):
        """
        Support reading a binary artifact that is shipped within a package.

        > Loaders that wish to support resource reading are expected to provide a method called
        > get_resource_reader(fullname) which returns an object implementing this ABC’s interface.

        ref: docs.python.org/3.10/library/importlib.html?highlight=traversableresources#importlib.abc.ResourceReader
        """
        return self.loader.get_resource_reader(fullname)


class ImportInterceptor(importlib.abc.MetaPathFinder):
    loading: dict[str, tuple[str, float]]
    tracing_socket: socket.socket
    events: queue.Queue

    @classmethod
    def connect(cls, socket_filename: str) -> "ImportInterceptor":
        tracing_socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        tracing_socket.connect(socket_filename)
        return cls(tracing_socket)

    def __init__(self, tracing_socket: socket.socket):
        self.loading = {}
        self.tracing_socket = tracing_socket
        self.events = queue.Queue(maxsize=16 * 1024)
        sender = threading.Thread(target=self._send, daemon=True)
        sender.start()

    def find_spec(self, fullname, path, target=None):
        if fullname in self.loading:
            return None
        self.load_start(fullname)
        spec = find_spec(fullname)
        if spec is None:
            self.load_end(fullname)
            return None
        spec.loader = InterceptedModuleLoader(fullname, spec.loader, self)
        return spec

    def load_start(self, name):
        t0 = time.monotonic()
        span_id = str(uuid.uuid4())
        self.emit(
            {"span_id": span_id, "timestamp": time.time(), "event": MODULE_LOAD_START, "attributes": {"name": name}}
        )
        self.loading[name] = (span_id, t0)

    def load_end(self, name):
        span_id, t0 = self.loading.pop(name, (None, None))
        if t0 is None:
            return
        latency = time.monotonic() - t0
        self.emit(
            {
                "span_id": span_id,
                "timestamp": time.time(),
                "event": MODULE_LOAD_END,
                "attributes": {
                    "name": name,
                    "latency": latency,
                },
            }
        )

    def emit(self, event):
        try:
            self.events.put_nowait(event)
        except queue.Full:
            logger.debug("failed to emit event: queue full")

    def _send(self):
        while True:
            event = self.events.get()
            try:
                msg = json.dumps(event).encode("utf-8")
            except BaseException as e:
                logger.debug(f"failed to serialize event: {e}")
                continue
            try:
                encoded_len = pack(MESSAGE_HEADER_FORMAT, len(msg))
                self.tracing_socket.send(encoded_len + msg)
            except OSError as e:
                logger.debug(f"failed to send event: {e}")

    def install(self):
        sys.meta_path = [self] + sys.meta_path  # type: ignore

    def remove(self):
        sys.meta_path.remove(self)  # type: ignore

    def __enter__(self):
        self.install()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.remove()


def _instrument_imports(socket_filename: str):
    if not supported_platform():
        logger.debug("unsupported platform, not instrumenting imports")
        return
    interceptor = ImportInterceptor.connect(socket_filename)
    interceptor.install()


def instrument_imports(socket_filename: str):
    try:
        _instrument_imports(socket_filename)
    except BaseException as e:
        logger.warning(f"failed to instrument imports: {e}")


def supported_platform():
    return sys.platform in ("linux", "darwin")


================================================
File: modal/_runtime/user_code_imports.py
================================================
# Copyright Modal Labs 2024
import importlib
import typing
from abc import ABCMeta, abstractmethod
from dataclasses import dataclass
from typing import Any, Callable, Optional, Sequence

import modal._object
import modal._runtime.container_io_manager
import modal.cls
from modal import Function
from modal._functions import _Function
from modal._partial_function import _find_partial_methods_for_user_cls, _PartialFunctionFlags
from modal._utils.async_utils import synchronizer
from modal._utils.function_utils import LocalFunctionError, is_async as get_is_async, is_global_object
from modal.exception import ExecutionError, InvalidError
from modal_proto import api_pb2

if typing.TYPE_CHECKING:
    import modal.app
    import modal.partial_function
    from modal._runtime.asgi import LifespanManager


@dataclass
class FinalizedFunction:
    callable: Callable[..., Any]
    is_async: bool
    is_generator: bool
    data_format: int  # api_pb2.DataFormat
    lifespan_manager: Optional["LifespanManager"] = None


class Service(metaclass=ABCMeta):
    """Common interface for singular functions and class-based "services"

    There are differences in the importing/finalization logic, and this
    "protocol"/abc basically defines a common interface for the two types
    of "Services" after the point of import.
    """

    user_cls_instance: Any
    app: Optional["modal.app._App"]
    service_deps: Optional[Sequence["modal._object._Object"]]

    @abstractmethod
    def get_finalized_functions(
        self, fun_def: api_pb2.Function, container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager"
    ) -> dict[str, "FinalizedFunction"]: ...


def construct_webhook_callable(
    user_defined_callable: Callable,
    webhook_config: api_pb2.WebhookConfig,
    container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager",
):
    # Note: aiohttp is a significant dependency of the `asgi` module, so we import it locally
    from modal._runtime import asgi

    # For webhooks, the user function is used to construct an asgi app:
    if webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP:
        # Function returns an asgi_app, which we can use as a callable.
        return asgi.asgi_app_wrapper(user_defined_callable(), container_io_manager)

    elif webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP:
        # Function returns an wsgi_app, which we can use as a callable
        return asgi.wsgi_app_wrapper(user_defined_callable(), container_io_manager)

    elif webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION:
        # Function is a webhook without an ASGI app. Create one for it.
        return asgi.asgi_app_wrapper(
            asgi.webhook_asgi_app(user_defined_callable, webhook_config.method, webhook_config.web_endpoint_docs),
            container_io_manager,
        )

    elif webhook_config.type == api_pb2.WEBHOOK_TYPE_WEB_SERVER:
        # Function spawns an HTTP web server listening at a port.
        user_defined_callable()

        # We intentionally try to connect to the external interface instead of the loopback
        # interface here so users are forced to expose the server. This allows us to potentially
        # change the implementation to use an external bridge in the future.
        host = asgi.get_ip_address(b"eth0")
        port = webhook_config.web_server_port
        startup_timeout = webhook_config.web_server_startup_timeout
        asgi.wait_for_web_server(host, port, timeout=startup_timeout)
        return asgi.asgi_app_wrapper(asgi.web_server_proxy(host, port), container_io_manager)
    else:
        raise InvalidError(f"Unrecognized web endpoint type {webhook_config.type}")


@dataclass
class ImportedFunction(Service):
    user_cls_instance: Any
    app: Optional["modal.app._App"]
    service_deps: Optional[Sequence["modal._object._Object"]]

    _user_defined_callable: Callable[..., Any]

    def get_finalized_functions(
        self, fun_def: api_pb2.Function, container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager"
    ) -> dict[str, "FinalizedFunction"]:
        # Check this property before we turn it into a method (overriden by webhooks)
        is_async = get_is_async(self._user_defined_callable)
        # Use the function definition for whether this is a generator (overriden by webhooks)
        is_generator = fun_def.function_type == api_pb2.Function.FUNCTION_TYPE_GENERATOR

        webhook_config = fun_def.webhook_config
        if not webhook_config.type:
            # for non-webhooks, the runnable is straight forward:
            return {
                "": FinalizedFunction(
                    callable=self._user_defined_callable,
                    is_async=is_async,
                    is_generator=is_generator,
                    data_format=api_pb2.DATA_FORMAT_PICKLE,
                )
            }

        web_callable, lifespan_manager = construct_webhook_callable(
            self._user_defined_callable, fun_def.webhook_config, container_io_manager
        )

        return {
            "": FinalizedFunction(
                callable=web_callable,
                lifespan_manager=lifespan_manager,
                is_async=True,
                is_generator=True,
                data_format=api_pb2.DATA_FORMAT_ASGI,
            )
        }


@dataclass
class ImportedClass(Service):
    user_cls_instance: Any
    app: Optional["modal.app._App"]
    service_deps: Optional[Sequence["modal._object._Object"]]

    _partial_functions: dict[str, "modal._partial_function._PartialFunction"]

    def get_finalized_functions(
        self, fun_def: api_pb2.Function, container_io_manager: "modal._runtime.container_io_manager.ContainerIOManager"
    ) -> dict[str, "FinalizedFunction"]:
        finalized_functions = {}
        for method_name, partial in self._partial_functions.items():
            partial = synchronizer._translate_in(partial)  # ugly
            user_func = partial.raw_f
            # Check this property before we turn it into a method (overriden by webhooks)
            is_async = get_is_async(user_func)
            # Use the function definition for whether this is a generator (overriden by webhooks)
            is_generator = partial.is_generator
            webhook_config = partial.webhook_config

            bound_func = user_func.__get__(self.user_cls_instance)

            if not webhook_config or webhook_config.type == api_pb2.WEBHOOK_TYPE_UNSPECIFIED:
                # for non-webhooks, the runnable is straight forward:
                finalized_function = FinalizedFunction(
                    callable=bound_func,
                    is_async=is_async,
                    is_generator=is_generator,
                    data_format=api_pb2.DATA_FORMAT_PICKLE,
                )
            else:
                web_callable, lifespan_manager = construct_webhook_callable(
                    bound_func, webhook_config, container_io_manager
                )
                finalized_function = FinalizedFunction(
                    callable=web_callable,
                    lifespan_manager=lifespan_manager,
                    is_async=True,
                    is_generator=True,
                    data_format=api_pb2.DATA_FORMAT_ASGI,
                )
            finalized_functions[method_name] = finalized_function
        return finalized_functions


def get_user_class_instance(cls: typing.Union[type, modal.cls.Cls], args: tuple, kwargs: dict[str, Any]) -> typing.Any:
    """Returns instance of the underlying class to be used as the `self`

    The input `cls` can either be the raw Python class the user has declared ("user class"),
    or an @app.cls-decorated version of it which is a modal.Cls-instance wrapping the user class.
    """
    if isinstance(cls, modal.cls.Cls):
        # globally @app.cls-decorated class
        modal_obj: modal.cls.Obj = cls(*args, **kwargs)
        modal_obj._entered = True  # ugly but prevents .local() from triggering additional enter-logic
        # TODO: unify lifecycle logic between .local() and container_entrypoint
        user_cls_instance = modal_obj._cached_user_cls_instance()
    else:
        # undecorated class (non-global decoration or serialized)
        user_cls_instance = cls(*args, **kwargs)

    return user_cls_instance


def import_single_function_service(
    function_def: api_pb2.Function,
    ser_cls,  # used only for @build functions
    ser_fun,
    cls_args,  #  used only for @build functions
    cls_kwargs,  #  used only for @build functions
) -> Service:
    """Imports a function dynamically, and locates the app.

    This is somewhat complex because we're dealing with 3 quite different type of functions:
    1. Functions defined in global scope and decorated in global scope (Function objects)
    2. Functions defined in global scope but decorated elsewhere (these will be raw callables)
    3. Serialized functions

    In addition, we also need to handle
    * Normal functions
    * Methods on classes (in which case we need to instantiate the object)

    This helper also handles web endpoints, ASGI/WSGI servers, and HTTP servers.

    In order to locate the app, we try two things:
    * If the function is a Function, we can get the app directly from it
    * Otherwise, use the app name and look it up from a global list of apps: this
      typically only happens in case 2 above, or in sometimes for case 3

    Note that `import_function` is *not* synchronized, because we need it to run on the main
    thread. This is so that any user code running in global scope (which executes as a part of
    the import) runs on the right thread.
    """
    user_defined_callable: Callable
    service_deps: Optional[Sequence["modal._object._Object"]] = None
    active_app: Optional[modal.app._App] = None

    if ser_fun is not None:
        # This is a serialized function we already fetched from the server
        cls, user_defined_callable = ser_cls, ser_fun
    else:
        # Load the module dynamically
        module = importlib.import_module(function_def.module_name)
        qual_name: str = function_def.function_name

        if not is_global_object(qual_name):
            raise LocalFunctionError("Attempted to load a function defined in a function scope")

        parts = qual_name.split(".")
        if len(parts) == 1:
            # This is a function
            cls = None
            f = getattr(module, qual_name)
            if isinstance(f, Function):
                function = synchronizer._translate_in(f)
                service_deps = function.deps(only_explicit_mounts=True)
                user_defined_callable = function.get_raw_f()
                active_app = function._app
            else:
                user_defined_callable = f
        elif len(parts) == 2:
            # This path should only be triggered by @build class builder methods and can be removed
            # once @build is deprecated.
            assert not function_def.use_method_name  # new "placeholder methods" should not be invoked directly!
            assert function_def.is_builder_function
            cls_name, fun_name = parts
            cls = getattr(module, cls_name)
            if isinstance(cls, modal.cls.Cls):
                # The cls decorator is in global scope
                _cls = synchronizer._translate_in(cls)
                user_defined_callable = _cls._callables[fun_name]
                # Intentionally not including these, since @build functions don't actually
                # forward the information from their parent class.
                # service_deps = _cls._get_class_service_function().deps(only_explicit_mounts=True)
                active_app = _cls._app
            else:
                # This is non-decorated class
                user_defined_callable = getattr(cls, fun_name)
        else:
            raise InvalidError(f"Invalid function qualname {qual_name}")

    # Instantiate the class if it's defined
    if cls:
        # This code is only used for @build methods on classes
        user_cls_instance = get_user_class_instance(cls, cls_args, cls_kwargs)
        # Bind the function to the instance as self (using the descriptor protocol!)
        user_defined_callable = user_defined_callable.__get__(user_cls_instance)
    else:
        user_cls_instance = None

    return ImportedFunction(
        user_cls_instance,
        active_app,
        service_deps,
        user_defined_callable,
    )


def import_class_service(
    function_def: api_pb2.Function,
    ser_cls,
    cls_args,
    cls_kwargs,
) -> Service:
    """
    This imports a full class to be able to execute any @method or webhook decorated methods.

    See import_function.
    """
    active_app: Optional["modal.app._App"]
    service_deps: Optional[Sequence["modal._object._Object"]]
    cls: typing.Union[type, modal.cls.Cls]

    if function_def.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED:
        assert ser_cls is not None
        cls = ser_cls
    else:
        # Load the module dynamically
        module = importlib.import_module(function_def.module_name)
        qual_name: str = function_def.function_name

        if not is_global_object(qual_name):
            raise LocalFunctionError("Attempted to load a class defined in a function scope")

        parts = qual_name.split(".")
        if not (
            len(parts) == 2 and parts[1] == "*"
        ):  # the "function name" of a class service "function placeholder" is expected to be "ClassName.*"
            raise ExecutionError(
                f"Internal error: Invalid 'service function' identifier {qual_name}. Please contact Modal support"
            )

        assert not function_def.use_method_name  # new "placeholder methods" should not be invoked directly!
        cls_name = parts[0]
        cls = getattr(module, cls_name)

    if isinstance(cls, modal.cls.Cls):
        # The cls decorator is in global scope
        _cls = synchronizer._translate_in(cls)
        method_partials = _cls._get_partial_functions()
        service_function: _Function = _cls._class_service_function
        service_deps = service_function.deps(only_explicit_mounts=True)
        active_app = service_function.app
    else:
        # Undecorated user class - find all methods
        method_partials = _find_partial_methods_for_user_cls(cls, _PartialFunctionFlags.all())
        service_deps = None
        active_app = None

    user_cls_instance = get_user_class_instance(cls, cls_args, cls_kwargs)

    return ImportedClass(
        user_cls_instance,
        active_app,
        service_deps,
        # TODO (elias/deven): instead of using method_partials here we should use a set of api_pb2.MethodDefinition
        method_partials,
    )


================================================
File: modal/_utils/__init__.py
================================================
# Copyright Modal Labs 2022


================================================
File: modal/_utils/app_utils.py
================================================
# Copyright Modal Labs 2024
# Temporary shim as we use this in the server
from .name_utils import *  # noqa


================================================
File: modal/_utils/async_utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
import concurrent.futures
import functools
import inspect
import itertools
import time
import typing
from collections.abc import AsyncGenerator, AsyncIterable, Awaitable, Iterable, Iterator
from contextlib import asynccontextmanager
from dataclasses import dataclass
from typing import (
    Any,
    Callable,
    Optional,
    TypeVar,
    Union,
    cast,
)

import synchronicity
from synchronicity.async_utils import Runner
from synchronicity.exceptions import NestedEventLoops
from typing_extensions import ParamSpec, assert_type

from ..exception import InvalidError
from .logger import logger

synchronizer = synchronicity.Synchronizer()


def synchronize_api(obj, target_module=None):
    if inspect.isclass(obj) or inspect.isfunction(obj):
        blocking_name = obj.__name__.lstrip("_")
    elif isinstance(obj, TypeVar):
        blocking_name = "_BLOCKING_" + obj.__name__
    else:
        blocking_name = None
    if target_module is None:
        target_module = obj.__module__
    return synchronizer.create_blocking(obj, blocking_name, target_module=target_module)


def retry(direct_fn=None, *, n_attempts=3, base_delay=0, delay_factor=2, timeout=90):
    """Decorator that calls an async function multiple times, with a given timeout.

    If a `base_delay` is provided, the function is given an exponentially
    increasing delay on each run, up until the maximum number of attempts.

    Usage:

    ```
    @retry
    async def may_fail_default():
        # ...
        pass

    @retry(n_attempts=5, base_delay=1)
    async def may_fail_delay():
        # ...
        pass
    ```
    """

    def decorator(fn):
        @functools.wraps(fn)
        async def f_wrapped(*args, **kwargs):
            delay = base_delay
            for i in range(n_attempts):
                t0 = time.time()
                try:
                    return await asyncio.wait_for(fn(*args, **kwargs), timeout=timeout)
                except asyncio.CancelledError:
                    logger.debug(f"Function {fn} was cancelled")
                    raise
                except Exception as e:
                    if i >= n_attempts - 1:
                        raise
                    logger.debug(
                        f"Failed invoking function {fn}: {e}"
                        f" (took {time.time() - t0}s, sleeping {delay}s"
                        f" and trying {n_attempts - i - 1} more times)"
                    )
                await asyncio.sleep(delay)
                delay *= delay_factor

        return f_wrapped

    if direct_fn is not None:
        # It's invoked like @retry
        return decorator(direct_fn)
    else:
        # It's invoked like @retry(n_attempts=...)
        return decorator


class TaskContext:
    """A structured group that helps manage stray tasks.

    This differs from the standard library `asyncio.TaskGroup` in that it cancels all tasks still
    running after exiting the context manager, rather than waiting for them to finish.

    A `TaskContext` can have an optional `grace` period in seconds, which will wait for a certain
    amount of time before cancelling all remaining tasks. This is useful for allowing tasks to
    gracefully exit when they determine that the context is shutting down.

    Usage:

    ```python notest
    async with TaskContext() as task_context:
        task = task_context.create_task(coro())
    ```
    """

    _loops: set[asyncio.Task]

    def __init__(self, grace: Optional[float] = None):
        self._grace = grace
        self._loops = set()

    async def start(self):
        # TODO: this only exists as a standalone method because Client doesn't have a proper ctx mgr
        self._tasks: set[asyncio.Task] = set()
        self._exited: asyncio.Event = asyncio.Event()  # Used to stop infinite loops

    @property
    def exited(self) -> bool:
        return self._exited.is_set()

    async def __aenter__(self):
        await self.start()
        return self

    async def stop(self):
        self._exited.set()
        await asyncio.sleep(0)  # Causes any just-created tasks to get started
        unfinished_tasks = [t for t in self._tasks if not t.done()]
        gather_future = None
        try:
            if self._grace is not None and unfinished_tasks:
                gather_future = asyncio.gather(*unfinished_tasks, return_exceptions=True)
                await asyncio.wait_for(gather_future, timeout=self._grace)
        except asyncio.TimeoutError:
            pass
        finally:
            # asyncio.wait_for cancels the future, but the CancelledError
            # still needs to be handled
            # (https://stackoverflow.com/a/63356323/2475114)
            if gather_future:
                try:
                    await gather_future
                except asyncio.CancelledError:
                    pass

            for task in self._tasks:
                if task.done() and not task.cancelled():
                    # Raise any exceptions if they happened.
                    # Only tasks without a done_callback will still be present in self._tasks
                    task.result()

                if task.done() or task in self._loops:  # Note: Legacy code, we can probably cancel loops.
                    continue

                # Cancel any remaining unfinished tasks.
                task.cancel()
            await asyncio.sleep(0)  # wake up coroutines waiting for cancellations

    async def __aexit__(self, exc_type, value, tb):
        await self.stop()

    def create_task(self, coro_or_task) -> asyncio.Task:
        if isinstance(coro_or_task, asyncio.Task):
            task = coro_or_task
        elif asyncio.iscoroutine(coro_or_task):
            loop = asyncio.get_event_loop()
            task = loop.create_task(coro_or_task)
        else:
            raise Exception(f"Object of type {type(coro_or_task)} is not a coroutine or Task")
        self._tasks.add(task)
        task.add_done_callback(self._tasks.discard)
        return task

    def infinite_loop(
        self, async_f, timeout: Optional[float] = 90, sleep: float = 10, log_exception: bool = True
    ) -> asyncio.Task:
        if isinstance(async_f, functools.partial):
            function_name = async_f.func.__qualname__
        else:
            function_name = async_f.__qualname__

        async def loop_coro() -> None:
            logger.debug(f"Starting infinite loop {function_name}")
            while not self.exited:
                try:
                    await asyncio.wait_for(async_f(), timeout=timeout)
                except Exception as exc:
                    if log_exception and isinstance(exc, asyncio.TimeoutError):
                        # Asyncio sends an empty message in this case, so let's use logger.error
                        logger.error(f"Loop attempt for {function_name} timed out")
                    elif log_exception:
                        # Propagate the exception to the logger
                        logger.exception(f"Loop attempt for {function_name} failed")
                try:
                    await asyncio.wait_for(self._exited.wait(), timeout=sleep)
                except asyncio.TimeoutError:
                    continue

            logger.debug(f"Exiting infinite loop for {function_name}")

        t = self.create_task(loop_coro())
        t.set_name(f"{function_name} loop")
        self._loops.add(t)
        t.add_done_callback(self._loops.discard)
        return t

    @staticmethod
    async def gather(*coros: Awaitable) -> Any:
        """Wait for a sequence of coroutines to finish, concurrently.

        This is similar to `asyncio.gather()`, but it uses TaskContext to cancel all remaining tasks
        if one fails with an exception other than `asyncio.CancelledError`. The native `asyncio`
        function does not cancel remaining tasks in this case, which can lead to surprises.

        For example, if you use `asyncio.gather(t1, t2, t3)` and t2 raises an exception, then t1 and
        t3 would continue running. With `TaskContext.gather(t1, t2, t3)`, they are cancelled.

        (It's still acceptable to use `asyncio.gather()` if you don't need cancellation — for
        example, if you're just gathering quick coroutines with no side-effects. Or if you're
        gathering the tasks with `return_exceptions=True`.)

        Usage:

        ```python notest
        # Example 1: Await three coroutines
        created_object, other_work, new_plumbing = await TaskContext.gather(
            create_my_object(),
            do_some_other_work(),
            fix_plumbing(),
        )

        # Example 2: Gather a list of coroutines
        coros = [a.load() for a in objects]
        results = await TaskContext.gather(*coros)
        ```
        """
        async with TaskContext() as tc:
            results = await asyncio.gather(*(tc.create_task(coro) for coro in coros))
        return results


def run_coro_blocking(coro):
    """Fairly hacky thing that's needed in some extreme cases.

    It's basically works like asyncio.run but unlike asyncio.run it also works
    with in the case an event loop is already running. It does this by basically
    moving the whole thing to a separate thread.
    """
    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
        fut = executor.submit(asyncio.run, coro)
        return fut.result()


async def queue_batch_iterator(q: asyncio.Queue, max_batch_size=100, debounce_time=0.015):
    """
    Read from a queue but return lists of items when queue is large

    Treats a None value as end of queue items
    """
    item_list: list[Any] = []

    while True:
        if q.empty() and len(item_list) > 0:
            yield item_list
            item_list = []
            await asyncio.sleep(debounce_time)

        res = await q.get()

        if len(item_list) >= max_batch_size:
            yield item_list
            item_list = []

        if res is None:
            if len(item_list) > 0:
                yield item_list
            break
        item_list.append(res)


class _WarnIfGeneratorIsNotConsumed:
    def __init__(self, gen, function_name: str):
        self.gen = gen
        self.function_name = function_name
        self.iterated = False
        self.warned = False

    def __aiter__(self):
        self.iterated = True
        return self.gen.__aiter__()

    async def __anext__(self):
        self.iterated = True
        return await self.gen.__anext__()

    async def asend(self, value):
        self.iterated = True
        return await self.gen.asend(value)

    def __repr__(self):
        return repr(self.gen)

    def __del__(self):
        if not self.iterated and not self.warned:
            self.warned = True
            logger.warning(
                f"Warning: the results of a call to {self.function_name} was not consumed, "
                "so the call will never be executed."
                f" Consider a for-loop like `for x in {self.function_name}(...)` or "
                "unpacking the generator using `list(...)`"
            )

    async def athrow(self, exc):
        return await self.gen.athrow(exc)

    async def aclose(self):
        return await self.gen.aclose()


synchronize_api(_WarnIfGeneratorIsNotConsumed)


class _WarnIfNonWrappedGeneratorIsNotConsumed(_WarnIfGeneratorIsNotConsumed):
    # used for non-synchronicity-wrapped generators and iterators
    def __iter__(self):
        self.iterated = True
        return iter(self.gen)

    def __next__(self):
        self.iterated = True
        return self.gen.__next__()

    def send(self, value):
        self.iterated = True
        return self.gen.send(value)


def warn_if_generator_is_not_consumed(function_name: Optional[str] = None):
    # https://gist.github.com/erikbern/01ae78d15f89edfa7f77e5c0a827a94d
    def decorator(gen_f):
        presented_func_name = function_name if function_name is not None else gen_f.__name__

        @functools.wraps(gen_f)
        def f_wrapped(*args, **kwargs):
            gen = gen_f(*args, **kwargs)
            if inspect.isasyncgen(gen):
                return _WarnIfGeneratorIsNotConsumed(gen, presented_func_name)
            else:
                return _WarnIfNonWrappedGeneratorIsNotConsumed(gen, presented_func_name)

        return f_wrapped

    return decorator


class AsyncOrSyncIterable:
    """Compatibility class for non-synchronicity wrapped async iterables to get
    both async and sync interfaces in the same way that synchronicity does (but on the main thread)
    so they can be "lazily" iterated using either `for _ in x` or `async for _ in x`

    nested_async_message is raised as an InvalidError if the async variant is called
    from an already async context, since that would otherwise deadlock the event loop
    """

    def __init__(self, async_iterable: typing.AsyncGenerator[Any, None], nested_async_message):
        self._async_iterable = async_iterable
        self.nested_async_message = nested_async_message

    def __aiter__(self):
        return self._async_iterable

    def __iter__(self):
        try:
            with Runner() as runner:
                yield from run_async_gen(runner, self._async_iterable)
        except NestedEventLoops:
            raise InvalidError(self.nested_async_message)


_shutdown_tasks = []


def on_shutdown(coro):
    # hook into event loop shutdown when all active tasks get cancelled
    async def wrapper():
        try:
            await asyncio.sleep(1e10)  # never awake except for exceptions
        finally:
            await coro
            raise

    _shutdown_tasks.append(asyncio.create_task(wrapper()))


T = TypeVar("T")
P = ParamSpec("P")
V = TypeVar("V")


def asyncify(f: Callable[P, T]) -> Callable[P, typing.Coroutine[None, None, T]]:
    """Convert a blocking function into one that runs in the current loop's executor."""

    @functools.wraps(f)
    async def wrapper(*args: P.args, **kwargs: P.kwargs):
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(None, functools.partial(f, *args, **kwargs))

    return wrapper


async def iterate_blocking(iterator: Iterator[T]) -> AsyncGenerator[T, None]:
    """Iterate over a blocking iterator in an async context."""

    loop = asyncio.get_running_loop()
    DONE = object()
    while True:
        obj = await loop.run_in_executor(None, next, iterator, DONE)
        if obj is DONE:
            break
        yield cast(T, obj)


@asynccontextmanager
async def asyncnullcontext(*args, **kwargs):
    """Async noop context manager.

    Note that for Python 3.10+ you can use contextlib.nullcontext() instead.

    Usage:
    async with asyncnullcontext():
        pass
    """
    yield


YIELD_TYPE = typing.TypeVar("YIELD_TYPE")
SEND_TYPE = typing.TypeVar("SEND_TYPE")


def run_async_gen(
    runner: Runner,
    gen: typing.AsyncGenerator[YIELD_TYPE, SEND_TYPE],
) -> typing.Generator[YIELD_TYPE, SEND_TYPE, None]:
    """Convert an async generator into a sync one"""
    # more or less copied from synchronicity's implementation:
    next_send: typing.Union[SEND_TYPE, None] = None
    next_yield: YIELD_TYPE
    exc: Optional[BaseException] = None
    while True:
        try:
            if exc:
                next_yield = runner.run(gen.athrow(exc))
            else:
                next_yield = runner.run(gen.asend(next_send))  # type: ignore[arg-type]
        except KeyboardInterrupt as e:
            raise e from None
        except StopAsyncIteration:
            break  # typically a graceful exit of the async generator
        try:
            next_send = yield next_yield
            exc = None
        except BaseException as err:
            exc = err


class aclosing(typing.Generic[T]):  # noqa
    # backport of Python contextlib.aclosing from Python 3.10
    def __init__(self, agen: AsyncGenerator[T, None]):
        self.agen = agen

    async def __aenter__(self) -> AsyncGenerator[T, None]:
        return self.agen

    async def __aexit__(self, exc, exc_type, tb):
        await self.agen.aclose()


async def sync_or_async_iter(iter: Union[Iterable[T], AsyncIterable[T]]) -> AsyncGenerator[T, None]:
    if hasattr(iter, "__aiter__"):
        agen = typing.cast(AsyncGenerator[T, None], iter)
        try:
            async for item in agen:
                yield item
        finally:
            if hasattr(agen, "aclose"):
                # All AsyncGenerator's have an aclose method
                # but some AsyncIterable's don't necessarily
                await agen.aclose()
    else:
        assert hasattr(iter, "__iter__"), "sync_or_async_iter requires an Iterable or AsyncGenerator"
        # This intentionally could block the event loop for the duration of calling __iter__ and __next__,
        # so in non-trivial cases (like passing lists and ranges) this could be quite a foot gun for users #
        # w/ async code (but they can work around it by always using async iterators)
        for item in typing.cast(Iterable[T], iter):
            yield item


@typing.overload
def async_zip(g1: AsyncGenerator[T, None], g2: AsyncGenerator[V, None], /) -> AsyncGenerator[tuple[T, V], None]: ...


@typing.overload
def async_zip(*generators: AsyncGenerator[T, None]) -> AsyncGenerator[tuple[T, ...], None]: ...


async def async_zip(*generators):
    tasks = []
    try:
        while True:
            try:

                async def next_item(gen):
                    return await gen.__anext__()

                tasks = [asyncio.create_task(next_item(gen)) for gen in generators]
                items = await asyncio.gather(*tasks)
                yield tuple(items)
            except StopAsyncIteration:
                break
    finally:
        cancelled_tasks = []
        for task in tasks:
            if not task.done():
                task.cancel()
                cancelled_tasks.append(task)
        try:
            await asyncio.gather(*cancelled_tasks)
        except asyncio.CancelledError:
            pass

        first_exception = None
        for gen in generators:
            try:
                await gen.aclose()
            except BaseException as e:
                if first_exception is None:
                    first_exception = e
                logger.exception(f"Error closing async generator: {e}")
        if first_exception is not None:
            raise first_exception


@dataclass
class ValueWrapper(typing.Generic[T]):
    value: T


@dataclass
class ExceptionWrapper:
    value: Exception


class StopSentinelType: ...


STOP_SENTINEL = StopSentinelType()


async def async_merge(*generators: AsyncGenerator[T, None]) -> AsyncGenerator[T, None]:
    """
    Asynchronously merges multiple async generators into a single async generator.

    This function takes multiple async generators and yields their values in the order
    they are produced. If any generator raises an exception, the exception is propagated.

    Args:
        *generators: One or more async generators to be merged.

    Yields:
        The values produced by the input async generators.

    Raises:
        Exception: If any of the input generators raises an exception, it is propagated.

    Usage:
    ```python
    import asyncio
    from modal._utils.async_utils import async_merge

    async def gen1():
        yield 1
        yield 2

    async def gen2():
        yield "a"
        yield "b"

    async def example():
        values = set()
        async for value in async_merge(gen1(), gen2()):
            values.add(value)

        return values

    # Output could be: {1, "a", 2, "b"} (order may vary)
    values = asyncio.run(example())
    assert values == {1, "a", 2, "b"}
    ```
    """
    queue: asyncio.Queue[Union[ValueWrapper[T], ExceptionWrapper]] = asyncio.Queue(maxsize=len(generators) * 10)

    async def producer(generator: AsyncGenerator[T, None]):
        try:
            async for item in generator:
                await queue.put(ValueWrapper(item))
        except Exception as e:
            await queue.put(ExceptionWrapper(e))

    tasks = {asyncio.create_task(producer(gen)) for gen in generators}
    new_output_task = asyncio.create_task(queue.get())

    try:
        while tasks:
            done, _ = await asyncio.wait(
                [*tasks, new_output_task],
                return_when=asyncio.FIRST_COMPLETED,
            )

            if new_output_task in done:
                item = new_output_task.result()
                if isinstance(item, ValueWrapper):
                    yield item.value
                else:
                    assert_type(item, ExceptionWrapper)
                    raise item.value

                new_output_task = asyncio.create_task(queue.get())

            finished_producers = done & tasks
            tasks -= finished_producers
            for finished_producer in finished_producers:
                # this is done in order to catch potential raised errors/cancellations
                # from within worker tasks as soon as they happen.
                await finished_producer

        while not queue.empty():
            item = await new_output_task
            if isinstance(item, ValueWrapper):
                yield item.value
            else:
                assert_type(item, ExceptionWrapper)
                raise item.value

            new_output_task = asyncio.create_task(queue.get())

    finally:
        if not new_output_task.done():
            new_output_task.cancel()
        for task in tasks:
            if not task.done():
                try:
                    task.cancel()
                    await task
                except asyncio.CancelledError:
                    pass


async def callable_to_agen(awaitable: Callable[[], Awaitable[T]]) -> AsyncGenerator[T, None]:
    yield await awaitable()


async def gather_cancel_on_exc(*coros_or_futures):
    input_tasks = [asyncio.ensure_future(t) for t in coros_or_futures]
    try:
        return await asyncio.gather(*input_tasks)
    except BaseException:
        for t in input_tasks:
            t.cancel()
        await asyncio.gather(*input_tasks, return_exceptions=False)  # handle cancellations
        raise


async def async_map(
    input_generator: AsyncGenerator[T, None],
    async_mapper_func: Callable[[T], Awaitable[V]],
    concurrency: int,
) -> AsyncGenerator[V, None]:
    queue: asyncio.Queue[Union[ValueWrapper[T], StopSentinelType]] = asyncio.Queue(maxsize=concurrency * 2)

    async def producer() -> AsyncGenerator[V, None]:
        async for item in input_generator:
            await queue.put(ValueWrapper(item))

        for _ in range(concurrency):
            await queue.put(STOP_SENTINEL)

        if False:
            # Need it to be an async generator for async_merge
            # but we don't want to yield anything
            yield

    async def worker() -> AsyncGenerator[V, None]:
        while True:
            item = await queue.get()
            if isinstance(item, ValueWrapper):
                yield await async_mapper_func(item.value)
            elif isinstance(item, ExceptionWrapper):
                raise item.value
            else:
                assert_type(item, StopSentinelType)
                break

    async with aclosing(async_merge(*[worker() for _ in range(concurrency)], producer())) as stream:
        async for item in stream:
            yield item


async def async_map_ordered(
    input_generator: AsyncGenerator[T, None],
    async_mapper_func: Callable[[T], Awaitable[V]],
    concurrency: int,
    buffer_size: Optional[int] = None,
) -> AsyncGenerator[V, None]:
    semaphore = asyncio.Semaphore(buffer_size or concurrency)

    async def mapper_func_wrapper(tup: tuple[int, T]) -> tuple[int, V]:
        return (tup[0], await async_mapper_func(tup[1]))

    async def counter() -> AsyncGenerator[int, None]:
        for i in itertools.count():
            await semaphore.acquire()
            yield i

    next_idx = 0
    buffer = {}

    async with aclosing(async_map(async_zip(counter(), input_generator), mapper_func_wrapper, concurrency)) as stream:
        async for output_idx, output_item in stream:
            buffer[output_idx] = output_item

            while next_idx in buffer:
                yield buffer[next_idx]
                semaphore.release()
                del buffer[next_idx]
                next_idx += 1


async def async_chain(*generators: AsyncGenerator[T, None]) -> AsyncGenerator[T, None]:
    try:
        for gen in generators:
            async for item in gen:
                yield item
    finally:
        first_exception = None
        for gen in generators:
            try:
                await gen.aclose()
            except BaseException as e:
                if first_exception is None:
                    first_exception = e
                logger.exception(f"Error closing async generator: {e}")
        if first_exception is not None:
            raise first_exception


================================================
File: modal/_utils/blob_utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
import dataclasses
import hashlib
import io
import os
import platform
import time
from collections.abc import AsyncIterator
from contextlib import AbstractContextManager, contextmanager
from pathlib import Path, PurePosixPath
from typing import TYPE_CHECKING, Any, BinaryIO, Callable, Optional, Union
from urllib.parse import urlparse

from modal_proto import api_pb2
from modal_proto.modal_api_grpc import ModalClientModal

from ..exception import ExecutionError
from .async_utils import TaskContext, retry
from .grpc_utils import retry_transient_errors
from .hash_utils import UploadHashes, get_upload_hashes
from .http_utils import ClientSessionRegistry
from .logger import logger

if TYPE_CHECKING:
    from .bytes_io_segment_payload import BytesIOSegmentPayload

# Max size for function inputs and outputs.
MAX_OBJECT_SIZE_BYTES = 2 * 1024 * 1024  # 2 MiB

#  If a file is LARGE_FILE_LIMIT bytes or larger, it's uploaded to blob store (s3) instead of going through grpc
#  It will also make sure to chunk the hash calculation to avoid reading the entire file into memory
LARGE_FILE_LIMIT = 4 * 1024 * 1024  # 4 MiB

# Max parallelism during map calls
BLOB_MAX_PARALLELISM = 10

# read ~16MiB chunks by default
DEFAULT_SEGMENT_CHUNK_SIZE = 2**24

# Files larger than this will be multipart uploaded. The server might request multipart upload for smaller files as
# well, but the limit will never be raised.
# TODO(dano): remove this once we stop requiring md5 for blobs
MULTIPART_UPLOAD_THRESHOLD = 1024**3


@retry(n_attempts=5, base_delay=0.5, timeout=None)
async def _upload_to_s3_url(
    upload_url,
    payload: "BytesIOSegmentPayload",
    content_md5_b64: Optional[str] = None,
    content_type: Optional[str] = "application/octet-stream",  # set to None to force omission of ContentType header
) -> str:
    """Returns etag of s3 object which is a md5 hex checksum of the uploaded content"""
    with payload.reset_on_error():  # ensure retries read the same data
        headers = {}
        if content_md5_b64 and use_md5(upload_url):
            headers["Content-MD5"] = content_md5_b64
        if content_type:
            headers["Content-Type"] = content_type

        async with ClientSessionRegistry.get_session().put(
            upload_url,
            data=payload,
            headers=headers,
            skip_auto_headers=["content-type"] if content_type is None else [],
        ) as resp:
            # S3 signal to slow down request rate.
            if resp.status == 503:
                logger.warning("Received SlowDown signal from S3, sleeping for 1 second before retrying.")
                await asyncio.sleep(1)

            if resp.status != 200:
                try:
                    text = await resp.text()
                except Exception:
                    text = "<no body>"
                raise ExecutionError(f"Put to url {upload_url} failed with status {resp.status}: {text}")

            # client side ETag checksum verification
            # the s3 ETag of a single part upload is a quoted md5 hex of the uploaded content
            etag = resp.headers["ETag"].strip()
            if etag.startswith(("W/", "w/")):  # see https://www.rfc-editor.org/rfc/rfc7232#section-2.3
                etag = etag[2:]
            if etag[0] == '"' and etag[-1] == '"':
                etag = etag[1:-1]
            remote_md5 = etag

            local_md5_hex = payload.md5_checksum().hexdigest()
            if local_md5_hex != remote_md5:
                raise ExecutionError(f"Local data and remote data checksum mismatch ({local_md5_hex} vs {remote_md5})")

            return remote_md5


async def perform_multipart_upload(
    data_file: Union[BinaryIO, io.BytesIO, io.FileIO],
    *,
    content_length: int,
    max_part_size: int,
    part_urls: list[str],
    completion_url: str,
    upload_chunk_size: int = DEFAULT_SEGMENT_CHUNK_SIZE,
    progress_report_cb: Optional[Callable] = None,
) -> None:
    from .bytes_io_segment_payload import BytesIOSegmentPayload

    upload_coros = []
    file_offset = 0
    num_bytes_left = content_length

    # Give each part its own IO reader object to avoid needing to
    # lock access to the reader's position pointer.
    data_file_readers: list[BinaryIO]
    if isinstance(data_file, io.BytesIO):
        view = data_file.getbuffer()  # does not copy data
        data_file_readers = [io.BytesIO(view) for _ in range(len(part_urls))]
    else:
        filename = data_file.name
        data_file_readers = [open(filename, "rb") for _ in range(len(part_urls))]

    for part_number, (data_file_rdr, part_url) in enumerate(zip(data_file_readers, part_urls), start=1):
        part_length_bytes = min(num_bytes_left, max_part_size)
        part_payload = BytesIOSegmentPayload(
            data_file_rdr,
            segment_start=file_offset,
            segment_length=part_length_bytes,
            chunk_size=upload_chunk_size,
            progress_report_cb=progress_report_cb,
        )
        upload_coros.append(_upload_to_s3_url(part_url, payload=part_payload, content_type=None))
        num_bytes_left -= part_length_bytes
        file_offset += part_length_bytes

    part_etags = await TaskContext.gather(*upload_coros)

    # The body of the complete_multipart_upload command needs some data in xml format:
    completion_body = "<CompleteMultipartUpload>\n"
    for part_number, etag in enumerate(part_etags, 1):
        completion_body += f"""<Part>\n<PartNumber>{part_number}</PartNumber>\n<ETag>"{etag}"</ETag>\n</Part>\n"""
    completion_body += "</CompleteMultipartUpload>"

    # etag of combined object should be md5 hex of concatendated md5 *bytes* from parts + `-{num_parts}`
    bin_hash_parts = [bytes.fromhex(etag) for etag in part_etags]

    expected_multipart_etag = hashlib.md5(b"".join(bin_hash_parts)).hexdigest() + f"-{len(part_etags)}"
    resp = await ClientSessionRegistry.get_session().post(
        completion_url, data=completion_body.encode("ascii"), skip_auto_headers=["content-type"]
    )
    if resp.status != 200:
        try:
            msg = await resp.text()
        except Exception:
            msg = "<no body>"
        raise ExecutionError(f"Error when completing multipart upload: {resp.status}\n{msg}")
    else:
        response_body = await resp.text()
        if expected_multipart_etag not in response_body:
            raise ExecutionError(
                f"Hash mismatch on multipart upload assembly: {expected_multipart_etag} not in {response_body}"
            )


def get_content_length(data: BinaryIO) -> int:
    # *Remaining* length of file from current seek position
    pos = data.tell()
    data.seek(0, os.SEEK_END)
    content_length = data.tell()
    data.seek(pos)
    return content_length - pos


async def _blob_upload(
    upload_hashes: UploadHashes, data: Union[bytes, BinaryIO], stub, progress_report_cb: Optional[Callable] = None
) -> str:
    if isinstance(data, bytes):
        data = io.BytesIO(data)

    content_length = get_content_length(data)

    req = api_pb2.BlobCreateRequest(
        content_md5=upload_hashes.md5_base64,
        content_sha256_base64=upload_hashes.sha256_base64,
        content_length=content_length,
    )
    resp = await retry_transient_errors(stub.BlobCreate, req)

    blob_id = resp.blob_id

    if resp.WhichOneof("upload_type_oneof") == "multipart":
        await perform_multipart_upload(
            data,
            content_length=content_length,
            max_part_size=resp.multipart.part_length,
            part_urls=resp.multipart.upload_urls,
            completion_url=resp.multipart.completion_url,
            upload_chunk_size=DEFAULT_SEGMENT_CHUNK_SIZE,
            progress_report_cb=progress_report_cb,
        )
    else:
        from .bytes_io_segment_payload import BytesIOSegmentPayload

        payload = BytesIOSegmentPayload(
            data, segment_start=0, segment_length=content_length, progress_report_cb=progress_report_cb
        )
        await _upload_to_s3_url(
            resp.upload_url,
            payload,
            # for single part uploads, we use server side md5 checksums
            content_md5_b64=upload_hashes.md5_base64,
        )

    if progress_report_cb:
        progress_report_cb(complete=True)

    return blob_id


async def blob_upload(payload: bytes, stub: ModalClientModal) -> str:
    size_mib = len(payload) / 1024 / 1024
    logger.debug(f"Uploading large blob of size {size_mib:.2f} MiB")
    t0 = time.time()
    if isinstance(payload, str):
        logger.warning("Blob uploading string, not bytes - auto-encoding as utf8")
        payload = payload.encode("utf8")
    upload_hashes = get_upload_hashes(payload)
    blob_id = await _blob_upload(upload_hashes, payload, stub)
    dur_s = max(time.time() - t0, 0.001)  # avoid division by zero
    throughput_mib_s = (size_mib) / dur_s
    logger.debug(f"Uploaded large blob of size {size_mib:.2f} MiB ({throughput_mib_s:.2f} MiB/s). {blob_id}")
    return blob_id


async def blob_upload_file(
    file_obj: BinaryIO,
    stub: ModalClientModal,
    progress_report_cb: Optional[Callable] = None,
    sha256_hex: Optional[str] = None,
    md5_hex: Optional[str] = None,
) -> str:
    upload_hashes = get_upload_hashes(file_obj, sha256_hex=sha256_hex, md5_hex=md5_hex)
    return await _blob_upload(upload_hashes, file_obj, stub, progress_report_cb)


@retry(n_attempts=5, base_delay=0.1, timeout=None)
async def _download_from_url(download_url: str) -> bytes:
    async with ClientSessionRegistry.get_session().get(download_url) as s3_resp:
        # S3 signal to slow down request rate.
        if s3_resp.status == 503:
            logger.warning("Received SlowDown signal from S3, sleeping for 1 second before retrying.")
            await asyncio.sleep(1)

        if s3_resp.status != 200:
            text = await s3_resp.text()
            raise ExecutionError(f"Get from url failed with status {s3_resp.status}: {text}")
        return await s3_resp.read()


async def blob_download(blob_id: str, stub: ModalClientModal) -> bytes:
    """Convenience function for reading all of the downloaded file into memory."""
    logger.debug(f"Downloading large blob {blob_id}")
    t0 = time.time()
    req = api_pb2.BlobGetRequest(blob_id=blob_id)
    resp = await retry_transient_errors(stub.BlobGet, req)
    data = await _download_from_url(resp.download_url)
    size_mib = len(data) / 1024 / 1024
    dur_s = max(time.time() - t0, 0.001)  # avoid division by zero
    throughput_mib_s = size_mib / dur_s
    logger.debug(f"Downloaded large blob {blob_id} of size {size_mib:.2f} MiB ({throughput_mib_s:.2f} MiB/s)")
    return data


async def blob_iter(blob_id: str, stub: ModalClientModal) -> AsyncIterator[bytes]:
    req = api_pb2.BlobGetRequest(blob_id=blob_id)
    resp = await retry_transient_errors(stub.BlobGet, req)
    download_url = resp.download_url
    async with ClientSessionRegistry.get_session().get(download_url) as s3_resp:
        # S3 signal to slow down request rate.
        if s3_resp.status == 503:
            logger.warning("Received SlowDown signal from S3, sleeping for 1 second before retrying.")
            await asyncio.sleep(1)

        if s3_resp.status != 200:
            text = await s3_resp.text()
            raise ExecutionError(f"Get from url failed with status {s3_resp.status}: {text}")

        async for chunk in s3_resp.content.iter_any():
            yield chunk


@dataclasses.dataclass
class FileUploadSpec:
    source: Callable[[], Union[AbstractContextManager, BinaryIO]]
    source_description: Any
    mount_filename: str

    use_blob: bool
    content: Optional[bytes]  # typically None if using blob, required otherwise
    sha256_hex: str
    md5_hex: str
    mode: int  # file permission bits (last 12 bits of st_mode)
    size: int


def _get_file_upload_spec(
    source: Callable[[], Union[AbstractContextManager, BinaryIO]],
    source_description: Any,
    mount_filename: PurePosixPath,
    mode: int,
) -> FileUploadSpec:
    with source() as fp:
        # Current position is ignored - we always upload from position 0
        fp.seek(0, os.SEEK_END)
        size = fp.tell()
        fp.seek(0)

        if size >= LARGE_FILE_LIMIT:
            # TODO(dano): remove the placeholder md5 once we stop requiring md5 for blobs
            md5_hex = "baadbaadbaadbaadbaadbaadbaadbaad" if size > MULTIPART_UPLOAD_THRESHOLD else None
            use_blob = True
            content = None
            hashes = get_upload_hashes(fp, md5_hex=md5_hex)
        else:
            use_blob = False
            content = fp.read()
            hashes = get_upload_hashes(content)

    return FileUploadSpec(
        source=source,
        source_description=source_description,
        mount_filename=mount_filename.as_posix(),
        use_blob=use_blob,
        content=content,
        sha256_hex=hashes.sha256_hex(),
        md5_hex=hashes.md5_hex(),
        mode=mode & 0o7777,
        size=size,
    )


def get_file_upload_spec_from_path(
    filename: Path, mount_filename: PurePosixPath, mode: Optional[int] = None
) -> FileUploadSpec:
    # Python appears to give files 0o666 bits on Windows (equal for user, group, and global),
    # so we mask those out to 0o755 for compatibility with POSIX-based permissions.
    mode = mode or os.stat(filename).st_mode & (0o7777 if platform.system() != "Windows" else 0o7755)
    return _get_file_upload_spec(
        lambda: open(filename, "rb"),
        filename,
        mount_filename,
        mode,
    )


def get_file_upload_spec_from_fileobj(fp: BinaryIO, mount_filename: PurePosixPath, mode: int) -> FileUploadSpec:
    @contextmanager
    def source():
        # We ignore position in stream and always upload from position 0
        fp.seek(0)
        yield fp

    return _get_file_upload_spec(
        source,
        str(fp),
        mount_filename,
        mode,
    )


def use_md5(url: str) -> bool:
    """This takes an upload URL in S3 and returns whether we should attach a checksum.

    It's only a workaround for missing functionality in moto.
    https://github.com/spulec/moto/issues/816
    """
    host = urlparse(url).netloc.split(":")[0]
    if host.endswith(".amazonaws.com") or host.endswith(".r2.cloudflarestorage.com"):
        return True
    elif host in ["127.0.0.1", "localhost", "172.21.0.1"]:
        return False
    else:
        raise Exception(f"Unknown S3 host: {host}")


================================================
File: modal/_utils/bytes_io_segment_payload.py
================================================
# Copyright Modal Labs 2024

import asyncio
import hashlib
from contextlib import contextmanager
from typing import BinaryIO, Callable, Optional

# Note: this module needs to import aiohttp in global scope
# This takes about 50ms and isn't needed in many cases for Modal execution
# To avoid this, we import it in local scope when needed (blob_utils.py)
from aiohttp import BytesIOPayload
from aiohttp.abc import AbstractStreamWriter

# read ~16MiB chunks by default
DEFAULT_SEGMENT_CHUNK_SIZE = 2**24


class BytesIOSegmentPayload(BytesIOPayload):
    """Modified bytes payload for concurrent sends of chunks from the same file.

    Adds:
    * read limit using remaining_bytes, in order to split files across streams
    * larger read chunk (to prevent excessive read contention between parts)
    * calculates an md5 for the segment

    Feels like this should be in some standard lib...
    """

    def __init__(
        self,
        bytes_io: BinaryIO,  # should *not* be shared as IO position modification is not locked
        segment_start: int,
        segment_length: int,
        chunk_size: int = DEFAULT_SEGMENT_CHUNK_SIZE,
        progress_report_cb: Optional[Callable] = None,
    ):
        # not thread safe constructor!
        super().__init__(bytes_io)
        self.initial_seek_pos = bytes_io.tell()
        self.segment_start = segment_start
        self.segment_length = segment_length
        # seek to start of file segment we are interested in, in order to make .size() evaluate correctly
        self._value.seek(self.initial_seek_pos + segment_start)
        assert self.segment_length <= super().size
        self.chunk_size = chunk_size
        self.progress_report_cb = progress_report_cb or (lambda *_, **__: None)
        self.reset_state()

    def reset_state(self):
        self._md5_checksum = hashlib.md5()
        self.num_bytes_read = 0
        self._value.seek(self.initial_seek_pos)

    @contextmanager
    def reset_on_error(self):
        try:
            yield
        except Exception as exc:
            try:
                self.progress_report_cb(reset=True)
            except Exception as cb_exc:
                raise cb_exc from exc
            raise exc
        finally:
            self.reset_state()

    @property
    def size(self) -> int:
        return self.segment_length

    def md5_checksum(self):
        return self._md5_checksum

    async def write(self, writer: "AbstractStreamWriter"):
        loop = asyncio.get_event_loop()

        async def safe_read():
            read_start = self.initial_seek_pos + self.segment_start + self.num_bytes_read
            self._value.seek(read_start)
            num_bytes = min(self.chunk_size, self.remaining_bytes())
            chunk = await loop.run_in_executor(None, self._value.read, num_bytes)

            await loop.run_in_executor(None, self._md5_checksum.update, chunk)
            self.num_bytes_read += len(chunk)
            return chunk

        chunk = await safe_read()
        while chunk and self.remaining_bytes() > 0:
            await writer.write(chunk)
            self.progress_report_cb(advance=len(chunk))
            chunk = await safe_read()
        if chunk:
            await writer.write(chunk)
            self.progress_report_cb(advance=len(chunk))

    def remaining_bytes(self):
        return self.segment_length - self.num_bytes_read


================================================
File: modal/_utils/deprecation.py
================================================
# Copyright Modal Labs 2024
import functools
import sys
import warnings
from datetime import date
from typing import Any, Callable, TypeVar

from typing_extensions import ParamSpec  # Needed for Python 3.9

from ..exception import DeprecationError, PendingDeprecationError

_INTERNAL_MODULES = ["modal", "synchronicity"]


def _is_internal_frame(frame):
    module = frame.f_globals["__name__"].split(".")[0]
    return module in _INTERNAL_MODULES


def deprecation_error(deprecated_on: tuple[int, int, int], msg: str):
    raise DeprecationError(f"Deprecated on {date(*deprecated_on)}: {msg}")


def deprecation_warning(
    deprecated_on: tuple[int, int, int], msg: str, *, pending: bool = False, show_source: bool = True
) -> None:
    """Issue a Modal deprecation warning with source optionally attributed to user code.

    See the implementation of the built-in [warnings.warn](https://docs.python.org/3/library/warnings.html#available-functions).
    """
    filename, lineno = "<unknown>", 0
    if show_source:
        # Find the last non-Modal line that triggered the warning
        try:
            frame = sys._getframe()
            while frame is not None and _is_internal_frame(frame):
                frame = frame.f_back
            if frame is not None:
                filename = frame.f_code.co_filename
                lineno = frame.f_lineno
        except ValueError:
            # Use the defaults from above
            pass

    warning_cls = PendingDeprecationError if pending else DeprecationError

    # This is a lower-level function that warnings.warn uses
    warnings.warn_explicit(f"{date(*deprecated_on)}: {msg}", warning_cls, filename, lineno)


P = ParamSpec("P")
R = TypeVar("R")


def renamed_parameter(
    date: tuple[int, int, int],
    old_name: str,
    new_name: str,
    show_source: bool = True,
) -> Callable[[Callable[P, R]], Callable[P, R]]:
    """Decorator for semi-gracefully changing a parameter name.

    Functions wrapped with this decorator can be defined using only the `new_name` of the parameter.
    If the function is invoked with the `old_name`, the wrapper will pass the value as a keyword
    argument for `new_name` and issue a Modal deprecation warning about the change.

    Note that this only prevents parameter renamings from breaking code at runtime.
    Type checking will fail when code uses `old_name`. To avoid this, the `old_name` can be
    preserved in the function signature with an `Annotated` type hint indicating the renaming.
    """

    def decorator(func: Callable[P, R]) -> Callable[P, R]:
        @functools.wraps(func)
        def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
            mut_kwargs: dict[str, Any] = locals()["kwargs"]  # Avoid referencing kwargs directly due to bug in sigtools
            if old_name in mut_kwargs:
                mut_kwargs[new_name] = mut_kwargs.pop(old_name)
                func_name = func.__qualname__.removeprefix("_")  # Avoid confusion when synchronicity-wrapped
                message = (
                    f"The '{old_name}' parameter of `{func_name}` has been renamed to '{new_name}'."
                    "\nUsing the old name will become an error in a future release. Please update your code."
                )
                deprecation_warning(date, message, show_source=show_source)

            return func(*args, **kwargs)

        return wrapper

    return decorator


def warn_on_renamed_autoscaler_settings(func: Callable[P, R]) -> Callable[P, R]:
    name_map = {
        "keep_warm": "min_containers",
        "concurrency_limit": "max_containers",
        "_experimental_buffer_containers": "buffer_containers",
        "container_idle_timeout": "scaledown_window",
    }

    @functools.wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        mut_kwargs: dict[str, Any] = locals()["kwargs"]  # Avoid referencing kwargs directly due to bug in sigtools

        substitutions = []
        old_params_used = name_map.keys() & mut_kwargs.keys()
        for old_param, new_param in name_map.items():
            if old_param in old_params_used:
                new_param = name_map[old_param]
                mut_kwargs[new_param] = mut_kwargs.pop(old_param)
                substitutions.append(f"- {old_param} -> {new_param}")

        if substitutions:
            substitution_string = "\n".join(substitutions)
            message = (
                "We have renamed several parameters related to autoscaling."
                " Please update your code to use the following new names:"
                f"\n\n{substitution_string}"
                "\n\nSee https://modal.com/docs/guide/modal-1-0-migration for more details."
            )
            deprecation_warning((2025, 2, 24), message, pending=True, show_source=True)

        return func(*args, **kwargs)

    return wrapper


================================================
File: modal/_utils/docker_utils.py
================================================
# Copyright Modal Labs 2024
import re
import shlex
from pathlib import Path
from typing import Optional, Sequence

from ..exception import InvalidError


def extract_copy_command_patterns(dockerfile_lines: Sequence[str]) -> list[str]:
    """
    Extract all COPY command sources from a Dockerfile.
    Combines multiline COPY commands into a single line.
    """
    copy_source_patterns: set[str] = set()
    current_command = ""
    copy_pattern = re.compile(r"^\s*COPY\s+(.+)$", re.IGNORECASE)

    # First pass: handle line continuations and collect full commands
    for line in dockerfile_lines:
        line = line.strip()
        if not line or line.startswith("#"):
            # ignore comments and empty lines
            continue

        if current_command:
            # Continue previous line
            current_command += " " + line.rstrip("\\").strip()
        else:
            # Start new command
            current_command = line.rstrip("\\").strip()

        if not line.endswith("\\"):
            # Command is complete

            match = copy_pattern.match(current_command)
            if match:
                args = match.group(1)
                parts = shlex.split(args)

                # COPY --from=... commands reference external sources and do not need a context mount.
                # https://docs.docker.com/reference/dockerfile/#copy---from
                if parts[0].startswith("--from="):
                    current_command = ""
                    continue

                if len(parts) >= 2:
                    # Last part is destination, everything else is a mount source
                    sources = parts[:-1]

                    for source in sources:
                        special_pattern = re.compile(r"^\s*--|\$\s*")
                        if special_pattern.match(source):
                            raise InvalidError(
                                f"COPY command: {source} using special flags/arguments/variables are not supported"
                            )

                        if source == ".":
                            copy_source_patterns.add("./**")
                        else:
                            copy_source_patterns.add(source)

            current_command = ""

    return list(copy_source_patterns)


def find_dockerignore_file(context_directory: Path, dockerfile_path: Optional[Path] = None) -> Optional[Path]:
    """
    Find dockerignore file relative to current context directory
    and if dockerfile path is provided, check if specific <dockerfile_name>.dockerignore
    file exists in the same directory as <dockerfile_name>
    Finds the most specific dockerignore file that exists.
    """

    def valid_dockerignore_file(fp):
        # fp has to exist
        if not fp.exists():
            return False
        # fp has to be subpath to current working directory
        if not fp.is_relative_to(context_directory):
            return False

        return True

    generic_name = ".dockerignore"
    possible_locations = []
    if dockerfile_path:
        specific_name = f"{dockerfile_path.name}.dockerignore"
        # 1. check if specific <dockerfile_name>.dockerignore file exists in the same directory as <dockerfile_name>
        possible_locations.append(dockerfile_path.parent / specific_name)
        # 2. check if generic .dockerignore file exists in the same directory as <dockerfile_name>
        possible_locations.append(dockerfile_path.parent / generic_name)

    # 3. check if generic .dockerignore file exists in current working directory
    possible_locations.append(context_directory / generic_name)

    return next((e for e in possible_locations if valid_dockerignore_file(e)), None)


================================================
File: modal/_utils/function_utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
import enum
import inspect
import os
from collections.abc import AsyncGenerator
from enum import Enum
from pathlib import Path, PurePosixPath
from typing import Any, Callable, Literal, Optional

from grpclib import GRPCError
from grpclib.exceptions import StreamTerminatedError
from synchronicity.exceptions import UserCodeException

import modal_proto
from modal_proto import api_pb2

from .._serialization import deserialize, deserialize_data_format, serialize
from .._traceback import append_modal_tb
from ..config import config, logger
from ..exception import (
    DeserializationError,
    ExecutionError,
    FunctionTimeoutError,
    InternalFailure,
    InvalidError,
    RemoteError,
)
from ..mount import ROOT_DIR, _is_modal_path, _Mount
from .blob_utils import MAX_OBJECT_SIZE_BYTES, blob_download, blob_upload
from .grpc_utils import RETRYABLE_GRPC_STATUS_CODES


class FunctionInfoType(Enum):
    PACKAGE = "package"
    FILE = "file"
    SERIALIZED = "serialized"
    NOTEBOOK = "notebook"


# TODO(elias): Add support for quoted/str annotations
CLASS_PARAM_TYPE_MAP: dict[type, tuple["api_pb2.ParameterType.ValueType", str]] = {
    str: (api_pb2.PARAM_TYPE_STRING, "string_default"),
    int: (api_pb2.PARAM_TYPE_INT, "int_default"),
}


class LocalFunctionError(InvalidError):
    """Raised if a function declared in a non-global scope is used in an impermissible way"""


def entrypoint_only_package_mount_condition(entrypoint_file):
    entrypoint_path = Path(entrypoint_file)

    def inner(filename):
        path = Path(filename)
        if path == entrypoint_path:
            return True
        if path.name == "__init__.py" and path.parent in entrypoint_path.parents:
            # ancestor __init__.py are included
            return True
        return False

    return inner


def is_global_object(object_qual_name: str):
    return "<locals>" not in object_qual_name.split(".")


def is_method_fn(object_qual_name: str):
    # methods have names like Cls.foo.
    if "<locals>" in object_qual_name:
        # functions can be nested in multiple local scopes.
        rest = object_qual_name.split("<locals>.")[-1]
        return len(rest.split(".")) > 1
    return len(object_qual_name.split(".")) > 1


def is_top_level_function(f: Callable) -> bool:
    """Returns True if this function is defined in global scope.

    Returns False if this function is locally scoped (including on a class).
    """
    return f.__name__ == f.__qualname__


def is_async(function):
    # TODO: this is somewhat hacky. We need to know whether the function is async or not in order to
    # coerce the input arguments to the right type. The proper way to do is to call the function and
    # see if you get a coroutine (or async generator) back. However at this point, it's too late to
    # coerce the type. For now let's make a determination based on inspecting the function definition.
    # This sometimes isn't correct, since a "vanilla" Python function can return a coroutine if it
    # wraps async code or similar. Let's revisit this shortly.
    if inspect.ismethod(function):
        function = function.__func__  # inspect the underlying function
    if inspect.iscoroutinefunction(function) or inspect.isasyncgenfunction(function):
        return True
    elif inspect.isfunction(function) or inspect.isgeneratorfunction(function):
        return False
    else:
        raise RuntimeError(f"Function {function} is a strange type {type(function)}")


def get_function_type(is_generator: Optional[bool]) -> "api_pb2.Function.FunctionType.ValueType":
    return api_pb2.Function.FUNCTION_TYPE_GENERATOR if is_generator else api_pb2.Function.FUNCTION_TYPE_FUNCTION


class FunctionInfo:
    """Utility that determines serialization/deserialization mechanisms for functions

    * Stored as file vs serialized
    * If serialized: how to serialize the function
    * If file: which module/function name should be used to retrieve

    Used for populating the definition of a remote function
    """

    raw_f: Optional[Callable[..., Any]]  # if None - this is a "class service function"
    function_name: str
    user_cls: Optional[type[Any]]
    module_name: Optional[str]

    _type: FunctionInfoType
    _file: Optional[str]
    _base_dir: str
    _remote_dir: Optional[PurePosixPath] = None

    def get_definition_type(self) -> "modal_proto.api_pb2.Function.DefinitionType.ValueType":
        if self.is_serialized():
            return modal_proto.api_pb2.Function.DEFINITION_TYPE_SERIALIZED
        else:
            return modal_proto.api_pb2.Function.DEFINITION_TYPE_FILE

    def is_service_class(self):
        if self.raw_f is None:
            assert self.user_cls
            return True
        return False

    # TODO: we should have a bunch of unit tests for this
    def __init__(
        self,
        f: Optional[Callable[..., Any]],
        serialized=False,
        name_override: Optional[str] = None,
        user_cls: Optional[type] = None,
    ):
        self.raw_f = f
        self.user_cls = user_cls

        if name_override is not None:
            self.function_name = name_override
        elif f is None and user_cls:
            # "service function" for running all methods of a class
            self.function_name = f"{user_cls.__name__}.*"
        elif f and user_cls:
            # Method may be defined on superclass of the wrapped class
            self.function_name = f"{user_cls.__name__}.{f.__name__}"
        else:
            self.function_name = f.__qualname__

        # If it's a cls, the @method could be defined in a base class in a different file.
        if user_cls is not None:
            module = inspect.getmodule(user_cls)
        else:
            module = inspect.getmodule(f)

        if getattr(module, "__package__", None) and not serialized:
            # This is a "real" module, eg. examples.logs.f
            # Get the package path
            # Note: __import__ always returns the top-level package.
            self._file = os.path.abspath(module.__file__)
            package_paths = {os.path.abspath(p) for p in __import__(module.__package__).__path__}
            # There might be multiple package paths in some weird cases
            base_dirs = [
                base_dir for base_dir in package_paths if os.path.commonpath((base_dir, self._file)) == base_dir
            ]

            if not base_dirs:
                logger.info(f"Module files: {self._file}")
                logger.info(f"Package paths: {package_paths}")
                logger.info(f"Base dirs: {base_dirs}")
                raise Exception("Wasn't able to find the package directory!")
            elif len(base_dirs) > 1:
                # Base_dirs should all be prefixes of each other since they all contain `module_file`.
                base_dirs.sort(key=len)
            self._base_dir = base_dirs[0]
            self.module_name = module.__spec__.name
            self._remote_dir = ROOT_DIR / PurePosixPath(module.__package__.split(".")[0])
            self._is_serialized = False
            self._type = FunctionInfoType.PACKAGE
        elif hasattr(module, "__file__") and not serialized:
            # This generally covers the case where it's invoked with
            # python foo/bar/baz.py

            # If it's a cls, the @method could be defined in a base class in a different file.
            self._file = os.path.abspath(inspect.getfile(module))
            self.module_name = inspect.getmodulename(self._file)
            self._base_dir = os.path.dirname(self._file)
            self._is_serialized = False
            self._type = FunctionInfoType.FILE
        else:
            self.module_name = None
            self._base_dir = os.path.abspath("")  # get current dir
            self._is_serialized = True  # either explicitly, or by being in a notebook
            if serialized:  # if explicit
                self._type = FunctionInfoType.SERIALIZED
            else:
                self._type = FunctionInfoType.NOTEBOOK

        if not self.is_serialized():
            # Sanity check that this function is defined in global scope
            # Unfortunately, there's no "clean" way to do this in Python
            qualname = f.__qualname__ if f else user_cls.__qualname__
            if not is_global_object(qualname):
                raise LocalFunctionError(
                    "Modal can only import functions defined in global scope unless they are `serialized=True`"
                )

    def is_serialized(self) -> bool:
        return self._is_serialized

    def serialized_function(self) -> bytes:
        # Note: this should only be called from .load() and not at function decoration time
        #       otherwise the serialized function won't have access to variables/side effect
        #        defined after it in the same file
        assert self.is_serialized()
        if self.raw_f:
            serialized_bytes = serialize(self.raw_f)
            logger.debug(f"Serializing {self.raw_f.__qualname__}, size is {len(serialized_bytes)}")
            return serialized_bytes
        else:
            logger.debug(f"Serializing function for class service function {self.user_cls.__qualname__} as empty")
            return b""

    def get_cls_vars(self) -> dict[str, Any]:
        if self.user_cls is not None:
            cls_vars = {
                attr: getattr(self.user_cls, attr)
                for attr in dir(self.user_cls)
                if not callable(getattr(self.user_cls, attr)) and not attr.startswith("__")
            }
            return cls_vars
        return {}

    def get_cls_var_attrs(self) -> dict[str, Any]:
        import dis
        import opcode

        LOAD_ATTR = opcode.opmap["LOAD_ATTR"]
        STORE_ATTR = opcode.opmap["STORE_ATTR"]

        func = self.raw_f
        code = func.__code__
        f_attr_ops = set()
        for instr in dis.get_instructions(code):
            if instr.opcode == LOAD_ATTR:
                f_attr_ops.add(instr.argval)
            elif instr.opcode == STORE_ATTR:
                f_attr_ops.add(instr.argval)

        cls_vars = self.get_cls_vars()
        f_attrs = {k: cls_vars[k] for k in cls_vars if k in f_attr_ops}
        return f_attrs

    def get_globals(self) -> dict[str, Any]:
        from .._vendor.cloudpickle import _extract_code_globals

        if self.raw_f is None:
            return {}

        func = self.raw_f
        while hasattr(func, "__wrapped__") and func is not func.__wrapped__:
            # Unwrap functions decorated using functools.wrapped (potentially multiple times)
            func = func.__wrapped__
        f_globals_ref = _extract_code_globals(func.__code__)
        f_globals = {k: func.__globals__[k] for k in f_globals_ref if k in func.__globals__}
        return f_globals

    def class_parameter_info(self) -> api_pb2.ClassParameterInfo:
        if not self.user_cls:
            return api_pb2.ClassParameterInfo()

        # TODO(elias): Resolve circular dependencies... maybe we'll need some cls_utils module
        from modal.cls import _get_class_constructor_signature, _use_annotation_parameters

        if not _use_annotation_parameters(self.user_cls):
            return api_pb2.ClassParameterInfo(format=api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PICKLE)

        # annotation parameters trigger strictly typed parametrization
        # which enables web endpoint for parametrized classes

        modal_parameters: list[api_pb2.ClassParameterSpec] = []
        signature = _get_class_constructor_signature(self.user_cls)
        for param in signature.parameters.values():
            has_default = param.default is not param.empty
            if param.annotation not in CLASS_PARAM_TYPE_MAP:
                raise InvalidError("modal.parameter() currently only support str or int types")
            param_type, default_field = CLASS_PARAM_TYPE_MAP[param.annotation]
            class_param_spec = api_pb2.ClassParameterSpec(name=param.name, has_default=has_default, type=param_type)
            if has_default:
                setattr(class_param_spec, default_field, param.default)
            modal_parameters.append(class_param_spec)

        return api_pb2.ClassParameterInfo(
            format=api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PROTO, schema=modal_parameters
        )

    def get_entrypoint_mount(self) -> dict[str, _Mount]:
        """
        Includes:
        * Implicit mount of the function itself (the module or package that the function is part of)

        Does not include:
        * Client mount
        * Explicit mounts added to the stub or function declaration
        * "Auto mounted" mounts, i.e. all mounts in sys.modules that are *not* installed in site-packages.
            These are typically local modules which are imported but not part of the running package

        """
        if self.is_serialized():
            # Don't auto-mount anything for serialized functions (including notebooks)
            return {}

        # make sure the function's own entrypoint is included:
        if self._type == FunctionInfoType.PACKAGE:
            top_level_package = self.module_name.split(".")[0]
            # TODO: add deprecation warning if the following entrypoint mount
            #  includes non-.py files, since we'll want to migrate to .py-only
            #  soon to get it consistent with the `add_local_python_source()`
            #  defaults.
            return {top_level_package: _Mount._from_local_python_packages(top_level_package)}
        elif self._type == FunctionInfoType.FILE:
            # TODO: inspect if this file is already included as part of
            #  a package mount, and skip it + reference that package
            #  instead if that's the case. This avoids possible module
            #  duplication bugs
            module_file = Path(self._file)
            container_module_name = module_file.stem
            remote_path = ROOT_DIR / module_file.name
            if not _is_modal_path(remote_path):
                return {
                    container_module_name: _Mount._from_local_file(
                        self._file,
                        remote_path=remote_path,
                    )
                }
        return {}  # this should never be reached...

    def get_tag(self):
        return self.function_name

    def is_nullary(self):
        signature = inspect.signature(self.raw_f)
        for param in signature.parameters.values():
            if param.kind in (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD):
                # variadic parameters are nullary
                continue
            if param.default is param.empty:
                return False
        return True


def callable_has_non_self_params(f: Callable[..., Any]) -> bool:
    """Return True if a callable (function, bound method, or unbound method) has parameters other than self.

    Used to ensure that @exit(), @asgi_app, and @wsgi_app functions don't have parameters.
    """
    return any(param.name != "self" for param in inspect.signature(f).parameters.values())


def callable_has_non_self_non_default_params(f: Callable[..., Any]) -> bool:
    """Return True if a callable (function, bound method, or unbound method) has non-default parameters other than self.

    Used for deprecation of default parameters in @asgi_app and @wsgi_app functions.
    """
    for param in inspect.signature(f).parameters.values():
        if param.name == "self":
            continue

        if param.default != inspect.Parameter.empty:
            continue

        return True
    return False


async def _stream_function_call_data(
    client, function_call_id: str, variant: Literal["data_in", "data_out"]
) -> AsyncGenerator[Any, None]:
    """Read from the `data_in` or `data_out` stream of a function call."""
    last_index = 0

    # TODO(gongy): generalize this logic as util for unary streams
    retries_remaining = 10
    delay_ms = 1

    if variant == "data_in":
        stub_fn = client.stub.FunctionCallGetDataIn
    elif variant == "data_out":
        stub_fn = client.stub.FunctionCallGetDataOut
    else:
        raise ValueError(f"Invalid variant {variant}")

    while True:
        req = api_pb2.FunctionCallGetDataRequest(function_call_id=function_call_id, last_index=last_index)
        try:
            async for chunk in stub_fn.unary_stream(req):
                if chunk.index <= last_index:
                    continue
                if chunk.data_blob_id:
                    message_bytes = await blob_download(chunk.data_blob_id, client.stub)
                else:
                    message_bytes = chunk.data
                message = deserialize_data_format(message_bytes, chunk.data_format, client)

                last_index = chunk.index
                yield message
        except (GRPCError, StreamTerminatedError) as exc:
            if retries_remaining > 0:
                retries_remaining -= 1
                if isinstance(exc, GRPCError):
                    if exc.status in RETRYABLE_GRPC_STATUS_CODES:
                        logger.debug(f"{variant} stream retrying with delay {delay_ms}ms due to {exc}")
                        await asyncio.sleep(delay_ms / 1000)
                        delay_ms = min(1000, delay_ms * 10)
                        continue
                elif isinstance(exc, StreamTerminatedError):
                    continue
            raise
        else:
            delay_ms = 1


OUTPUTS_TIMEOUT = 55.0  # seconds
ATTEMPT_TIMEOUT_GRACE_PERIOD = 5  # seconds


def exc_with_hints(exc: BaseException):
    """mdmd:hidden"""
    if isinstance(exc, ImportError) and exc.msg == "attempted relative import with no known parent package":
        exc.msg += """\n
HINT: For relative imports to work, you might need to run your modal app as a module. Try:
- `python -m my_pkg.my_app` instead of `python my_pkg/my_app.py`
- `modal deploy my_pkg.my_app` instead of `modal deploy my_pkg/my_app.py`
"""
    elif isinstance(
        exc, RuntimeError
    ) and "CUDA error: no kernel image is available for execution on the device" in str(exc):
        msg = (
            exc.args[0]
            + """\n
HINT: This error usually indicates an outdated CUDA version. Older versions of torch (<=1.12)
come with CUDA 10.2 by default. If pinning to an older torch version, you can specify a CUDA version
manually, for example:
-  image.pip_install("torch==1.12.1+cu116", find_links="https://download.pytorch.org/whl/torch_stable.html")
"""
        )
        exc.args = (msg,)

    return exc


async def _process_result(result: api_pb2.GenericResult, data_format: int, stub, client=None):
    if result.WhichOneof("data_oneof") == "data_blob_id":
        data = await blob_download(result.data_blob_id, stub)
    else:
        data = result.data

    if result.status == api_pb2.GenericResult.GENERIC_STATUS_TIMEOUT:
        raise FunctionTimeoutError(result.exception)
    elif result.status == api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE:
        raise InternalFailure(result.exception)
    elif result.status != api_pb2.GenericResult.GENERIC_STATUS_SUCCESS:
        if data:
            try:
                exc = deserialize(data, client)
            except DeserializationError as deser_exc:
                raise ExecutionError(
                    "Could not deserialize remote exception due to local error:\n"
                    + f"{deser_exc}\n"
                    + "This can happen if your local environment does not have the remote exception definitions.\n"
                    + "Here is the remote traceback:\n"
                    + f"{result.traceback}"
                ) from deser_exc.__cause__
            except Exception as deser_exc:
                raise ExecutionError(
                    "Could not deserialize remote exception due to local error:\n"
                    + f"{deser_exc}\n"
                    + "Here is the remote traceback:\n"
                    + f"{result.traceback}"
                ) from deser_exc
            if not isinstance(exc, BaseException):
                raise ExecutionError(f"Got remote exception of incorrect type {type(exc)}")

            if result.serialized_tb:
                try:
                    tb_dict = deserialize(result.serialized_tb, client)
                    line_cache = deserialize(result.tb_line_cache, client)
                    append_modal_tb(exc, tb_dict, line_cache)
                except Exception:
                    pass
            uc_exc = UserCodeException(exc_with_hints(exc))
            raise uc_exc
        raise RemoteError(result.exception)

    try:
        return deserialize_data_format(data, data_format, client)
    except ModuleNotFoundError as deser_exc:
        raise ExecutionError(
            "Could not deserialize result due to error:\n"
            f"{deser_exc}\n"
            "This can happen if your local environment does not have a module that was used to construct the result. \n"
        ) from deser_exc


async def _create_input(
    args, kwargs, client, *, idx: Optional[int] = None, method_name: Optional[str] = None
) -> api_pb2.FunctionPutInputsItem:
    """Serialize function arguments and create a FunctionInput protobuf,
    uploading to blob storage if needed.
    """
    if idx is None:
        idx = 0
    if method_name is None:
        method_name = ""  # proto compatible

    args_serialized = serialize((args, kwargs))

    if len(args_serialized) > MAX_OBJECT_SIZE_BYTES:
        args_blob_id = await blob_upload(args_serialized, client.stub)

        return api_pb2.FunctionPutInputsItem(
            input=api_pb2.FunctionInput(
                args_blob_id=args_blob_id,
                data_format=api_pb2.DATA_FORMAT_PICKLE,
                method_name=method_name,
            ),
            idx=idx,
        )
    else:
        return api_pb2.FunctionPutInputsItem(
            input=api_pb2.FunctionInput(
                args=args_serialized,
                data_format=api_pb2.DATA_FORMAT_PICKLE,
                method_name=method_name,
            ),
            idx=idx,
        )


def _get_suffix_from_web_url_info(url_info: api_pb2.WebUrlInfo) -> str:
    if url_info.truncated:
        suffix = " [grey70](label truncated)[/grey70]"
    elif url_info.label_stolen:
        suffix = " [grey70](label stolen)[/grey70]"
    else:
        suffix = ""
    return suffix


class FunctionCreationStatus:
    # TODO(michael) this really belongs with other output-related code
    # but moving it here so we can use it when loading a function with output disabled
    tag: str
    response: Optional[api_pb2.FunctionCreateResponse] = None

    def __init__(self, resolver, tag):
        self.resolver = resolver
        self.tag = tag

    def __enter__(self):
        self.status_row = self.resolver.add_status_row()
        self.status_row.message(f"Creating function {self.tag}...")
        return self

    def set_response(self, resp: api_pb2.FunctionCreateResponse):
        self.response = resp

    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type:
            raise exc_val

        if not self.response:
            self.status_row.finish(f"Unknown error when creating function {self.tag}")

        elif self.response.function.web_url:
            url_info = self.response.function.web_url_info
            requires_proxy_auth = self.response.function.webhook_config.requires_proxy_auth
            proxy_auth_suffix = " 🔑" if requires_proxy_auth else ""
            # Ensure terms used here match terms used in modal.com/docs/guide/webhook-urls doc.
            suffix = _get_suffix_from_web_url_info(url_info)
            # TODO: this is only printed when we're showing progress. Maybe move this somewhere else.
            web_url = self.response.handle_metadata.web_url
            self.status_row.finish(
                f"Created web function {self.tag} => [magenta underline]{web_url}[/magenta underline]"
                f"{proxy_auth_suffix}{suffix}"
            )

            # Print custom domain in terminal
            for custom_domain in self.response.function.custom_domain_info:
                custom_domain_status_row = self.resolver.add_status_row()
                custom_domain_status_row.finish(
                    f"Custom domain for {self.tag} => [magenta underline]{custom_domain.url}[/magenta underline]"
                )
        else:
            self.status_row.finish(f"Created function {self.tag}.")
            if self.response.function.method_definitions_set:
                for method_definition in self.response.function.method_definitions.values():
                    if method_definition.web_url:
                        url_info = method_definition.web_url_info
                        suffix = _get_suffix_from_web_url_info(url_info)
                        class_web_endpoint_method_status_row = self.resolver.add_status_row()
                        class_web_endpoint_method_status_row.finish(
                            f"Created web endpoint for {method_definition.function_name} => [magenta underline]"
                            f"{method_definition.web_url}[/magenta underline]{suffix}"
                        )
                        for custom_domain in method_definition.custom_domain_info:
                            custom_domain_status_row = self.resolver.add_status_row()
                            custom_domain_status_row.finish(
                                f"Custom domain for {method_definition.function_name} => [magenta underline]"
                                f"{custom_domain.url}[/magenta underline]"
                            )


class IncludeSourceMode(enum.Enum):
    INCLUDE_NOTHING = False  # can only be set in source, can't be set in config
    INCLUDE_MAIN_PACKAGE = True  # also represented by AUTOMOUNT=0 in config
    INCLUDE_FIRST_PARTY = "legacy"  # mounts all "local" modules in sys.modules - represented by AUTOMOUNT=1 in config


def get_include_source_mode(function_or_app_specific) -> IncludeSourceMode:
    """Which "automount" behavior should a function use

    function_or_app_specific: explicit value given in the @function or @cls decorator, in an App constructor, or None

    If function_or_app_specific is specified, validate and return the IncludeSourceMode
    If function_or_app_specific is None, infer it from config
    """
    if function_or_app_specific is not None:
        if not isinstance(function_or_app_specific, bool):
            raise ValueError(
                f"Invalid `include_source` value: {function_or_app_specific}. Use one of:\n"
                f"True - include function's package source\n"
                f"False - include no Python source (module expected to be present in Image)\n"
            )

        # explicitly set in app/function
        return IncludeSourceMode(function_or_app_specific)

    # note that the automount config boolean isn't a 1-1 mapping with include_source!
    legacy_automount_mode: bool = config.get("automount")
    return IncludeSourceMode.INCLUDE_FIRST_PARTY if legacy_automount_mode else IncludeSourceMode.INCLUDE_MAIN_PACKAGE


================================================
File: modal/_utils/grpc_testing.py
================================================
# Copyright Modal Labs 2023
import contextlib
import inspect
import logging
import typing
from collections import Counter, defaultdict
from collections.abc import Awaitable
from typing import Any, Callable

import grpclib.server
from grpclib import GRPCError, Status

from modal.config import logger

if typing.TYPE_CHECKING:
    from test.conftest import MockClientServicer


def patch_mock_servicer(cls):
    """Adds an `.intercept()` context manager method

    This allows for context-local tracking and assertions of all calls
    performed on the servicer during a context, e.g.:

    ```python notest
    with servicer.intercept() as ctx:
        await some_complex_method()
    assert ctx.calls == [("SomeMethod", MyMessage(foo="bar"))]
    ```
    Also allows to set a predefined queue of responses, temporarily replacing
    a mock servicer's default responses for a method:

    ```python notest
    with servicer.intercept() as ctx:
        ctx.add_response("SomeMethod", [
            MyResponse(bar="baz")
        ])
        ctx.add_response("SomeMethod", [
            MyResponse(bar="baz2")
        ])
        await service_stub.SomeMethod(Empty())  # receives MyResponse(bar="baz")
        await service_stub.SomeMethod(Empty())  # receives MyResponse(bar="baz2")
    ```

    Also patches all unimplemented abstract methods in a mock servicer with default error implementations.
    """

    async def fallback(self, stream) -> None:
        raise GRPCError(Status.UNIMPLEMENTED, "Not implemented in mock servicer " + repr(cls))

    @contextlib.contextmanager
    def intercept(servicer):
        ctx = InterceptionContext(servicer)
        servicer.interception_context = ctx
        yield ctx
        ctx._assert_responses_consumed()
        servicer.interception_context = None

    cls.intercept = intercept
    cls.interception_context = None

    def patch_grpc_method(method_name, original_method):
        async def patched_method(servicer_self, stream):
            try:
                ctx = servicer_self.interception_context
                if ctx:
                    intercepted_stream = await InterceptedStream(ctx, method_name, stream).initialize()
                    custom_responder = ctx._next_custom_responder(method_name, intercepted_stream.request_message)
                    if custom_responder:
                        return await custom_responder(servicer_self, intercepted_stream)
                    else:
                        # use default servicer, but intercept messages for assertions
                        return await original_method(servicer_self, intercepted_stream)
                else:
                    return await original_method(servicer_self, stream)
            except GRPCError:
                raise
            except Exception:
                logger.exception("Error in mock servicer responder:")
                raise

        return patched_method

    # Fill in the remaining methods on the class
    for name in dir(cls):
        method = getattr(cls, name)
        if getattr(method, "__isabstractmethod__", False):
            setattr(cls, name, patch_grpc_method(name, fallback))
        elif name[0].isupper() and inspect.isfunction(method):
            setattr(cls, name, patch_grpc_method(name, method))

    cls.__abstractmethods__ = frozenset()
    return cls


class ResponseNotConsumed(Exception):
    def __init__(self, unconsumed_requests: list[str]):
        self.unconsumed_requests = unconsumed_requests
        request_count = Counter(unconsumed_requests)
        super().__init__(f"Expected but did not receive the following requests: {request_count}")


class InterceptionContext:
    def __init__(self, servicer):
        self._servicer = servicer
        self.calls: list[tuple[str, Any]] = []  # List[Tuple[method_name, message]]
        self.custom_responses: dict[str, list[tuple[Callable[[Any], bool], list[Any]]]] = defaultdict(list)
        self.custom_defaults: dict[str, Callable[["MockClientServicer", grpclib.server.Stream], Awaitable[None]]] = {}

    def add_response(
        self, method_name: str, first_payload, *, request_filter: Callable[[Any], bool] = lambda req: True
    ):
        """Adds one response payload to an expected queue of responses for a method.

        These responses will be used once each instead of calling the MockServicer's
        implementation of the method.

        The interception context will throw an exception on exit if not all of the added
        responses have been consumed.
        """
        self.custom_responses[method_name].append((request_filter, [first_payload]))

    def set_responder(
        self, method_name: str, responder: Callable[["MockClientServicer", grpclib.server.Stream], Awaitable[None]]
    ):
        """Replace the default responder from the MockClientServicer with a custom implementation

        ```python notest
        def custom_responder(servicer, stream):
            request = stream.recv_message()
            await stream.send_message(api_pb2.SomeMethodResponse(foo=123))

        with servicer.intercept() as ctx:
            ctx.set_responder("SomeMethod", custom_responder)
        ```

        Responses added via `.add_response()` take precedence over the use of this replacement
        """
        self.custom_defaults[method_name] = responder

    def pop_request(self, method_name):
        # fast forward to the next request of type method_name
        # dropping any preceding requests if there is a match
        # returns the payload of the request
        for i, (_method_name, msg) in enumerate(self.calls):
            if _method_name == method_name:
                self.calls = self.calls[i + 1 :]
                return msg

        raise KeyError(f"No message of that type in call list: {self.calls}")

    def get_requests(self, method_name: str) -> list[Any]:
        if not hasattr(self._servicer, method_name):
            # we check this to prevent things like `assert ctx.get_requests("ASdfFunctionCreate") == 0` passing
            raise ValueError(f"{method_name} not in MockServicer - did you spell it right?")
        return [msg for _method_name, msg in self.calls if _method_name == method_name]

    def _add_recv(self, method_name: str, msg):
        self.calls.append((method_name, msg))

    def _next_custom_responder(self, method_name, request):
        method_responses = self.custom_responses[method_name]
        for i, (request_filter, response_messages) in enumerate(method_responses):
            try:
                request_matches = request_filter(request)
            except Exception:
                logging.exception("Error when filtering requests")
                raise

            if request_matches:
                next_response_messages = response_messages
                self.custom_responses[method_name] = method_responses[:i] + method_responses[i + 1 :]
                break
        else:
            custom_default = self.custom_defaults.get(method_name)
            if not custom_default:
                return None
            return custom_default

        # build a new temporary responder based on the next queued response messages (added via add_response)
        async def responder(servicer_self, stream):
            await stream.recv_message()  # get the input message so we can track that
            for msg in next_response_messages:
                await stream.send_message(msg)

        return responder

    def _assert_responses_consumed(self):
        unconsumed = []
        for method_name, queued_responses in self.custom_responses.items():
            unconsumed += [method_name] * len(queued_responses)

        if unconsumed:
            raise ResponseNotConsumed(unconsumed)


class InterceptedStream:
    def __init__(self, interception_context: InterceptionContext, method_name: str, stream):
        self.interception_context = interception_context
        self.method_name = method_name
        self.stream = stream
        self.request_message = None

    async def initialize(self):
        self.request_message = await self.recv_message()
        return self

    async def recv_message(self):
        if self.request_message:
            ret = self.request_message
            self.request_message = None
            return ret

        msg = await self.stream.recv_message()
        self.interception_context._add_recv(self.method_name, msg)
        return msg

    async def send_message(self, msg):
        await self.stream.send_message(msg)

    def __getattr__(self, attr):
        return getattr(self.stream, attr)


================================================
File: modal/_utils/grpc_utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
import contextlib
import platform
import socket
import time
import typing
import urllib.parse
import uuid
from collections.abc import AsyncIterator
from typing import (
    Any,
    Optional,
    TypeVar,
)

import grpclib.client
import grpclib.config
import grpclib.events
import grpclib.protocol
import grpclib.stream
from google.protobuf.message import Message
from grpclib import GRPCError, Status
from grpclib.exceptions import StreamTerminatedError
from grpclib.protocol import H2Protocol

from modal.exception import AuthError, ConnectionError
from modal_version import __version__

from .logger import logger

RequestType = TypeVar("RequestType", bound=Message)
ResponseType = TypeVar("ResponseType", bound=Message)

if typing.TYPE_CHECKING:
    import modal.client

# Monkey patches grpclib to have a Modal User Agent header.
grpclib.client.USER_AGENT = "modal-client/{version} ({sys}; {py}/{py_ver})'".format(
    version=__version__,
    sys=platform.system(),
    py=platform.python_implementation(),
    py_ver=platform.python_version(),
).lower()


class Subchannel:
    protocol: H2Protocol
    created_at: float
    requests: int

    def __init__(self, protocol: H2Protocol) -> None:
        self.protocol = protocol
        self.created_at = time.time()
        self.requests = 0

    def connected(self):
        if hasattr(self.protocol.handler, "connection_lost"):
            # AbstractHandler doesn't have connection_lost, but Handler does
            return not self.protocol.handler.connection_lost  # type: ignore
        return True


RETRYABLE_GRPC_STATUS_CODES = [
    Status.DEADLINE_EXCEEDED,
    Status.UNAVAILABLE,
    Status.CANCELLED,
    Status.INTERNAL,
]


def create_channel(
    server_url: str,
    metadata: dict[str, str] = {},
) -> grpclib.client.Channel:
    """Creates a grpclib.Channel.

    Either to be used directly by a GRPC stub, or indirectly used through the channel pool.
    See `create_channel`.
    """
    o = urllib.parse.urlparse(server_url)

    channel: grpclib.client.Channel
    config = grpclib.config.Configuration(
        http2_connection_window_size=64 * 1024 * 1024,  # 64 MiB
        http2_stream_window_size=64 * 1024 * 1024,  # 64 MiB
    )

    if o.scheme == "unix":
        channel = grpclib.client.Channel(path=o.path, config=config)  # probably pointless to use a pool ever
    elif o.scheme in ("http", "https"):
        target = o.netloc
        parts = target.split(":")
        assert 1 <= len(parts) <= 2, "Invalid target location: " + target
        ssl = o.scheme.endswith("s")
        host = parts[0]
        port = int(parts[1]) if len(parts) == 2 else 443 if ssl else 80
        channel = grpclib.client.Channel(host, port, ssl=ssl, config=config)
    else:
        raise Exception(f"Unknown scheme: {o.scheme}")

    target = o.path if o.scheme == "unix" else o.netloc
    logger.debug(f"Connecting to {target} using scheme {o.scheme}")

    # Inject metadata for the client.
    async def send_request(event: grpclib.events.SendRequest) -> None:
        for k, v in metadata.items():
            event.metadata[k] = v

        logger.debug(f"Sending request to {event.method_name}")

    grpclib.events.listen(channel, grpclib.events.SendRequest, send_request)

    return channel


async def connect_channel(channel: grpclib.client.Channel):
    """Connects socket (potentially raising errors raising to connectivity."""
    await channel.__connect__()


if typing.TYPE_CHECKING:
    import modal.client


async def unary_stream(
    method: "modal.client.UnaryStreamWrapper[RequestType, ResponseType]",
    request: RequestType,
    metadata: Optional[Any] = None,
) -> AsyncIterator[ResponseType]:
    # TODO: remove this, since we have a method now
    async for item in method.unary_stream(request, metadata):
        yield item


async def retry_transient_errors(
    fn: "modal.client.UnaryUnaryWrapper[RequestType, ResponseType]",
    *args,
    base_delay: float = 0.1,
    max_delay: float = 1,
    delay_factor: float = 2,
    max_retries: Optional[int] = 3,
    additional_status_codes: list = [],
    attempt_timeout: Optional[float] = None,  # timeout for each attempt
    total_timeout: Optional[float] = None,  # timeout for the entire function call
    attempt_timeout_floor=2.0,  # always have at least this much timeout (only for total_timeout)
) -> ResponseType:
    """Retry on transient gRPC failures with back-off until max_retries is reached.
    If max_retries is None, retry forever."""

    delay = base_delay
    n_retries = 0

    status_codes = [*RETRYABLE_GRPC_STATUS_CODES, *additional_status_codes]

    idempotency_key = str(uuid.uuid4())

    t0 = time.time()
    if total_timeout is not None:
        total_deadline = t0 + total_timeout
    else:
        total_deadline = None

    while True:
        metadata = [("x-idempotency-key", idempotency_key), ("x-retry-attempt", str(n_retries))]
        if n_retries > 0:
            metadata.append(("x-retry-delay", str(time.time() - t0)))
        timeouts = []
        if attempt_timeout is not None:
            timeouts.append(attempt_timeout)
        if total_timeout is not None:
            timeouts.append(max(total_deadline - time.time(), attempt_timeout_floor))
        if timeouts:
            timeout = min(timeouts)  # In case the function provided both types of timeouts
        else:
            timeout = None
        try:
            return await fn(*args, metadata=metadata, timeout=timeout)
        except (StreamTerminatedError, GRPCError, OSError, asyncio.TimeoutError, AttributeError) as exc:
            if isinstance(exc, GRPCError) and exc.status not in status_codes:
                if exc.status == Status.UNAUTHENTICATED:
                    raise AuthError(exc.message)
                else:
                    raise exc

            if max_retries is not None and n_retries >= max_retries:
                final_attempt = True
            elif total_deadline is not None and time.time() + delay + attempt_timeout_floor >= total_deadline:
                final_attempt = True
            else:
                final_attempt = False

            if final_attempt:
                if isinstance(exc, OSError):
                    raise ConnectionError(str(exc))
                elif isinstance(exc, asyncio.TimeoutError):
                    raise ConnectionError(str(exc))
                else:
                    raise exc

            if isinstance(exc, AttributeError) and "_write_appdata" not in str(exc):
                # StreamTerminatedError are not properly raised in grpclib<=0.4.7
                # fixed in https://github.com/vmagamedov/grpclib/issues/185
                # TODO: update to newer version (>=0.4.8) once stable
                raise exc

            logger.debug(f"Retryable failure {repr(exc)} {n_retries=} {delay=} for {fn.name}")

            n_retries += 1

            await asyncio.sleep(delay)
            delay = min(delay * delay_factor, max_delay)


def find_free_port() -> int:
    """
    Find a free TCP port, useful for testing.

    WARN: if a returned free port is not bound immediately by the caller, that same port
    may be returned in subsequent calls to this function, potentially creating port collisions.
    """
    with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
        s.bind(("", 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return s.getsockname()[1]


def get_proto_oneof(message: Message, oneof_group: str) -> Optional[Message]:
    oneof_field = message.WhichOneof(oneof_group)
    if oneof_field is None:
        return None

    return getattr(message, oneof_field)


================================================
File: modal/_utils/hash_utils.py
================================================
# Copyright Modal Labs 2022
import base64
import dataclasses
import hashlib
import time
from typing import BinaryIO, Callable, Optional, Sequence, Union

from modal.config import logger

HASH_CHUNK_SIZE = 65536


def _update(hashers: Sequence[Callable[[bytes], None]], data: Union[bytes, BinaryIO]) -> None:
    if isinstance(data, bytes):
        for hasher in hashers:
            hasher(data)
    else:
        assert not isinstance(data, (bytearray, memoryview))  # https://github.com/microsoft/pyright/issues/5697
        pos = data.tell()
        while True:
            chunk = data.read(HASH_CHUNK_SIZE)
            if not isinstance(chunk, bytes):
                raise ValueError(f"Only accepts bytes or byte buffer objects, not {type(chunk)} buffers")
            if not chunk:
                break
            for hasher in hashers:
                hasher(chunk)
        data.seek(pos)


def get_sha256_hex(data: Union[bytes, BinaryIO]) -> str:
    t0 = time.monotonic()
    hasher = hashlib.sha256()
    _update([hasher.update], data)
    logger.debug("get_sha256_hex took %.3fs", time.monotonic() - t0)
    return hasher.hexdigest()


def get_sha256_base64(data: Union[bytes, BinaryIO]) -> str:
    t0 = time.monotonic()
    hasher = hashlib.sha256()
    _update([hasher.update], data)
    logger.debug("get_sha256_base64 took %.3fs", time.monotonic() - t0)
    return base64.b64encode(hasher.digest()).decode("ascii")


def get_md5_base64(data: Union[bytes, BinaryIO]) -> str:
    t0 = time.monotonic()
    hasher = hashlib.md5()
    _update([hasher.update], data)
    logger.debug("get_md5_base64 took %.3fs", time.monotonic() - t0)
    return base64.b64encode(hasher.digest()).decode("utf-8")


@dataclasses.dataclass
class UploadHashes:
    md5_base64: str
    sha256_base64: str

    def md5_hex(self) -> str:
        return base64.b64decode(self.md5_base64).hex()

    def sha256_hex(self) -> str:
        return base64.b64decode(self.sha256_base64).hex()


def get_upload_hashes(
    data: Union[bytes, BinaryIO], sha256_hex: Optional[str] = None, md5_hex: Optional[str] = None
) -> UploadHashes:
    t0 = time.monotonic()
    hashers = {}

    if not sha256_hex:
        sha256 = hashlib.sha256()
        hashers["sha256"] = sha256
    if not md5_hex:
        md5 = hashlib.md5()
        hashers["md5"] = md5

    if hashers:
        updaters = [h.update for h in hashers.values()]
        _update(updaters, data)

    if sha256_hex:
        sha256_base64 = base64.b64encode(bytes.fromhex(sha256_hex)).decode("ascii")
    else:
        sha256_base64 = base64.b64encode(hashers["sha256"].digest()).decode("ascii")

    if md5_hex:
        md5_base64 = base64.b64encode(bytes.fromhex(md5_hex)).decode("ascii")
    else:
        md5_base64 = base64.b64encode(hashers["md5"].digest()).decode("ascii")

    hashes = UploadHashes(
        md5_base64=md5_base64,
        sha256_base64=sha256_base64,
    )

    logger.debug("get_upload_hashes took %.3fs (%s)", time.monotonic() - t0, hashers.keys())
    return hashes


================================================
File: modal/_utils/http_utils.py
================================================
# Copyright Modal Labs 2022
import contextlib
from typing import TYPE_CHECKING, Optional

# Note: importing aiohttp seems to take about 100ms, and it's not really necessarily,
# unless we need to work with blobs. So that's why we import it lazily instead.

if TYPE_CHECKING:
    from aiohttp import ClientSession
    from aiohttp.web import Application

from .async_utils import on_shutdown


def _http_client_with_tls(timeout: Optional[float]) -> "ClientSession":
    """Create a new HTTP client session with standard, bundled TLS certificates.

    This is necessary to prevent client issues on some system where Python does
    not come pre-installed with specific TLS certificates that are necessary to
    connect to AWS S3 bucket URLs.

    Specifically: the error "unable to get local issuer certificate" when making
    an aiohttp request.
    """
    import ssl

    import certifi
    from aiohttp import ClientSession, ClientTimeout, TCPConnector

    ssl_context = ssl.create_default_context(cafile=certifi.where())
    connector = TCPConnector(ssl=ssl_context)
    return ClientSession(connector=connector, timeout=ClientTimeout(total=timeout))


class ClientSessionRegistry:
    _client_session: "ClientSession"
    _client_session_active: bool = False

    @staticmethod
    def get_session():
        if not ClientSessionRegistry._client_session_active:
            ClientSessionRegistry._client_session = _http_client_with_tls(timeout=None)
            ClientSessionRegistry._client_session_active = True
            on_shutdown(ClientSessionRegistry.close_session())
        return ClientSessionRegistry._client_session

    @staticmethod
    async def close_session():
        if ClientSessionRegistry._client_session_active:
            await ClientSessionRegistry._client_session.close()
            ClientSessionRegistry._client_session_active = False


@contextlib.asynccontextmanager
async def run_temporary_http_server(app: "Application"):
    # Allocates a random port, runs a server in a context manager
    # This is used in various tests
    import socket

    from aiohttp.web_runner import AppRunner, SockSite

    sock = socket.socket()
    sock.bind(("", 0))
    port = sock.getsockname()[1]
    host = f"http://127.0.0.1:{port}"

    runner = AppRunner(app)
    await runner.setup()
    site = SockSite(runner, sock=sock)
    await site.start()
    try:
        yield host
    finally:
        await runner.cleanup()


================================================
File: modal/_utils/logger.py
================================================
# Copyright Modal Labs 2022
import logging
import os


def configure_logger(logger: logging.Logger, log_level: str, log_format: str):
    ch = logging.StreamHandler()
    log_level_numeric = logging.getLevelName(log_level.upper())
    logger.setLevel(log_level_numeric)
    ch.setLevel(log_level_numeric)

    if log_format.upper() == "JSON":
        # This is primarily for modal internal use.
        # pythonjsonlogger is already installed in the environment.
        from pythonjsonlogger import jsonlogger

        json_formatter = jsonlogger.JsonFormatter(
            fmt=(
                "%(asctime)s %(levelname)s [%(name)s] [%(filename)s:%(lineno)d] "
                "[dd.service=%(dd.service)s dd.env=%(dd.env)s dd.version=%(dd.version)s dd.trace_id=%(dd.trace_id)s "
                "dd.span_id=%(dd.span_id)s] "
                "- %(message)s"
            ),
            datefmt="%Y-%m-%dT%H:%M:%S%z",
        )

        ch.setFormatter(json_formatter)
    else:
        ch.setFormatter(logging.Formatter("[%(threadName)s] %(asctime)s %(message)s", datefmt="%Y-%m-%dT%H:%M:%S%z"))

    logger.addHandler(ch)


log_level = os.environ.get("MODAL_LOGLEVEL", "WARNING")
log_format = os.environ.get("MODAL_LOG_FORMAT", "STRING")

logger = logging.getLogger("modal-utils")
configure_logger(logger, log_level, log_format)


================================================
File: modal/_utils/mount_utils.py
================================================
# Copyright Modal Labs 2022
import posixpath
import typing
from collections.abc import Mapping, Sequence
from pathlib import PurePath, PurePosixPath
from typing import Union

from ..cloud_bucket_mount import _CloudBucketMount
from ..exception import InvalidError
from ..network_file_system import _NetworkFileSystem
from ..volume import _Volume

T = typing.TypeVar("T", bound=Union[_Volume, _NetworkFileSystem, _CloudBucketMount])


def validate_mount_points(
    display_name: str,
    volume_likes: Mapping[Union[str, PurePosixPath], T],
) -> list[tuple[str, T]]:
    """Mount point path validation for volumes and network file systems."""

    if not isinstance(volume_likes, dict):
        raise InvalidError(
            f"`volume_likes` should be a dict[str | PurePosixPath, {display_name}], got {type(volume_likes)} instead"
        )

    validated = []
    for path, vol in volume_likes.items():
        path = PurePath(path).as_posix()
        abs_path = posixpath.abspath(path)

        if path != abs_path:
            raise InvalidError(f"{display_name} {path} must be a canonical, absolute path.")
        elif abs_path == "/":
            raise InvalidError(f"{display_name} {path} cannot be mounted into root directory.")
        elif abs_path == "/root":
            raise InvalidError(f"{display_name} {path} cannot be mounted at '/root'.")
        elif abs_path == "/tmp":
            raise InvalidError(f"{display_name} {path} cannot be mounted at '/tmp'.")
        validated.append((path, vol))
    return validated


def validate_network_file_systems(
    network_file_systems: Mapping[Union[str, PurePosixPath], _NetworkFileSystem],
):
    validated_network_file_systems = validate_mount_points("NetworkFileSystem", network_file_systems)

    for path, network_file_system in validated_network_file_systems:
        if not isinstance(network_file_system, (_NetworkFileSystem)):
            raise InvalidError(
                f"Object of type {type(network_file_system)} mounted at '{path}' "
                + "is not useable as a network file system."
            )

    return validated_network_file_systems


def validate_volumes(
    volumes: Mapping[Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]],
) -> Sequence[tuple[str, Union[_Volume, _CloudBucketMount]]]:
    validated_volumes = validate_mount_points("Volume", volumes)
    # We don't support mounting a modal.Volume in more than one location,
    # but the same CloudBucketMount object can be used in more than one location.
    volume_to_paths: dict[_Volume, list[str]] = {}
    for path, volume in validated_volumes:
        if not isinstance(volume, (_Volume, _CloudBucketMount)):
            raise InvalidError(f"Object of type {type(volume)} mounted at '{path}' is not usable as a volume.")
        elif isinstance(volume, (_Volume)):
            volume_to_paths.setdefault(volume, []).append(path)
    for paths in volume_to_paths.values():
        if len(paths) > 1:
            conflicting = ", ".join(paths)
            raise InvalidError(
                f"The same Volume cannot be mounted in multiple locations for the same function: {conflicting}"
            )

    return validated_volumes


================================================
File: modal/_utils/name_utils.py
================================================
# Copyright Modal Labs 2022
import re

from ..exception import InvalidError

# https://www.rfc-editor.org/rfc/rfc1035
subdomain_regex = re.compile("^(?![0-9]+$)(?!-)[a-z0-9-]{,63}(?<!-)$")


def is_valid_subdomain_label(label: str) -> bool:
    return subdomain_regex.match(label) is not None


def replace_invalid_subdomain_chars(label: str) -> str:
    return re.sub("[^a-z0-9-]", "-", label.lower())


def is_valid_object_name(name: str) -> bool:
    return (
        # Limit object name length
        len(name) <= 64
        # Limit character set
        and re.match("^[a-zA-Z0-9-_.]+$", name) is not None
        # Avoid collisions with App IDs
        and re.match("^ap-[a-zA-Z0-9]{22}$", name) is None
    )


def is_valid_environment_name(name: str) -> bool:
    # first char is alnum, the rest allows other chars
    return len(name) <= 64 and re.match(r"^[a-zA-Z0-9][a-zA-Z0-9-_.]+$", name) is not None


def is_valid_tag(tag: str) -> bool:
    """Tags are alphanumeric, dashes, periods, and underscores, and must be 50 characters or less"""
    pattern = r"^[a-zA-Z0-9._-]{1,50}$"
    return bool(re.match(pattern, tag))


def check_object_name(name: str, object_type: str) -> None:
    message = (
        f"Invalid {object_type} name: '{name}'."
        "\n\nNames may contain only alphanumeric characters, dashes, periods, and underscores,"
        " must be shorter than 64 characters, and cannot conflict with App ID strings."
    )
    if not is_valid_object_name(name):
        raise InvalidError(message)


def check_environment_name(name: str) -> None:
    message = (
        f"Invalid environment name: '{name}'."
        "\n\nEnvironment names can only start with alphanumeric characters,"
        " may contain only alphanumeric characters, dashes, periods, and underscores,"
        " and must be shorter than 64 characters."
    )
    if not is_valid_environment_name(name):
        raise InvalidError(message)


================================================
File: modal/_utils/package_utils.py
================================================
# Copyright Modal Labs 2022
import importlib
import importlib.util
import typing
from importlib.metadata import PackageNotFoundError, files
from pathlib import Path

from ..exception import ModuleNotMountable


def get_file_formats(module):
    try:
        module_files = files(module)
        if not module_files:
            return []

        endings = [str(p).split(".")[-1] for p in module_files if "." in str(p)]
        return list(set(endings))
    except PackageNotFoundError:
        return []


BINARY_FORMATS = ["so", "S", "s", "asm"]  # TODO


def get_module_mount_info(module_name: str) -> typing.Sequence[tuple[bool, Path]]:
    """Returns a list of tuples [(is_dir, path)] describing how to mount a given module."""
    file_formats = get_file_formats(module_name)
    if set(BINARY_FORMATS) & set(file_formats):
        raise ModuleNotMountable(f"{module_name} can't be mounted because it contains binary file(s).")
    try:
        spec = importlib.util.find_spec(module_name)
    except Exception as exc:
        raise ModuleNotMountable(str(exc))

    entries = []
    if spec is None:
        raise ModuleNotMountable(f"{module_name} has no spec - might not be installed?")
    elif spec.submodule_search_locations:
        entries = [(True, Path(path)) for path in spec.submodule_search_locations if Path(path).exists()]
    else:
        # Individual file
        filename = spec.origin
        if filename is not None and Path(filename).exists():
            entries = [(False, Path(filename))]
    if not entries:
        raise ModuleNotMountable(f"{module_name} has no mountable paths")
    return entries


def parse_major_minor_version(version_string: str) -> tuple[int, int]:
    parts = version_string.split(".")
    if len(parts) < 2:
        raise ValueError("version_string must have at least an 'X.Y' format")
    try:
        major = int(parts[0])
        minor = int(parts[1])
    except ValueError:
        raise ValueError("version_string must have at least an 'X.Y' format with integral major/minor values")

    return major, minor


================================================
File: modal/_utils/pattern_utils.py
================================================
# Copyright Modal Labs 2024
"""Pattern matching library ported from https://github.com/moby/patternmatcher.

This is the same pattern-matching logic used by Docker, except it is written in
Python rather than Go. Also, the original Go library has a couple deprecated
functions that we don't implement in this port.

The main way to use this library is by constructing a `FilePatternMatcher` object,
then asking it whether file paths match any of its patterns.
"""

import enum
import os
import re
from typing import Optional, TextIO

escape_chars = frozenset(".+()|{}$")


class MatchType(enum.IntEnum):
    UNKNOWN = 0
    EXACT = 1
    PREFIX = 2
    SUFFIX = 3
    REGEXP = 4


class Pattern:
    """Defines a single regex pattern used to filter file paths."""

    def __init__(self) -> None:
        """Initialize a new Pattern instance."""
        self.match_type = MatchType.UNKNOWN
        self.cleaned_pattern = ""
        self.dirs: list[str] = []
        self.regexp: Optional[re.Pattern] = None
        self.exclusion = False

    def __str__(self) -> str:
        """Return the cleaned pattern as the string representation."""
        return self.cleaned_pattern

    def compile(self, separator: str) -> None:
        """Compile the pattern into a regular expression.

        Args:
            separator (str): The path separator (e.g., '/' or '\\').

        Raises:
            ValueError: If the pattern is invalid.
        """
        reg_str = "^"
        pattern = self.cleaned_pattern

        esc_separator = separator
        if separator == "\\":
            esc_separator = "\\\\"

        self.match_type = MatchType.EXACT
        i = 0
        pattern_length = len(pattern)
        while i < pattern_length:
            ch = pattern[i]
            if ch == "*":
                if (i + 1) < pattern_length and pattern[i + 1] == "*":
                    # Handle '**'
                    i += 1  # Skip the second '*'
                    # Treat '**/' as '**' so eat the '/'
                    if (i + 1) < pattern_length and pattern[i + 1] == separator:
                        i += 1  # Skip the '/'
                    if i + 1 == pattern_length:
                        # Pattern ends with '**'
                        if self.match_type == MatchType.EXACT:
                            self.match_type = MatchType.PREFIX
                        else:
                            reg_str += ".*"
                            self.match_type = MatchType.REGEXP
                    else:
                        # '**' in the middle
                        reg_str += f"(.*{esc_separator})?"
                        self.match_type = MatchType.REGEXP

                    if i == 1:
                        self.match_type = MatchType.SUFFIX
                else:
                    # Single '*'
                    reg_str += f"[^{esc_separator}]*"
                    self.match_type = MatchType.REGEXP
            elif ch == "?":
                # Single '?'
                reg_str += f"[^{esc_separator}]"
                self.match_type = MatchType.REGEXP
            elif ch in escape_chars:
                reg_str += "\\" + ch
            elif ch == "\\":
                # Escape next character
                if separator == "\\":
                    reg_str += esc_separator
                    i += 1
                    continue
                if (i + 1) < pattern_length:
                    reg_str += "\\" + pattern[i + 1]
                    i += 1  # Skip the escaped character
                    self.match_type = MatchType.REGEXP
                else:
                    reg_str += "\\"
            elif ch == "[" or ch == "]":
                reg_str += ch
                self.match_type = MatchType.REGEXP
            else:
                reg_str += ch
            i += 1

        if self.match_type != MatchType.REGEXP:
            return

        reg_str += "$"

        try:
            self.regexp = re.compile(reg_str)
            self.match_type = MatchType.REGEXP
        except re.error as e:
            raise ValueError(f"Bad pattern: {pattern}") from e

    def match(self, path: str) -> bool:
        """Check if the path matches the pattern."""
        if self.match_type == MatchType.UNKNOWN:
            self.compile(os.path.sep)

        if self.match_type == MatchType.EXACT:
            return path == self.cleaned_pattern
        elif self.match_type == MatchType.PREFIX:
            # Strip trailing '**'
            return path.startswith(self.cleaned_pattern[:-2])
        elif self.match_type == MatchType.SUFFIX:
            # Strip leading '**'
            suffix = self.cleaned_pattern[2:]
            if path.endswith(suffix):
                return True
            # '**/foo' matches 'foo'
            if suffix[0] == os.path.sep and path == suffix[1:]:
                return True
            else:
                return False
        elif self.match_type == MatchType.REGEXP:
            return self.regexp.match(path) is not None
        else:
            return False


def read_ignorefile(reader: TextIO) -> list[str]:
    """Read an ignore file from a reader and return the list of file patterns to
    ignore, applying the following rules:

    - An UTF8 BOM header (if present) is stripped. (Python does this already)
    - Lines starting with "#" are considered comments and are skipped.

    For remaining lines:

    - Leading and trailing whitespace is removed from each ignore pattern.
    - It uses `os.path.normpath` to get the shortest/cleanest path for ignore
      patterns.
    - Leading forward-slashes ("/") are removed from ignore patterns, so
      "/some/path" and "some/path" are considered equivalent.

    Args:
        reader (file-like object): The input stream to read from.

    Returns:
        list: A list of patterns to ignore.
    """
    if reader is None:
        return []

    excludes: list[str] = []

    for line in reader:
        pattern = line.rstrip("\n\r")

        # Lines starting with "#" are ignored
        if pattern.startswith("#"):
            continue

        pattern = pattern.strip()
        if pattern == "":
            continue

        # Normalize absolute paths to paths relative to the context
        # (taking care of '!' prefix)
        invert = pattern[0] == "!"
        if invert:
            pattern = pattern[1:].strip()

        if len(pattern) > 0:
            pattern = os.path.normpath(pattern)
            pattern = pattern.replace(os.sep, "/")
            if len(pattern) > 1 and pattern[0] == "/":
                pattern = pattern[1:]

        if invert:
            pattern = "!" + pattern

        excludes.append(pattern)

    return excludes


================================================
File: modal/_utils/rand_pb_testing.py
================================================
# Copyright Modal Labs 2023
"""Utilities to generate random valid Protobuf messages for testing.

This is based on https://github.com/yupingso/randomproto but customizable for
Modal, with random seeds, and it supports oneofs, and Protobuf v4.
"""

import string
from random import Random
from typing import Any, Callable, Optional, TypeVar, Union

from google.protobuf.descriptor import Descriptor, FieldDescriptor

T = TypeVar("T")

_FIELD_RANDOM_GENERATOR: dict[int, Callable[[Random], Any]] = {
    FieldDescriptor.TYPE_DOUBLE: lambda rand: rand.normalvariate(0, 1),
    FieldDescriptor.TYPE_FLOAT: lambda rand: rand.normalvariate(0, 1),
    FieldDescriptor.TYPE_INT32: lambda rand: int.from_bytes(rand.randbytes(4), "little", signed=True),
    FieldDescriptor.TYPE_INT64: lambda rand: int.from_bytes(rand.randbytes(8), "little", signed=True),
    FieldDescriptor.TYPE_UINT32: lambda rand: int.from_bytes(rand.randbytes(4), "little"),
    FieldDescriptor.TYPE_UINT64: lambda rand: int.from_bytes(rand.randbytes(8), "little"),
    FieldDescriptor.TYPE_SINT32: lambda rand: int.from_bytes(rand.randbytes(4), "little", signed=True),
    FieldDescriptor.TYPE_SINT64: lambda rand: int.from_bytes(rand.randbytes(8), "little", signed=True),
    FieldDescriptor.TYPE_FIXED32: lambda rand: int.from_bytes(rand.randbytes(4), "little"),
    FieldDescriptor.TYPE_FIXED64: lambda rand: int.from_bytes(rand.randbytes(8), "little"),
    FieldDescriptor.TYPE_SFIXED32: lambda rand: int.from_bytes(rand.randbytes(4), "little", signed=True),
    FieldDescriptor.TYPE_SFIXED64: lambda rand: int.from_bytes(rand.randbytes(8), "little", signed=True),
    FieldDescriptor.TYPE_BOOL: lambda rand: rand.choice([True, False]),
    FieldDescriptor.TYPE_STRING: lambda rand: "".join(
        rand.choice(string.printable) for _ in range(int(rand.expovariate(0.15)))
    ),
    FieldDescriptor.TYPE_BYTES: lambda rand: rand.randbytes(int(rand.expovariate(0.15))),
}


def _fill(msg, desc: Descriptor, rand: Random) -> None:
    field: FieldDescriptor
    oneof_fields: set[str] = set()
    for oneof in desc.oneofs:
        oneof_field: Union[FieldDescriptor, None] = rand.choice(list(oneof.fields) + [None])
        if oneof_field is not None:
            oneof_fields.add(oneof_field.name)
    for field in desc.fields:
        if field.containing_oneof is not None and field.name not in oneof_fields:
            continue
        is_message = field.type == FieldDescriptor.TYPE_MESSAGE
        is_repeated = field.label == FieldDescriptor.LABEL_REPEATED
        if is_message:
            msg_field = getattr(msg, field.name)
            if is_repeated:
                num = rand.randint(0, 2)
                for _ in range(num):
                    element = msg_field.add()
                    _fill(element, field.message_type, rand)
            else:
                _fill(msg_field, field.message_type, rand)
        else:
            if field.type == FieldDescriptor.TYPE_ENUM:
                enum_values = [x.number for x in field.enum_type.values]
                generator = lambda rand: rand.choice(enum_values)  # noqa: E731

            else:
                generator = _FIELD_RANDOM_GENERATOR[field.type]
            if is_repeated:
                num = rand.randint(0, 2)
                msg_field = getattr(msg, field.name)
                for _ in range(num):
                    msg_field.append(generator(rand))
            else:
                setattr(msg, field.name, generator(rand))


def rand_pb(proto: type[T], rand: Optional[Random] = None) -> T:
    """Generate a pseudorandom protobuf message.

    ```python notest
    rand = random.Random(42)
    definition = rand_pb(api_pb2.Function, rand)
    ```
    """
    if rand is None:
        rand = Random(0)  # note: deterministic seed if not specified
    msg = proto()
    _fill(msg, proto.DESCRIPTOR, rand)  # type: ignore
    return msg


================================================
File: modal/_utils/shell_utils.py
================================================
# Copyright Modal Labs 2024

import asyncio
import contextlib
import errno
import os
import select
import sys
from collections.abc import Coroutine
from typing import Callable, Optional

from modal._pty import raw_terminal, set_nonblocking

from .async_utils import asyncify


def write_to_fd(fd: int, data: bytes):
    loop = asyncio.get_event_loop()
    future = loop.create_future()

    def try_write():
        nonlocal data
        try:
            nbytes = os.write(fd, data)
            data = data[nbytes:]
            if not data:
                loop.remove_writer(fd)
                future.set_result(None)
        except OSError as e:
            if e.errno == errno.EAGAIN:
                # Wait for the next write notification
                return
            # Fail if it's not EAGAIN
            loop.remove_writer(fd)
            future.set_exception(e)

    loop.add_writer(fd, try_write)
    return future


@contextlib.asynccontextmanager
async def stream_from_stdin(handle_input: Callable[[bytes, int], Coroutine], use_raw_terminal=False):
    """Stream from terminal stdin to the handle_input provided by the method"""
    quit_pipe_read, quit_pipe_write = os.pipe()

    set_nonblocking(sys.stdin.fileno())

    @asyncify
    def _read_stdin() -> Optional[bytes]:
        nonlocal quit_pipe_read
        # TODO: Windows support.
        (readable, _, _) = select.select([sys.stdin.buffer, quit_pipe_read], [], [], 5)
        if quit_pipe_read in readable:
            return None
        if sys.stdin.buffer in readable:
            return sys.stdin.buffer.read()
        # we had 5 seconds of no input. send an empty string as a "heartbeat" to the server.
        return b""

    async def _write():
        message_index = 1
        while True:
            data = await _read_stdin()
            if data is None:
                return

            await handle_input(data, message_index)

            message_index += 1

    write_task = asyncio.create_task(_write())

    if use_raw_terminal:
        with raw_terminal():
            yield
    else:
        yield
    os.write(quit_pipe_write, b"\n")
    write_task.cancel()


================================================
File: modal/_vendor/__init__.py
================================================
# Copyright Modal Labs 2024


================================================
File: modal/_vendor/a2wsgi_wsgi.py
================================================
"""
Vendored version of a2wsgi v1.10.2.

We vendor only a2wsgi/wsgi.py, plus type annotations, to convert WSGI apps into ASGI protocol
versions using the `WSGIMiddleware` class.

This is a well-tested library marked as an optional dependency of uvicorn. It doesn't have the
issues with buffering request streams that asgiref has, and it also is simpler, only requiring a
standard `concurrent.futures.ThreadPoolExecutor`.

---

   Copyright 2022 abersheeran

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
"""

import asyncio
import contextvars
import functools
import os
import sys
import typing
from concurrent.futures import ThreadPoolExecutor
from types import TracebackType
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Literal,
    Optional,
    Protocol,
    Tuple,
    Type,
    TypedDict,
    Union,
)
from collections.abc import Awaitable, Iterable


## BEGIN a2wsgi/asgi_typing.py

if sys.version_info >= (3, 11):
    from typing import NotRequired
else:
    from typing_extensions import NotRequired


class ASGIVersions(TypedDict):
    spec_version: str
    version: Literal["3.0"]


class HTTPScope(TypedDict):
    type: Literal["http"]
    asgi: ASGIVersions
    http_version: str
    method: str
    scheme: str
    path: str
    raw_path: NotRequired[bytes]
    query_string: bytes
    root_path: str
    headers: Iterable[tuple[bytes, bytes]]
    client: NotRequired[tuple[str, int]]
    server: NotRequired[tuple[str, Optional[int]]]
    state: NotRequired[dict[str, Any]]
    extensions: NotRequired[dict[str, dict[object, object]]]


class WebSocketScope(TypedDict):
    type: Literal["websocket"]
    asgi: ASGIVersions
    http_version: str
    scheme: str
    path: str
    raw_path: bytes
    query_string: bytes
    root_path: str
    headers: Iterable[tuple[bytes, bytes]]
    client: NotRequired[tuple[str, int]]
    server: NotRequired[tuple[str, Optional[int]]]
    subprotocols: Iterable[str]
    state: NotRequired[dict[str, Any]]
    extensions: NotRequired[dict[str, dict[object, object]]]


class LifespanScope(TypedDict):
    type: Literal["lifespan"]
    asgi: ASGIVersions
    state: NotRequired[dict[str, Any]]


WWWScope = Union[HTTPScope, WebSocketScope]
Scope = Union[HTTPScope, WebSocketScope, LifespanScope]


class HTTPRequestEvent(TypedDict):
    type: Literal["http.request"]
    body: bytes
    more_body: NotRequired[bool]


class HTTPResponseStartEvent(TypedDict):
    type: Literal["http.response.start"]
    status: int
    headers: NotRequired[Iterable[tuple[bytes, bytes]]]
    trailers: NotRequired[bool]


class HTTPResponseBodyEvent(TypedDict):
    type: Literal["http.response.body"]
    body: NotRequired[bytes]
    more_body: NotRequired[bool]


class HTTPDisconnectEvent(TypedDict):
    type: Literal["http.disconnect"]


class WebSocketConnectEvent(TypedDict):
    type: Literal["websocket.connect"]


class WebSocketAcceptEvent(TypedDict):
    type: Literal["websocket.accept"]
    subprotocol: NotRequired[str]
    headers: NotRequired[Iterable[tuple[bytes, bytes]]]


class WebSocketReceiveEvent(TypedDict):
    type: Literal["websocket.receive"]
    bytes: NotRequired[bytes]
    text: NotRequired[str]


class WebSocketSendEvent(TypedDict):
    type: Literal["websocket.send"]
    bytes: NotRequired[bytes]
    text: NotRequired[str]


class WebSocketDisconnectEvent(TypedDict):
    type: Literal["websocket.disconnect"]
    code: int


class WebSocketCloseEvent(TypedDict):
    type: Literal["websocket.close"]
    code: NotRequired[int]
    reason: NotRequired[str]


class LifespanStartupEvent(TypedDict):
    type: Literal["lifespan.startup"]


class LifespanShutdownEvent(TypedDict):
    type: Literal["lifespan.shutdown"]


class LifespanStartupCompleteEvent(TypedDict):
    type: Literal["lifespan.startup.complete"]


class LifespanStartupFailedEvent(TypedDict):
    type: Literal["lifespan.startup.failed"]
    message: str


class LifespanShutdownCompleteEvent(TypedDict):
    type: Literal["lifespan.shutdown.complete"]


class LifespanShutdownFailedEvent(TypedDict):
    type: Literal["lifespan.shutdown.failed"]
    message: str


ReceiveEvent = Union[
    HTTPRequestEvent,
    HTTPDisconnectEvent,
    WebSocketConnectEvent,
    WebSocketReceiveEvent,
    WebSocketDisconnectEvent,
    LifespanStartupEvent,
    LifespanShutdownEvent,
]

SendEvent = Union[
    HTTPResponseStartEvent,
    HTTPResponseBodyEvent,
    HTTPDisconnectEvent,
    WebSocketAcceptEvent,
    WebSocketSendEvent,
    WebSocketCloseEvent,
    LifespanStartupCompleteEvent,
    LifespanStartupFailedEvent,
    LifespanShutdownCompleteEvent,
    LifespanShutdownFailedEvent,
]

Receive = Callable[[], Awaitable[ReceiveEvent]]

Send = Callable[[SendEvent], Awaitable[None]]

ASGIApp = Callable[[Scope, Receive, Send], Awaitable[None]]

## END a2wsgi/asgi_typing.py


## BEGIN a2wsgi/wsgi_typing.py

class CGIRequiredDefined(TypedDict):
    # The HTTP request method, such as GET or POST. This cannot ever be an
    # empty string, and so is always required.
    REQUEST_METHOD: str
    # When HTTP_HOST is not set, these variables can be combined to determine
    # a default.
    # SERVER_NAME and SERVER_PORT are required strings and must never be empty.
    SERVER_NAME: str
    SERVER_PORT: str
    # The version of the protocol the client used to send the request.
    # Typically this will be something like "HTTP/1.0" or "HTTP/1.1" and
    # may be used by the application to determine how to treat any HTTP
    # request headers. (This variable should probably be called REQUEST_PROTOCOL,
    # since it denotes the protocol used in the request, and is not necessarily
    # the protocol that will be used in the server's response. However, for
    # compatibility with CGI we have to keep the existing name.)
    SERVER_PROTOCOL: str

class CGIOptionalDefined(TypedDict, total=False):
    REQUEST_URI: str
    REMOTE_ADDR: str
    REMOTE_PORT: str
    # The initial portion of the request URL’s “path” that corresponds to the
    # application object, so that the application knows its virtual “location”.
    # This may be an empty string, if the application corresponds to the “root”
    # of the server.
    SCRIPT_NAME: str
    # The remainder of the request URL’s “path”, designating the virtual
    # “location” of the request’s target within the application. This may be an
    # empty string, if the request URL targets the application root and does
    # not have a trailing slash.
    PATH_INFO: str
    # The portion of the request URL that follows the “?”, if any. May be empty
    # or absent.
    QUERY_STRING: str
    # The contents of any Content-Type fields in the HTTP request. May be empty
    # or absent.
    CONTENT_TYPE: str
    # The contents of any Content-Length fields in the HTTP request. May be empty
    # or absent.
    CONTENT_LENGTH: str


class InputStream(Protocol):
    """
    An input stream (file-like object) from which the HTTP request body bytes can be
    read. (The server or gateway may perform reads on-demand as requested by the
    application, or it may pre- read the client's request body and buffer it in-memory
    or on disk, or use any other technique for providing such an input stream, according
    to its preference.)
    """

    def read(self, size: int = -1, /) -> bytes:
        """
        The server is not required to read past the client's specified Content-Length,
        and should simulate an end-of-file condition if the application attempts to read
        past that point. The application should not attempt to read more data than is
        specified by the CONTENT_LENGTH variable.
        A server should allow read() to be called without an argument, and return the
        remainder of the client's input stream.
        A server should return empty bytestrings from any attempt to read from an empty
        or exhausted input stream.
        """
        raise NotImplementedError

    def readline(self, limit: int = -1, /) -> bytes:
        """
        Servers should support the optional "size" argument to readline(), but as in
        WSGI 1.0, they are allowed to omit support for it.
        (In WSGI 1.0, the size argument was not supported, on the grounds that it might
        have been complex to implement, and was not often used in practice... but then
        the cgi module started using it, and so practical servers had to start
        supporting it anyway!)
        """
        raise NotImplementedError

    def readlines(self, hint: int = -1, /) -> list[bytes]:
        """
        Note that the hint argument to readlines() is optional for both caller and
        implementer. The application is free not to supply it, and the server or gateway
        is free to ignore it.
        """
        raise NotImplementedError


class ErrorStream(Protocol):
    """
    An output stream (file-like object) to which error output can be written,
    for the purpose of recording program or other errors in a standardized and
    possibly centralized location. This should be a "text mode" stream;
    i.e., applications should use "\n" as a line ending, and assume that it will
    be converted to the correct line ending by the server/gateway.
    (On platforms where the str type is unicode, the error stream should accept
    and log arbitrary unicode without raising an error; it is allowed, however,
    to substitute characters that cannot be rendered in the stream's encoding.)
    For many servers, wsgi.errors will be the server's main error log. Alternatively,
    this may be sys.stderr, or a log file of some sort. The server's documentation
    should include an explanation of how to configure this or where to find the
    recorded output. A server or gateway may supply different error streams to
    different applications, if this is desired.
    """

    def flush(self) -> None:
        """
        Since the errors stream may not be rewound, servers and gateways are free to
        forward write operations immediately, without buffering. In this case, the
        flush() method may be a no-op. Portable applications, however, cannot assume
        that output is unbuffered or that flush() is a no-op. They must call flush()
        if they need to ensure that output has in fact been written.
        (For example, to minimize intermingling of data from multiple processes writing
        to the same error log.)
        """
        raise NotImplementedError

    def write(self, s: str, /) -> Any:
        raise NotImplementedError

    def writelines(self, seq: list[str], /) -> Any:
        raise NotImplementedError


WSGIDefined = TypedDict(
    "WSGIDefined",
    {
        "wsgi.version": tuple[int, int],  # e.g. (1, 0)
        "wsgi.url_scheme": str,  # e.g. "http" or "https"
        "wsgi.input": InputStream,
        "wsgi.errors": ErrorStream,
        # This value should evaluate true if the application object may be simultaneously
        # invoked by another thread in the same process, and should evaluate false otherwise.
        "wsgi.multithread": bool,
        # This value should evaluate true if an equivalent application object may be
        # simultaneously invoked by another process, and should evaluate false otherwise.
        "wsgi.multiprocess": bool,
        # This value should evaluate true if the server or gateway expects (but does
        # not guarantee!) that the application will only be invoked this one time during
        # the life of its containing process. Normally, this will only be true for a
        # gateway based on CGI (or something similar).
        "wsgi.run_once": bool,
    },
)


class Environ(CGIRequiredDefined, CGIOptionalDefined, WSGIDefined):
    """
    WSGI Environ
    """


ExceptionInfo = tuple[type[BaseException], BaseException, Optional[TracebackType]]

# https://peps.python.org/pep-3333/#the-write-callable
WriteCallable = Callable[[bytes], None]


class StartResponse(Protocol):
    def __call__(
        self,
        status: str,
        response_headers: list[tuple[str, str]],
        exc_info: Optional[ExceptionInfo] = None,
        /,
    ) -> WriteCallable:
        raise NotImplementedError


IterableChunks = Iterable[bytes]

WSGIApp = Callable[[Environ, StartResponse], IterableChunks]

## END a2wsgi/wsgi_typing.py


## BEGIN a2wsgi/wsgi.py

class Body:
    def __init__(self, loop: asyncio.AbstractEventLoop, receive: Receive) -> None:
        self.buffer = bytearray()
        self.loop = loop
        self.receive = receive
        self._has_more = True

    @property
    def has_more(self) -> bool:
        if self._has_more or self.buffer:
            return True
        return False

    def _receive_more_data(self) -> bytes:
        if not self._has_more:
            return b""
        future = asyncio.run_coroutine_threadsafe(self.receive(), loop=self.loop)
        message = future.result()
        self._has_more = message.get("more_body", False)
        return message.get("body", b"")

    def read(self, size: int = -1) -> bytes:
        while size == -1 or size > len(self.buffer):
            self.buffer.extend(self._receive_more_data())
            if not self._has_more:
                break
        if size == -1:
            result = bytes(self.buffer)
            self.buffer.clear()
        else:
            result = bytes(self.buffer[:size])
            del self.buffer[:size]
        return result

    def readline(self, limit: int = -1) -> bytes:
        while True:
            lf_index = self.buffer.find(b"\n", 0, limit if limit > -1 else None)
            if lf_index != -1:
                result = bytes(self.buffer[: lf_index + 1])
                del self.buffer[: lf_index + 1]
                return result
            elif limit != -1:
                result = bytes(self.buffer[:limit])
                del self.buffer[:limit]
                return result
            if not self._has_more:
                break
            self.buffer.extend(self._receive_more_data())

        result = bytes(self.buffer)
        self.buffer.clear()
        return result

    def readlines(self, hint: int = -1) -> list[bytes]:
        if not self.has_more:
            return []
        if hint == -1:
            raw_data = self.read(-1)
            bytelist = raw_data.split(b"\n")
            if raw_data[-1] == 10:  # 10 -> b"\n"
                bytelist.pop(len(bytelist) - 1)
            return [line + b"\n" for line in bytelist]
        return [self.readline() for _ in range(hint)]

    def __iter__(self) -> typing.Generator[bytes, None, None]:
        while self.has_more:
            yield self.readline()


ENC, ESC = sys.getfilesystemencoding(), "surrogateescape"


def unicode_to_wsgi(u):
    """Convert an environment variable to a WSGI "bytes-as-unicode" string"""
    return u.encode(ENC, ESC).decode("iso-8859-1")


def build_environ(scope: HTTPScope, body: Body) -> Environ:
    """
    Builds a scope and request body into a WSGI environ object.
    """
    script_name = scope.get("root_path", "").encode("utf8").decode("latin1")
    path_info = scope["path"].encode("utf8").decode("latin1")
    if path_info.startswith(script_name):
        path_info = path_info[len(script_name) :]

    script_name_environ_var = os.environ.get("SCRIPT_NAME", "")
    if script_name_environ_var:
        script_name = unicode_to_wsgi(script_name_environ_var)

    environ: Environ = {
        "asgi.scope": scope,  # type: ignore a2wsgi
        "REQUEST_METHOD": scope["method"],
        "SCRIPT_NAME": script_name,
        "PATH_INFO": path_info,
        "QUERY_STRING": scope["query_string"].decode("ascii"),
        "SERVER_PROTOCOL": f"HTTP/{scope['http_version']}",
        "wsgi.version": (1, 0),
        "wsgi.url_scheme": scope.get("scheme", "http"),
        "wsgi.input": body,
        "wsgi.errors": sys.stdout,
        "wsgi.multithread": True,
        "wsgi.multiprocess": True,
        "wsgi.run_once": False,
    }

    # Get server name and port - required in WSGI, not in ASGI
    server_addr, server_port = scope.get("server") or ("localhost", 80)
    environ["SERVER_NAME"] = server_addr
    environ["SERVER_PORT"] = str(server_port or 0)

    # Get client IP address
    client = scope.get("client")
    if client is not None:
        addr, port = client
        environ["REMOTE_ADDR"] = addr
        environ["REMOTE_PORT"] = str(port)

    # Go through headers and make them into environ entries
    for name, value in scope.get("headers", []):
        name = name.decode("latin1")
        if name == "content-length":
            corrected_name = "CONTENT_LENGTH"
        elif name == "content-type":
            corrected_name = "CONTENT_TYPE"
        else:
            corrected_name = f"HTTP_{name}".upper().replace("-", "_")
        # HTTPbis say only ASCII chars are allowed in headers, but we latin1 just in case
        value = value.decode("latin1")
        if corrected_name in environ:
            value = environ[corrected_name] + "," + value
        environ[corrected_name] = value
    return environ


class WSGIMiddleware:
    """
    Convert WSGIApp to ASGIApp.
    """

    def __init__(
        self, app: WSGIApp, workers: int = 10, send_queue_size: int = 10
    ) -> None:
        self.app = app
        self.send_queue_size = send_queue_size
        self.executor = ThreadPoolExecutor(
            thread_name_prefix="WSGI", max_workers=workers
        )

    async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:
        if scope["type"] == "http":
            responder = WSGIResponder(self.app, self.executor, self.send_queue_size)
            return await responder(scope, receive, send)

        if scope["type"] == "websocket":
            await send({"type": "websocket.close", "code": 1000})
            return

        if scope["type"] == "lifespan":
            message = await receive()
            assert message["type"] == "lifespan.startup"
            await send({"type": "lifespan.startup.complete"})
            message = await receive()
            assert message["type"] == "lifespan.shutdown"
            await send({"type": "lifespan.shutdown.complete"})
            return


class WSGIResponder:
    def __init__(
        self, app: WSGIApp, executor: ThreadPoolExecutor, send_queue_size: int
    ) -> None:
        self.app = app
        self.executor = executor
        self.loop = asyncio.get_event_loop()
        self.send_queue = asyncio.Queue(send_queue_size)
        self.response_started = False
        self.exc_info: typing.Any = None

    async def __call__(self, scope: HTTPScope, receive: Receive, send: Send) -> None:
        body = Body(self.loop, receive)
        environ = build_environ(scope, body)
        sender = None
        try:
            sender = self.loop.create_task(self.sender(send))
            context = contextvars.copy_context()
            func = functools.partial(context.run, self.wsgi)
            await self.loop.run_in_executor(
                self.executor, func, environ, self.start_response
            )
            await self.send_queue.put(None)
            await self.send_queue.join()
            await asyncio.wait_for(sender, None)
            if self.exc_info is not None:
                raise self.exc_info[0].with_traceback(
                    self.exc_info[1], self.exc_info[2]
                )
        finally:
            if sender and not sender.done():
                sender.cancel()  # pragma: no cover

    def send(self, message: typing.Optional[SendEvent]) -> None:
        future = asyncio.run_coroutine_threadsafe(
            self.send_queue.put(message),
            loop=self.loop,
        )
        future.result()

    async def sender(self, send: Send) -> None:
        while True:
            message = await self.send_queue.get()
            self.send_queue.task_done()
            if message is None:
                return
            await send(message)

    def start_response(
        self,
        status: str,
        response_headers: list[tuple[str, str]],
        exc_info: typing.Optional[ExceptionInfo] = None,
    ) -> WriteCallable:
        self.exc_info = exc_info
        if not self.response_started:
            self.response_started = True
            status_code_string, _ = status.split(" ", 1)
            status_code = int(status_code_string)
            headers = [
                (name.strip().encode("latin1").lower(), value.strip().encode("latin1"))
                for name, value in response_headers
            ]
            self.send(
                {
                    "type": "http.response.start",
                    "status": status_code,
                    "headers": headers,
                }
            )
        return lambda chunk: self.send(
            {"type": "http.response.body", "body": chunk, "more_body": True}
        )

    def wsgi(self, environ: Environ, start_response: StartResponse) -> None:
        iterable = self.app(environ, start_response)
        try:
            for chunk in iterable:
                self.send(
                    {"type": "http.response.body", "body": chunk, "more_body": True}
                )

            self.send({"type": "http.response.body", "body": b""})
        finally:
            getattr(iterable, "close", lambda: None)()

## END a2wsgi/wsgi.py



================================================
File: modal/_vendor/tblib.py
================================================
"""
Vendored version of tblib v3.0.0.

We vendor only tblib/__init__.py because we don't use tblib's pickling features (we use cloudpickle instead).

---

BSD 2-Clause License

Copyright (c) 2013-2023, Ionel Cristian Mărieș. All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted provided that the
following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following
disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following
disclaimer in the documentation and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES,
INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
"""
import re
import sys
from types import CodeType

__version__ = '3.0.0'
__all__ = 'Traceback', 'TracebackParseError', 'Frame', 'Code'

FRAME_RE = re.compile(r'^\s*File "(?P<co_filename>.+)", line (?P<tb_lineno>\d+)(, in (?P<co_name>.+))?$')


class _AttrDict(dict):
    __slots__ = ()

    def __getattr__(self, name):
        try:
            return self[name]
        except KeyError:
            raise AttributeError(name) from None


# noinspection PyPep8Naming
class __traceback_maker(Exception):
    pass


class TracebackParseError(Exception):
    pass


class Code:
    """
    Class that replicates just enough of the builtin Code object to enable serialization and traceback rendering.
    """

    co_code = None

    def __init__(self, code):
        self.co_filename = code.co_filename
        self.co_name = code.co_name
        self.co_argcount = 0
        self.co_kwonlyargcount = 0
        self.co_varnames = ()
        self.co_nlocals = 0
        self.co_stacksize = 0
        self.co_flags = 64
        self.co_firstlineno = 0


class Frame:
    """
    Class that replicates just enough of the builtin Frame object to enable serialization and traceback rendering.

    Args:

        get_locals (callable): A function that take a frame argument and returns a dict.

            See :class:`Traceback` class for example.
    """

    def __init__(self, frame, *, get_locals=None):
        self.f_locals = {} if get_locals is None else get_locals(frame)
        self.f_globals = {k: v for k, v in frame.f_globals.items() if k in ('__file__', '__name__')}
        self.f_code = Code(frame.f_code)
        self.f_lineno = frame.f_lineno

    def clear(self):
        """
        For compatibility with PyPy 3.5;
        clear() was added to frame in Python 3.4
        and is called by traceback.clear_frames(), which
        in turn is called by unittest.TestCase.assertRaises
        """


class Traceback:
    """
    Class that wraps builtin Traceback objects.

    Args:
        get_locals (callable): A function that take a frame argument and returns a dict.

            Ideally you will only return exactly what you need, and only with simple types that can be json serializable.

            Example:

            .. code:: python

                def get_locals(frame):
                    if frame.f_locals.get("__tracebackhide__"):
                        return {"__tracebackhide__": True}
                    else:
                        return {}
    """

    tb_next = None

    def __init__(self, tb, *, get_locals=None):
        self.tb_frame = Frame(tb.tb_frame, get_locals=get_locals)
        self.tb_lineno = int(tb.tb_lineno)

        # Build in place to avoid exceeding the recursion limit
        tb = tb.tb_next
        prev_traceback = self
        cls = type(self)
        while tb is not None:
            traceback = object.__new__(cls)
            traceback.tb_frame = Frame(tb.tb_frame, get_locals=get_locals)
            traceback.tb_lineno = int(tb.tb_lineno)
            prev_traceback.tb_next = traceback
            prev_traceback = traceback
            tb = tb.tb_next

    def as_traceback(self):
        """
        Convert to a builtin Traceback object that is usable for raising or rendering a stacktrace.
        """
        current = self
        top_tb = None
        tb = None
        while current:
            f_code = current.tb_frame.f_code
            code = compile('\n' * (current.tb_lineno - 1) + 'raise __traceback_maker', current.tb_frame.f_code.co_filename, 'exec')
            if hasattr(code, 'replace'):
                # Python 3.8 and newer
                code = code.replace(co_argcount=0, co_filename=f_code.co_filename, co_name=f_code.co_name, co_freevars=(), co_cellvars=())
            else:
                code = CodeType(
                    0,
                    code.co_kwonlyargcount,
                    code.co_nlocals,
                    code.co_stacksize,
                    code.co_flags,
                    code.co_code,
                    code.co_consts,
                    code.co_names,
                    code.co_varnames,
                    f_code.co_filename,
                    f_code.co_name,
                    code.co_firstlineno,
                    code.co_lnotab,
                    (),
                    (),
                )

            # noinspection PyBroadException
            try:
                exec(code, dict(current.tb_frame.f_globals), dict(current.tb_frame.f_locals))  # noqa: S102
            except Exception:
                next_tb = sys.exc_info()[2].tb_next
                if top_tb is None:
                    top_tb = next_tb
                if tb is not None:
                    tb.tb_next = next_tb
                tb = next_tb
                del next_tb

            current = current.tb_next
        try:
            return top_tb
        finally:
            del top_tb
            del tb

    to_traceback = as_traceback

    def as_dict(self):
        """
        Converts to a dictionary representation. You can serialize the result to JSON as it only has
        builtin objects like dicts, lists, ints or strings.
        """
        if self.tb_next is None:
            tb_next = None
        else:
            tb_next = self.tb_next.as_dict()

        code = {
            'co_filename': self.tb_frame.f_code.co_filename,
            'co_name': self.tb_frame.f_code.co_name,
        }
        frame = {
            'f_globals': self.tb_frame.f_globals,
            'f_locals': self.tb_frame.f_locals,
            'f_code': code,
            'f_lineno': self.tb_frame.f_lineno,
        }
        return {
            'tb_frame': frame,
            'tb_lineno': self.tb_lineno,
            'tb_next': tb_next,
        }

    to_dict = as_dict

    @classmethod
    def from_dict(cls, dct):
        """
        Creates an instance from a dictionary with the same structure as ``.as_dict()`` returns.
        """
        if dct['tb_next']:
            tb_next = cls.from_dict(dct['tb_next'])
        else:
            tb_next = None

        code = _AttrDict(
            co_filename=dct['tb_frame']['f_code']['co_filename'],
            co_name=dct['tb_frame']['f_code']['co_name'],
        )
        frame = _AttrDict(
            f_globals=dct['tb_frame']['f_globals'],
            f_locals=dct['tb_frame'].get('f_locals', {}),
            f_code=code,
            f_lineno=dct['tb_frame']['f_lineno'],
        )
        tb = _AttrDict(
            tb_frame=frame,
            tb_lineno=dct['tb_lineno'],
            tb_next=tb_next,
        )
        return cls(tb, get_locals=get_all_locals)

    @classmethod
    def from_string(cls, string, strict=True):
        """
        Creates an instance by parsing a stacktrace. Strict means that parsing stops when lines are not indented by at least two spaces
        anymore.
        """
        frames = []
        header = strict

        for line in string.splitlines():
            line = line.rstrip()
            if header:
                if line == 'Traceback (most recent call last):':
                    header = False
                continue
            frame_match = FRAME_RE.match(line)
            if frame_match:
                frames.append(frame_match.groupdict())
            elif line.startswith('  '):
                pass
            elif strict:
                break  # traceback ended

        if frames:
            previous = None
            for frame in reversed(frames):
                previous = _AttrDict(
                    frame,
                    tb_frame=_AttrDict(
                        frame,
                        f_globals=_AttrDict(
                            __file__=frame['co_filename'],
                            __name__='?',
                        ),
                        f_locals={},
                        f_code=_AttrDict(frame),
                        f_lineno=int(frame['tb_lineno']),
                    ),
                    tb_next=previous,
                )
            return cls(previous)
        else:
            raise TracebackParseError('Could not find any frames in %r.' % string)


def get_all_locals(frame):
    return dict(frame.f_locals)

================================================
File: modal/cli/__init__.py
================================================
# Copyright Modal Labs 2022


================================================
File: modal/cli/_download.py
================================================
# Copyright Modal Labs 2023
import asyncio
import os
import shutil
import sys
from collections.abc import AsyncIterator
from pathlib import Path, PurePosixPath
from typing import Callable, Optional, Union

from click import UsageError

from modal._utils.async_utils import TaskContext
from modal.config import logger
from modal.network_file_system import _NetworkFileSystem
from modal.volume import FileEntry, FileEntryType, _Volume

PIPE_PATH = Path("-")


async def _volume_download(
    volume: Union[_NetworkFileSystem, _Volume],
    remote_path: str,
    local_destination: Path,
    overwrite: bool,
    progress_cb: Callable,
):
    is_pipe = local_destination == PIPE_PATH

    q: asyncio.Queue[tuple[Optional[Path], Optional[FileEntry]]] = asyncio.Queue()
    num_consumers = 1 if is_pipe else 10  # concurrency limit for downloading files

    async def producer():
        iterator: AsyncIterator[FileEntry]
        if isinstance(volume, _Volume):
            iterator = volume.iterdir(remote_path, recursive=True)
        else:
            iterator = volume.iterdir(remote_path)  # NFS still supports "glob" paths

        async for entry in iterator:
            if is_pipe:
                await q.put((None, entry))
            else:
                start_path = Path(remote_path).parent.as_posix().split("*")[0]
                rel_path = PurePosixPath(entry.path).relative_to(start_path.lstrip("/"))
                if local_destination.is_dir():
                    output_path = local_destination / rel_path
                else:
                    output_path = local_destination
                if output_path.exists():
                    if overwrite:
                        if output_path.is_file():
                            os.remove(output_path)
                        else:
                            shutil.rmtree(output_path)
                    else:
                        raise UsageError(
                            f"Output path '{output_path}' already exists. Use --force to overwrite the output directory"
                        )
                await q.put((output_path, entry))
        # No more entries to process; issue one shutdown message for each consumer.
        for _ in range(num_consumers):
            await q.put((None, None))

    async def consumer():
        while True:
            output_path, entry = await q.get()
            if entry is None:
                return
            try:
                if is_pipe:
                    if entry.type == FileEntryType.FILE:
                        progress_task_id = progress_cb(name=entry.path, size=entry.size)
                        async for chunk in volume.read_file(entry.path):
                            sys.stdout.buffer.write(chunk)
                            progress_cb(task_id=progress_task_id, advance=len(chunk))
                        progress_cb(task_id=progress_task_id, complete=True)
                else:
                    if entry.type == FileEntryType.FILE:
                        progress_task_id = progress_cb(name=entry.path, size=entry.size)
                        output_path.parent.mkdir(parents=True, exist_ok=True)
                        with output_path.open("wb") as fp:
                            b = 0
                            async for chunk in volume.read_file(entry.path):
                                b += fp.write(chunk)
                                progress_cb(task_id=progress_task_id, advance=len(chunk))
                        logger.debug(f"Wrote {b} bytes to {output_path}")
                        progress_cb(task_id=progress_task_id, complete=True)
                    elif entry.type == FileEntryType.DIRECTORY:
                        output_path.mkdir(parents=True, exist_ok=True)
            finally:
                q.task_done()

    consumers = [consumer() for _ in range(num_consumers)]
    await TaskContext.gather(producer(), *consumers)
    progress_cb(complete=True)
    sys.stdout.flush()


================================================
File: modal/cli/_traceback.py
================================================
# Copyright Modal Labs 2024
"""Helper functions related to displaying tracebacks in the CLI."""

import functools
import re
import warnings
from typing import Optional

from rich.console import Console, RenderResult, group
from rich.panel import Panel
from rich.syntax import Syntax
from rich.text import Text
from rich.traceback import PathHighlighter, Stack, Traceback, install

from ..exception import DeprecationError, PendingDeprecationError, ServerWarning


@group()
def _render_stack(self, stack: Stack) -> RenderResult:
    """Patched variant of rich.Traceback._render_stack that uses the line from the modal StackSummary,
    when the file isn't available to be read locally."""

    path_highlighter = PathHighlighter()
    theme = self.theme
    code_cache: dict[str, str] = {}
    line_cache = getattr(stack, "line_cache", {})
    task_id = None

    def read_code(filename: str) -> str:
        code = code_cache.get(filename)
        if code is None:
            with open(filename, encoding="utf-8", errors="replace") as code_file:
                code = code_file.read()
            code_cache[filename] = code
        return code

    exclude_frames: Optional[range] = None
    if self.max_frames != 0:
        exclude_frames = range(
            self.max_frames // 2,
            len(stack.frames) - self.max_frames // 2,
        )

    excluded = False
    for frame_index, frame in enumerate(stack.frames):
        if exclude_frames and frame_index in exclude_frames:
            excluded = True
            continue

        if excluded:
            assert exclude_frames is not None
            yield Text(
                f"\n... {len(exclude_frames)} frames hidden ...",
                justify="center",
                style="traceback.error",
            )
            excluded = False

        first = frame_index == 0
        # Patched Modal-specific code.
        if frame.filename.startswith("<") and ":" in frame.filename:
            next_task_id, frame_filename = frame.filename.split(":", 1)
            next_task_id = next_task_id.strip("<>")
        else:
            frame_filename = frame.filename
            next_task_id = None
        suppressed = any(frame_filename.startswith(path) for path in self.suppress)

        if next_task_id != task_id:
            task_id = next_task_id
            yield ""
            yield Text(
                f"...Remote call to Modal Function ({task_id})...",
                justify="center",
                style="green",
            )

        text = Text.assemble(
            path_highlighter(Text(frame_filename, style="pygments.string")),
            (":", "pygments.text"),
            (str(frame.lineno), "pygments.number"),
            " in ",
            (frame.name, "pygments.function"),
            style="pygments.text",
        )
        if not frame_filename.startswith("<") and not first:
            yield ""

        yield text
        if not suppressed:
            try:
                code = read_code(frame_filename)
                lexer_name = self._guess_lexer(frame_filename, code)
                syntax = Syntax(
                    code,
                    lexer_name,
                    theme=theme,
                    line_numbers=True,
                    line_range=(
                        frame.lineno - self.extra_lines,
                        frame.lineno + self.extra_lines,
                    ),
                    highlight_lines={frame.lineno},
                    word_wrap=self.word_wrap,
                    code_width=88,
                    indent_guides=self.indent_guides,
                    dedent=False,
                )
                yield ""
            except Exception as error:
                # Patched Modal-specific code.
                line = line_cache.get((frame_filename, frame.lineno))
                if line:
                    try:
                        lexer_name = self._guess_lexer(frame_filename, line)
                        yield ""
                        yield Syntax(
                            line,
                            lexer_name,
                            theme=theme,
                            line_numbers=True,
                            line_range=(0, 1),
                            highlight_lines={frame.lineno},
                            word_wrap=self.word_wrap,
                            code_width=88,
                            indent_guides=self.indent_guides,
                            dedent=False,
                            start_line=frame.lineno,
                        )
                    except Exception:
                        yield Text.assemble(
                            (f"\n{error}", "traceback.error"),
                        )
                yield ""
            else:
                yield syntax


def setup_rich_traceback() -> None:
    from_exception = Traceback.from_exception

    @functools.wraps(Traceback.from_exception)
    def _from_exception(exc_type, exc_value, *args, **kwargs):
        """Patch from_exception to grab the Modal line_cache and store it with the
        Stack object, so it's available to render_stack at display time."""

        line_cache = getattr(exc_value, "__line_cache__", {})
        tb = from_exception(exc_type, exc_value, *args, **kwargs)
        for stack in tb.trace.stacks:
            stack.line_cache = line_cache  # type: ignore
        return tb

    Traceback._render_stack = _render_stack  # type: ignore
    Traceback.from_exception = _from_exception  # type: ignore

    import click
    import grpclib
    import synchronicity
    import typer

    install(suppress=[synchronicity, grpclib, click, typer], extra_lines=1)


def highlight_modal_deprecation_warnings() -> None:
    """Patch the warnings module to make client deprecation warnings more salient in the CLI."""
    base_showwarning = warnings.showwarning

    def showwarning(warning, category, filename, lineno, file=None, line=None):
        if issubclass(category, (DeprecationError, PendingDeprecationError, ServerWarning)):
            content = str(warning)
            if re.match(r"^\d{4}-\d{2}-\d{2}", content):
                date = content[:10]
                message = content[11:].strip()
            else:
                date = ""
                message = content
            try:
                with open(filename, encoding="utf-8", errors="replace") as code_file:
                    source = code_file.readlines()[lineno - 1].strip()
                message = f"{message}\n\nSource: {filename}:{lineno}\n  {source}"
            except OSError:
                # e.g., when filename is "<unknown>"; raises FileNotFoundError on posix but OSError on windows
                pass
            if issubclass(category, ServerWarning):
                title = "Modal Warning"
            else:
                title = "Modal Deprecation Warning"
            if date:
                title += f" ({date})"
            panel = Panel(
                message,
                border_style="yellow",
                title=title,
                title_align="left",
            )
            Console().print(panel)
        else:
            base_showwarning(warning, category, filename, lineno, file=None, line=None)

    warnings.showwarning = showwarning


================================================
File: modal/cli/app.py
================================================
# Copyright Modal Labs 2022
import re
from typing import Optional, Union

import rich
import typer
from click import UsageError
from rich.table import Column
from rich.text import Text
from typer import Argument

from modal._object import _get_environment_name
from modal._utils.async_utils import synchronizer
from modal._utils.deprecation import deprecation_warning
from modal.client import _Client
from modal.environments import ensure_env
from modal_proto import api_pb2

from .utils import ENV_OPTION, display_table, get_app_id_from_name, stream_app_logs, timestamp_to_local

APP_IDENTIFIER = Argument("", help="App name or ID")
NAME_OPTION = typer.Option("", "-n", "--name", help="Deprecated: Pass App name as a positional argument")

app_cli = typer.Typer(name="app", help="Manage deployed and running apps.", no_args_is_help=True)

APP_STATE_TO_MESSAGE = {
    api_pb2.APP_STATE_DEPLOYED: Text("deployed", style="green"),
    api_pb2.APP_STATE_DETACHED: Text("ephemeral (detached)", style="green"),
    api_pb2.APP_STATE_DETACHED_DISCONNECTED: Text("ephemeral (detached)", style="green"),
    api_pb2.APP_STATE_DISABLED: Text("disabled", style="dim"),
    api_pb2.APP_STATE_EPHEMERAL: Text("ephemeral", style="green"),
    api_pb2.APP_STATE_INITIALIZING: Text("initializing...", style="yellow"),
    api_pb2.APP_STATE_STOPPED: Text("stopped", style="blue"),
    api_pb2.APP_STATE_STOPPING: Text("stopping...", style="blue"),
}


@synchronizer.create_blocking
async def get_app_id(app_identifier: str, env: Optional[str], client: Optional[_Client] = None) -> str:
    """Resolve an app_identifier that may be a name or an ID into an ID."""
    if re.match(r"^ap-[a-zA-Z0-9]{22}$", app_identifier):
        return app_identifier
    return await get_app_id_from_name.aio(app_identifier, env, client)


def warn_on_name_option(command: str, app_identifier: str, name: str) -> str:
    if name:
        message = (
            "Passing an App name using --name is deprecated;"
            " App names can now be passed directly as positional arguments:"
            f"\n\n    modal app {command} {name} ..."
        )
        deprecation_warning((2024, 8, 15), message, show_source=False)
        return name
    return app_identifier


@app_cli.command("list")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: bool = False):
    """List Modal apps that are currently deployed/running or recently stopped."""
    env = ensure_env(env)
    client = await _Client.from_env()

    resp: api_pb2.AppListResponse = await client.stub.AppList(
        api_pb2.AppListRequest(environment_name=_get_environment_name(env))
    )

    columns: list[Union[Column, str]] = [
        Column("App ID", min_width=25),  # Ensure that App ID is not truncated in slim terminals
        "Description",
        "State",
        "Tasks",
        "Created at",
        "Stopped at",
    ]
    rows: list[list[Union[Text, str]]] = []
    for app_stats in resp.apps:
        state = APP_STATE_TO_MESSAGE.get(app_stats.state, Text("unknown", style="gray"))
        rows.append(
            [
                app_stats.app_id,
                app_stats.description,
                state,
                str(app_stats.n_running_tasks),
                timestamp_to_local(app_stats.created_at, json),
                timestamp_to_local(app_stats.stopped_at, json),
            ]
        )

    env_part = f" in environment '{env}'" if env else ""
    display_table(columns, rows, json, title=f"Apps{env_part}")


@app_cli.command("logs", no_args_is_help=True)
def logs(
    app_identifier: str = APP_IDENTIFIER,
    *,
    name: str = NAME_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    """Show App logs, streaming while active.

    **Examples:**

    Get the logs based on an app ID:

    ```
    modal app logs ap-123456
    ```

    Get the logs for a currently deployed App based on its name:

    ```
    modal app logs my-app
    ```

    """
    app_identifier = warn_on_name_option("logs", app_identifier, name)
    app_id = get_app_id(app_identifier, env)
    stream_app_logs(app_id)


@app_cli.command("rollback", no_args_is_help=True, context_settings={"ignore_unknown_options": True})
@synchronizer.create_blocking
async def rollback(
    app_identifier: str = APP_IDENTIFIER,
    version: str = typer.Argument("", help="Target version for rollback."),
    *,
    env: Optional[str] = ENV_OPTION,
):
    """Redeploy a previous version of an App.

    Note that the App must currently be in a "deployed" state.
    Rollbacks will appear as a new deployment in the App history, although
    the App state will be reset to the state at the time of the previous deployment.

    **Examples:**

    Rollback an App to its previous version:

    ```
    modal app rollback my-app
    ```

    Rollback an App to a specific version:

    ```
    modal app rollback my-app v3
    ```

    Rollback an App using its App ID instead of its name:

    ```
    modal app rollback ap-abcdefghABCDEFGH123456
    ```

    """
    env = ensure_env(env)
    client = await _Client.from_env()
    app_id = await get_app_id.aio(app_identifier, env, client)
    if not version:
        version_number = -1
    else:
        if m := re.match(r"v(\d+)", version):
            version_number = int(m.group(1))
        else:
            raise UsageError(f"Invalid version specifer: {version}")
    req = api_pb2.AppRollbackRequest(app_id=app_id, version=version_number)
    await client.stub.AppRollback(req)
    rich.print("[green]✓[/green] Deployment rollback successful!")


@app_cli.command("stop", no_args_is_help=True)
@synchronizer.create_blocking
async def stop(
    app_identifier: str = APP_IDENTIFIER,
    *,
    name: str = NAME_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    """Stop an app."""
    app_identifier = warn_on_name_option("stop", app_identifier, name)
    client = await _Client.from_env()
    app_id = await get_app_id.aio(app_identifier, env)
    req = api_pb2.AppStopRequest(app_id=app_id, source=api_pb2.APP_STOP_SOURCE_CLI)
    await client.stub.AppStop(req)


@app_cli.command("history", no_args_is_help=True)
@synchronizer.create_blocking
async def history(
    app_identifier: str = APP_IDENTIFIER,
    *,
    env: Optional[str] = ENV_OPTION,
    name: str = NAME_OPTION,
    json: bool = False,
):
    """Show App deployment history, for a currently deployed app

    **Examples:**

    Get the history based on an app ID:

    ```
    modal app history ap-123456
    ```

    Get the history for a currently deployed App based on its name:

    ```
    modal app history my-app
    ```

    """
    app_identifier = warn_on_name_option("history", app_identifier, name)
    env = ensure_env(env)
    client = await _Client.from_env()
    app_id = await get_app_id.aio(app_identifier, env, client)
    resp = await client.stub.AppDeploymentHistory(api_pb2.AppDeploymentHistoryRequest(app_id=app_id))

    columns = [
        "Version",
        "Time deployed",
        "Client",
        "Deployed by",
    ]
    rows = []
    deployments_with_tags = False
    for idx, app_stats in enumerate(resp.app_deployment_histories):
        style = "bold green" if idx == 0 else ""

        row = [
            Text(f"v{app_stats.version}", style=style),
            Text(timestamp_to_local(app_stats.deployed_at, json), style=style),
            Text(app_stats.client_version, style=style),
            Text(app_stats.deployed_by, style=style),
        ]

        if app_stats.tag:
            deployments_with_tags = True
            row.append(Text(app_stats.tag, style=style))

        rows.append(row)

    if deployments_with_tags:
        columns.append("Tag")

    rows = sorted(rows, key=lambda x: int(str(x[0])[1:]), reverse=True)
    display_table(columns, rows, json)


================================================
File: modal/cli/config.py
================================================
# Copyright Modal Labs 2022
import typer
from rich.console import Console

from modal.config import _profile, _store_user_config, config
from modal.environments import Environment

config_cli = typer.Typer(
    name="config",
    help="""
    Manage client configuration for the current profile.

    Refer to https://modal.com/docs/reference/modal.config for a full explanation
    of what these options mean, and how to set them.
    """,
    no_args_is_help=True,
)


@config_cli.command(help="Show current configuration values (debugging command).")
def show(redact: bool = typer.Option(True, help="Redact the `token_secret` value.")):
    # This is just a test command
    config_dict = config.to_dict()
    if redact and config_dict.get("token_secret"):
        config_dict["token_secret"] = "***"

    console = Console()
    console.print(config_dict)


SET_DEFAULT_ENV_HELP = """Set the default Modal environment for the active profile

The default environment of a profile is used when no --env flag is passed to `modal run`, `modal deploy` etc.

If no default environment is set, and there exists multiple environments in a workspace, an error will be raised
when running a command that requires an environment.
"""


@config_cli.command(help=SET_DEFAULT_ENV_HELP)
def set_environment(environment_name: str):
    # Confirm that the environment exists by looking it up
    Environment.from_name(environment_name).hydrate()
    _store_user_config({"environment": environment_name})
    typer.echo(f"New default environment for profile {_profile}: {environment_name}")


@config_cli.command(hidden=True)
def set(key: str, value: str):
    _store_user_config({key: value})


================================================
File: modal/cli/container.py
================================================
# Copyright Modal Labs 2022
from typing import Optional, Union

import typer
from rich.text import Text

from modal._object import _get_environment_name
from modal._pty import get_pty_info
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal.cli.utils import ENV_OPTION, display_table, is_tty, stream_app_logs, timestamp_to_local
from modal.client import _Client
from modal.config import config
from modal.container_process import _ContainerProcess
from modal.environments import ensure_env
from modal.stream_type import StreamType
from modal_proto import api_pb2

container_cli = typer.Typer(name="container", help="Manage and connect to running containers.", no_args_is_help=True)


@container_cli.command("list")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: bool = False):
    """List all containers that are currently running."""
    env = ensure_env(env)
    client = await _Client.from_env()
    environment_name = _get_environment_name(env)
    res: api_pb2.TaskListResponse = await client.stub.TaskList(
        api_pb2.TaskListRequest(environment_name=environment_name)
    )

    column_names = ["Container ID", "App ID", "App Name", "Start Time"]
    rows: list[list[Union[Text, str]]] = []
    res.tasks.sort(key=lambda task: task.started_at, reverse=True)
    for task_stats in res.tasks:
        rows.append(
            [
                task_stats.task_id,
                task_stats.app_id,
                task_stats.app_description,
                timestamp_to_local(task_stats.started_at, json) if task_stats.started_at else "Pending",
            ]
        )

    display_table(column_names, rows, json=json, title=f"Active Containers in environment: {environment_name}")


@container_cli.command("logs")
def logs(container_id: str = typer.Argument(help="Container ID")):
    """Show logs for a specific container, streaming while active."""
    stream_app_logs(task_id=container_id)


@container_cli.command("exec")
@synchronizer.create_blocking
async def exec(
    pty: Optional[bool] = typer.Option(default=None, help="Run the command using a PTY."),
    container_id: str = typer.Argument(help="Container ID"),
    command: list[str] = typer.Argument(
        help="A command to run inside the container.\n\n"
        "To pass command-line flags or options, add `--` before the start of your commands. "
        "For example: `modal container exec <id> -- /bin/bash -c 'echo hi'`"
    ),
):
    """Execute a command in a container."""

    if pty is None:
        pty = is_tty()

    client = await _Client.from_env()

    req = api_pb2.ContainerExecRequest(
        task_id=container_id,
        command=command,
        pty_info=get_pty_info(shell=True) if pty else None,
        runtime_debug=config.get("function_runtime_debug"),
    )
    res: api_pb2.ContainerExecResponse = await client.stub.ContainerExec(req)

    if pty:
        await _ContainerProcess(res.exec_id, client).attach()
    else:
        # TODO: redirect stderr to its own stream?
        await _ContainerProcess(res.exec_id, client, stdout=StreamType.STDOUT, stderr=StreamType.STDOUT).wait()


@container_cli.command("stop")
@synchronizer.create_blocking
async def stop(container_id: str = typer.Argument(help="Container ID")):
    """Stop a currently-running container and reassign its in-progress inputs.

    This will send the container a SIGINT signal that Modal will handle.
    """
    client = await _Client.from_env()
    request = api_pb2.ContainerStopRequest(task_id=container_id)
    await retry_transient_errors(client.stub.ContainerStop, request)


================================================
File: modal/cli/dict.py
================================================
# Copyright Modal Labs 2024
from typing import Optional

import typer
from rich.console import Console
from typer import Argument, Option, Typer

from modal._resolver import Resolver
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table, timestamp_to_local
from modal.client import _Client
from modal.dict import _Dict
from modal.environments import ensure_env
from modal_proto import api_pb2

dict_cli = Typer(
    name="dict",
    no_args_is_help=True,
    help="Manage `modal.Dict` objects and inspect their contents.",
)


@dict_cli.command(name="create", rich_help_panel="Management")
@synchronizer.create_blocking
async def create(name: str, *, env: Optional[str] = ENV_OPTION):
    """Create a named Dict object.

    Note: This is a no-op when the Dict already exists.
    """
    d = _Dict.from_name(name, environment_name=env, create_if_missing=True)
    client = await _Client.from_env()
    resolver = Resolver(client=client)
    await resolver.load(d)


@dict_cli.command(name="list", rich_help_panel="Management")
@synchronizer.create_blocking
async def list_(*, json: bool = False, env: Optional[str] = ENV_OPTION):
    """List all named Dicts."""
    env = ensure_env(env)
    client = await _Client.from_env()
    request = api_pb2.DictListRequest(environment_name=env)
    response = await retry_transient_errors(client.stub.DictList, request)

    rows = [(d.name, timestamp_to_local(d.created_at, json)) for d in response.dicts]
    display_table(["Name", "Created at"], rows, json)


@dict_cli.command("clear", rich_help_panel="Management")
@synchronizer.create_blocking
async def clear(name: str, *, yes: bool = YES_OPTION, env: Optional[str] = ENV_OPTION):
    """Clear the contents of a named Dict by deleting all of its data."""
    d = _Dict.from_name(name, environment_name=env)
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the contents of modal.Dict '{name}'?",
            default=False,
            abort=True,
        )
    await d.clear()


@dict_cli.command(name="delete", rich_help_panel="Management")
@synchronizer.create_blocking
async def delete(name: str, *, yes: bool = YES_OPTION, env: Optional[str] = ENV_OPTION):
    """Delete a named Dict and all of its data."""
    # Lookup first to validate the name, even though delete is a staticmethod
    await _Dict.from_name(name, environment_name=env).hydrate()
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.Dict '{name}'?",
            default=False,
            abort=True,
        )
    await _Dict.delete(name, environment_name=env)


@dict_cli.command(name="get", rich_help_panel="Inspection")
@synchronizer.create_blocking
async def get(name: str, key: str, *, env: Optional[str] = ENV_OPTION):
    """Print the value for a specific key.

    Note: When using the CLI, keys are always interpreted as having a string type.
    """
    d = _Dict.from_name(name, environment_name=env)
    console = Console()
    val = await d.get(key)
    console.print(val)


def _display(input: str, use_repr: bool) -> str:
    val = repr(input) if use_repr else str(input)
    return val[:80] + "..." if len(val) > 80 else val


@dict_cli.command(name="items", rich_help_panel="Inspection")
@synchronizer.create_blocking
async def items(
    name: str,
    n: int = Argument(default=20, help="Limit the number of entries shown"),
    *,
    all: bool = Option(False, "-a", "--all", help="Ignore N and print all entries in the Dict (may be slow)"),
    use_repr: bool = Option(False, "-r", "--repr", help="Display items using `repr()` to see more details"),
    json: bool = False,
    env: Optional[str] = ENV_OPTION,
):
    """Print the contents of a Dict.

    Note: By default, this command truncates the contents. Use the `N` argument to control the
    amount of data shown or the `--all` option to retrieve the entire Dict, which may be slow.
    """
    d = _Dict.from_name(name, environment_name=env)

    i, items = 0, []
    async for key, val in d.items():
        i += 1
        if not json and not all and i > n:
            items.append(("...", "..."))
            break
        else:
            if json:
                display_item = key, val
            else:
                display_item = _display(key, use_repr), _display(val, use_repr)  # type: ignore  # mypy/issue/12056
            items.append(display_item)

    display_table(["Key", "Value"], items, json)


================================================
File: modal/cli/entry_point.py
================================================
# Copyright Modal Labs 2022
import subprocess
from typing import Optional

import typer
from rich.console import Console
from rich.rule import Rule

from modal._utils.async_utils import synchronizer

from . import run
from .app import app_cli
from .config import config_cli
from .container import container_cli
from .dict import dict_cli
from .environment import environment_cli
from .launch import launch_cli
from .network_file_system import nfs_cli
from .profile import profile_cli
from .queues import queue_cli
from .secret import secret_cli
from .token import _new_token, token_cli
from .volume import volume_cli


def version_callback(value: bool):
    if value:
        from modal_version import __version__

        typer.echo(f"modal client version: {__version__}")
        raise typer.Exit()


entrypoint_cli_typer = typer.Typer(
    no_args_is_help=True,
    add_completion=False,
    rich_markup_mode="markdown",
    help="""
    Modal is the fastest way to run code in the cloud.

    See the website at https://modal.com/ for documentation and more information
    about running code on Modal.
    """,
)


@entrypoint_cli_typer.callback()
def modal(
    ctx: typer.Context,
    version: bool = typer.Option(None, "--version", callback=version_callback),
):
    pass


def check_path():
    """Checks whether the `modal` executable is on the path and usable."""
    url = "https://modal.com/docs/guide/troubleshooting#command-not-found-errors"
    try:
        subprocess.run(["modal", "--help"], capture_output=True)
        # TODO(erikbern): check returncode?
        return
    except FileNotFoundError:
        text = (
            "[red]The `[white]modal[/white]` command was not found on your path!\n"
            "You may need to add it to your path or use `[white]python -m modal[/white]` as a workaround.[/red]\n"
        )
    except PermissionError:
        text = (
            "[red]The `[white]modal[/white]` command is not executable!\n"
            "You may need to give it permissions or use `[white]python -m modal[/white]` as a workaround.[/red]\n"
        )
    text += f"See more information here:\n\n[link={url}]{url}[/link]\n"
    console = Console()
    console.print(text)
    console.print(Rule(style="white"))


@synchronizer.create_blocking
async def setup(profile: Optional[str] = None):
    check_path()

    # Fetch a new token (same as `modal token new` but redirect to /home once finishes)
    await _new_token(profile=profile, next_url="/home")


# Commands
entrypoint_cli_typer.command("deploy", no_args_is_help=True)(run.deploy)
entrypoint_cli_typer.command("serve", no_args_is_help=True)(run.serve)
entrypoint_cli_typer.command("shell")(run.shell)
entrypoint_cli_typer.add_typer(launch_cli)

# Deployments
entrypoint_cli_typer.add_typer(app_cli, rich_help_panel="Deployments")
entrypoint_cli_typer.add_typer(container_cli, rich_help_panel="Deployments")

# Storage
entrypoint_cli_typer.add_typer(dict_cli, rich_help_panel="Storage")
entrypoint_cli_typer.add_typer(nfs_cli, rich_help_panel="Storage")
entrypoint_cli_typer.add_typer(secret_cli, rich_help_panel="Storage")
entrypoint_cli_typer.add_typer(queue_cli, rich_help_panel="Storage")
entrypoint_cli_typer.add_typer(volume_cli, rich_help_panel="Storage")

# Configuration
entrypoint_cli_typer.add_typer(config_cli, rich_help_panel="Configuration")
entrypoint_cli_typer.add_typer(environment_cli, rich_help_panel="Configuration")
entrypoint_cli_typer.add_typer(profile_cli, rich_help_panel="Configuration")
entrypoint_cli_typer.add_typer(token_cli, rich_help_panel="Configuration")

# Hide setup from help as it's redundant with modal token new, but nicer for onboarding
entrypoint_cli_typer.command("setup", help="Bootstrap Modal's configuration.", rich_help_panel="Onboarding")(setup)

# Special handling for modal run, which is more complicated
entrypoint_cli = typer.main.get_command(entrypoint_cli_typer)
entrypoint_cli.add_command(run.run, name="run")  # type: ignore
entrypoint_cli.list_commands(None)  # type: ignore

if __name__ == "__main__":
    # this module is only called from tests, otherwise the parent package __main__.py is used as the entrypoint
    entrypoint_cli()


================================================
File: modal/cli/environment.py
================================================
# Copyright Modal Labs 2023
from typing import Annotated, Optional, Union

import typer
from click import UsageError
from grpclib import GRPCError, Status
from rich.text import Text

from modal import environments
from modal._utils.name_utils import check_environment_name
from modal.cli.utils import display_table
from modal.config import config
from modal.exception import InvalidError

ENVIRONMENT_HELP_TEXT = """Create and interact with Environments

Environments are sub-divisons of workspaces, allowing you to deploy the same app
in different namespaces. Each environment has their own set of Secrets and any
lookups performed from an app in an environment will by default look for entities
in the same environment.

Typical use cases for environments include having one for development and one for
production, to prevent overwriting production apps when developing new features
while still being able to deploy changes to a live environment.
"""

environment_cli = typer.Typer(name="environment", help=ENVIRONMENT_HELP_TEXT, no_args_is_help=True)


class RenderableBool(Text):
    def __init__(self, value: bool):
        self.value = value

    def __rich__(self):
        return repr(self.value)


@environment_cli.command(name="list", help="List all environments in the current workspace")
def list_(json: Optional[bool] = False):
    envs = environments.list_environments()

    # determine which environment is currently active, prioritizing the local default
    # over the server default
    active_env = config.get("environment")
    for env in envs:
        if env.default is True and active_env is None:
            active_env = env.name

    table_data = []
    for item in envs:
        is_active = item.name == active_env
        is_active_display: Union[Text, str] = str(is_active) if json else RenderableBool(is_active)
        row = [item.name, item.webhook_suffix, is_active_display]
        table_data.append(row)
    display_table(["name", "web suffix", "active"], table_data, json=json)


ENVIRONMENT_CREATE_HELP = """Create a new environment in the current workspace"""


@environment_cli.command(name="create", help=ENVIRONMENT_CREATE_HELP)
def create(name: Annotated[str, typer.Argument(help="Name of the new environment. Must be unique. Case sensitive")]):
    check_environment_name(name)

    try:
        environments.create_environment(name)
    except GRPCError as exc:
        if exc.status == Status.INVALID_ARGUMENT:
            raise InvalidError(exc.message)
        raise
    typer.echo(f"Environment created: {name}")


ENVIRONMENT_DELETE_HELP = """Delete an environment in the current workspace

Deletes all apps in the selected environment and deletes the environment irrevocably.
"""


@environment_cli.command(name="delete", help=ENVIRONMENT_DELETE_HELP)
def delete(
    name: str = typer.Argument(help="Name of the environment to be deleted. Case sensitive"),
    confirm: bool = typer.Option(default=False, help="Set this flag to delete without prompting for confirmation"),
):
    if not confirm:
        typer.confirm(
            (
                f"Are you sure you want to irrevocably delete the environment '{name}' and"
                " all its associated apps and secrets?"
            ),
            default=False,
            abort=True,
        )

    environments.delete_environment(name)
    typer.echo(f"Environment deleted: {name}")


ENVIRONMENT_UPDATE_HELP = """Update the name or web suffix of an environment"""


@environment_cli.command(name="update", help=ENVIRONMENT_UPDATE_HELP)
def update(
    current_name: str,
    set_name: Optional[str] = typer.Option(default=None, help="New name of the environment"),
    set_web_suffix: Optional[str] = typer.Option(
        default=None, help="New web suffix of environment (empty string is no suffix)"
    ),
):
    if set_name is None and set_web_suffix is None:
        raise UsageError("You need to at least one new property (using --set-name or --set-web-suffix)")

    if set_name:
        check_environment_name(set_name)

    try:
        environments.update_environment(current_name, new_name=set_name, new_web_suffix=set_web_suffix)
    except GRPCError as exc:
        if exc.status == Status.INVALID_ARGUMENT:
            raise InvalidError(exc.message)
        raise

    typer.echo("Environment updated")


================================================
File: modal/cli/import_refs.py
================================================
# Copyright Modal Labs 2023
"""Load or import Python modules from the CLI.

For example, the function reference of `modal run some_file.py::app.foo_func`
or the app lookup of `modal deploy some_file.py`.

These functions are only called by the Modal CLI, not in tasks.
"""

import dataclasses
import importlib
import importlib.util
import inspect
import sys
import types
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Union, cast

import click
from rich.console import Console
from rich.markdown import Markdown

from modal._utils.deprecation import deprecation_warning
from modal.app import App, LocalEntrypoint
from modal.cls import Cls
from modal.exception import InvalidError, _CliUserExecutionError
from modal.functions import Function


@dataclasses.dataclass
class ImportRef:
    file_or_module: str
    use_module_mode: bool  # i.e. using the -m flag

    # object_path is a .-delimited path to the object to execute, or a parent from which to infer the object
    # e.g.
    # function or local_entrypoint in module scope
    # app in module scope [+ method name]
    # app [+ function/entrypoint on that app]
    object_path: str = dataclasses.field(default="")


def parse_import_ref(object_ref: str, use_module_mode: bool = False) -> ImportRef:
    if object_ref.find("::") > 1:
        file_or_module, object_path = object_ref.split("::", 1)
    elif object_ref.find(":") > 1:
        raise InvalidError(f"Invalid object reference: {object_ref}. Did you mean '::' instead of ':'?")
    else:
        file_or_module, object_path = object_ref, ""

    return ImportRef(file_or_module, use_module_mode, object_path)


DEFAULT_APP_NAME = "app"


def import_file_or_module(import_ref: ImportRef, base_cmd: str = ""):
    if "" not in sys.path:
        # When running from a CLI like `modal run`
        # the current working directory isn't added to sys.path
        # so we add it in order to make module path specification possible
        sys.path.insert(0, "")  # "" means the current working directory

    if not import_ref.file_or_module.endswith(".py") or import_ref.use_module_mode:
        if not import_ref.use_module_mode:
            deprecation_warning(
                (2025, 2, 6),
                f"Using Python module paths will require using the -m flag in a future version of Modal.\n"
                f"Use `{base_cmd} -m {import_ref.file_or_module}` instead.",
                pending=True,
                show_source=False,
            )
        try:
            module = importlib.import_module(import_ref.file_or_module)
        except Exception as exc:
            raise _CliUserExecutionError(import_ref.file_or_module) from exc
    else:
        # when using a script path, that scripts directory should also be on the path as it is
        # with `python some/script.py`
        full_path = Path(import_ref.file_or_module).resolve()
        if "." in full_path.name.removesuffix(".py"):
            raise InvalidError(
                f"Invalid Modal source filename: {full_path.name!r}."
                "\n\nSource filename cannot contain additional period characters."
            )
        sys.path.insert(0, str(full_path.parent))

        module_name = inspect.getmodulename(import_ref.file_or_module)
        assert module_name is not None
        # Import the module - see https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly
        spec = importlib.util.spec_from_file_location(module_name, import_ref.file_or_module)
        assert spec is not None
        module = importlib.util.module_from_spec(spec)
        sys.modules[module_name] = module
        try:
            assert spec.loader
            spec.loader.exec_module(module)
        except Exception as exc:
            raise _CliUserExecutionError(str(full_path)) from exc

    return module


@dataclass(frozen=True)
class MethodReference:
    """This helps with deferring method reference until after the class gets instantiated by the CLI"""

    cls: Cls
    method_name: str


Runnable = Union[Function, MethodReference, LocalEntrypoint]


@dataclass(frozen=True)
class CLICommand:
    names: list[str]
    runnable: Runnable
    is_web_endpoint: bool
    priority: int


class AutoRunPriority:
    MODULE_LOCAL_ENTRYPOINT = 0
    MODULE_FUNCTION = 1
    APP_LOCAL_ENTRYPOINT = 2
    APP_FUNCTION = 3


def list_cli_commands(
    module: types.ModuleType,
) -> list[CLICommand]:
    """
    Extracts all runnables found either directly in the input module, or in any of the Apps listed in that module

    Runnables includes all Functions, (class) Methods and Local Entrypoints, including web endpoints.

    The returned list consists of tuples:
    ([name1, name2...], Runnable)

    Where the first name is always the module level name if such a name exists
    """
    apps = cast(list[tuple[str, App]], inspect.getmembers(module, lambda x: isinstance(x, App)))

    all_runnables: dict[Runnable, list[str]] = defaultdict(list)
    priorities: dict[Runnable, int] = defaultdict(lambda: AutoRunPriority.APP_FUNCTION)
    for app_name, app in apps:
        for name, local_entrypoint in app.registered_entrypoints.items():
            all_runnables[local_entrypoint].append(f"{app_name}.{name}")
            priorities[local_entrypoint] = AutoRunPriority.APP_LOCAL_ENTRYPOINT
        for name, function in app.registered_functions.items():
            if name.endswith(".*"):
                continue
            all_runnables[function].append(f"{app_name}.{name}")
            priorities[function] = AutoRunPriority.APP_FUNCTION
        for cls_name, cls in app.registered_classes.items():
            for method_name in cls._get_method_names():
                method_ref = MethodReference(cls, method_name)
                all_runnables[method_ref].append(f"{app_name}.{cls_name}.{method_name}")
                priorities[method_ref] = AutoRunPriority.APP_FUNCTION

    # If any class or function is exported as a module level object, use that
    # as the preferred name by putting it first in the list
    module_level_entities = cast(
        list[tuple[str, Runnable]],
        inspect.getmembers(module, lambda x: isinstance(x, (Function, Cls, LocalEntrypoint))),
    )
    for name, entity in module_level_entities:
        if isinstance(entity, Cls) and entity._is_local():
            for method_name in entity._get_method_names():
                method_ref = MethodReference(entity, method_name)
                all_runnables.setdefault(method_ref, []).insert(0, f"{name}.{method_name}")
                priorities[method_ref] = AutoRunPriority.MODULE_FUNCTION
        elif isinstance(entity, Function) and entity._is_local():
            all_runnables.setdefault(entity, []).insert(0, name)
            priorities[entity] = AutoRunPriority.MODULE_FUNCTION
        elif isinstance(entity, LocalEntrypoint):
            all_runnables.setdefault(entity, []).insert(0, name)
            priorities[entity] = AutoRunPriority.MODULE_LOCAL_ENTRYPOINT

    def _is_web_endpoint(runnable: Runnable) -> bool:
        if isinstance(runnable, Function) and runnable._is_web_endpoint():
            return True
        elif isinstance(runnable, MethodReference):
            # this is a bit yucky but can hopefully get cleaned up with Cls cleanup:
            method_partial = runnable.cls._get_partial_functions()[runnable.method_name]
            if method_partial._is_web_endpoint():
                return True

        return False

    return [
        CLICommand(names, runnable, _is_web_endpoint(runnable), priority=priorities[runnable])
        for runnable, names in all_runnables.items()
    ]


def filter_cli_commands(
    cli_commands: list[CLICommand],
    name_prefix: str,
    accept_local_entrypoints: bool = True,
    accept_web_endpoints: bool = True,
) -> list[CLICommand]:
    """Filters by name and type of runnable

    Returns generator of (matching names list, CLICommand)
    """

    def _is_accepted_type(cli_command: CLICommand) -> bool:
        if not accept_local_entrypoints and isinstance(cli_command.runnable, LocalEntrypoint):
            return False
        if not accept_web_endpoints and cli_command.is_web_endpoint:
            return False
        return True

    res = []
    for cli_command in cli_commands:
        if not _is_accepted_type(cli_command):
            continue

        if name_prefix in cli_command.names:
            # exact name match
            res.append(cli_command)
            continue

        if not name_prefix:
            # no name specified, return all reachable runnables
            res.append(cli_command)
            continue

        # partial matches e.g. app or class name - should we even allow this?
        prefix_matches = [x for x in cli_command.names if x.startswith(f"{name_prefix}.")]
        if prefix_matches:
            res.append(cli_command)
    return res


def import_app(app_ref: str):
    # TODO: remove when integration tests have been migrated to import_app_from_ref
    return import_app_from_ref(parse_import_ref(app_ref))


def import_app_from_ref(import_ref: ImportRef, base_cmd: str = "") -> App:
    # TODO: default could be to just pick up any app regardless if it's called DEFAULT_APP_NAME
    #  as long as there is a single app in the module?
    import_path = import_ref.file_or_module
    object_path = import_ref.object_path or DEFAULT_APP_NAME

    module = import_file_or_module(import_ref, base_cmd)

    if "." in object_path:
        raise click.UsageError(f"{object_path} is not a Modal App")

    app = getattr(module, object_path)

    if app is None:
        error_console = Console(stderr=True)
        error_console.print(f"[bold red]Could not find Modal app '{object_path}' in {import_path}.[/bold red]")

        if not object_path:
            guidance_msg = Markdown(
                f"Expected to find an app variable named **`{DEFAULT_APP_NAME}`** (the default app name). "
                "If your `modal.App` is assigned to a different variable name, "
                "you must specify it in the app ref argument. "
                f"For example an App variable `app_2 = modal.App()` in `{import_path}` would "
                f"be specified as `{import_path}::app_2`."
            )
            error_console.print(guidance_msg)

        sys.exit(1)

    if not isinstance(app, App):
        raise click.UsageError(f"{app} is not a Modal App")

    return app


def _show_function_ref_help(app_ref: ImportRef, base_cmd: str) -> None:
    object_path = app_ref.object_path
    import_path = app_ref.file_or_module
    error_console = Console(stderr=True)
    if object_path:
        error_console.print(
            f"[bold red]Could not find Modal function or local entrypoint"
            f" '{object_path}' in '{import_path}'.[/bold red]"
        )
    else:
        error_console.print(
            f"[bold red]No function was specified, and no [green]`app`[/green] variable "
            f"could be found in '{import_path}'.[/bold red]"
        )
    guidance_msg = f"""
Usage:
{base_cmd} <file_or_module_path>::<function_name>

Given the following example `app.py`:
```
app = modal.App()

@app.function()
def foo():
    ...
```
You would run foo as [bold green]{base_cmd} app.py::foo[/bold green]"""
    error_console.print(guidance_msg)


def _get_runnable_app(runnable: Runnable) -> App:
    if isinstance(runnable, Function):
        return runnable.app
    elif isinstance(runnable, MethodReference):
        return runnable.cls._get_app()
    else:
        assert isinstance(runnable, LocalEntrypoint)
        return runnable.app


def import_and_filter(
    import_ref: ImportRef, *, base_cmd: str, accept_local_entrypoint=True, accept_webhook=False
) -> tuple[Optional[Runnable], list[CLICommand]]:
    """Takes a function ref string and returns a single determined "runnable" to use, and a list of all available
    runnables.

    The function ref can leave out partial information (apart from the file name/module)
    as long as the runnable is uniquely identifiable by the provided information.

    When there are multiple runnables within the provided ref, the following rules should
    be followed:

    1. if there is a single local_entrypoint, that one is used
    2. if there is a single {function, class} that one is used
    3. if there is a single method (within a class) that one is used
    """
    # all commands:
    module = import_file_or_module(import_ref, base_cmd)
    cli_commands = list_cli_commands(module)

    # all commands that satisfy local entrypoint/accept webhook limitations AND object path prefix
    filtered_commands = filter_cli_commands(
        cli_commands, import_ref.object_path, accept_local_entrypoint, accept_webhook
    )

    all_usable_commands = filter_cli_commands(cli_commands, "", accept_local_entrypoint, accept_webhook)

    if filtered_commands:
        # if there is a single command with "highest run prio" - use that
        filtered_commands_by_prio = defaultdict(list)
        for cmd in filtered_commands:
            filtered_commands_by_prio[cmd.priority].append(cmd)

        _, highest_prio_commands = min(filtered_commands_by_prio.items())
        if len(highest_prio_commands) == 1:
            cli_command = highest_prio_commands[0]
            return cli_command.runnable, all_usable_commands

    # otherwise, just return the list of all commands
    return None, all_usable_commands


================================================
File: modal/cli/launch.py
================================================
# Copyright Modal Labs 2023
import asyncio
import inspect
import json
import os
from pathlib import Path
from typing import Any, Optional

from typer import Typer

from ..exception import _CliUserExecutionError
from ..output import enable_output
from ..runner import run_app
from .import_refs import ImportRef, _get_runnable_app, import_file_or_module

launch_cli = Typer(
    name="launch",
    no_args_is_help=True,
    help="""
    Open a serverless app instance on Modal.

    This command is in preview and may change in the future.
    """,
)


def _launch_program(name: str, filename: str, detach: bool, args: dict[str, Any]) -> None:
    os.environ["MODAL_LAUNCH_ARGS"] = json.dumps(args)

    program_path = str(Path(__file__).parent / "programs" / filename)
    base_cmd = f"modal launch {name}"
    module = import_file_or_module(ImportRef(program_path, use_module_mode=False), base_cmd=base_cmd)
    entrypoint = module.main

    app = _get_runnable_app(entrypoint)
    app.set_description(base_cmd)

    # `launch/` scripts must have a `local_entrypoint()` with no args, for simplicity here.
    func = entrypoint.info.raw_f
    isasync = inspect.iscoroutinefunction(func)
    with enable_output():
        with run_app(app, detach=detach):
            try:
                if isasync:
                    asyncio.run(func())
                else:
                    func()
            except Exception as exc:
                raise _CliUserExecutionError(inspect.getsourcefile(func)) from exc


@launch_cli.command(name="jupyter", help="Start Jupyter Lab on Modal.")
def jupyter(
    cpu: int = 8,
    memory: int = 32768,
    gpu: Optional[str] = None,
    timeout: int = 3600,
    image: str = "ubuntu:22.04",
    add_python: Optional[str] = "3.11",
    mount: Optional[str] = None,  # Adds a local directory to the jupyter container
    volume: Optional[str] = None,  # Attach a persisted `modal.Volume` by name (creating if missing).
    detach: bool = False,  # Run the app in "detached" mode to persist after local client disconnects
):
    args = {
        "cpu": cpu,
        "memory": memory,
        "gpu": gpu,
        "timeout": timeout,
        "image": image,
        "add_python": add_python,
        "mount": mount,
        "volume": volume,
    }
    _launch_program("jupyter", "run_jupyter.py", detach, args)


@launch_cli.command(name="vscode", help="Start Visual Studio Code on Modal.")
def vscode(
    cpu: int = 8,
    memory: int = 32768,
    gpu: Optional[str] = None,
    image: str = "debian:12",
    timeout: int = 3600,
    mount: Optional[str] = None,  # Create a `modal.Mount` from a local directory.
    volume: Optional[str] = None,  # Attach a persisted `modal.Volume` by name (creating if missing).
    detach: bool = False,  # Run the app in "detached" mode to persist after local client disconnects
):
    args = {
        "cpu": cpu,
        "memory": memory,
        "gpu": gpu,
        "image": image,
        "timeout": timeout,
        "mount": mount,
        "volume": volume,
    }
    _launch_program("vscode", "vscode.py", detach, args)


================================================
File: modal/cli/network_file_system.py
================================================
# Copyright Modal Labs 2022
import os
import sys
from pathlib import Path
from typing import Optional

import typer
from click import UsageError
from grpclib import GRPCError, Status
from rich.console import Console
from rich.syntax import Syntax
from rich.table import Table
from typer import Argument, Typer

import modal
from modal._location import display_location
from modal._output import OutputManager, ProgressHandler
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal.cli._download import _volume_download
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table, timestamp_to_local
from modal.client import _Client
from modal.environments import ensure_env
from modal.network_file_system import _NetworkFileSystem
from modal_proto import api_pb2

nfs_cli = Typer(name="nfs", help="Read and edit `modal.NetworkFileSystem` file systems.", no_args_is_help=True)


@nfs_cli.command(name="list", help="List the names of all network file systems.", rich_help_panel="Management")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
    env = ensure_env(env)

    client = await _Client.from_env()
    response = await retry_transient_errors(
        client.stub.SharedVolumeList, api_pb2.SharedVolumeListRequest(environment_name=env)
    )
    env_part = f" in environment '{env}'" if env else ""
    column_names = ["Name", "Location", "Created at"]
    rows = []
    for item in response.items:
        rows.append(
            [
                item.label,
                display_location(item.cloud_provider),
                timestamp_to_local(item.created_at, json),
            ]
        )
    display_table(column_names, rows, json, title=f"Shared Volumes{env_part}")


def gen_usage_code(label):
    return f"""
@app.function(network_file_systems={{"/my_vol": modal.NetworkFileSystem.from_name("{label}")}})
def some_func():
    os.listdir("/my_vol")
"""


@nfs_cli.command(name="create", help="Create a named network file system.", rich_help_panel="Management")
def create(
    name: str,
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    modal.NetworkFileSystem.create_deployed(name, environment_name=env)
    console = Console()
    console.print(f"Created volume '{name}'. \n\nCode example:\n")
    usage = Syntax(gen_usage_code(name), "python")
    console.print(usage)


@nfs_cli.command(
    name="ls",
    help="List files and directories in a network file system.",
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def ls(
    volume_name: str,
    path: str = typer.Argument(default="/"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _NetworkFileSystem.from_name(volume_name)
    try:
        entries = await volume.listdir(path)
    except GRPCError as exc:
        if exc.status in (Status.INVALID_ARGUMENT, Status.NOT_FOUND):
            raise UsageError(exc.message)
        raise

    if sys.stdout.isatty():
        console = Console()
        console.print(f"Directory listing of '{path}' in '{volume_name}'")
        table = Table()

        table.add_column("filename")
        table.add_column("type")

        for entry in entries:
            filetype = "dir" if entry.type == api_pb2.FileEntry.FileType.DIRECTORY else "file"
            table.add_row(entry.path, filetype)
        console.print(table)
    else:
        for entry in entries:
            print(entry.path)


@nfs_cli.command(
    name="put",
    help="""Upload a file or directory to a network file system.

Remote parent directories will be created as needed.

Ending the REMOTE_PATH with a forward slash (/), it's assumed to be a directory and the file
will be uploaded with its current name under that directory.
""",
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def put(
    volume_name: str,
    local_path: str,
    remote_path: str = typer.Argument(default="/"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _NetworkFileSystem.from_name(volume_name)
    if remote_path.endswith("/"):
        remote_path = remote_path + os.path.basename(local_path)
    console = Console()

    if Path(local_path).is_dir():
        progress_handler = ProgressHandler(type="upload", console=console)
        with progress_handler.live:
            await volume.add_local_dir(local_path, remote_path, progress_cb=progress_handler.progress)
            progress_handler.progress(complete=True)
        console.print(OutputManager.step_completed(f"Uploaded directory '{local_path}' to '{remote_path}'"))

    elif "*" in local_path:
        raise UsageError("Glob uploads are currently not supported")
    else:
        progress_handler = ProgressHandler(type="upload", console=console)
        with progress_handler.live:
            written_bytes = await volume.add_local_file(local_path, remote_path, progress_cb=progress_handler.progress)
            progress_handler.progress(complete=True)
        console.print(
            OutputManager.step_completed(
                f"Uploaded file '{local_path}' to '{remote_path}' ({written_bytes} bytes written)"
            )
        )


class CliError(Exception):
    def __init__(self, message):
        self.message = message


@nfs_cli.command(name="get", rich_help_panel="File operations")
@synchronizer.create_blocking
async def get(
    volume_name: str,
    remote_path: str,
    local_destination: str = typer.Argument("."),
    force: bool = False,
    env: Optional[str] = ENV_OPTION,
):
    """Download a file from a network file system.

    Specifying a glob pattern (using any `*` or `**` patterns) as the `remote_path` will download
    all matching files, preserving their directory structure.

    For example, to download an entire network file system into `dump_volume`:

    ```
    modal nfs get <volume-name> "**" dump_volume
    ```

    Use "-" as LOCAL_DESTINATION to write file contents to standard output.
    """
    ensure_env(env)
    destination = Path(local_destination)
    volume = _NetworkFileSystem.from_name(volume_name)
    console = Console()
    progress_handler = ProgressHandler(type="download", console=console)
    with progress_handler.live:
        await _volume_download(volume, remote_path, destination, force, progress_cb=progress_handler.progress)
    console.print(OutputManager.step_completed("Finished downloading files to local!"))


@nfs_cli.command(
    name="rm", help="Delete a file or directory from a network file system.", rich_help_panel="File operations"
)
@synchronizer.create_blocking
async def rm(
    volume_name: str,
    remote_path: str,
    recursive: bool = typer.Option(False, "-r", "--recursive", help="Delete directory recursively"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _NetworkFileSystem.from_name(volume_name)
    try:
        await volume.remove_file(remote_path, recursive=recursive)
    except GRPCError as exc:
        if exc.status in (Status.NOT_FOUND, Status.INVALID_ARGUMENT):
            raise UsageError(exc.message)
        raise


@nfs_cli.command(
    name="delete",
    help="Delete a named, persistent modal.NetworkFileSystem.",
    rich_help_panel="Management",
)
@synchronizer.create_blocking
async def delete(
    nfs_name: str = Argument(help="Name of the modal.NetworkFileSystem to be deleted. Case sensitive"),
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    # Lookup first to validate the name, even though delete is a staticmethod
    await _NetworkFileSystem.from_name(nfs_name, environment_name=env).hydrate()
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.NetworkFileSystem '{nfs_name}'?",
            default=False,
            abort=True,
        )

    await _NetworkFileSystem.delete(nfs_name, environment_name=env)


================================================
File: modal/cli/profile.py
================================================
# Copyright Modal Labs 2022

import asyncio
import os
from typing import Optional

import typer
from rich.console import Console
from rich.json import JSON
from rich.table import Table

from modal._utils.async_utils import synchronizer
from modal.config import Config, _lookup_workspace, _profile, config_profiles, config_set_active_profile
from modal.exception import AuthError

profile_cli = typer.Typer(name="profile", help="Switch between Modal profiles.", no_args_is_help=True)


@profile_cli.command(help="Change the active Modal profile.")
def activate(profile: str = typer.Argument(..., help="Modal profile to activate.")):
    config_set_active_profile(profile)


@profile_cli.command(help="Print the currently active Modal profile.")
def current():
    typer.echo(_profile)


@profile_cli.command(name="list", help="Show all Modal profiles and highlight the active one.")
@synchronizer.create_blocking
async def list_(json: Optional[bool] = False):
    config = Config()
    profiles = config_profiles()
    lookup_coros = [
        _lookup_workspace(
            config.get("server_url", profile),
            config.get("token_id", profile, use_env=False),
            config.get("token_secret", profile, use_env=False),
        )
        for profile in profiles
    ]
    responses = await asyncio.gather(*lookup_coros, return_exceptions=True)

    rows = []
    for profile, resp in zip(profiles, responses):
        active = profile == _profile
        if isinstance(resp, AuthError):
            workspace = "Unknown (authentication failure)"
        elif isinstance(resp, TimeoutError):
            workspace = "Unknown (timed out)"
        elif isinstance(resp, Exception):
            # Catch-all for other exceptions, like incorrect server url
            workspace = "Unknown (profile misconfigured)"
        else:
            assert hasattr(resp, "username")
            workspace = resp.username
        content = ["•" if active else "", profile, workspace]
        rows.append((active, content))

    env_based_workspace: Optional[str] = None
    if "MODAL_TOKEN_ID" in os.environ:
        try:
            env_based_resp = await _lookup_workspace(
                config.get("server_url", _profile),
                os.environ.get("MODAL_TOKEN_ID"),
                os.environ.get("MODAL_TOKEN_SECRET"),
            )
            env_based_workspace = env_based_resp.username
        except AuthError:
            env_based_workspace = "Unknown (authentication failure)"

    console = Console()
    highlight = "bold green" if env_based_workspace is None else "yellow"
    if json:
        json_data = []
        for active, content in rows:
            json_data.append({"name": content[1], "workspace": content[2], "active": active})
        console.print(JSON.from_data(json_data))
    else:
        table = Table(" ", "Profile", "Workspace")
        for active, content in rows:
            table.add_row(*content, style=highlight if active else "dim")
        console.print(table)

    if env_based_workspace is not None:
        console.print(
            f"Using [bold]{env_based_workspace}[/bold] workspace based on environment variables", style="yellow"
        )


================================================
File: modal/cli/queues.py
================================================
# Copyright Modal Labs 2024
from typing import Optional

import typer
from rich.console import Console
from typer import Argument, Option, Typer

from modal._resolver import Resolver
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table, timestamp_to_local
from modal.client import _Client
from modal.environments import ensure_env
from modal.queue import _Queue
from modal_proto import api_pb2

queue_cli = Typer(
    name="queue",
    no_args_is_help=True,
    help="Manage `modal.Queue` objects and inspect their contents.",
)

PARTITION_OPTION = Option(
    None,
    "-p",
    "--partition",
    help="Name of the partition to use, otherwise use the default (anonymous) partition.",
)


@queue_cli.command(name="create", rich_help_panel="Management")
@synchronizer.create_blocking
async def create(name: str, *, env: Optional[str] = ENV_OPTION):
    """Create a named Queue.

    Note: This is a no-op when the Queue already exists.
    """
    q = _Queue.from_name(name, environment_name=env, create_if_missing=True)
    client = await _Client.from_env()
    resolver = Resolver(client=client)
    await resolver.load(q)


@queue_cli.command(name="delete", rich_help_panel="Management")
@synchronizer.create_blocking
async def delete(name: str, *, yes: bool = YES_OPTION, env: Optional[str] = ENV_OPTION):
    """Delete a named Queue and all of its data."""
    # Lookup first to validate the name, even though delete is a staticmethod
    await _Queue.from_name(name, environment_name=env).hydrate()
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.Queue '{name}'?",
            default=False,
            abort=True,
        )
    await _Queue.delete(name, environment_name=env)


@queue_cli.command(name="list", rich_help_panel="Management")
@synchronizer.create_blocking
async def list_(*, json: bool = False, env: Optional[str] = ENV_OPTION):
    """List all named Queues."""
    env = ensure_env(env)

    max_total_size = 100_000
    client = await _Client.from_env()
    request = api_pb2.QueueListRequest(environment_name=env, total_size_limit=max_total_size + 1)
    response = await retry_transient_errors(client.stub.QueueList, request)

    rows = [
        (
            q.name,
            timestamp_to_local(q.created_at, json),
            str(q.num_partitions),
            str(q.total_size) if q.total_size <= max_total_size else f">{max_total_size}",
        )
        for q in response.queues
    ]
    display_table(["Name", "Created at", "Partitions", "Total size"], rows, json)


@queue_cli.command(name="clear", rich_help_panel="Management")
@synchronizer.create_blocking
async def clear(
    name: str,
    partition: Optional[str] = PARTITION_OPTION,
    all: bool = Option(False, "-a", "--all", help="Clear the contents of all partitions."),
    yes: bool = YES_OPTION,
    *,
    env: Optional[str] = ENV_OPTION,
):
    """Clear the contents of a queue by removing all of its data."""
    q = _Queue.from_name(name, environment_name=env)
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the contents of modal.Queue '{name}'?",
            default=False,
            abort=True,
        )
    await q.clear(partition=partition, all=all)


@queue_cli.command(name="peek", rich_help_panel="Inspection")
@synchronizer.create_blocking
async def peek(
    name: str, n: int = Argument(1), partition: Optional[str] = PARTITION_OPTION, *, env: Optional[str] = ENV_OPTION
):
    """Print the next N items in the queue or queue partition (without removal)."""
    q = _Queue.from_name(name, environment_name=env)
    console = Console()
    i = 0
    async for item in q.iterate(partition=partition):
        console.print(item)
        i += 1
        if i >= n:
            break


@queue_cli.command(name="len", rich_help_panel="Inspection")
@synchronizer.create_blocking
async def len(
    name: str,
    partition: Optional[str] = PARTITION_OPTION,
    total: bool = Option(False, "-t", "--total", help="Compute the sum of the queue lengths across all partitions"),
    *,
    env: Optional[str] = ENV_OPTION,
):
    """Print the length of a queue partition or the total length of all partitions."""
    q = _Queue.from_name(name, environment_name=env)
    console = Console()
    console.print(await q.len(partition=partition, total=total))


================================================
File: modal/cli/run.py
================================================
# Copyright Modal Labs 2022
import asyncio
import functools
import inspect
import platform
import re
import shlex
import sys
import time
from dataclasses import dataclass
from functools import partial
from typing import Any, Callable, Optional, get_type_hints

import click
import typer
from click import ClickException
from typing_extensions import TypedDict

from .._functions import _FunctionSpec
from ..app import App, LocalEntrypoint
from ..config import config
from ..environments import ensure_env
from ..exception import ExecutionError, InvalidError, _CliUserExecutionError
from ..functions import Function
from ..image import Image
from ..output import enable_output
from ..runner import deploy_app, interactive_shell, run_app
from ..serving import serve_app
from ..volume import Volume
from .import_refs import (
    CLICommand,
    MethodReference,
    _get_runnable_app,
    import_and_filter,
    import_app_from_ref,
    parse_import_ref,
)
from .utils import ENV_OPTION, ENV_OPTION_HELP, is_tty, stream_app_logs


class ParameterMetadata(TypedDict):
    name: str
    default: Any
    annotation: Any
    type_hint: Any
    kind: Any


class AnyParamType(click.ParamType):
    name = "any"

    def convert(self, value, param, ctx):
        return value


option_parsers = {
    "str": str,
    "int": int,
    "float": float,
    "bool": bool,
    "datetime.datetime": click.DateTime(),
    "Any": AnyParamType(),
}


class NoParserAvailable(InvalidError):
    pass


@dataclass
class FnSignature:
    parameters: dict[str, ParameterMetadata]
    has_variadic_args: bool


def _get_signature(f: Callable[..., Any], is_method: bool = False) -> FnSignature:
    try:
        type_hints = get_type_hints(f)
    except Exception as exc:
        # E.g., if entrypoint type hints cannot be evaluated by local Python runtime
        msg = "Unable to generate command line interface for app entrypoint. See traceback above for details."
        raise ExecutionError(msg) from exc

    has_variadic_args = False

    if is_method:
        self = None  # Dummy, doesn't matter
        f = functools.partial(f, self)

    signature: dict[str, ParameterMetadata] = {}
    for param in inspect.signature(f).parameters.values():
        if param.kind == inspect.Parameter.VAR_POSITIONAL:
            has_variadic_args = True
        else:
            signature[param.name] = {
                "name": param.name,
                "default": param.default,
                "annotation": param.annotation,
                "type_hint": type_hints.get(param.name, "Any"),
                "kind": param.kind,
            }

    if has_variadic_args and len(signature) > 0:
        raise InvalidError("Functions with variable-length positional arguments (*args) cannot have other parameters.")

    return FnSignature(signature, has_variadic_args)


def _get_param_type_as_str(annot: Any) -> str:
    """Return annotation as a string, handling various spellings for optional types."""
    annot_str = str(annot)
    annot_patterns = [
        r"typing\.Optional\[([\w.]+)\]",
        r"typing\.Union\[([\w.]+), NoneType\]",
        r"([\w.]+) \| None",
        r"<class '([\w\.]+)'>",
    ]
    for pat in annot_patterns:
        m = re.match(pat, annot_str)
        if m is not None:
            return m.group(1)
    return annot_str


def _add_click_options(func, parameters: dict[str, ParameterMetadata]):
    """Adds @click.option based on function signature

    Kind of like typer, but using options instead of positional arguments
    """
    for param in parameters.values():
        param_type_str = _get_param_type_as_str(param["type_hint"])
        param_name = param["name"].replace("_", "-")
        cli_name = "--" + param_name
        if param_type_str == "bool":
            cli_name += "/--no-" + param_name

        parser = option_parsers.get(param_type_str)
        if parser is None:
            msg = f"Parameter `{param_name}` has unparseable annotation: {param['annotation']!r}"
            raise NoParserAvailable(msg)
        kwargs: Any = {
            "type": parser,
        }
        if param["default"] is not inspect.Signature.empty:
            kwargs["default"] = param["default"]
        else:
            kwargs["required"] = True

        click.option(cli_name, **kwargs)(func)
    return func


def _get_clean_app_description(func_ref: str) -> str:
    # If possible, consider the 'ref' argument the start of the app's args. Everything
    # before it Modal CLI cruft (eg. `modal run --detach`).
    try:
        func_ref_arg_idx = sys.argv.index(func_ref)
        return " ".join(sys.argv[func_ref_arg_idx:])
    except ValueError:
        return " ".join(sys.argv)


def _write_local_result(result_path: str, res: Any):
    if isinstance(res, str):
        mode = "wt"
    elif isinstance(res, bytes):
        mode = "wb"
    else:
        res_type = type(res).__name__
        raise InvalidError(f"Function must return str or bytes when using `--write-result`; got {res_type}.")
    with open(result_path, mode) as fid:
        fid.write(res)


def _make_click_function(app, signature: FnSignature, inner: Callable[[tuple[str, ...], dict[str, Any]], Any]):
    @click.pass_context
    def f(ctx, **kwargs):
        if signature.has_variadic_args:
            assert len(kwargs) == 0
            args = ctx.args
        else:
            args = ()

        show_progress: bool = ctx.obj["show_progress"]
        with enable_output(show_progress):
            with run_app(
                app,
                detach=ctx.obj["detach"],
                environment_name=ctx.obj["env"],
                interactive=ctx.obj["interactive"],
            ):
                res = inner(args, kwargs)

            if result_path := ctx.obj["result_path"]:
                _write_local_result(result_path, res)

    return f


def _get_click_command_for_function(app: App, function: Function):
    if function.is_generator:
        raise InvalidError("`modal run` is not supported for generator functions")

    signature = _get_signature(function.info.raw_f)

    def _inner(args, click_kwargs):
        return function.remote(*args, **click_kwargs)

    f = _make_click_function(app, signature, _inner)

    with_click_options = _add_click_options(f, signature.parameters)

    if signature.has_variadic_args:
        return click.command(context_settings={"ignore_unknown_options": True, "allow_extra_args": True})(
            with_click_options
        )
    else:
        return click.command(with_click_options)


def _get_click_command_for_cls(app: App, method_ref: MethodReference):
    parameters: dict[str, ParameterMetadata]
    cls = method_ref.cls
    method_name = method_ref.method_name

    cls_signature = _get_signature(cls._get_user_cls())

    if cls_signature.has_variadic_args:
        raise InvalidError("Modal classes cannot have variable-length positional arguments (*args).")

    partial_functions = cls._get_partial_functions()

    if method_name in ("*", ""):
        # auto infer method name - not sure if we have to support this...
        method_names = list(partial_functions.keys())
        if len(method_names) == 1:
            method_name = method_names[0]
        else:
            raise click.UsageError(
                f"Please specify a specific method of {cls._get_name()} to run, e.g. `modal run foo.py::MyClass.bar`"  # noqa: E501
            )

    partial_function = partial_functions[method_name]
    fun_signature = _get_signature(partial_function._get_raw_f(), is_method=True)

    # TODO(erikbern): assert there's no overlap?
    parameters = dict(**cls_signature.parameters, **fun_signature.parameters)  # Pool all arguments

    def _inner(args, click_kwargs):
        # unpool class and method arguments
        # TODO(erikbern): this code is a bit hacky
        cls_kwargs = {k: click_kwargs[k] for k in cls_signature.parameters}
        fun_kwargs = {k: click_kwargs[k] for k in fun_signature.parameters}

        instance = cls(**cls_kwargs)
        method: Function = getattr(instance, method_name)
        return method.remote(*args, **fun_kwargs)

    f = _make_click_function(app, fun_signature, _inner)
    with_click_options = _add_click_options(f, parameters)

    if fun_signature.has_variadic_args:
        return click.command(context_settings={"ignore_unknown_options": True, "allow_extra_args": True})(
            with_click_options
        )
    else:
        return click.command(with_click_options)


def _get_click_command_for_local_entrypoint(app: App, entrypoint: LocalEntrypoint):
    func = entrypoint.info.raw_f
    isasync = inspect.iscoroutinefunction(func)

    signature = _get_signature(func)

    @click.pass_context
    def f(ctx, *args, **kwargs):
        if ctx.obj["detach"]:
            print(
                "Note that running a local entrypoint in detached mode only keeps the last "
                "triggered Modal function alive after the parent process has been killed or disconnected."
            )

        if signature.has_variadic_args:
            assert len(args) == 0 and len(kwargs) == 0
            args = ctx.args

        show_progress: bool = ctx.obj["show_progress"]
        with enable_output(show_progress):
            with run_app(
                app,
                detach=ctx.obj["detach"],
                environment_name=ctx.obj["env"],
                interactive=ctx.obj["interactive"],
            ):
                try:
                    if isasync:
                        res = asyncio.run(func(*args, **kwargs))
                    else:
                        res = func(*args, **kwargs)
                except Exception as exc:
                    raise _CliUserExecutionError(inspect.getsourcefile(func)) from exc

            if result_path := ctx.obj["result_path"]:
                _write_local_result(result_path, res)

    with_click_options = _add_click_options(f, signature.parameters)

    if signature.has_variadic_args:
        return click.command(context_settings={"ignore_unknown_options": True, "allow_extra_args": True})(
            with_click_options
        )
    else:
        return click.command(with_click_options)


def _get_runnable_list(all_usable_commands: list[CLICommand]) -> str:
    usable_command_lines = []
    for cmd in all_usable_commands:
        cmd_names = " / ".join(cmd.names)
        usable_command_lines.append(cmd_names)

    return "\n".join(usable_command_lines)


class RunGroup(click.Group):
    def get_command(self, ctx, func_ref):
        # note: get_command here is run before the "group logic" in the `run` logic below
        # so to ensure that `env` has been globally populated before user code is loaded, it
        # needs to be handled here, and not in the `run` logic below
        ctx.ensure_object(dict)
        ctx.obj["env"] = ensure_env(ctx.params["env"])

        import_ref = parse_import_ref(func_ref, use_module_mode=ctx.params["m"])
        runnable, all_usable_commands = import_and_filter(
            import_ref, base_cmd="modal run", accept_local_entrypoint=True, accept_webhook=False
        )
        if not runnable:
            help_header = (
                "Specify a Modal Function or local entrypoint to run. E.g.\n"
                f"> modal run {import_ref.file_or_module}::my_function [..args]"
            )

            if all_usable_commands:
                help_footer = f"'{import_ref.file_or_module}' has the following functions and local entrypoints:\n"
                help_footer += _get_runnable_list(all_usable_commands)
            else:
                help_footer = f"'{import_ref.file_or_module}' has no functions or local entrypoints."

            raise ClickException(f"{help_header}\n\n{help_footer}")

        app = _get_runnable_app(runnable)

        if app.description is None:
            app.set_description(_get_clean_app_description(func_ref))

        if isinstance(runnable, LocalEntrypoint):
            click_command = _get_click_command_for_local_entrypoint(app, runnable)
        elif isinstance(runnable, Function):
            click_command = _get_click_command_for_function(app, runnable)
        elif isinstance(runnable, MethodReference):
            click_command = _get_click_command_for_cls(app, runnable)
        else:
            # This should be unreachable...
            raise ValueError(f"{runnable} is neither function, local entrypoint or class/method")
        return click_command


@click.group(
    cls=RunGroup,
    subcommand_metavar="FUNC_REF",
)
@click.option("-w", "--write-result", help="Write return value (which must be str or bytes) to this local path.")
@click.option("-q", "--quiet", is_flag=True, help="Don't show Modal progress indicators.")
@click.option("-d", "--detach", is_flag=True, help="Don't stop the app if the local process dies or disconnects.")
@click.option("-i", "--interactive", is_flag=True, help="Run the app in interactive mode.")
@click.option("-e", "--env", help=ENV_OPTION_HELP, default=None)
@click.option("-m", is_flag=True, help="Interpret argument as a Python module path instead of a file/script path")
@click.pass_context
def run(ctx, write_result, detach, quiet, interactive, env, m):
    """Run a Modal function or local entrypoint.

    `FUNC_REF` should be of the format `{file or module}::{function name}`.
    Alternatively, you can refer to the function via the app:

    `{file or module}::{app variable name}.{function name}`

    **Examples:**

    To run the hello_world function (or local entrypoint) in my_app.py:

    ```
    modal run my_app.py::hello_world
    ```

    If your module only has a single app and your app has a
    single local entrypoint (or single function), you can omit the app and
    function parts:

    ```
    modal run my_app.py
    ```

    Instead of pointing to a file, you can also use the Python module path, which
    by default will ensure that your remote functions will use the same module
    names as they do locally.

    ```
    modal run -m my_project.my_app
    ```
    """
    ctx.ensure_object(dict)
    ctx.obj["result_path"] = write_result
    ctx.obj["detach"] = detach  # if subcommand would be a click command...
    ctx.obj["show_progress"] = False if quiet else True
    ctx.obj["interactive"] = interactive


def deploy(
    app_ref: str = typer.Argument(..., help="Path to a Python file with an app to deploy"),
    name: str = typer.Option("", help="Name of the deployment."),
    env: str = ENV_OPTION,
    stream_logs: bool = typer.Option(False, help="Stream logs from the app upon deployment."),
    tag: str = typer.Option("", help="Tag the deployment with a version."),
    use_module_mode: bool = typer.Option(
        False, "-m", help="Interpret argument as a Python module path instead of a file/script path"
    ),
):
    """Deploy a Modal application.

    **Usage:**
    modal deploy my_script.py
    modal deploy -m my_package.my_mod
    """
    # this ensures that lookups without environment specification use the same env as specified
    env = ensure_env(env)

    import_ref = parse_import_ref(app_ref, use_module_mode=use_module_mode)
    app = import_app_from_ref(import_ref, base_cmd="modal deploy")

    if name is None:
        name = app.name

    with enable_output():
        res = deploy_app(app, name=name, environment_name=env or "", tag=tag)

    if stream_logs:
        stream_app_logs(app_id=res.app_id, app_logs_url=res.app_logs_url)


def serve(
    app_ref: str = typer.Argument(..., help="Path to a Python file with an app."),
    timeout: Optional[float] = None,
    env: str = ENV_OPTION,
    use_module_mode: bool = typer.Option(
        False, "-m", help="Interpret argument as a Python module path instead of a file/script path"
    ),
):
    """Run a web endpoint(s) associated with a Modal app and hot-reload code.

    **Examples:**

    ```
    modal serve hello_world.py
    ```
    """
    env = ensure_env(env)
    import_ref = parse_import_ref(app_ref, use_module_mode=use_module_mode)
    app = import_app_from_ref(import_ref, base_cmd="modal serve")
    if app.description is None:
        app.set_description(_get_clean_app_description(app_ref))

    with enable_output():
        with serve_app(app, import_ref, environment_name=env):
            if timeout is None:
                timeout = config["serve_timeout"]
            if timeout is None:
                timeout = float("inf")
            while timeout > 0:
                t = min(timeout, 3600)
                time.sleep(t)
                timeout -= t


def shell(
    container_or_function: Optional[str] = typer.Argument(
        default=None,
        help=(
            "ID of running container, or path to a Python file containing a Modal App."
            " Can also include a function specifier, like `module.py::func`, if the file defines multiple functions."
        ),
        metavar="REF",
    ),
    cmd: str = typer.Option("/bin/bash", "-c", "--cmd", help="Command to run inside the Modal image."),
    env: str = ENV_OPTION,
    image: Optional[str] = typer.Option(
        default=None, help="Container image tag for inside the shell (if not using REF)."
    ),
    add_python: Optional[str] = typer.Option(default=None, help="Add Python to the image (if not using REF)."),
    volume: Optional[list[str]] = typer.Option(
        default=None,
        help=(
            "Name of a `modal.Volume` to mount inside the shell at `/mnt/{name}` (if not using REF)."
            " Can be used multiple times."
        ),
    ),
    cpu: Optional[int] = typer.Option(default=None, help="Number of CPUs to allocate to the shell (if not using REF)."),
    memory: Optional[int] = typer.Option(
        default=None, help="Memory to allocate for the shell, in MiB (if not using REF)."
    ),
    gpu: Optional[str] = typer.Option(
        default=None,
        help="GPUs to request for the shell, if any. Examples are `any`, `a10g`, `a100:4` (if not using REF).",
    ),
    cloud: Optional[str] = typer.Option(
        default=None,
        help=(
            "Cloud provider to run the shell on. Possible values are `aws`, `gcp`, `oci`, `auto` (if not using REF)."
        ),
    ),
    region: Optional[str] = typer.Option(
        default=None,
        help=(
            "Region(s) to run the container on. "
            "Can be a single region or a comma-separated list to choose from (if not using REF)."
        ),
    ),
    pty: Optional[bool] = typer.Option(default=None, help="Run the command using a PTY."),
    use_module_mode: bool = typer.Option(
        False, "-m", help="Interpret argument as a Python module path instead of a file/script path"
    ),
):
    """Run a command or interactive shell inside a Modal container.

    **Examples:**

    Start an interactive shell inside the default Debian-based image:

    ```
    modal shell
    ```

    Start an interactive shell with the spec for `my_function` in your App
    (uses the same image, volumes, mounts, etc.):

    ```
    modal shell hello_world.py::my_function
    ```

    Or, if you're using a [modal.Cls](/docs/reference/modal.Cls), you can refer to a `@modal.method` directly:

    ```
    modal shell hello_world.py::MyClass.my_method
    ```

    Start a `python` shell:

    ```
    modal shell hello_world.py --cmd=python
    ```

    Run a command with your function's spec and pipe the output to a file:

    ```
    modal shell hello_world.py -c 'uv pip list' > env.txt
    ```
    """
    env = ensure_env(env)

    if pty is None:
        pty = is_tty()

    if platform.system() == "Windows":
        raise InvalidError("`modal shell` is currently not supported on Windows")

    app = App("modal shell")

    if container_or_function is not None:
        # `modal shell` with a container ID is a special case, alias for `modal container exec`.
        if (
            container_or_function.startswith("ta-")
            and len(container_or_function[3:]) > 0
            and container_or_function[3:].isalnum()
        ):
            from .container import exec

            exec(container_id=container_or_function, command=shlex.split(cmd), pty=pty)
            return

        import_ref = parse_import_ref(container_or_function, use_module_mode=use_module_mode)
        runnable, all_usable_commands = import_and_filter(
            import_ref, base_cmd="modal shell", accept_local_entrypoint=False, accept_webhook=True
        )
        if not runnable:
            help_header = (
                "Specify a Modal function to start a shell session for. E.g.\n"
                f"> modal shell {import_ref.file_or_module}::my_function"
            )

            if all_usable_commands:
                help_footer = f"The selected module '{import_ref.file_or_module}' has the following choices:\n\n"
                help_footer += _get_runnable_list(all_usable_commands)
            else:
                help_footer = f"The selected module '{import_ref.file_or_module}' has no Modal functions or classes."

            raise ClickException(f"{help_header}\n\n{help_footer}")

        function_spec: _FunctionSpec
        if isinstance(runnable, MethodReference):
            # TODO: let users specify a class instead of a method, since they use the same environment
            class_service_function = runnable.cls._get_class_service_function()
            function_spec = class_service_function.spec
        elif isinstance(runnable, Function):
            function_spec = runnable.spec
        else:
            raise ValueError("Referenced entity is not a Modal function or class")

        start_shell = partial(
            interactive_shell,
            image=function_spec.image,
            mounts=function_spec.mounts,
            secrets=function_spec.secrets,
            network_file_systems=function_spec.network_file_systems,
            gpu=function_spec.gpus,
            cloud=function_spec.cloud,
            cpu=function_spec.cpu,
            memory=function_spec.memory,
            volumes=function_spec.volumes,
            region=function_spec.scheduler_placement.proto.regions if function_spec.scheduler_placement else None,
            pty=pty,
            proxy=function_spec.proxy,
        )
    else:
        modal_image = Image.from_registry(image, add_python=add_python) if image else None
        volumes = {} if volume is None else {f"/mnt/{vol}": Volume.from_name(vol) for vol in volume}
        start_shell = partial(
            interactive_shell,
            image=modal_image,
            cpu=cpu,
            memory=memory,
            gpu=gpu,
            cloud=cloud,
            volumes=volumes,
            region=region.split(",") if region else [],
            pty=pty,
        )

    # NB: invoking under bash makes --cmd a lot more flexible.
    cmds = shlex.split(f'/bin/bash -c "{cmd}"')
    start_shell(app, cmds=cmds, environment_name=env, timeout=3600)


================================================
File: modal/cli/secret.py
================================================
# Copyright Modal Labs 2022
import os
import platform
import subprocess
from tempfile import NamedTemporaryFile
from typing import Optional

import click
import typer
from rich.console import Console
from rich.syntax import Syntax

from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal.cli.utils import ENV_OPTION, display_table, timestamp_to_local
from modal.client import _Client
from modal.environments import ensure_env
from modal.secret import _Secret
from modal_proto import api_pb2

secret_cli = typer.Typer(name="secret", help="Manage secrets.", no_args_is_help=True)


@secret_cli.command("list", help="List your published secrets.")
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
    env = ensure_env(env)
    client = await _Client.from_env()
    response = await retry_transient_errors(client.stub.SecretList, api_pb2.SecretListRequest(environment_name=env))
    column_names = ["Name", "Created at", "Last used at"]
    rows = []

    for item in response.items:
        rows.append(
            [
                item.label,
                timestamp_to_local(item.created_at, json),
                timestamp_to_local(item.last_used_at, json) if item.last_used_at else "-",
            ]
        )

    env_part = f" in environment '{env}'" if env else ""
    display_table(column_names, rows, json, title=f"Secrets{env_part}")


@secret_cli.command("create", help="Create a new secret. Use `--force` to overwrite an existing one.")
@synchronizer.create_blocking
async def create(
    secret_name,
    keyvalues: list[str] = typer.Argument(..., help="Space-separated KEY=VALUE items"),
    env: Optional[str] = ENV_OPTION,
    force: bool = typer.Option(False, "--force", help="Overwrite the secret if it already exists."),
):
    env = ensure_env(env)
    env_dict = {}
    for arg in keyvalues:
        if "=" in arg:
            key, value = arg.split("=", 1)
            if value == "-":
                value = get_text_from_editor(key)
            env_dict[key] = value
        else:
            raise click.UsageError(
                """Each item should be of the form <KEY>=VALUE. To enter secrets using your $EDITOR, use `<KEY>=-`.

E.g.

modal secret create my-credentials username=john password=-
"""
            )

    if not env_dict:
        raise click.UsageError("You need to specify at least one key for your secret")

    # Create secret
    await _Secret.create_deployed(secret_name, env_dict, overwrite=force)

    # Print code sample
    console = Console()
    env_var_code = "\n    ".join(f'os.getenv("{name}")' for name in env_dict.keys()) if env_dict else "..."
    example_code = f"""
@app.function(secrets=[modal.Secret.from_name("{secret_name}")])
def some_function():
    {env_var_code}
"""
    plural_s = "s" if len(env_dict) > 1 else ""
    console.print(
        f"""Created a new secret '{secret_name}' with the key{plural_s} {", ".join(repr(k) for k in env_dict.keys())}"""
    )
    console.print("\nUse it in to your Modal app using:\n")
    console.print(Syntax(example_code, "python"))


def get_text_from_editor(key) -> str:
    with NamedTemporaryFile("w+", prefix="secret_buffer", suffix=".txt") as bufferfile:
        if platform.system() != "Windows":
            editor = os.getenv("EDITOR", "vi")
            input(f"Pressing enter will open an external editor ({editor}) for editing '{key}'...")
            status_code = subprocess.call([editor, bufferfile.name])
        else:
            # not tested, but according to https://stackoverflow.com/questions/1442841/lauch-default-editor-like-webbrowser-module
            # this should open an editor on Windows...
            input("Pressing enter will open an external editor to allow you to edit the secret value...")
            status_code = os.system(bufferfile.name)

        if status_code != 0:
            raise ValueError(
                "Something went wrong with the external editor. "
                "Try again, or use '--' as the value to pass input through stdin instead"
            )

        bufferfile.seek(0)
        return bufferfile.read()


================================================
File: modal/cli/token.py
================================================
# Copyright Modal Labs 2022
import getpass
from typing import Optional

import typer

from modal._utils.async_utils import synchronizer
from modal.token_flow import _new_token, _set_token

token_cli = typer.Typer(name="token", help="Manage tokens.", no_args_is_help=True)

profile_option = typer.Option(
    None,
    help=(
        "Modal profile to set credentials for. If unspecified "
        "(and MODAL_PROFILE environment variable is not set), "
        "uses the workspace name associated with the credentials."
    ),
)
activate_option = typer.Option(
    True,
    help="Activate the profile containing this token after creation.",
)

verify_option = typer.Option(
    True,
    help="Make a test request to verify the new credentials.",
)


@token_cli.command(
    name="set",
    help=(
        "Set account credentials for connecting to Modal. "
        "If not provided with the command, you will be prompted to enter your credentials."
    ),
)
@synchronizer.create_blocking
async def set(
    token_id: Optional[str] = typer.Option(None, help="Account token ID."),
    token_secret: Optional[str] = typer.Option(None, help="Account token secret."),
    profile: Optional[str] = profile_option,
    activate: bool = activate_option,
    verify: bool = verify_option,
):
    if token_id is None:
        token_id = getpass.getpass("Token ID:")
    if token_secret is None:
        token_secret = getpass.getpass("Token secret:")
    await _set_token(token_id, token_secret, profile=profile, activate=activate, verify=verify)


@token_cli.command(name="new", help="Create a new token by using an authenticated web session.")
@synchronizer.create_blocking
async def new(
    profile: Optional[str] = profile_option,
    activate: bool = activate_option,
    verify: bool = verify_option,
    source: Optional[str] = None,
):
    await _new_token(profile=profile, activate=activate, verify=verify, source=source)


================================================
File: modal/cli/utils.py
================================================
# Copyright Modal Labs 2022
import asyncio
from collections.abc import Sequence
from datetime import datetime
from json import dumps
from typing import Optional, Union

import typer
from click import UsageError
from grpclib import GRPCError, Status
from rich.console import Console
from rich.table import Column, Table
from rich.text import Text

from modal_proto import api_pb2

from .._output import OutputManager, get_app_logs_loop
from .._utils.async_utils import synchronizer
from ..client import _Client
from ..environments import ensure_env
from ..exception import NotFoundError


@synchronizer.create_blocking
async def stream_app_logs(
    app_id: Optional[str] = None, task_id: Optional[str] = None, app_logs_url: Optional[str] = None
):
    client = await _Client.from_env()
    output_mgr = OutputManager(status_spinner_text=f"Tailing logs for {app_id}")
    try:
        with output_mgr.show_status_spinner():
            await get_app_logs_loop(client, output_mgr, app_id=app_id, task_id=task_id, app_logs_url=app_logs_url)
    except asyncio.CancelledError:
        pass
    except GRPCError as exc:
        if exc.status in (Status.INVALID_ARGUMENT, Status.NOT_FOUND):
            raise UsageError(exc.message)
        else:
            raise
    except KeyboardInterrupt:
        pass


@synchronizer.create_blocking
async def get_app_id_from_name(name: str, env: Optional[str], client: Optional[_Client] = None) -> str:
    if client is None:
        client = await _Client.from_env()
    env_name = ensure_env(env)
    request = api_pb2.AppGetByDeploymentNameRequest(
        namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, name=name, environment_name=env_name
    )
    try:
        resp = await client.stub.AppGetByDeploymentName(request)
    except GRPCError as exc:
        if exc.status in (Status.INVALID_ARGUMENT, Status.NOT_FOUND):
            raise UsageError(exc.message or "")
        raise
    if not resp.app_id:
        env_comment = f" in the '{env_name}' environment" if env_name else ""
        raise NotFoundError(f"Could not find a deployed app named '{name}'{env_comment}.")
    return resp.app_id


def timestamp_to_local(ts: float, isotz: bool = True) -> str:
    if ts > 0:
        locale_tz = datetime.now().astimezone().tzinfo
        dt = datetime.fromtimestamp(ts, tz=locale_tz)
        if isotz:
            return dt.isoformat(sep=" ", timespec="seconds")
        else:
            return f"{datetime.strftime(dt, '%Y-%m-%d %H:%M')} {locale_tz.tzname(dt)}"
    else:
        return None


def _plain(text: Union[Text, str]) -> str:
    return text.plain if isinstance(text, Text) else text


def is_tty() -> bool:
    return Console().is_terminal


def display_table(
    columns: Sequence[Union[Column, str]],
    rows: Sequence[Sequence[Union[Text, str]]],
    json: bool = False,
    title: str = "",
):
    def col_to_str(col: Union[Column, str]) -> str:
        return str(col.header) if isinstance(col, Column) else col

    console = Console()
    if json:
        json_data = [{col_to_str(col): _plain(row[i]) for i, col in enumerate(columns)} for row in rows]
        console.print_json(dumps(json_data))
    else:
        table = Table(*columns, title=title)
        for row in rows:
            table.add_row(*row)
        console.print(table)


ENV_OPTION_HELP = """Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the `MODAL_ENVIRONMENT` variable.
Otherwise, raises an error if the workspace has multiple environments.
"""
ENV_OPTION = typer.Option(None, "-e", "--env", help=ENV_OPTION_HELP)

YES_OPTION = typer.Option(False, "-y", "--yes", help="Run without pausing for confirmation.")


================================================
File: modal/cli/volume.py
================================================
# Copyright Modal Labs 2022
import os
import sys
from pathlib import Path
from typing import Optional

import typer
from click import UsageError
from grpclib import GRPCError, Status
from rich.console import Console
from rich.syntax import Syntax
from typer import Argument, Option, Typer

import modal
from modal._output import OutputManager, ProgressHandler
from modal._utils.async_utils import synchronizer
from modal._utils.grpc_utils import retry_transient_errors
from modal.cli._download import _volume_download
from modal.cli.utils import ENV_OPTION, YES_OPTION, display_table, timestamp_to_local
from modal.client import _Client
from modal.environments import ensure_env
from modal.volume import _Volume, _VolumeUploadContextManager
from modal_proto import api_pb2

volume_cli = Typer(
    name="volume",
    no_args_is_help=True,
    help="""
    Read and edit `modal.Volume` volumes.

    Note: users of `modal.NetworkFileSystem` should use the `modal nfs` command instead.
    """,
)


def humanize_filesize(value: int) -> str:
    if value < 0:
        raise ValueError("value should be >= 0")
    suffix = (" KiB", " MiB", " GiB", " TiB", " PiB", " EiB", " ZiB")
    format = "%.1f"
    base = 1024
    bytes_ = float(value)
    if bytes_ < base:
        return f"{bytes_:0.0f} B"
    for i, s in enumerate(suffix):
        unit = base ** (i + 2)
        if bytes_ < unit:
            break
    return format % (base * bytes_ / unit) + s


@volume_cli.command(name="create", help="Create a named, persistent modal.Volume.", rich_help_panel="Management")
def create(
    name: str,
    env: Optional[str] = ENV_OPTION,
    version: Optional[int] = Option(default=None, help="VolumeFS version. (Experimental)"),
):
    env_name = ensure_env(env)
    modal.Volume.create_deployed(name, environment_name=env, version=version)
    usage_code = f"""
@app.function(volumes={{"/my_vol": modal.Volume.from_name("{name}")}})
def some_func():
    os.listdir("/my_vol")
"""

    console = Console()
    console.print(f"Created Volume '{name}' in environment '{env_name}'. \n\nCode example:\n")
    usage = Syntax(usage_code, "python")
    console.print(usage)


@volume_cli.command(name="get", rich_help_panel="File operations")
@synchronizer.create_blocking
async def get(
    volume_name: str,
    remote_path: str,
    local_destination: str = Argument("."),
    force: bool = False,
    env: Optional[str] = ENV_OPTION,
):
    """Download files from a modal.Volume object.

    If a folder is passed for REMOTE_PATH, the contents of the folder will be downloaded
    recursively, including all subdirectories.

    **Example**

    ```
    modal volume get <volume_name> logs/april-12-1.txt
    modal volume get <volume_name> / volume_data_dump
    ```

    Use "-" as LOCAL_DESTINATION to write file contents to standard output.
    """
    ensure_env(env)
    destination = Path(local_destination)
    volume = _Volume.from_name(volume_name, environment_name=env)
    console = Console()
    progress_handler = ProgressHandler(type="download", console=console)
    with progress_handler.live:
        await _volume_download(volume, remote_path, destination, force, progress_cb=progress_handler.progress)
    console.print(OutputManager.step_completed("Finished downloading files to local!"))


@volume_cli.command(
    name="list",
    help="List the details of all modal.Volume volumes in an Environment.",
    rich_help_panel="Management",
)
@synchronizer.create_blocking
async def list_(env: Optional[str] = ENV_OPTION, json: Optional[bool] = False):
    env = ensure_env(env)
    client = await _Client.from_env()
    response = await retry_transient_errors(client.stub.VolumeList, api_pb2.VolumeListRequest(environment_name=env))
    env_part = f" in environment '{env}'" if env else ""
    column_names = ["Name", "Created at"]
    rows = []
    for item in response.items:
        rows.append([item.label, timestamp_to_local(item.created_at, json)])
    display_table(column_names, rows, json, title=f"Volumes{env_part}")


@volume_cli.command(
    name="ls",
    help="List files and directories in a modal.Volume volume.",
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def ls(
    volume_name: str,
    path: str = Argument(default="/"),
    json: bool = False,
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    vol = _Volume.from_name(volume_name, environment_name=env)

    try:
        entries = await vol.listdir(path)
    except GRPCError as exc:
        if exc.status in (Status.INVALID_ARGUMENT, Status.NOT_FOUND):
            raise UsageError(exc.message)
        raise

    if not json and not sys.stdout.isatty():
        # Legacy behavior -- I am not sure why exactly we did this originally but I don't want to break it
        for entry in entries:
            print(entry.path)
    else:
        rows = []
        for entry in entries:
            if entry.type == api_pb2.FileEntry.FileType.DIRECTORY:
                filetype = "dir"
            elif entry.type == api_pb2.FileEntry.FileType.SYMLINK:
                filetype = "link"
            else:
                filetype = "file"
            rows.append(
                (
                    entry.path.encode("unicode_escape").decode("utf-8"),
                    filetype,
                    timestamp_to_local(entry.mtime, False),
                    humanize_filesize(entry.size),
                )
            )
        columns = ["Filename", "Type", "Created/Modified", "Size"]
        title = f"Directory listing of '{path}' in '{volume_name}'"
        display_table(columns, rows, json, title)


@volume_cli.command(
    name="put",
    help="""Upload a file or directory to a modal.Volume.

Remote parent directories will be created as needed.

Ending the REMOTE_PATH with a forward slash (/), it's assumed to be a directory
and the file will be uploaded with its current name under that directory.
""",
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def put(
    volume_name: str,
    local_path: str = Argument(),
    remote_path: str = Argument(default="/"),
    force: bool = Option(False, "-f", "--force", help="Overwrite existing files."),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    vol = await _Volume.from_name(volume_name, environment_name=env).hydrate()

    if remote_path.endswith("/"):
        remote_path = remote_path + os.path.basename(local_path)
    console = Console()
    progress_handler = ProgressHandler(type="upload", console=console)

    if Path(local_path).is_dir():
        with progress_handler.live:
            try:
                async with _VolumeUploadContextManager(
                    vol.object_id, vol._client, progress_cb=progress_handler.progress, force=force
                ) as batch:
                    batch.put_directory(local_path, remote_path)
            except FileExistsError as exc:
                raise UsageError(str(exc))
        console.print(OutputManager.step_completed(f"Uploaded directory '{local_path}' to '{remote_path}'"))
    elif "*" in local_path:
        raise UsageError("Glob uploads are currently not supported")
    else:
        with progress_handler.live:
            try:
                async with _VolumeUploadContextManager(
                    vol.object_id, vol._client, progress_cb=progress_handler.progress, force=force
                ) as batch:
                    batch.put_file(local_path, remote_path)

            except FileExistsError as exc:
                raise UsageError(str(exc))
        console.print(OutputManager.step_completed(f"Uploaded file '{local_path}' to '{remote_path}'"))


@volume_cli.command(
    name="rm", help="Delete a file or directory from a modal.Volume.", rich_help_panel="File operations"
)
@synchronizer.create_blocking
async def rm(
    volume_name: str,
    remote_path: str,
    recursive: bool = Option(False, "-r", "--recursive", help="Delete directory recursively"),
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _Volume.from_name(volume_name, environment_name=env)
    try:
        await volume.remove_file(remote_path, recursive=recursive)
    except GRPCError as exc:
        if exc.status in (Status.NOT_FOUND, Status.INVALID_ARGUMENT):
            raise UsageError(exc.message)
        raise


@volume_cli.command(
    name="cp",
    help=(
        "Copy within a modal.Volume. "
        "Copy source file to destination file or multiple source files to destination directory."
    ),
    rich_help_panel="File operations",
)
@synchronizer.create_blocking
async def cp(
    volume_name: str,
    paths: list[str],  # accepts multiple paths, last path is treated as destination path
    env: Optional[str] = ENV_OPTION,
):
    ensure_env(env)
    volume = _Volume.from_name(volume_name, environment_name=env)
    *src_paths, dst_path = paths
    await volume.copy_files(src_paths, dst_path)


@volume_cli.command(
    name="delete",
    help="Delete a named, persistent modal.Volume.",
    rich_help_panel="Management",
)
@synchronizer.create_blocking
async def delete(
    volume_name: str = Argument(help="Name of the modal.Volume to be deleted. Case sensitive"),
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    # Lookup first to validate the name, even though delete is a staticmethod
    await _Volume.from_name(volume_name, environment_name=env).hydrate()
    if not yes:
        typer.confirm(
            f"Are you sure you want to irrevocably delete the modal.Volume '{volume_name}'?",
            default=False,
            abort=True,
        )

    await _Volume.delete(volume_name, environment_name=env)


@volume_cli.command(
    name="rename",
    help="Rename a modal.Volume.",
    rich_help_panel="Management",
)
@synchronizer.create_blocking
async def rename(
    old_name: str,
    new_name: str,
    yes: bool = YES_OPTION,
    env: Optional[str] = ENV_OPTION,
):
    if not yes:
        typer.confirm(
            f"Are you sure you want rename the modal.Volume '{old_name}'? This may break any Apps currently using it.",
            default=False,
            abort=True,
        )

    await _Volume.rename(old_name, new_name, environment_name=env)


================================================
File: modal/cli/programs/__init__.py
================================================
# Copyright Modal Labs 2023


================================================
File: modal/cli/programs/run_jupyter.py
================================================
# Copyright Modal Labs 2023
# type: ignore
import json
import os
import secrets
import socket
import subprocess
import threading
import time
import webbrowser
from typing import Any

from modal import App, Image, Queue, Secret, Volume, forward

# Passed by `modal launch` locally via CLI, plumbed to remote runner through secrets.
args: dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_ARGS", "{}"))

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default

image = Image.from_registry(args.get("image"), add_python=args.get("add_python")).pip_install("jupyterlab")

if args.get("mount"):
    image = image.add_local_dir(
        args.get("mount"),
        remote_path="/root/lab/mount",
    )

volume = (
    Volume.from_name(
        args.get("volume"),
        create_if_missing=True,
    )
    if args.get("volume")
    else None
)
volumes = {"/root/lab/volume": volume} if volume else {}


def wait_for_port(url: str, q: Queue):
    start_time = time.monotonic()
    while True:
        try:
            with socket.create_connection(("localhost", 8888), timeout=30.0):
                break
        except OSError as exc:
            time.sleep(0.01)
            if time.monotonic() - start_time >= 30.0:
                raise TimeoutError("Waited too long for port 8888 to accept connections") from exc
    q.put(url)


@app.function(
    image=image,
    cpu=args.get("cpu"),
    memory=args.get("memory"),
    gpu=args.get("gpu"),
    timeout=args.get("timeout"),
    secrets=[Secret.from_dict({"MODAL_LAUNCH_ARGS": json.dumps(args)})],
    volumes=volumes,
    concurrency_limit=1 if volume else None,
)
def run_jupyter(q: Queue):
    os.makedirs("/root/lab", exist_ok=True)
    token = secrets.token_urlsafe(13)
    with forward(8888) as tunnel:
        url = tunnel.url + "/?token=" + token
        threading.Thread(target=wait_for_port, args=(url, q)).start()
        print("\nJupyter on Modal, opening in browser...")
        print(f"   -> {url}\n")
        subprocess.run(
            [
                "jupyter",
                "lab",
                "--no-browser",
                "--allow-root",
                "--ip=0.0.0.0",
                "--port=8888",
                "--notebook-dir=/root/lab",
                "--LabApp.allow_origin='*'",
                "--LabApp.allow_remote_access=1",
            ],
            env={**os.environ, "JUPYTER_TOKEN": token, "SHELL": "/bin/bash"},
            stderr=subprocess.DEVNULL,
        )
    q.put("done")


@app.local_entrypoint()
def main():
    with Queue.ephemeral() as q:
        run_jupyter.spawn(q)
        url = q.get()
        time.sleep(1)  # Give Jupyter a chance to start up
        webbrowser.open(url)
        assert q.get() == "done"


================================================
File: modal/cli/programs/vscode.py
================================================
# Copyright Modal Labs 2023
# type: ignore
import json
import os
import secrets
import socket
import subprocess
import threading
import time
import webbrowser
from typing import Any

from modal import App, Image, Queue, Secret, Volume, forward

# Passed by `modal launch` locally via CLI, plumbed to remote runner through secrets.
args: dict[str, Any] = json.loads(os.environ.get("MODAL_LAUNCH_ARGS", "{}"))

CODE_SERVER_INSTALLER = "https://code-server.dev/install.sh"
CODE_SERVER_ENTRYPOINT = (
    "https://raw.githubusercontent.com/coder/code-server/refs/tags/v4.96.1/ci/release-image/entrypoint.sh"
)
FIXUD_INSTALLER = "https://github.com/boxboat/fixuid/releases/download/v0.6.0/fixuid-0.6.0-linux-$ARCH.tar.gz"


app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default
image = (
    Image.from_registry(args.get("image"), add_python="3.11")
    .apt_install("curl", "dumb-init", "git", "git-lfs")
    .run_commands(
        f"curl -fsSL {CODE_SERVER_INSTALLER} | sh",
        f"curl -fsSL {CODE_SERVER_ENTRYPOINT}  > /code-server.sh",
        "chmod u+x /code-server.sh",
    )
    .run_commands(
        'ARCH="$(dpkg --print-architecture)"'
        f' && curl -fsSL "{FIXUD_INSTALLER}" | tar -C /usr/local/bin -xzf - '
        " && chown root:root /usr/local/bin/fixuid"
        " && chmod 4755 /usr/local/bin/fixuid"
        " && mkdir -p /etc/fixuid"
        ' && echo "user: root" >> /etc/fixuid/config.yml'
        ' && echo "group: root" >> /etc/fixuid/config.yml'
    )
    .run_commands("mkdir /home/coder")
    .env({"ENTRYPOINTD": ""})
)

if args.get("mount"):
    image = image.add_local_dir(
        args.get("mount"),
        remote_path="/home/coder/mount",
    )

volume = (
    Volume.from_name(
        args.get("volume"),
        create_if_missing=True,
    )
    if args.get("volume")
    else None
)
volumes = {"/home/coder/volume": volume} if volume else {}


def wait_for_port(data: tuple[str, str], q: Queue):
    start_time = time.monotonic()
    while True:
        try:
            with socket.create_connection(("localhost", 8080), timeout=30.0):
                break
        except OSError as exc:
            time.sleep(0.01)
            if time.monotonic() - start_time >= 30.0:
                raise TimeoutError("Waited too long for port 8080 to accept connections") from exc
    q.put(data)


@app.function(
    image=image,
    cpu=args.get("cpu"),
    memory=args.get("memory"),
    gpu=args.get("gpu"),
    timeout=args.get("timeout"),
    secrets=[Secret.from_dict({"MODAL_LAUNCH_ARGS": json.dumps(args)})],
    volumes=volumes,
    max_containers=1 if volume else None,
)
def run_vscode(q: Queue):
    os.chdir("/home/coder")
    token = secrets.token_urlsafe(13)
    with forward(8080) as tunnel:
        url = tunnel.url
        print("\nVS Code on Modal, opening in browser...")
        print(f"   -> {url}")
        print(f"   -> password: {token}\n")
        threading.Thread(target=wait_for_port, args=((url, token), q)).start()
        subprocess.run(
            ["/code-server.sh", "--bind-addr", "0.0.0.0:8080", "."],
            env={**os.environ, "SHELL": "/bin/bash", "PASSWORD": token},
        )
    q.put("done")


@app.local_entrypoint()
def main():
    with Queue.ephemeral() as q:
        run_vscode.spawn(q)
        url, token = q.get()
        time.sleep(1)  # Give VS Code a chance to start up
        webbrowser.open(url)
        assert q.get() == "done"


================================================
File: modal/extensions/__init__.py
================================================
# Copyright Modal Labs 2022


================================================
File: modal/extensions/ipython.py
================================================
# Copyright Modal Labs 2022
import atexit
import logging
import sys
from typing import Any

from modal import App
from modal._utils.async_utils import run_coro_blocking
from modal.config import config, logger

app_ctx: Any


def load_ipython_extension(ipython):
    global app_ctx

    # Set logger for notebook sys.stdout
    logger.addHandler(logging.StreamHandler(stream=sys.stdout))
    logger.setLevel(config["loglevel"])

    # Create an app and provide it in the IPython app
    app = App()
    ipython.push({"app": app})

    app_ctx = app.run()

    # Notebooks have an event loop present, but we want this function
    # to be blocking. This is fairly hacky.
    run_coro_blocking(app_ctx.__aenter__())

    def exit_app():
        print("Exiting modal app")
        run_coro_blocking(app_ctx.__aexit__(None, None, None))

    atexit.register(exit_app)


def unload_ipython_extension(ipython):
    global app_ctx

    run_coro_blocking(app_ctx.__aexit__(None, None, None))


================================================
File: modal/requirements/README.md
================================================
# Modal Image builder configuration

This directory contains `modal.Image` specifications that vary across
"image builder" versions.

The `base-images.json` file specifies the versions used for Modal's
various `Image` constructor methods.

The versioned requirements files enumerate the dependencies needed by
the Modal client library when it is running inside a Modal container.

The container requirements are a subset of the dependencies required by the
client for local operation (i.e., to run or deploy Modal apps). Additionally,
we aim to pin specific versions rather than allowing a range as we do for the
installation dependencies.

From version `2024.04`, the requirements specify the entire dependency tree,
and not just the first-order dependencies.

Note that for `2023.12`, there is a separate requirements file that is used for
Python 3.12.

================================================
File: modal/requirements/2023.12.312.txt
================================================
# Pins Modal dependencies installed within the container runtime.
aiohttp==3.9.1
aiostream==0.4.4
asgiref==3.5.2
certifi>=2022.12.07
cloudpickle==2.2.0
fastapi==0.88.0
fastprogress==1.0.0
grpclib==0.4.7
importlib_metadata==4.8.1
ipython>=7.34.0
protobuf>=3.19.0
python-multipart>=0.0.5
rich==12.3.0
tblib==1.7.0
toml==0.10.2
typer==0.6.1
types-certifi==2021.10.8.3
types-toml==0.10.4
typeguard>=3.0.0

================================================
File: modal/requirements/2023.12.txt
================================================
# Pins Modal dependencies installed within the container runtime.
aiohttp==3.8.3
aiostream==0.4.4
asgiref==3.5.2
certifi>=2022.12.07
cloudpickle==2.0.0;python_version<'3.11'
cloudpickle==2.2.0;python_version>='3.11'
ddtrace==1.5.2;python_version<'3.11'
fastapi==0.88.0
fastprogress==1.0.0
grpclib==0.4.3
importlib_metadata==4.8.1
ipython>=7.34.0
protobuf>=3.19.0
python-multipart>=0.0.5
rich==12.3.0
tblib==1.7.0
toml==0.10.2
typer==0.6.1
types-certifi==2021.10.8.3
types-toml==0.10.4
typeguard>=3.0.0


================================================
File: modal/requirements/2024.04.txt
================================================
aiohttp==3.9.3
aiosignal==1.3.1
aiostream==0.5.2
annotated-types==0.6.0
anyio==4.3.0
async-timeout==4.0.3 ; python_version < "3.11"
attrs==23.2.0
certifi==2024.2.2
exceptiongroup==1.2.0 ; python_version < "3.11"
fastapi==0.110.0
frozenlist==1.4.1
grpclib==0.4.7
h2==4.1.0
hpack==4.0.0
hyperframe==6.0.1
idna==3.6
markdown-it-py==3.0.0
mdurl==0.1.2
multidict==6.0.5
protobuf==4.25.3
pydantic==2.6.4
pydantic_core==2.16.3
Pygments==2.17.2
python-multipart==0.0.9
rich==13.7.1
sniffio==1.3.1
starlette==0.36.3
typing_extensions==4.10.0
yarl==1.9.4

================================================
File: modal/requirements/2024.10.txt
================================================
aiohappyeyeballs==2.4.3
aiohttp==3.10.8
aiosignal==1.3.1
async-timeout==4.0.3 ; python_version < "3.11"
attrs==24.2.0
certifi==2024.8.30
frozenlist==1.4.1
grpclib==0.4.7
h2==4.1.0
hpack==4.0.0
hyperframe==6.0.1
idna==3.10
multidict==6.1.0
protobuf>=3.20,<6
typing_extensions==4.12.2
yarl==1.13.1


================================================
File: modal/requirements/base-images.json
================================================
{
    "debian": {
        "2024.10": "bookworm",
        "2024.04": "bookworm",
        "2023.12": "bullseye"
    },
    "python": {
        "2024.10": ["3.9.20", "3.10.15", "3.11.10", "3.12.6", "3.13.0"],
        "2024.04": ["3.9.19", "3.10.14", "3.11.8", "3.12.2"],
        "2023.12": ["3.9.15", "3.10.8", "3.11.0", "3.12.1"]
    },
    "micromamba": {
        "2024.10": "1.5.10",
        "2024.04": "1.5.8",
        "2023.12": "1.3.1"
    },
    "package_tools": {
        "2024.10": "pip wheel uv",
        "2024.04": "pip wheel uv",
        "2023.12": "pip"
    }
}

================================================
File: modal_docs/__init__.py
================================================
# Copyright Modal Labs 2023


================================================
File: modal_docs/gen_cli_docs.py
================================================
# Copyright Modal Labs 2023
import inspect
import sys
from pathlib import Path
from typing import Optional, cast

from click import Command, Context, Group

from modal.cli.entry_point import entrypoint_cli


# Adapted from typer_cli, since it's incompatible with the latest version of typer
# (see https://github.com/tiangolo/typer-cli/issues/50)
def get_docs_for_click(
    obj: Command,
    ctx: Context,
    *,
    indent: int = 0,
    name: str = "",
    call_prefix: str = "",
) -> str:
    docs = "#" * (1 + indent)
    command_name = name or obj.name
    if call_prefix:
        command_name = f"{call_prefix} {command_name}"
    title = f"`{command_name}`" if command_name else "CLI"
    docs += f" {title}\n\n"
    if obj.help:
        docs += f"{inspect.cleandoc(obj.help)}\n\n"
    usage_pieces = obj.collect_usage_pieces(ctx)
    if usage_pieces:
        docs += "**Usage**:\n\n"
        docs += "```shell\n"
        if command_name:
            docs += f"{command_name} "
        docs += f"{' '.join(usage_pieces)}\n"
        docs += "```\n\n"
    args = []
    opts = []
    for param in obj.get_params(ctx):
        rv = param.get_help_record(ctx)
        if rv is not None:
            if getattr(param, "hidden", False):
                continue
            if param.param_type_name == "argument":
                args.append(rv)
            elif param.param_type_name == "option":
                opts.append(rv)
    if args:
        docs += "**Arguments**:\n\n"
        for arg_name, arg_help in args:
            docs += f"* `{arg_name}`"
            if arg_help:
                docs += f": {arg_help}"
            docs += "\n"
        docs += "\n"
    if opts:
        docs += "**Options**:\n\n"
        for opt_name, opt_help in opts:
            docs += f"* `{opt_name}`"
            if opt_help:
                docs += f": {opt_help}"
            docs += "\n"
        docs += "\n"
    if obj.epilog:
        docs += f"{obj.epilog}\n\n"
    if isinstance(obj, Group):
        group: Group = cast(Group, obj)
        commands = group.list_commands(ctx)
        if commands:
            docs += "**Commands**:\n\n"
            for command in commands:
                command_obj = group.get_command(ctx, command)
                assert command_obj
                if command_obj.hidden:
                    continue
                docs += f"* `{command_obj.name}`"
                command_help = command_obj.get_short_help_str(limit=250)
                if command_help:
                    docs += f": {command_help}"
                docs += "\n"
            docs += "\n"
        for command in commands:
            command_obj = group.get_command(ctx, command)
            if command_obj.hidden:
                continue
            assert command_obj
            use_prefix = ""
            if command_name:
                use_prefix += f"{command_name}"
            docs += get_docs_for_click(obj=command_obj, ctx=ctx, indent=indent + 1, call_prefix=use_prefix)
    return docs


def run(output_dirname: Optional[str]) -> None:
    entrypoint: Group = cast(Group, entrypoint_cli)
    ctx = Context(entrypoint)
    commands = entrypoint.list_commands(ctx)

    for command in commands:
        command_obj = entrypoint.get_command(ctx, command)
        if command_obj.hidden:
            continue
        docs = get_docs_for_click(obj=command_obj, ctx=ctx, call_prefix="modal")

        if output_dirname:
            output_dir = Path(output_dirname)
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = output_dir / f"{command}.md"
            print("Writing to", output_file)
            output_file.write_text(docs)
        else:
            print(docs)


if __name__ == "__main__":
    run(None if len(sys.argv) <= 1 else sys.argv[1])


================================================
File: modal_docs/gen_reference_docs.py
================================================
# Copyright Modal Labs 2023
import importlib
import inspect
import json
import os
import sys
import warnings
from typing import NamedTuple

from synchronicity.synchronizer import FunctionWithAio

from .mdmd.mdmd import (
    Category,
    class_str,
    default_filter,
    function_str,
    module_items,
    module_str,
    object_is_private,
    package_filter,
)


class DocItem(NamedTuple):
    label: str
    category: Category
    document: str
    in_sidebar: bool = True


def validate_doc_item(docitem: DocItem) -> DocItem:
    # Check that unwanted strings aren't leaking into our docs.
    bad_strings = [
        # Presence of a to-do inside a `DocItem` usually indicates it's been
        # placed inside a function signature definition or right underneath it, before the body.
        # Fix by moving the to-do into the body or above the signature.
        "TODO:"
    ]
    for line in docitem.document.splitlines():
        for bad_str in bad_strings:
            if bad_str in line:
                msg = f"Found unwanted string '{bad_str}' in content for item '{docitem.label}'. Problem line: {line}"
                raise ValueError(msg)
    return docitem


def run(output_dir: str = None):
    """Generate Modal docs."""
    import modal

    ordered_doc_items: list[DocItem] = []
    documented_items = set()

    def filter_non_aio(module, name):
        return not name.lower().startswith("aio")

    def filter_already_documented(module, name):
        item = getattr(module, name)
        try:
            if item in documented_items:
                return False
        except TypeError:  # unhashable stuff
            print(f"Warning: could not document item {name}: {item}:")
            return False
        documented_items.add(item)
        return True

    def modal_default_filter(module, name):
        return default_filter(module, name) and filter_non_aio(module, name) and filter_already_documented(module, name)

    def top_level_filter(module, name):
        item = getattr(module, name)
        if object_is_private(name, item) or inspect.ismodule(item):
            return False
        return package_filter("modal") and filter_already_documented(module, name) and filter_non_aio(module, name)

    base_title_level = "#"
    forced_module_docs = [
        ("modal.call_graph", "modal.call_graph"),
        ("modal.container_process", "modal.container_process"),
        ("modal.gpu", "modal.gpu"),
        ("modal.runner", "modal.runner"),
        ("modal.io_streams", "modal.io_streams"),
        ("modal.file_io", "modal.file_io"),
    ]
    # These aren't defined in `modal`, but should still be documented as top-level entries.
    forced_members: set[str] = set()
    # These are excluded from the sidebar, typically to 'soft release' some documentation.
    sidebar_excluded: set[str] = set()

    for title, modulepath in forced_module_docs:
        module = importlib.import_module(modulepath)
        document = module_str(modulepath, module, title_level=base_title_level, filter_items=modal_default_filter)
        if document:
            ordered_doc_items.append(
                validate_doc_item(
                    DocItem(
                        label=title,
                        category=Category.MODULE,
                        document=document,
                        in_sidebar=title not in sidebar_excluded,
                    )
                )
            )

    def f(module, member_name):
        return top_level_filter(module, member_name) or (member_name in forced_members)

    # now add all remaining top level modal.X entries
    for qual_name, item_name, item in module_items(modal, filter_items=f):
        if object_is_private(item_name, item):
            continue  # skip stuff that's part of explicit `handle_objects` above

        title = f"modal.{item_name}"
        if inspect.isclass(item):
            content = f"{base_title_level} {qual_name}\n\n" + class_str(item_name, item, base_title_level)
            category = Category.CLASS
        elif inspect.isroutine(item) or isinstance(item, FunctionWithAio):
            content = f"{base_title_level} {qual_name}\n\n" + function_str(item_name, item)
            category = Category.FUNCTION
        elif inspect.ismodule(item):
            continue  # skipping imported modules
        else:
            warnings.warn(f"Not sure how to document: {item_name} ({item})")
            continue
        ordered_doc_items.append(
            validate_doc_item(
                DocItem(
                    label=title,
                    category=category,
                    document=content,
                    in_sidebar=title not in sidebar_excluded,
                )
            )
        )
    ordered_doc_items.sort()

    for modulepath in ["modal.exception", "modal.config"]:
        module = importlib.import_module(modulepath)
        document = module_str(modulepath, module, title_level=base_title_level, filter_items=modal_default_filter)
        ordered_doc_items.append(
            DocItem(
                label=modulepath,
                category=Category.MODULE,
                document=document,
            )
        )

    # TODO: add some way of documenting our .aio sub-methods

    make_markdown_docs(
        ordered_doc_items,
        output_dir,
    )


def make_markdown_docs(items: list[DocItem], output_dir: str = None):
    def _write_file(rel_path: str, data: str):
        if output_dir is None:
            print(f"<<< {rel_path}")
            print(data)
            print(f">>> {rel_path}")
            return

        filename = os.path.join(output_dir, rel_path)
        print("Writing to", filename)
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, "w") as fp:
            fp.write(data)

    sidebar_items = []
    for item in items:
        if item.in_sidebar:
            sidebar_items.append(
                {
                    "label": item.label,
                    "category": item.category.value,
                }
            )
        _write_file(f"{item.label}.md", item.document)

    sidebar_data = {"items": sidebar_items}
    _write_file("sidebar.json", json.dumps(sidebar_data))


if __name__ == "__main__":
    # running this module outputs docs to stdout for inspection, useful for debugging
    run(None if len(sys.argv) <= 1 else sys.argv[1])


================================================
File: modal_docs/mdmd/__init__.py
================================================
# Copyright Modal Labs 2023


================================================
File: modal_docs/mdmd/mdmd.py
================================================
# Copyright Modal Labs 2023
"""mdmd - MoDal MarkDown"""

import inspect
import warnings
from enum import Enum, EnumMeta
from types import ModuleType
from typing import Callable

import synchronicity.synchronizer

from .signatures import get_signature


def format_docstring(docstring: str):
    if docstring is None:
        docstring = ""
    else:
        docstring = inspect.cleandoc(docstring)

    if docstring and not docstring.endswith("\n"):
        docstring += "\n"

    return docstring


def function_str(name: str, func):
    signature = get_signature(name, func)
    decl = f"""```python
{signature}
```\n\n"""
    docstring = format_docstring(func.__doc__)
    return decl + docstring


def class_str(name, obj, title_level="##"):
    def qual_name(cls):
        if cls.__module__ == "builtins":
            return cls.__name__
        return f"{cls.__module__}.{cls.__name__}"

    bases = [qual_name(b) for b in obj.__bases__]
    bases_str = f"({', '.join(bases)})" if bases else ""
    decl = f"""```python
class {name}{bases_str}
```\n\n"""
    parts = [decl]
    docstring = format_docstring(obj.__doc__)

    if isinstance(obj, EnumMeta) and not docstring:
        # Python 3.11 removed the docstring from enums
        docstring = "An enumeration.\n"

    if docstring:
        parts.append(docstring + "\n")

    if isinstance(obj, EnumMeta):
        enum_vals = "\n".join(f"* `{k}`" for k in obj.__members__.keys())
        parts.append(f"The possible values are:\n\n{enum_vals}\n")

    else:
        init = inspect.unwrap(obj.__init__)

        if (inspect.isfunction(init) or inspect.ismethod(init)) and not object_is_private("constructor", init):
            parts.append(function_str("__init__", init))

    member_title_level = title_level + "#"

    entries = {}

    def rec_update_attributes(cls):
        # first bases, then class itself
        for base_cls in cls.__bases__:
            rec_update_attributes(base_cls)
        entries.update(cls.__dict__)

    rec_update_attributes(obj)

    for member_name, member in entries.items():
        if isinstance(member, classmethod) or isinstance(member, staticmethod):
            # get the original function definition instead of the descriptor object
            member = getattr(obj, member_name)
        elif isinstance(member, property):
            member = member.fget
        elif isinstance(member, (synchronicity.synchronizer.FunctionWithAio, synchronicity.synchronizer.MethodWithAio)):
            member = member._func

        if object_is_private(member_name, member):
            continue

        if callable(member):
            parts.append(f"{member_title_level} {member_name}\n\n")
            parts.append(function_str(member_name, member))

    return "".join(parts)


def module_str(header, module, title_level="#", filter_items: Callable[[ModuleType, str], bool] = None):
    header = [f"{title_level} {header}\n\n"]
    docstring = format_docstring(module.__doc__)
    if docstring:
        header.append(docstring + "\n")

    object_docs = []
    member_title_level = title_level + "#"
    for qual_name, name, item in module_items(module, filter_items):
        try:
            if hasattr(item, "__wrapped__"):
                item = item.__wrapped__
        except KeyError:
            pass
        except:
            print("failed on", qual_name, name, item)
            raise
        if inspect.isclass(item):
            classdoc = class_str(name, item, title_level=member_title_level)
            object_docs.append(f"{member_title_level} {qual_name}\n\n")
            object_docs.append(classdoc)
        elif callable(item):
            funcdoc = function_str(name, item)
            object_docs.append(f"{member_title_level} {qual_name}\n\n")
            object_docs.append(funcdoc)
        else:
            item_doc = getattr(module, f"__doc__{name}", None)
            if item_doc:
                # variable documentation
                object_docs.append(f"{member_title_level} {qual_name}\n\n")
                object_docs.append(item_doc)
            else:
                warnings.warn(f"Not sure how to document: {name} ({item}")

    if object_docs:
        return "".join(header + object_docs)
    return ""


def object_is_private(name, obj):
    docstring = inspect.getdoc(obj)
    if docstring is None:
        docstring = ""
    module = getattr(obj, "__module__", None)  # obj is class
    if not module:
        cls = getattr(obj, "__class__", None)  # obj is instance
        if cls:
            module = getattr(cls, "__module__", None)
    if module == "builtins":
        return True

    if docstring.lstrip().startswith("mdmd:hidden") or name.startswith("_"):
        return True

    return False


def default_filter(module, item_name):
    """Include non-private objects defined in the module itself"""
    item = getattr(module, item_name)
    if object_is_private(item_name, item) or inspect.ismodule(item):
        return False
    member_module = getattr(item, "__module__", type(item).__module__)
    return member_module == module.__name__


def package_filter(module_prefix: str):
    """Include non-private objects defined in any module with the prefix `module_prefix`"""

    def return_filter(module, item_name):
        item = getattr(module, item_name)
        if object_is_private(item_name, item) or inspect.ismodule(item):
            return False
        member_module = getattr(item, "__module__", type(item).__module__)
        return member_module.startswith(module_prefix)

    return return_filter


def module_items(module, filter_items: Callable[[ModuleType, str], bool] = None):
    """Returns filtered members of module"""
    if filter_items is None:
        # default filter is to only include classes and functions declared (or whose type is declared) in the file
        filter_items = default_filter

    for member_name, member in inspect.getmembers(module):
        # only modal items
        if not filter_items(module, member_name):
            continue

        qual_name = f"{module.__name__}.{member_name}"
        yield qual_name, member_name, member


class Category(Enum):
    FUNCTION = "function"
    CLASS = "class"
    MODULE = "module"


================================================
File: modal_docs/mdmd/signatures.py
================================================
# Copyright Modal Labs 2023
import ast
import inspect
import re
import textwrap
import warnings

from synchronicity.synchronizer import FunctionWithAio


def _signature_from_ast(func) -> tuple[str, str]:
    """Get function signature, including decorators and comments, from source code

    Traverses functools.wraps-wrappings to get source of underlying function.

    Has the advantage over inspect.signature that it can get decorators, default arguments and comments verbatim
    from the function definition.
    """
    src = inspect.getsource(func)
    src = textwrap.dedent(src)

    def get_source_segment(src, fromline, fromcol, toline, tocol) -> str:
        lines = src.split("\n")
        lines = lines[fromline - 1 : toline]
        lines[-1] = lines[-1][:tocol]
        lines[0] = lines[0][fromcol:]
        return "\n".join(lines)

    tree = ast.parse(src)
    func_def = list(ast.iter_child_nodes(tree))[0]
    assert isinstance(func_def, (ast.FunctionDef, ast.AsyncFunctionDef))
    decorator_starts = [(item.lineno, item.col_offset - 1) for item in func_def.decorator_list]
    declaration_start = min([(func_def.lineno, func_def.col_offset)] + decorator_starts)
    body_start = min((item.lineno, item.col_offset) for item in func_def.body)

    return (
        func_def.name,
        get_source_segment(src, declaration_start[0], declaration_start[1], body_start[0], body_start[1] - 1).strip(),
    )


def get_signature(name, callable) -> str:
    """A problem with using *only* this method is that the wrapping method signature will not be respected.
    TODO: use source parsing *only* to extract default arguments, comments (and possibly decorators) and "merge"
          that definition with the outer-most definition."""

    if not (inspect.isfunction(callable) or inspect.ismethod(callable) or isinstance(callable, FunctionWithAio)):
        assert hasattr(callable, "__call__")
        callable = callable.__call__

    try:
        original_name, definition_source = _signature_from_ast(callable)
    except Exception:
        warnings.warn(f"Could not get source signature for {name}. Using fallback.")
        original_name = name
        definition_source = f"def {name}{inspect.signature(callable)}"

    if original_name != name:
        # ugly name and definition replacement hack when needed
        definition_source = definition_source.replace(f"def {original_name}", f"def {name}")

    if (
        "async def" in definition_source
        and not inspect.iscoroutinefunction(callable)
        and not inspect.isasyncgenfunction(callable)
    ):
        # hack to "reset" signature to a blocking one if the underlying source definition is async
        # but the wrapper function isn't (like when synchronicity wraps an async function as a blocking one)
        definition_source = definition_source.replace("async def", "def")
        definition_source = definition_source.replace("asynccontextmanager", "contextmanager")
        definition_source = definition_source.replace("AsyncIterator", "Iterator")

    # remove any synchronicity-internal decorators
    definition_source, _ = re.subn(r"^\s*@synchronizer\..*\n", "", definition_source)

    return definition_source


================================================
File: modal_global_objects/__init__.py
================================================
# Copyright Modal Labs 2022


================================================
File: modal_global_objects/images/__init__.py
================================================
# Copyright Modal Labs 2024


================================================
File: modal_global_objects/images/base_images.py
================================================
# Copyright Modal Labs 2022
import os
import sys
from typing import cast

import modal
from modal.image import SUPPORTED_PYTHON_SERIES, ImageBuilderVersion


def dummy():
    pass


if __name__ == "__main__":
    _, name = sys.argv
    constructor = getattr(modal.Image, name)

    builder_version = os.environ.get("MODAL_IMAGE_BUILDER_VERSION")
    assert builder_version, "Script requires MODAL_IMAGE_BUILDER_VERSION environment variable"
    python_versions = SUPPORTED_PYTHON_SERIES[cast(ImageBuilderVersion, builder_version)]

    app = modal.App(f"build-{name.replace('_', '-')}-image")
    for v in python_versions:
        app.function(image=constructor(python_version=v), name=f"{v}")(dummy)

    with modal.enable_output():
        with app.run():
            pass


================================================
File: modal_global_objects/mounts/__init__.py
================================================
# Copyright Modal Labs 2024


================================================
File: modal_global_objects/mounts/modal_client_package.py
================================================
# Copyright Modal Labs 2022
from modal.config import config
from modal.mount import (
    client_mount_name,
    create_client_mount,
)
from modal_proto import api_pb2


def publish_client_mount(client):
    mount = create_client_mount()
    name = client_mount_name()
    profile_environment = config.get("environment")
    # TODO: change how namespaces work, so we don't have to use unrelated workspaces when deploying to global?.
    mount._deploy(
        client_mount_name(),
        api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
        client=client,
        environment_name=profile_environment,
    )
    print(f"✅ Deployed client mount {name} to global namespace.")


def main(client=None):
    publish_client_mount(client)


if __name__ == "__main__":
    main()


================================================
File: modal_global_objects/mounts/python_standalone.py
================================================
# Copyright Modal Labs 2022
import shutil
import tempfile
import urllib.request

from modal.config import config
from modal.exception import NotFoundError
from modal.mount import (
    PYTHON_STANDALONE_VERSIONS,
    Mount,
    python_standalone_mount_name,
)
from modal_proto import api_pb2


def publish_python_standalone_mount(client, version: str) -> None:
    release, full_version = PYTHON_STANDALONE_VERSIONS[version]

    libc = "gnu"
    arch = "x86_64_v3"
    url = (
        "https://github.com/indygreg/python-build-standalone/releases/download"
        + f"/{release}/cpython-{full_version}+{release}-{arch}-unknown-linux-gnu-install_only.tar.gz"
    )

    profile_environment = config.get("environment")
    mount_name = python_standalone_mount_name(f"{version}-{libc}")
    try:
        Mount.from_name(mount_name, namespace=api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL).hydrate(client)
        print(f"✅ Found existing mount {mount_name} in global namespace.")
    except NotFoundError:
        print(f"📦 Unpacking python-build-standalone for {version}-{libc}.")
        with tempfile.TemporaryDirectory() as d:
            urllib.request.urlretrieve(url, f"{d}/cpython.tar.gz")
            shutil.unpack_archive(f"{d}/cpython.tar.gz", d)
            print(f"🌐 Downloaded and unpacked archive to {d}.")
            python_mount = Mount._from_local_dir(f"{d}/python")
            python_mount._deploy(
                mount_name,
                api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
                client=client,
                environment_name=profile_environment,
            )
            print(f"✅ Deployed mount {mount_name} to global namespace.")


def main(client=None):
    for version in PYTHON_STANDALONE_VERSIONS:
        publish_python_standalone_mount(client, version)


if __name__ == "__main__":
    main()


================================================
File: modal_proto/__init__.py
================================================
# Copyright Modal Labs 2024


================================================
File: modal_proto/options.proto
================================================
// Defines custom options used internally at Modal.
// Custom options must be in the range 50000-99999.
// Reference: https://protobuf.dev/programming-guides/proto2/#customoptions
syntax = "proto3";

import "google/protobuf/descriptor.proto";

package modal.options;

extend google.protobuf.FieldOptions {
  optional bool audit_target_attr = 50000;
}

extend google.protobuf.MethodOptions {
  optional string audit_event_name = 50000;
  optional string audit_event_description = 50001;
}


================================================
File: modal_version/__init__.py
================================================
# Copyright Modal Labs 2022
"""Specifies the `modal.__version__` number for the client package."""

from ._version_generated import build_number

# As long as we're on 0.*, all versions are published automatically
major_number = 0

# Bump this manually on breaking changes, then reset the number in _version_generated.py
minor_number = 73

# Right now, automatically increment the patch number in CI
__version__ = f"{major_number}.{minor_number}.{max(build_number, 0)}"


================================================
File: modal_version/__main__.py
================================================
# Copyright Modal Labs 2024
from . import __version__

if __name__ == "__main__":
    print(__version__)


================================================
File: modal_version/_version_generated.py
================================================
# Copyright Modal Labs 2025

# Note: Reset this value to -1 whenever you make a minor `0.X` release of the client.
build_number = 78  # git: 8254f56


================================================
File: protoc_plugin/plugin.py
================================================
#!/usr/bin/env python
# Copyright Modal Labs 2024
# built by modifying grpclib.plugin.main, see https://github.com/vmagamedov/grpclib
# original: Copyright (c) 2019  , Vladimir Magamedov
import os
import sys
from collections import deque
from collections.abc import Collection, Iterator
from contextlib import contextmanager
from pathlib import Path
from typing import Any, Deque, NamedTuple, Optional

from google.protobuf.compiler.plugin_pb2 import CodeGeneratorRequest, CodeGeneratorResponse
from google.protobuf.descriptor_pb2 import DescriptorProto, FileDescriptorProto
from grpclib import const

_CARDINALITY = {
    (False, False): const.Cardinality.UNARY_UNARY,
    (True, False): const.Cardinality.STREAM_UNARY,
    (False, True): const.Cardinality.UNARY_STREAM,
    (True, True): const.Cardinality.STREAM_STREAM,
}


class Method(NamedTuple):
    name: str
    cardinality: const.Cardinality
    request_type: str
    reply_type: str


class Service(NamedTuple):
    name: str
    methods: list[Method]


class Buffer:
    def __init__(self) -> None:
        self._lines: list[str] = []
        self._indent = 0

    def add(self, string: str, *args: Any, **kwargs: Any) -> None:
        line = " " * self._indent * 4 + string.format(*args, **kwargs)
        self._lines.append(line.rstrip(" "))

    @contextmanager
    def indent(self) -> Iterator[None]:
        self._indent += 1
        try:
            yield
        finally:
            self._indent -= 1

    def content(self) -> str:
        return "\n".join(self._lines) + "\n"


def render(
    proto_file: str,
    imports: Collection[str],
    services: Collection[Service],
    grpclib_module: str,
) -> str:
    buf = Buffer()
    buf.add("# Generated by the Modal Protocol Buffers compiler. DO NOT EDIT!")
    buf.add("# source: {}", proto_file)
    buf.add("# plugin: {}", __name__)
    if not services:
        return buf.content()

    buf.add("")
    for mod in imports:
        buf.add("import {}", mod)

    buf.add("import typing")
    buf.add("if typing.TYPE_CHECKING:")
    with buf.indent():
        buf.add("import modal.client")

    for service in services:
        buf.add("")
        buf.add("")
        grpclib_stub_name = f"{service.name}Stub"
        buf.add("class {}Modal:", service.name)
        with buf.indent():
            buf.add("")
            buf.add(
                f"def __init__(self, grpclib_stub: {grpclib_module}.{grpclib_stub_name}, "
                + """client: "modal.client._Client") -> None:"""
            )
            with buf.indent():
                if len(service.methods) == 0:
                    buf.add("pass")
                for method in service.methods:
                    name, cardinality, request_type, reply_type = method
                    wrapper_cls: str
                    if cardinality is const.Cardinality.UNARY_UNARY:
                        wrapper_cls = "modal.client.UnaryUnaryWrapper"
                    elif cardinality is const.Cardinality.UNARY_STREAM:
                        wrapper_cls = "modal.client.UnaryStreamWrapper"
                    # elif cardinality is const.Cardinality.STREAM_UNARY:
                    #     wrapper_cls = StreamUnaryWrapper
                    # elif cardinality is const.Cardinality.STREAM_STREAM:
                    #     wrapper_cls = StreamStreamWrapper
                    else:
                        raise TypeError(cardinality)

                    original_method = f"grpclib_stub.{name}"
                    buf.add(f"self.{name} = {wrapper_cls}({original_method}, client)")

    return buf.content()


def _get_proto(request: CodeGeneratorRequest, name: str) -> FileDescriptorProto:
    return next(f for f in request.proto_file if f.name == name)


def _strip_proto(proto_file_path: str) -> str:
    for suffix in [".protodevel", ".proto"]:
        if proto_file_path.endswith(suffix):
            return proto_file_path[: -len(suffix)]

    return proto_file_path


def _base_module_name(proto_file_path: str) -> str:
    basename = _strip_proto(proto_file_path)
    return basename.replace("-", "_").replace("/", ".")


def _proto2pb2_module_name(proto_file_path: str) -> str:
    return _base_module_name(proto_file_path) + "_pb2"


def _proto2grpc_module_name(proto_file_path: str) -> str:
    return _base_module_name(proto_file_path) + "_grpc"


def _type_names(
    proto_file: FileDescriptorProto,
    message_type: DescriptorProto,
    parents: Optional[Deque[str]] = None,
) -> Iterator[tuple[str, str]]:
    if parents is None:
        parents = deque()

    proto_name_parts = [""]
    if proto_file.package:
        proto_name_parts.append(proto_file.package)
    proto_name_parts.extend(parents)
    proto_name_parts.append(message_type.name)

    py_name_parts = [_proto2pb2_module_name(proto_file.name)]
    py_name_parts.extend(parents)
    py_name_parts.append(message_type.name)

    yield ".".join(proto_name_parts), ".".join(py_name_parts)

    parents.append(message_type.name)
    for nested in message_type.nested_type:
        yield from _type_names(proto_file, nested, parents=parents)
    parents.pop()


def main() -> None:
    with os.fdopen(sys.stdin.fileno(), "rb") as inp:
        request = CodeGeneratorRequest.FromString(inp.read())

    types_map: dict[str, str] = {}
    for pf in request.proto_file:
        for mt in pf.message_type:
            types_map.update(_type_names(pf, mt))

    response = CodeGeneratorResponse()

    # See https://github.com/protocolbuffers/protobuf/blob/v3.12.0/docs/implementing_proto3_presence.md  # noqa
    if hasattr(CodeGeneratorResponse, "Feature"):
        response.supported_features = CodeGeneratorResponse.FEATURE_PROTO3_OPTIONAL

    for file_to_generate in request.file_to_generate:
        proto_file = _get_proto(request, file_to_generate)
        module_name = _proto2grpc_module_name(file_to_generate)
        grpclib_module_path = Path(module_name.replace(".", "/") + ".py")

        imports = ["modal._utils.grpc_utils", module_name]

        services = []
        for service in proto_file.service:
            methods = []
            for method in service.method:
                cardinality = _CARDINALITY[(method.client_streaming, method.server_streaming)]
                methods.append(
                    Method(
                        name=method.name,
                        cardinality=cardinality,
                        request_type=types_map[method.input_type],
                        reply_type=types_map[method.output_type],
                    )
                )
            services.append(Service(name=service.name, methods=methods))

        file = response.file.add()

        file.name = str(grpclib_module_path.with_name("modal_" + grpclib_module_path.name))
        file.content = render(
            proto_file=proto_file.name, imports=imports, services=services, grpclib_module=module_name
        )

    with os.fdopen(sys.stdout.fileno(), "wb") as out:
        out.write(response.SerializeToString())


if __name__ == "__main__":
    main()


================================================
File: test/__init__.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/aio_test.py
================================================
# Copyright Modal Labs 2023
import pytest


@pytest.mark.asyncio
async def test_new(servicer, client):
    from modal import App

    app = App()

    async with app.run(client=client):
        pass


================================================
File: test/app_composition_test.py
================================================
# Copyright Modal Labs 2024
from test.helpers import deploy_app_externally


def test_app_composition_includes_all_functions(servicer, credentials, supports_dir, monkeypatch, client):
    print(deploy_app_externally(servicer, credentials, "multifile_project.main", cwd=supports_dir))
    assert servicer.n_functions == 5
    assert {
        "/root/multifile_project/__init__.py",
        "/root/multifile_project/main.py",
        "/root/multifile_project/a.py",
        "/root/multifile_project/b.py",
        "/root/multifile_project/c.py",
    } == set(servicer.files_name2sha.keys())
    assert len(servicer.secrets) == 1  # secret from B should be included
    assert servicer.n_mounts == 1  # mounts should not be duplicated, and the automount for the package includes all


================================================
File: test/app_test.py
================================================
# Copyright Modal Labs 2022
import asyncio
import logging
import pytest
import time

from grpclib import GRPCError, Status

from modal import App, Image, Mount, Secret, Stub, Volume, enable_output, web_endpoint
from modal._partial_function import _parse_custom_domains
from modal._utils.async_utils import synchronizer
from modal.exception import DeprecationError, ExecutionError, InvalidError, NotFoundError
from modal.runner import deploy_app, deploy_stub, run_app
from modal_proto import api_pb2

from .supports import module_1, module_2


def square(x):
    return x**2


@pytest.mark.asyncio
async def test_redeploy(servicer, client):
    app = App(image=Image.debian_slim().pip_install("pandas"))
    app.function()(square)

    # Deploy app
    res = await deploy_app.aio(app, "my-app", client=client)
    assert res.app_id == "ap-1"
    assert servicer.app_objects["ap-1"]["square"] == "fu-1"
    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]

    # Redeploy, make sure all ids are the same
    res = await deploy_app.aio(app, "my-app", client=client)
    assert res.app_id == "ap-1"
    assert servicer.app_objects["ap-1"]["square"] == "fu-1"
    assert servicer.app_state_history[res.app_id] == [
        api_pb2.APP_STATE_INITIALIZING,
        api_pb2.APP_STATE_DEPLOYED,
        api_pb2.APP_STATE_DEPLOYED,
    ]

    # Deploy to a different name, ids should change
    res = await deploy_app.aio(app, "my-app-xyz", client=client)
    assert res.app_id == "ap-2"
    assert servicer.app_objects["ap-2"]["square"] == "fu-2"
    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]


def dummy():
    pass


# Should exit without waiting for the "logs_timeout" grace period.
@pytest.mark.timeout(5)
def test_create_object_internal_exception(servicer, client):
    servicer.function_create_error = GRPCError(Status.INTERNAL, "Function create failed")

    app = App()
    app.function()(dummy)

    with servicer.intercept() as ctx:
        with pytest.raises(GRPCError) as excinfo:
            with enable_output():  # this activates the log streaming loop, which could potentially hold up context exit
                with app.run(client=client):
                    pass

    assert len(ctx.get_requests("FunctionCreate")) == 4  # some retries are applied to internal errors
    assert excinfo.value.status == Status.INTERNAL
    assert len(ctx.get_requests("AppClientDisconnect")) == 1


@pytest.mark.timeout(5)
def test_create_object_invalid_exception(servicer, client):
    servicer.function_create_error = GRPCError(Status.INVALID_ARGUMENT, "something was invalid")

    app = App()
    app.function()(dummy)

    with servicer.intercept() as ctx:
        with pytest.raises(InvalidError, match="something was invalid"):  # error should be converted
            with enable_output():  # this activates the log streaming loop, which could potentially hold up context exit
                with app.run(client=client):
                    pass
    assert len(ctx.get_requests("FunctionCreate")) == 1  # no retries on an invalid request
    assert len(ctx.get_requests("AppClientDisconnect")) == 1


def test_deploy_falls_back_to_app_name(servicer, client):
    named_app = App(name="foo_app")
    deploy_app(named_app, client=client)
    assert "foo_app" in servicer.deployed_apps


def test_deploy_uses_deployment_name_if_specified(servicer, client):
    named_app = App(name="foo_app")
    deploy_app(named_app, "bar_app", client=client)
    assert "bar_app" in servicer.deployed_apps
    assert "foo_app" not in servicer.deployed_apps


def test_run_function_without_app_error():
    app = App()
    dummy_modal = app.function()(dummy)

    with pytest.raises(ExecutionError) as excinfo:
        dummy_modal.remote()

    assert "hydrated" in str(excinfo.value)


def test_missing_attr():
    """Trying to call a non-existent function on the App should produce
    an understandable error message."""

    app = App()
    with pytest.raises(AttributeError):
        app.fun()  # type: ignore


def test_same_function_name(caplog):
    app = App()

    # Add first function
    with caplog.at_level(logging.WARNING):
        app.function()(module_1.square)
    assert len(caplog.records) == 0

    # Add second function: check warning
    with caplog.at_level(logging.WARNING):
        app.function()(module_2.square)
    assert len(caplog.records) == 1
    assert "module_1" in caplog.text
    assert "module_2" in caplog.text
    assert "square" in caplog.text


def test_run_state(client, servicer):
    app = App()
    with app.run(client=client):
        assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_EPHEMERAL]


def test_deploy_state(client, servicer):
    app = App()
    res = deploy_app(app, "foobar", client=client)
    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]


def test_detach_state(client, servicer):
    app = App()
    with app.run(client=client, detach=True):
        assert servicer.app_state_history[app.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DETACHED]


@pytest.mark.asyncio
async def test_grpc_protocol(client, servicer):
    app = App()
    async with app.run(client=client):
        await asyncio.sleep(0.01)  # wait for heartbeat
    assert len(servicer.requests) == 3
    assert isinstance(servicer.requests[0], api_pb2.AppCreateRequest)
    assert isinstance(servicer.requests[1], api_pb2.AppHeartbeatRequest)
    assert isinstance(servicer.requests[2], api_pb2.AppClientDisconnectRequest)


async def web1(x):
    return {"square": x**2}


async def web2(x):
    return {"cube": x**3}


def test_registered_web_endpoints(client, servicer):
    app = App()
    app.function()(square)
    app.function()(web_endpoint()(web1))
    app.function()(web_endpoint()(web2))

    @app.cls(serialized=True)
    class Cls:
        @web_endpoint()
        def cls_web_endpoint(self):
            pass

    assert app.registered_web_endpoints == ["web1", "web2", "Cls.cls_web_endpoint"]


def test_init_types():
    with pytest.raises(InvalidError):
        # singular secret to plural argument
        App(secrets=Secret.from_dict())  # type: ignore
    with pytest.raises(InvalidError):
        # not a Secret Object
        App(secrets=[{"foo": "bar"}])  # type: ignore
    with pytest.raises(InvalidError):
        # should be an Image
        App(image=Secret.from_dict())  # type: ignore

    App(
        image=Image.debian_slim().pip_install("pandas"),
        secrets=[Secret.from_dict()],
        mounts=[Mount._from_local_file(__file__)],  # TODO: remove
    )


def test_set_image_on_app_as_attribute():
    # TODO: do we want to deprecate this syntax? It's kind of random for image to
    #     have a reserved name in the blueprint, and being the only of the construction
    #     arguments that can be set on the instance after construction
    custom_img = Image.debian_slim().apt_install("emacs")
    app = App(image=custom_img)
    assert app._get_default_image() == custom_img


def test_redeploy_delete_objects(servicer, client):
    # Deploy an app with objects d1 and d2
    app = App()
    app.function(name="d1")(dummy)
    app.function(name="d2")(dummy)
    res = deploy_app(app, "xyz", client=client)

    # Check objects
    assert set(servicer.app_objects[res.app_id].keys()) == {"d1", "d2"}

    # Deploy an app with objects d2 and d3
    app = App()
    app.function(name="d2")(dummy)
    app.function(name="d3")(dummy)
    res = deploy_app(app, "xyz", client=client)

    # Make sure d1 is deleted
    assert set(servicer.app_objects[res.app_id].keys()) == {"d2", "d3"}


@pytest.mark.asyncio
async def test_unhydrate(servicer, client):
    app = App()

    f = app.function()(dummy)

    assert not f.is_hydrated
    async with app.run(client=client):
        assert f.is_hydrated

    # After app finishes, it should unhydrate
    assert not f.is_hydrated


def test_keyboard_interrupt(servicer, client):
    app = App()
    app.function()(square)
    with app.run(client=client):
        # The exit handler should catch this interrupt and exit gracefully
        raise KeyboardInterrupt()


def test_function_image_positional():
    app = App()
    image = Image.debian_slim()

    with pytest.raises(InvalidError) as excinfo:

        @app.function(image)  # type: ignore
        def f():
            pass

    assert "function(image=image)" in str(excinfo.value)


def test_function_decorator_on_class():
    app = App()
    with pytest.raises(TypeError, match="cannot be used on a class"):

        @app.function()
        class Foo:
            pass


@pytest.mark.asyncio
async def test_deploy_disconnect(servicer, client):
    app = App()
    app.function(secrets=[Secret.from_name("nonexistent-secret")])(square)

    with pytest.raises(NotFoundError):
        await deploy_app.aio(app, "my-app", client=client)

    assert servicer.app_state_history["ap-1"] == [
        api_pb2.APP_STATE_INITIALIZING,
        api_pb2.APP_STATE_STOPPED,
    ]


def test_parse_custom_domains():
    assert len(_parse_custom_domains(None)) == 0
    assert len(_parse_custom_domains(["foo.com", "bar.com"])) == 2
    with pytest.raises(AssertionError):
        assert _parse_custom_domains("foo.com")


def test_hydrated_other_app_object_gets_referenced(servicer, client):
    app = App("my-app")
    with servicer.intercept() as ctx:
        with Volume.ephemeral(client=client) as vol:
            app.function(volumes={"/vol": vol})(dummy)  # implicitly load vol
            deploy_app(app, client=client)
            function_create_req: api_pb2.FunctionCreateRequest = ctx.pop_request("FunctionCreate")
            assert vol.object_id in {obj.object_id for obj in function_create_req.function.object_dependencies}


def test_hasattr():
    app = App()
    assert not hasattr(app, "xyz")


def test_app(client):
    app = App()
    square_modal = app.function()(square)

    with app.run(client=client):
        square_modal.remote(42)


def test_non_string_app_name():
    with pytest.raises(InvalidError, match="Must be string"):
        App(Image.debian_slim())  # type: ignore


def test_function_named_app():
    # Make sure we have a helpful warning when a user's function is named "app"
    # as it might collide with the App variable name (in particular if people
    # find & replace "stub" with "app").
    app = App()

    with pytest.warns(match="app"):

        @app.function(serialized=True)
        def app(): ...


def test_stub():
    with pytest.warns(match="App"):
        Stub()


def test_deploy_stub(servicer, client):
    app = App("xyz")
    deploy_app(app, client=client)
    with pytest.raises(DeprecationError, match="deploy_app"):
        deploy_stub(app, client=client)


def test_app_logs(servicer, client):
    app = App()
    f = app.function()(dummy)

    with app.run(client=client):
        f.remote()

    logs = [data for data in app._logs(client=client)]
    assert logs == ["hello, world (1)\n"]


def test_app_interactive(servicer, client, capsys):
    app = App()

    async def app_logs_pty(servicer, stream):
        await stream.recv_message()

        # Enable PTY
        await stream.send_message(api_pb2.TaskLogsBatch(pty_exec_id="ta-123"))

        # Send some data (should be written raw to stdout)
        log = api_pb2.TaskLogs(data="some data\n", file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT)
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="xyz", items=[log]))

        # Send an EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True, task_id="ta-123"))

        # Terminate app
        await stream.send_message(api_pb2.TaskLogsBatch(app_done=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("AppGetLogs", app_logs_pty)

        with enable_output():
            with app.run(client=client):
                time.sleep(0.1)

    captured = capsys.readouterr()
    assert captured.out.endswith("\nsome data\n\r")


def test_app_interactive_no_output(servicer, client):
    app = App()

    with pytest.warns(match="Interactive mode is disabled because no output manager is active"):
        with app.run(client=client, interactive=True):
            # Verify that interactive mode was disabled
            assert not app.is_interactive


def test_show_progress_deprecations(client, monkeypatch):
    app = App()

    # If show_progress is set to True, raise that this is deprecated
    with pytest.raises(DeprecationError, match="enable_output"):
        with app.run(client=client, show_progress=True):
            pass

    # If show_progress is set to False, warn that this has no effect
    with pytest.warns(DeprecationError, match="no effect"):
        with app.run(client=client, show_progress=False):
            pass


@pytest.mark.asyncio
async def test_deploy_from_container(servicer, container_client):
    app = App(image=Image.debian_slim().pip_install("pandas"))
    app.function()(square)

    # Deploy app
    res = await deploy_app.aio(app, "my-app", client=container_client)
    assert res.app_id == "ap-1"
    assert servicer.app_objects["ap-1"]["square"] == "fu-1"
    assert servicer.app_state_history[res.app_id] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]


def test_app_create_bad_environment_name_error(client):
    environment_name = "this=is@not.allowed"
    app = App()
    with pytest.raises(InvalidError, match="Invalid Environment name"):
        with run_app(
            app, environment_name=environment_name, client=client
        ):  # TODO: why isn't environment_name an argument to app.run?
            pass

    assert len(asyncio.all_tasks(synchronizer._loop)) == 1  # no trailing tasks, except the `loop_inner` ever-task


def test_overriding_function_warning(caplog):
    app = App()

    @app.function(serialized=True)
    def func():  # type: ignore
        return 1

    assert len(caplog.messages) == 0

    app_2 = App()
    app_2.include(app)

    assert len(caplog.messages) == 0

    app_3 = App()

    app_3.include(app)
    app_3.include(app_2)

    assert len(caplog.messages) == 0

    app_4 = App()

    @app_4.function(serialized=True)  # type: ignore
    def func():  # noqa: F811
        return 2

    assert len(caplog.messages) == 0

    app_3.include(app_4)
    assert "Overriding existing function" in caplog.messages[0]


@pytest.mark.parametrize("name", ["", " ", "no way", "my-app!", "a" * 65])
def test_lookup_invalid_name(name):
    with pytest.raises(InvalidError, match="Invalid App name"):
        App.lookup(name)


================================================
File: test/asgi_wrapper_test.py
================================================
# Copyright Modal Labs 2024
import asyncio
import pytest

import fastapi
from starlette.requests import ClientDisconnect

from modal._runtime.asgi import asgi_app_wrapper
from modal._runtime.execution_context import _set_current_context_ids


class DummyException(Exception):
    pass


app = fastapi.FastAPI()


@app.get("/")
def sync_index():
    return {"some_result": "foo"}


@app.get("/error")
def sync_error():
    raise DummyException()


@app.post("/async_reading_body")
async def async_index_reading_body(req: fastapi.Request):
    body = await req.body()
    return {"some_result": body}


@app.get("/async_error")
async def async_error():
    raise DummyException()


@app.get("/streaming_response")
async def streaming_response():
    from fastapi.responses import StreamingResponse

    async def stream_bytes():
        yield b"foo"
        yield b"bar"

    return StreamingResponse(stream_bytes())


def _asgi_get_scope(path, method="GET"):
    return {
        "type": "http",
        "method": method,
        "path": path,
        "query_string": "",
        "headers": [],
    }


class MockIOManager:
    class get_data_in:
        @staticmethod
        async def aio(_function_call_id):
            yield {"type": "http.request", "body": b"some_body"}
            await asyncio.sleep(10)


@pytest.mark.asyncio
@pytest.mark.timeout(1)
async def test_success():
    mock_manager = MockIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, mock_manager)
    asgi_scope = _asgi_get_scope("/")
    outputs = [output async for output in wrapped_app(asgi_scope)]
    assert len(outputs) == 2
    before_body = outputs[0]
    assert before_body["status"] == 200
    assert before_body["type"] == "http.response.start"
    body = outputs[1]
    assert body["body"] == b'{"some_result":"foo"}'
    assert body["type"] == "http.response.body"


@pytest.mark.asyncio
@pytest.mark.parametrize("endpoint_url", ["/error", "/async_error"])
@pytest.mark.timeout(1)
async def test_endpoint_exception(endpoint_url):
    mock_manager = MockIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, mock_manager)
    asgi_scope = _asgi_get_scope(endpoint_url)
    outputs = []

    with pytest.raises(DummyException):
        async for output in wrapped_app(asgi_scope):
            outputs.append(output)

    assert len(outputs) == 2
    before_body = outputs[0]
    assert before_body["status"] == 500
    assert before_body["type"] == "http.response.start"
    body = outputs[1]
    assert body["body"] == b"Internal Server Error"
    assert body["type"] == "http.response.body"


class BrokenIOManager:
    class get_data_in:
        @staticmethod
        async def aio(_function_call_id):
            raise DummyException("error while fetching data")
            yield  # noqa (makes this a generator)


@pytest.mark.asyncio
@pytest.mark.timeout(1)
async def test_broken_io_unused(caplog):
    # if IO channel breaks, but the endpoint doesn't actually use
    # any of the body data, it should be allowed to output its data
    # and not raise an exception - but print a warning since it's unexpected
    mock_manager = BrokenIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, mock_manager)
    asgi_scope = _asgi_get_scope("/")
    outputs = []

    async for output in wrapped_app(asgi_scope):
        outputs.append(output)

    assert len(outputs) == 2
    assert outputs[0]["status"] == 200
    assert outputs[1]["body"] == b'{"some_result":"foo"}'
    assert "Internal error" in caplog.text
    assert "DummyException: error while fetching data" in caplog.text


@pytest.mark.asyncio
@pytest.mark.timeout(10)
async def test_broken_io_used():
    mock_manager = BrokenIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, mock_manager)
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")
    outputs = []
    with pytest.raises(ClientDisconnect):
        async for output in wrapped_app(asgi_scope):
            outputs.append(output)

    assert len(outputs) == 2
    assert outputs[0]["status"] == 500


class SlowIOManager:
    class get_data_in:
        @staticmethod
        async def aio(_function_call_id):
            await asyncio.sleep(5)
            yield  # makes this an async generator


@pytest.mark.asyncio
@pytest.mark.timeout(2)
async def test_first_message_timeout(monkeypatch):
    monkeypatch.setattr("modal._runtime.asgi.FIRST_MESSAGE_TIMEOUT_SECONDS", 0.1)  # simulate timeout
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, SlowIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")
    outputs = []
    with pytest.raises(ClientDisconnect):
        async for output in wrapped_app(asgi_scope):
            outputs.append(output)

    assert outputs[0]["status"] == 502
    assert b"Missing request" in outputs[1]["body"]


@pytest.mark.asyncio
async def test_cancellation_cleanup(caplog):
    # this test mostly exists to get some coverage on the cancellation/error paths and
    # ensure nothing unexpected happens there
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, SlowIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")
    outputs = []

    async def app_runner():
        async for output in wrapped_app(asgi_scope):
            outputs.append(output)

    app_runner_task = asyncio.create_task(app_runner())
    await asyncio.sleep(0.1)  # let it get started
    app_runner_task.cancel()
    await asyncio.sleep(0.1)  # let it shut down
    assert len(outputs) == 0
    assert caplog.text == ""  # make sure there are no junk traces about dangling tasks etc.


@pytest.mark.asyncio
async def test_streaming_response():
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(app, SlowIOManager())
    asgi_scope = _asgi_get_scope("/streaming_response", "GET")
    outputs = []
    async for output in wrapped_app(asgi_scope):
        outputs.append(output)
    assert outputs == [
        {"headers": [], "status": 200, "type": "http.response.start"},
        {"body": b"foo", "more_body": True, "type": "http.response.body"},
        {"body": b"bar", "more_body": True, "type": "http.response.body"},
        {"body": b"", "more_body": False, "type": "http.response.body"},
    ]


class StreamingIOManager:
    class get_data_in:
        @staticmethod
        async def aio(_function_call_id):
            yield {"type": "http.request", "body": b"foo", "more_body": True}
            yield {"type": "http.request", "body": b"bar", "more_body": True}
            yield {"type": "http.request", "body": b"baz", "more_body": False}
            yield {"type": "http.request", "body": b"this should not be read", "more_body": False}


@pytest.mark.asyncio
async def test_streaming_body():
    _set_current_context_ids(["in-123"], ["fc-123"])

    wrapped_app, lifespan_manager = asgi_app_wrapper(app, StreamingIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")
    outputs = []
    async for output in wrapped_app(asgi_scope):
        outputs.append(output)
    assert outputs[1] == {"type": "http.response.body", "body": b'{"some_result":"foobarbaz"}'}


@pytest.mark.asyncio
async def test_cancellation_while_waiting_for_first_input():
    # due to an asyncio edge case of cancellation + wait_for(future) resolution there
    # are scenarios in which an asgi task cancellation doesn't actually stop the underlying
    # fetch_data_in task, causing either warnings on shutdown or even infinite stalling on
    # shutdown.
    _set_current_context_ids(["in-123"], ["fc-123"])
    fut: asyncio.Future[None] = asyncio.Future()

    class StreamingIOManager:
        class get_data_in:
            @staticmethod
            async def aio(_function_call_id):
                await fut  # we never resolve this, unlike in test_cancellation_first_message_race_cleanup
                yield

    wrapped_app, _ = asgi_app_wrapper(app, StreamingIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")

    first_app_output = asyncio.create_task(wrapped_app(asgi_scope).__anext__())  # type: ignore
    await asyncio.sleep(0.1)  # ensure we are in wait_for(first_message_task)
    first_app_output.cancel()
    await asyncio.sleep(0.1)  # resume event loop to resolve tasks if possible
    remaining_tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
    assert len(remaining_tasks) == 0


@pytest.mark.asyncio
async def test_cancellation_when_first_input_arrives():
    # due to an asyncio edge case of cancellation + wait_for(future) resolution there
    # are scenarios in which an asgi task cancellation doesn't actually stop the underlying
    # fetch_data_in task, causing either warnings on shutdown or even infinite stalling on
    # shutdown.
    _set_current_context_ids(["in-123"], ["fc-123"])
    fut: asyncio.Future[None] = asyncio.Future()

    class StreamingIOManager:
        class get_data_in:
            @staticmethod
            async def aio(_function_call_id):
                await fut
                yield {"type": "http.request", "body": b"foo", "more_body": True}
                while 1:
                    yield  # simulate infinite stream

    wrapped_app, _ = asgi_app_wrapper(app, StreamingIOManager())
    asgi_scope = _asgi_get_scope("/async_reading_body", "POST")

    first_app_output = asyncio.create_task(wrapped_app(asgi_scope).__anext__())  # type: ignore
    await asyncio.sleep(0.1)  # ensure we are in wait_for(first_message_task)
    # now lets unblock get_data_in, supplying a request to the waiting asgi app
    # fut.set_result(None)
    # but at the same time, before we resume the event loop, we cancel the full input task
    fut.set_result(None)
    first_app_output.cancel()
    await asyncio.sleep(0.1)  # resume event loop to resolve tasks if possible
    remaining_tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
    assert len(remaining_tasks) == 0


@pytest.mark.asyncio
async def test_lifespan_supported():
    lifespan_startup_complete = False
    lifespan_shutdown_complete = False

    async def asgi_app(scope, receive, send):
        if scope["type"] == "lifespan":
            while True:
                message = await receive()
                if message["type"] == "lifespan.startup":
                    nonlocal lifespan_startup_complete
                    lifespan_startup_complete = True
                    await send({"type": "lifespan.startup.complete"})
                elif message["type"] == "lifespan.shutdown":
                    nonlocal lifespan_shutdown_complete
                    lifespan_shutdown_complete = True
                    await send({"type": "lifespan.shutdown.complete"})
                    return
        else:
            await send({"type": "http.response.start", "status": 200})
            await send({"type": "http.response.body", "body": b'{"some_result":"foo"}'})

    mock_manager = MockIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(asgi_app, mock_manager)

    asyncio.create_task(lifespan_manager.background_task())
    await lifespan_manager.lifespan_startup()

    assert lifespan_startup_complete

    asgi_scope = _asgi_get_scope("/")
    outputs = [output async for output in wrapped_app(asgi_scope)]
    assert len(outputs) == 2
    before_body = outputs[0]
    assert before_body["status"] == 200
    assert before_body["type"] == "http.response.start"
    body = outputs[1]
    assert body["body"] == b'{"some_result":"foo"}'
    assert body["type"] == "http.response.body"

    await lifespan_manager.lifespan_shutdown()

    assert lifespan_shutdown_complete

    assert lifespan_manager._lifespan_supported


@pytest.mark.asyncio
async def test_lifespan_unsupported():
    async def asgi_app(scope, receive, send):
        if scope["type"] == "lifespan":
            raise Exception("broken lifespan scope handler")
        else:
            await send({"type": "http.response.start", "status": 200})
            await send({"type": "http.response.body", "body": b'{"some_result":"foo"}'})

    mock_manager = MockIOManager()
    _set_current_context_ids(["in-123"], ["fc-123"])
    wrapped_app, lifespan_manager = asgi_app_wrapper(asgi_app, mock_manager)

    # Failing lifespan should not affect the app
    asyncio.create_task(lifespan_manager.background_task())
    await lifespan_manager.lifespan_startup()

    # works with stuff after
    asgi_scope = _asgi_get_scope("/")
    outputs = [output async for output in wrapped_app(asgi_scope)]
    assert len(outputs) == 2
    before_body = outputs[0]
    assert before_body["status"] == 200
    assert before_body["type"] == "http.response.start"
    body = outputs[1]
    assert body["body"] == b'{"some_result":"foo"}'
    assert body["type"] == "http.response.body"

    await lifespan_manager.lifespan_shutdown()

    assert not lifespan_manager._lifespan_supported


================================================
File: test/async_utils_test.py
================================================
# Copyright Modal Labs 2022
import asyncio
import functools
import logging
import os
import platform
import pytest
import subprocess
import sys
import textwrap

import pytest_asyncio
from synchronicity import Synchronizer

from modal._utils import async_utils
from modal._utils.async_utils import (
    TaskContext,
    aclosing,
    async_chain,
    async_map,
    async_map_ordered,
    async_merge,
    async_zip,
    callable_to_agen,
    queue_batch_iterator,
    retry,
    sync_or_async_iter,
    synchronize_api,
    warn_if_generator_is_not_consumed,
)
from test import helpers


@pytest_asyncio.fixture(autouse=True)
async def no_dangling_tasks():
    yield
    assert not asyncio.all_tasks() - {asyncio.tasks.current_task()}


skip_github_non_linux = pytest.mark.skipif(
    (os.environ.get("GITHUB_ACTIONS") == "true" and platform.system() != "Linux"),
    reason="sleep is inaccurate on GitHub Actions runners.",
)


class SampleException(Exception):
    pass


class FailNTimes:
    def __init__(self, n_failures, exc=SampleException("Something bad happened")):
        self.n_failures = n_failures
        self.n_calls = 0
        self.exc = exc

    async def __call__(self, x):
        self.n_calls += 1
        if self.n_calls <= self.n_failures:
            raise self.exc
        else:
            return x + 1


@pytest.mark.asyncio
async def test_retry():
    f_retry = retry(FailNTimes(2))
    assert await f_retry(42) == 43

    with pytest.raises(SampleException):
        f_retry = retry(FailNTimes(3))
        assert await f_retry(42) == 43

    f_retry = retry(n_attempts=5)(FailNTimes(4))
    assert await f_retry(42) == 43

    with pytest.raises(SampleException):
        f_retry = retry(n_attempts=5)(FailNTimes(5))
        assert await f_retry(42) == 43


@pytest.mark.asyncio
async def test_task_context():
    async with TaskContext() as task_context:
        t = task_context.create_task(asyncio.sleep(0.1))
        assert not t.done()
        # await asyncio.sleep(0.0)
    await asyncio.sleep(0.0)  # just waste a loop step for the cancellation to go through
    assert t.cancelled()


@pytest.mark.asyncio
async def test_task_context_grace():
    async with TaskContext(grace=0.2) as task_context:
        u = task_context.create_task(asyncio.sleep(0.1))
        v = task_context.create_task(asyncio.sleep(0.3))
        assert not u.done()
        assert not v.done()
    await asyncio.sleep(0.0)
    assert u.done()
    assert v.cancelled()


@skip_github_non_linux
@pytest.mark.asyncio
async def test_task_context_infinite_loop():
    async with TaskContext(grace=0.01) as task_context:
        counter = 0

        async def f():
            nonlocal counter
            counter += 1

        t = task_context.infinite_loop(f, sleep=0.1)
        assert not t.done()
        await asyncio.sleep(0.35)
        assert counter == 4  # at 0.00, 0.10, 0.20, 0.30
    await asyncio.sleep(0.0)  # just waste a loop step for the cancellation to go through
    assert not t.cancelled()
    assert t.done()
    assert counter == 4  # should be exited immediately


@skip_github_non_linux
@pytest.mark.asyncio
async def test_task_context_infinite_loop_non_functions():
    async with TaskContext(grace=0.01) as task_context:

        async def f(x):
            pass

        task_context.infinite_loop(lambda: f(123))
        task_context.infinite_loop(functools.partial(f, 123))


@skip_github_non_linux
@pytest.mark.asyncio
async def test_task_context_infinite_loop_timeout(caplog):
    async with TaskContext(grace=0.01) as task_context:

        async def f():
            await asyncio.sleep(5.0)

        task_context.infinite_loop(f, timeout=0.1)
        await asyncio.sleep(0.15)

    # TODO(elias): Find the tests that leak `Task was destroyed but it is pending` warnings into this test
    # so we can assert a single record here:
    # assert len(caplog.records) == 1
    for record in caplog.records:
        if "timed out" in caplog.text:
            break
    else:
        assert False, "no timeout"


@pytest.mark.asyncio
async def test_task_context_gather():
    state = "none"

    async def t1(error=False):
        nonlocal state
        await asyncio.sleep(0.1)
        state = "t1"
        if error:
            raise ValueError()

    async def t2():
        nonlocal state
        await asyncio.sleep(0.2)
        state = "t2"

    await asyncio.gather(t1(), t2())
    assert state == "t2"

    # On t1 error: asyncio.gather() does not cancel t2, which is bad behavior.
    state = "none"
    with pytest.raises(ValueError):
        await asyncio.gather(t1(error=True), t2())
    assert state == "t1"
    await asyncio.sleep(0.2)
    assert state == "t2"  # t2 still runs because asyncio.gather() does not cancel tasks

    # On t1 error: TaskContext.gather() should cancel the remaining tasks.
    state = "none"
    with pytest.raises(ValueError):
        await TaskContext.gather(t1(error=True), t2())
    assert state == "t1"
    await asyncio.sleep(0.2)
    assert state == "t1"


DEBOUNCE_TIME = 0.1


@pytest.mark.asyncio
async def test_queue_batch_iterator():
    queue: asyncio.Queue = asyncio.Queue()
    await queue.put(1)
    drained_items = []

    async def drain_queue(logs_queue):
        async for batch in queue_batch_iterator(logs_queue, debounce_time=DEBOUNCE_TIME):
            drained_items.extend(batch)

    async with TaskContext(grace=0.0) as tc:
        tc.create_task(drain_queue(queue))

        # Make sure the queue gets drained.
        await asyncio.sleep(0.001)

        assert len(drained_items) == 1

        # Add items to the queue and a sentinel while it's still waiting for DEBOUNCE_TIME.
        await queue.put(2)
        await queue.put(3)
        await queue.put(None)

        await asyncio.sleep(DEBOUNCE_TIME + 0.001)

        assert len(drained_items) == 3


@pytest.mark.asyncio
async def test_warn_if_generator_is_not_consumed(caplog):
    @warn_if_generator_is_not_consumed()
    async def my_generator():
        yield 42

    with caplog.at_level(logging.WARNING):
        g = my_generator()
        assert "my_generator" in repr(g)
        del g  # Force destructor

    assert len(caplog.records) == 1
    assert "my_generator" in caplog.text
    assert "for" in caplog.text
    assert "list" in caplog.text


def test_warn_if_generator_is_not_consumed_sync(caplog):
    @warn_if_generator_is_not_consumed()
    def my_generator():
        yield 42

    with caplog.at_level(logging.WARNING):
        g = my_generator()
        assert "my_generator" in repr(g)
        del g  # Force destructor

    assert len(caplog.records) == 1
    assert "my_generator" in caplog.text
    assert "for" in caplog.text
    assert "list" in caplog.text


@pytest.mark.asyncio
async def test_no_warn_if_generator_is_consumed(caplog):
    @warn_if_generator_is_not_consumed()
    async def my_generator():
        yield 42

    with caplog.at_level(logging.WARNING):
        g = my_generator()
        async for _ in g:
            pass
        del g  # Force destructor

    assert len(caplog.records) == 0


def test_exit_handler():
    result = None
    sync = Synchronizer()

    async def cleanup():
        nonlocal result
        result = "bye"

    async def _setup_code():
        async_utils.on_shutdown(cleanup())

    setup_code = sync.create_blocking(_setup_code)
    setup_code()

    sync._close_loop()  # this is called on exit by synchronicity, which shuts down the event loop
    assert result == "bye"


def test_synchronize_api_blocking_name():
    class _MyClass:
        async def foo(self):
            await asyncio.sleep(0.1)
            return "bar"

    async def _myfunc():
        await asyncio.sleep(0.1)
        return "bar"

    MyClass = synchronize_api(_MyClass)
    assert MyClass.__name__ == "MyClass"
    assert MyClass().foo() == "bar"

    myfunc = synchronize_api(_myfunc)
    assert myfunc.__name__ == "myfunc"
    assert myfunc() == "bar"


@pytest.mark.asyncio
async def test_aclosing():
    result = []
    states = []

    async def foo():
        states.append("enter")
        try:
            yield 1
            yield 2
        finally:
            states.append("exit")

    # test that things are cleaned up when we fully exhaust the generator
    async with aclosing(foo()) as stream:
        async for it in stream:
            result.append(it)

    assert sorted(result) == [1, 2]
    assert states == ["enter", "exit"]

    # test that things are cleaned up when we exit the context manager without fully exhausting the generator
    states.clear()
    result.clear()
    async with aclosing(foo()) as stream:
        async for it in stream:
            break

    assert result == []
    assert states == ["enter", "exit"]


@pytest.mark.asyncio
async def test_sync_or_async_iter_sync_gen():
    result = []

    def sync_gen():
        yield 4
        yield 5
        yield 6

    async for i in sync_or_async_iter(sync_gen()):
        result.append(i)
    assert result == [4, 5, 6]


@pytest.mark.asyncio
async def test_sync_or_async_iter_async_gen():
    result = []
    states = []

    async def async_gen():
        states.append("enter")
        try:
            yield 1
            await asyncio.sleep(0.1)
            yield 2
            await asyncio.sleep(0.1)
            yield 3
        finally:
            states.append("exit")

    # test that things are cleaned up when we fully exhaust the generator
    async for i in sync_or_async_iter(async_gen()):
        result.append(i)
    assert result == [1, 2, 3]
    assert states == ["enter", "exit"]

    # test that things are cleaned up when we exit the context manager without fully exhausting the generator
    result.clear()
    states.clear()
    async with aclosing(sync_or_async_iter(async_gen())) as stream:
        async for _ in stream:
            break
    assert states == ["enter", "exit"]
    assert result == []


@pytest.mark.asyncio
async def test_async_zip():
    states = []
    result = []

    async def gen(x):
        states.append(f"enter {x}")
        try:
            await asyncio.sleep(0.1)
            yield x
            yield x + 1
        finally:
            await asyncio.sleep(0)
            states.append(f"exit {x}")

    async with aclosing(async_zip(gen(1), gen(5), gen(10))) as stream:
        async for item in stream:
            result.append(item)

    assert result == [(1, 5, 10), (2, 6, 11)]
    assert states == ["enter 1", "enter 5", "enter 10", "exit 1", "exit 5", "exit 10"]


@pytest.mark.asyncio
async def test_async_zip_different_lengths():
    states = []
    result = []

    async def gen_short():
        states.append("enter short")
        try:
            await asyncio.sleep(0.1)
            yield 1
            yield 2
        finally:
            await asyncio.sleep(0)
            states.append("exit short")

    async def gen_long():
        states.append("enter long")
        try:
            await asyncio.sleep(0.1)
            yield 3
            yield 4
            yield 5
            yield 6

        finally:
            await asyncio.sleep(0)
            states.append("exit long")

    async with aclosing(async_zip(gen_short(), gen_long())) as stream:
        async for item in stream:
            result.append(item)

    assert result == [(1, 3), (2, 4)]
    assert states == ["enter short", "enter long", "exit short", "exit long"]


@pytest.mark.asyncio
async def test_async_zip_exception():
    states = []
    result = []

    async def gen(x):
        states.append(f"enter {x}")
        try:
            await asyncio.sleep(0.1)
            yield x
            if x == 1:
                raise SampleException("test")
            yield x + 1
        finally:
            await asyncio.sleep(0)
            states.append(f"exit {x}")

    with pytest.raises(SampleException):
        async with aclosing(async_zip(gen(1), gen(5))) as stream:
            async for item in stream:
                result.append(item)

    assert result == [(1, 5)]
    assert states == ["enter 1", "enter 5", "exit 1", "exit 5"]


@pytest.mark.asyncio
async def test_async_zip_parallel():
    ev1 = asyncio.Event()
    ev2 = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        ev1.set()
        yield 1
        await ev2.wait()
        yield 2

    async def gen2():
        await ev1.wait()
        yield 3
        await asyncio.sleep(0.1)
        ev2.set()
        yield 4

    result = []
    async for item in async_zip(gen1(), gen2()):
        result.append(item)

    assert result == [(1, 3), (2, 4)]


@pytest.mark.asyncio
async def test_async_zip_cancellation():
    ev = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        await ev.wait()
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    async def zip_coro():
        async with aclosing(async_zip(gen1(), gen2())) as stream:
            async for _ in stream:
                pass

    zip_task = asyncio.create_task(zip_coro())
    await asyncio.sleep(0.1)
    zip_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await zip_task


@pytest.mark.asyncio
async def test_async_zip_producer_cancellation():
    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    await asyncio.sleep(0.1)
    with pytest.raises(asyncio.CancelledError):
        async with aclosing(async_zip(gen1(), gen2())) as stream:
            async for _ in stream:
                pass


@pytest.mark.asyncio
async def test_async_merge():
    result = []
    states = []

    ev1 = asyncio.Event()
    ev2 = asyncio.Event()

    async def gen1():
        states.append("gen1 enter")
        try:
            await asyncio.sleep(0.1)
            yield 1
            ev1.set()
            await ev2.wait()
            yield 2
        finally:
            await asyncio.sleep(0)
            states.append("gen1 exit")

    async def gen2():
        states.append("gen2 enter")
        try:
            await ev1.wait()
            yield 3
            await asyncio.sleep(0.1)
            ev2.set()
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("gen2 exit")

    async for item in async_merge(gen1(), gen2()):
        result.append(item)

    assert result == [1, 3, 4, 2]
    assert states == [
        "gen1 enter",
        "gen2 enter",
        "gen2 exit",
        "gen1 exit",
    ]


@pytest.mark.asyncio
async def test_async_merge_cleanup():
    states = []

    ev1 = asyncio.Event()
    ev2 = asyncio.Event()

    async def gen1():
        states.append("gen1 enter")
        try:
            await asyncio.sleep(0.1)
            yield 1
            ev1.set()
            await ev2.wait()
            yield 2
        finally:
            await asyncio.sleep(0)
            states.append("gen1 exit")

    async def gen2():
        states.append("gen2 enter")
        try:
            await ev1.wait()
            yield 3
            await asyncio.sleep(0.1)
            ev2.set()
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("gen2 exit")

    async with aclosing(async_merge(gen1(), gen2())) as stream:
        async for _ in stream:
            break

    assert sorted(states) == [
        "gen1 enter",
        "gen1 exit",
        "gen2 enter",
        "gen2 exit",
    ]


@pytest.mark.asyncio
async def test_async_merge_exception():
    result = []
    states = []

    async def gen1():
        states.append("gen1 enter")
        try:
            await asyncio.sleep(0.1)
            yield 1
            await asyncio.sleep(0.1)  # ensure that 4 gets added by gen2 before the exception cancels it
            raise SampleException("test")
        finally:
            await asyncio.sleep(0)
            states.append("gen1 exit")

    async def gen2():
        states.append("gen2 enter")
        try:
            yield 3
            await asyncio.sleep(0.1)
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("gen2 exit")

    with pytest.raises(SampleException):
        async for item in async_merge(gen1(), gen2()):
            result.append(item)

    assert sorted(result) == [1, 3, 4]
    assert sorted(states) == [
        "gen1 enter",
        "gen1 exit",
        "gen2 enter",
        "gen2 exit",
    ]


@pytest.mark.asyncio
async def test_async_merge_cancellation():
    ev = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        await ev.wait()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    async def merge_coro():
        async with aclosing(async_merge(gen1(), gen2())) as stream:
            async for _ in stream:
                pass

    merge_task = asyncio.create_task(merge_coro())
    await asyncio.sleep(0.1)
    merge_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await merge_task


@pytest.mark.asyncio
async def test_async_merge_producer_cancellation():
    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    await asyncio.sleep(0.1)
    with pytest.raises(asyncio.CancelledError):
        async with aclosing(async_merge(gen1(), gen2())) as stream:
            async for _ in stream:
                pass


@pytest.mark.asyncio
async def test_callable_to_agen():
    async def foo():
        await asyncio.sleep(0.1)
        return 42

    result = []
    async for item in callable_to_agen(foo):
        result.append(item)
    assert result == [await foo()]


@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map(in_order):
    result = []
    states = []

    async def foo():
        states.append("enter")
        try:
            yield 1
            yield 2
            yield 3
        finally:
            states.append("exit")

    async def mapper(x):
        await asyncio.sleep(0.1)  # Simulate some async work
        return x * 2

    if in_order:
        async for item in async_map_ordered(foo(), mapper, concurrency=3):
            result.append(item)
    else:
        async for item in async_map(foo(), mapper, concurrency=3):
            result.append(item)

    assert sorted(result) == [2, 4, 6]
    assert states == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_input_exception_async_producer(in_order):
    # test exception async producer
    states = []

    async def mapper_func(x):
        await asyncio.sleep(0.1)
        return x * 2

    async def gen():
        states.append("enter")
        try:
            for i in range(5):
                if i == 3:
                    raise SampleException("test")
                yield i
        finally:
            states.append("exit")

    with pytest.raises(SampleException):
        if in_order:
            async for _ in async_map_ordered(gen(), mapper_func, concurrency=3):
                pass
        else:
            async for _ in async_map(gen(), mapper_func, concurrency=3):
                pass

    assert sorted(states) == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_input_cancellation_async_producer(in_order):
    # test cancelling async_map while waiting for input
    states = []

    async def mapper_func(x):
        await asyncio.sleep(0.1)
        return x * 2

    async def gen():
        states.append("enter")
        try:
            for i in range(5):
                if i == 3:
                    raise asyncio.CancelledError()
                yield i
        finally:
            states.append("exit")

    with pytest.raises(asyncio.CancelledError):
        if in_order:
            async for _ in async_map_ordered(gen(), mapper_func, concurrency=3):
                pass
        else:
            async for _ in async_map(gen(), mapper_func, concurrency=3):
                pass

    assert sorted(states) == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_cancellation_waiting_for_input(in_order):
    # test cancelling async_map while waiting for input
    result = []
    states = []

    async def mapper_func(x):
        return x * 2

    blocking_event = asyncio.Event()

    async def gen():
        states.append("enter")
        try:
            await blocking_event.wait()
            yield 1
        finally:
            states.append("exit")

    async def mapper_coro():
        if in_order:
            async for item in async_map_ordered(gen(), mapper_func, concurrency=3):
                result.append(item)
        else:
            async for item in async_map(gen(), mapper_func, concurrency=3):
                result.append(item)

    mapper_task = asyncio.create_task(mapper_coro())
    await asyncio.sleep(0.1)
    mapper_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await mapper_task

    assert sorted(result) == []
    assert sorted(states) == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_input_exception_sync_producer(in_order):
    # test exception sync producer
    states = []

    async def mapper_func(x):
        await asyncio.sleep(0.1)
        return x * 2

    def gen():
        states.append("enter")
        try:
            for i in range(5):
                if i == 3:
                    raise SampleException("test")
                yield i
        finally:
            states.append("exit")

    with pytest.raises(SampleException):
        if in_order:
            async for _ in async_map_ordered(sync_or_async_iter(gen()), mapper_func, concurrency=3):
                pass
        else:
            async for _ in async_map(sync_or_async_iter(gen()), mapper_func, concurrency=3):
                pass

    assert sorted(states) == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_output_exception_async_func(in_order):
    # test cancelling async mapper function
    result = []
    states = []

    def gen():
        states.append("enter")
        try:
            yield from range(5)
        finally:
            states.append("exit")

    async def mapper_func(x):
        await asyncio.sleep(0.1)
        if x == 3:
            raise SampleException("test")
        return x * 2

    with pytest.raises(SampleException):
        if in_order:
            async for item in async_map_ordered(sync_or_async_iter(gen()), mapper_func, concurrency=3):
                result.append(item)
        else:
            async for item in async_map(sync_or_async_iter(gen()), mapper_func, concurrency=3):
                result.append(item)

    assert sorted(result) == [0, 2, 4]
    assert states == ["enter", "exit"]


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_streaming_input(in_order):
    # ensure we can stream input
    # and dont buffer all the items and return them after
    result = []
    states = []

    async def gen():
        states.append("enter")
        try:
            yield 1
            await asyncio.sleep(1)
            yield 2
            yield 3
        finally:
            states.append("exit")

    async def mapper(x):
        await asyncio.sleep(0.1)
        return x * 2

    import time

    start = time.time()
    if in_order:
        async for item in async_map_ordered(gen(), mapper, concurrency=3):
            if item == 2:
                assert time.time() - start < 0.5
            else:
                assert time.time() - start > 0.5
            result.append(item)
    else:
        async for item in async_map(gen(), mapper, concurrency=3):
            if item == 2:
                assert time.time() - start < 0.5
            else:
                assert time.time() - start > 0.5
            result.append(item)

    assert result == [2, 4, 6]
    assert states == ["enter", "exit"]


@pytest.mark.asyncio
async def test_async_map_concurrency():
    active_mappers = 0
    active_mappers_history = []

    async def mapper(x):
        nonlocal active_mappers
        active_mappers += 1
        active_mappers_history.append(active_mappers)
        await asyncio.sleep(0.1)  # Simulate some async work
        active_mappers -= 1
        return x * 2

    result = [item async for item in async_map(sync_or_async_iter(range(10)), mapper, concurrency=3)]
    assert sorted(result) == [x * 2 for x in range(10)]
    assert max(active_mappers_history) == 3
    assert active_mappers_history.count(3) >= 7  # 2, ... 3, 4, 5 and 6, 7, 8 (9 *could* also be active with 3)


@pytest.mark.asyncio
@pytest.mark.parametrize("in_order", [True, False])
async def test_async_map_ordering(in_order):
    result = []
    ev = asyncio.Event()

    async def foo():
        yield 1
        yield 2
        yield 3

    async def mapper(x):
        if x == 1:
            await ev.wait()

        if x == 2:
            ev.set()

        return x * 2

    if in_order:
        async for item in async_map_ordered(foo(), mapper, concurrency=3):
            result.append(item)
        assert result == [2, 4, 6]
    else:
        async for item in async_map(foo(), mapper, concurrency=3):
            result.append(item)
        assert result == [4, 6, 2]


@pytest.mark.asyncio
async def test_async_map_ordered_buffer_size():
    processing = []

    async def mapper(x: int) -> int:
        processing.append(x)
        # Item 0 will block, causing buffer to fill up
        if x == 0:
            await asyncio.sleep(0.2)
        await asyncio.sleep(0.01)
        return x

    async def inputs():
        for i in range(100):
            yield i

    # Use small buffer_size to ensure we don't process too far ahead
    results = []
    async for result in async_map_ordered(inputs(), mapper, concurrency=5, buffer_size=3):
        # Check that we never processed more than buffer_size + 1 items ahead
        # (+1 because one item is being yielded while buffer_size items are buffered)
        assert max(processing) - result <= 3
        results.append(result)

    assert results == list(range(100))


@pytest.mark.asyncio
async def test_async_map_ordered_buffer_size2():
    result = []
    ev = asyncio.Event()
    cancel_ev = asyncio.Event()

    async def mapper_func(x: int) -> int:
        if x == 3:
            # Item 3 will block, causing buffer to fill up
            await ev.wait()

        if x == 10:
            # Item 10 will unblock the buffer
            # but with concurrency=3, it will never be reached
            ev.set()
        return x

    async def gen():
        for i in range(100):
            yield i

    async def mapper_coro():
        async with aclosing(async_map_ordered(gen(), mapper_func, concurrency=3)) as stream:
            async for item in stream:
                result.append(item)
                if item == 2:
                    cancel_ev.set()

    mapper_task = asyncio.create_task(mapper_coro())
    await cancel_ev.wait()
    mapper_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await mapper_task

    assert sorted(result) == [0, 1, 2]


@pytest.mark.asyncio
async def test_async_chain():
    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    async def gen3():
        yield 5
        yield 6

    result = []
    async for item in async_chain(gen1(), gen2(), gen3()):
        result.append(item)

    assert result == [1, 2, 3, 4, 5, 6]


@pytest.mark.asyncio
async def test_async_chain_sequential():
    ev = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        await ev.wait()
        yield 2

    async def gen2():
        yield 3
        ev.set()
        await asyncio.sleep(0.1)
        yield 4

    results = []

    async def concat_coro():
        async with aclosing(async_chain(gen1(), gen2())) as stream:
            async for item in stream:
                results.append(item)

    concat_task = asyncio.create_task(concat_coro())
    await asyncio.sleep(0.5)
    concat_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await concat_task

    assert results == [1]


@pytest.mark.asyncio
async def test_async_chain_exception():
    # test exception bubbling up
    result = []
    states = []

    async def gen1():
        states.append("enter 1")
        try:
            yield 1
            yield 2
        finally:
            states.append("exit 1")

    async def gen2():
        states.append("enter 2")
        try:
            await asyncio.sleep(0.1)
            yield 3
            raise SampleException("test")
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("exit 2")

    with pytest.raises(SampleException):
        async for item in async_chain(gen1(), gen2()):
            result.append(item)

    assert result == [1, 2, 3]
    assert states == ["enter 1", "exit 1", "enter 2", "exit 2"]


@pytest.mark.asyncio
async def test_async_chain_cancellation():
    ev = asyncio.Event()

    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        await ev.wait()
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    async def concat_coro():
        async with aclosing(async_chain(gen1(), gen2())) as stream:
            async for _ in stream:
                pass

    concat_task = asyncio.create_task(concat_coro())
    await asyncio.sleep(0.1)
    concat_task.cancel()
    with pytest.raises(asyncio.CancelledError):
        await concat_task


@pytest.mark.asyncio
async def test_async_chain_producer_cancellation():
    async def gen1():
        await asyncio.sleep(0.1)
        yield 1
        raise asyncio.CancelledError()
        yield 2

    async def gen2():
        yield 3
        await asyncio.sleep(0.1)
        yield 4

    await asyncio.sleep(0.1)
    with pytest.raises(asyncio.CancelledError):
        async with aclosing(async_chain(gen1(), gen2())) as stream:
            async for _ in stream:
                pass


@pytest.mark.asyncio
async def test_async_chain_cleanup():
    # test cleanup of generators
    result = []
    states = []

    async def gen1():
        states.append("enter 1")
        try:
            await asyncio.sleep(0.1)
            yield 1
            yield 2
        finally:
            await asyncio.sleep(0)
            states.append("exit 1")

    async def gen2():
        states.append("enter 2")
        try:
            yield 3
            await asyncio.sleep(0.1)
            yield 4
        finally:
            await asyncio.sleep(0)
            states.append("exit 2")

    async with aclosing(async_chain(gen1(), gen2())) as stream:
        async for item in stream:
            result.append(item)
            if item == 3:
                break

    assert result == [1, 2, 3]
    assert states == ["enter 1", "exit 1", "enter 2", "exit 2"]


def test_sigint_run_async_gen_shuts_down_gracefully():
    code = textwrap.dedent(
        """
    import asyncio
    import time
    from itertools import count
    from synchronicity.async_utils import Runner
    from modal._utils.async_utils import run_async_gen
    async def async_gen():
        print("enter")
        try:
            for i in count():
                yield i
                await asyncio.sleep(0.1)
        finally:
            # this could be either CancelledError or GeneratorExit depending on timing
            # CancelledError happens if sigint is during this generator's await
            # GeneratorExit is during the yielded block in the sync caller
            print("cancel")
            await asyncio.sleep(0.1)
            print("bye")
    try:
        with Runner() as runner:
            for res in run_async_gen(runner, async_gen()):
                print("res", res)
    except KeyboardInterrupt:
        print("KeyboardInterrupt")
    """
    )

    p = helpers.PopenWithCtrlC(
        [sys.executable, "-u", "-c", code],
        encoding="utf8",
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )

    def line():
        s = p.stdout.readline().rstrip("\n")
        if s == "":
            print(p.stderr.read())
            raise Exception("no stdout")
        print(s)
        return s

    assert line() == "enter"
    assert line() == "res 0"
    assert line() == "res 1"

    p.send_ctrl_c()
    print("sent ctrl-C")
    while (nextline := line()).startswith("res"):
        pass
    assert nextline == "cancel"
    assert line() == "bye"
    assert line() == "KeyboardInterrupt"
    assert p.wait() == 0
    assert p.stdout.read() == ""
    assert p.stderr.read() == ""


================================================
File: test/blob_test.py
================================================
# Copyright Modal Labs 2022

import pytest
import random

from modal._utils.async_utils import synchronize_api
from modal._utils.blob_utils import (
    blob_download as _blob_download,
    blob_upload as _blob_upload,
    blob_upload_file as _blob_upload_file,
)
from modal.exception import ExecutionError

from .supports.skip import skip_old_py

blob_upload = synchronize_api(_blob_upload)
blob_download = synchronize_api(_blob_download)
blob_upload_file = synchronize_api(_blob_upload_file)


@pytest.mark.asyncio
async def test_blob_put_get(servicer, blob_server, client):
    # Upload
    blob_id = await blob_upload.aio(b"Hello, world", client.stub)

    # Download
    data = await blob_download.aio(blob_id, client.stub)
    assert data == b"Hello, world"


@pytest.mark.asyncio
async def test_blob_put_failure(servicer, blob_server, client):
    with pytest.raises(ExecutionError):
        await blob_upload.aio(b"FAILURE", client.stub)


@pytest.mark.asyncio
async def test_blob_get_failure(servicer, blob_server, client):
    with pytest.raises(ExecutionError):
        await blob_download.aio("bl-failure", client.stub)


@pytest.mark.asyncio
async def test_blob_large(servicer, blob_server, client):
    data = b"*" * 10_000_000
    blob_id = await blob_upload.aio(data, client.stub)
    assert await blob_download.aio(blob_id, client.stub) == data


@skip_old_py("random.randbytes() was introduced in python 3.9", (3, 9))
@pytest.mark.asyncio
async def test_blob_multipart(servicer, blob_server, client, monkeypatch, tmp_path):
    monkeypatch.setattr("modal._utils.blob_utils.DEFAULT_SEGMENT_CHUNK_SIZE", 128)
    multipart_threshold = 1024
    servicer.blob_multipart_threshold = multipart_threshold
    # - set high # of parts, to test concurrency correctness
    # - make last part significantly shorter than rest, creating uneven upload time.
    data_len = (256 * multipart_threshold) + (multipart_threshold // 2)
    data = random.randbytes(data_len)  # random data will not hide byte re-ordering corruption
    blob_id = await blob_upload.aio(data, client.stub)
    assert await blob_download.aio(blob_id, client.stub) == data

    data_len = (256 * multipart_threshold) + (multipart_threshold // 2)
    data = random.randbytes(data_len)  # random data will not hide byte re-ordering corruption
    data_filepath = tmp_path / "temp.bin"
    data_filepath.write_bytes(data)
    blob_id = await blob_upload_file.aio(data_filepath.open("rb"), client.stub)
    assert await blob_download.aio(blob_id, client.stub) == data


def test_sync(blob_server, client):
    # just tests that tests running blocking calls that upload to blob storage don't deadlock
    blob_upload(b"adsfadsf", client.stub)


================================================
File: test/cli_imports_test.py
================================================
# Copyright Modal Labs 2023
import pytest
import types
from typing import cast

from modal.app import App, LocalEntrypoint
from modal.cli.import_refs import (
    AutoRunPriority,
    CLICommand,
    ImportRef,
    MethodReference,
    import_and_filter,
    import_file_or_module,
    list_cli_commands,
    parse_import_ref,
)
from modal.exception import InvalidError, PendingDeprecationError
from modal.functions import Function
from modal.partial_function import method, web_server

# Some helper vars for import_stub tests:
local_entrypoint_src = """
import modal

app = modal.App()
@app.local_entrypoint()
def main():
    pass
"""
python_module_src = """
import modal
app = modal.App("FOO", include_source=True)  # TODO: remove include_source=True)
other_app = modal.App("BAR", include_source=True)  # TODO: remove include_source=True)
@other_app.function()
def func():
    pass
@app.cls()
class Parent:
    @modal.method()
    def meth(self):
        pass

assert not __package__
"""

python_package_src = """
import modal
app = modal.App("FOO", include_source=True)  # TODO: remove include_source=True)
other_app = modal.App("BAR", include_source=True)  # TODO: remove include_source=True)
@other_app.function()
def func():
    pass
assert __package__ == "pack005"
"""

python_subpackage_src = """
import modal
app = modal.App("FOO", include_source=True)  # TODO: remove include_source=True)
other_app = modal.App("BAR", include_source=True)  # TODO: remove include_source=True)
@other_app.function()
def func():
    pass
assert __package__ == "pack007.sub009"
"""

python_file_src = """
import modal
app = modal.App("FOO", include_source=True)  # TODO: remove include_source=True)
other_app = modal.App("BAR", include_source=True)  # TODO: remove include_source=True)
@other_app.function()
def func():
    pass

assert __package__ == ""
"""

empty_dir_with_python_file = {"mod000.py": python_module_src}


dir_containing_python_package = {
    "dir001": {"sub002": {"mod003.py": python_module_src, "subfile004.py": python_file_src}},
    "pack005": {
        "file006.py": python_file_src,
        "mod007.py": python_package_src,
        "local008.py": local_entrypoint_src,
        "__init__.py": "",
        "sub009": {"mod010.py": python_subpackage_src, "__init__.py": "", "subfile011.py": python_file_src},
    },
}


@pytest.mark.parametrize(
    ["dir_structure", "ref", "returned_runnable_type", "num_error_choices"],
    [
        # # file syntax
        (empty_dir_with_python_file, "mod000.py", type(None), 2),
        (empty_dir_with_python_file, "mod000.py::app", MethodReference, 2),
        (empty_dir_with_python_file, "mod000.py::other_app", Function, 2),
        (dir_containing_python_package, "pack005/file006.py", Function, 1),
        (dir_containing_python_package, "pack005/sub009/subfile011.py", Function, 1),
        (dir_containing_python_package, "dir001/sub002/subfile004.py", Function, 1),
        # # python module syntax
        (empty_dir_with_python_file, "mod000::func", Function, 2),
        (empty_dir_with_python_file, "mod000::other_app.func", Function, 2),
        (empty_dir_with_python_file, "mod000::app.func", type(None), 2),
        (empty_dir_with_python_file, "mod000::Parent.meth", MethodReference, 2),
        (empty_dir_with_python_file, "mod000::other_app", Function, 2),
        (dir_containing_python_package, "pack005.mod007", Function, 1),
        (dir_containing_python_package, "pack005.mod007::other_app", Function, 1),
        (dir_containing_python_package, "pack005/local008.py::app.main", LocalEntrypoint, 1),
    ],
)
def test_import_and_filter(dir_structure, ref, mock_dir, returned_runnable_type, num_error_choices):
    with mock_dir(dir_structure):
        import_ref = parse_import_ref(ref)
        runnable, all_usable_commands = import_and_filter(
            import_ref, base_cmd="dummy", accept_local_entrypoint=True, accept_webhook=False
        )
        print(all_usable_commands)
        assert isinstance(runnable, returned_runnable_type)
        assert len(all_usable_commands) == num_error_choices


def test_import_and_filter_2(monkeypatch, supports_on_path):
    def import_runnable(object_path, accept_local_entrypoint=False, accept_webhook=False):
        return import_and_filter(
            ImportRef("import_and_filter_source", use_module_mode=True, object_path=object_path),
            base_cmd="",
            accept_local_entrypoint=accept_local_entrypoint,
            accept_webhook=accept_webhook,
        )

    runnable, all_usable_commands = import_runnable(
        "app_with_one_web_function", accept_webhook=False, accept_local_entrypoint=True
    )
    assert runnable is None
    assert len(all_usable_commands) == 4

    assert import_runnable("app_with_one_web_function", accept_webhook=True)[0]
    assert import_runnable("app_with_one_function_one_web_endpoint", accept_webhook=False)[0]

    runnable, all_usable_commands = import_runnable("app_with_one_function_one_web_endpoint", accept_webhook=True)
    assert runnable is None
    assert len(all_usable_commands) == 7

    runnable, all_usable_commands = import_runnable("app_with_one_web_method", accept_webhook=False)
    assert runnable is None
    assert len(all_usable_commands) == 3

    assert import_runnable("app_with_one_web_method", accept_webhook=True)[0]

    assert isinstance(
        import_runnable("app_with_local_entrypoint_and_function", accept_local_entrypoint=True)[0], LocalEntrypoint
    )
    assert isinstance(
        import_runnable("app_with_local_entrypoint_and_function", accept_local_entrypoint=False)[0], Function
    )


def test_import_package_and_module_names(monkeypatch, supports_dir):
    # We try to reproduce the package/module naming standard that the `python` command line tool uses,
    # i.e. when loading using a module path (-m flag w/ python) you get a fully qualified package/module name
    # but when loading using a filename, some/mod.py it will not have a __package__

    # The biggest difference is that __name__ of the imported "entrypoint" script
    # is __main__ when using `python` but in the Modal runtime it's the name of the
    # file minus the ".py", since Modal has its own __main__
    monkeypatch.chdir(supports_dir)
    mod1 = import_file_or_module(ImportRef("assert_package", use_module_mode=True))
    assert mod1.__package__ == ""
    assert mod1.__name__ == "assert_package"

    monkeypatch.chdir(supports_dir.parent)
    with pytest.warns(PendingDeprecationError, match=r"\s-m\s"):
        # TODO: this should use use_module_mode=True once we remove the deprecation warning
        mod2 = import_file_or_module(ImportRef("test.supports.assert_package", use_module_mode=False))

    assert mod2.__package__ == "test.supports"
    assert mod2.__name__ == "test.supports.assert_package"

    mod3 = import_file_or_module(ImportRef("supports/assert_package.py", use_module_mode=False))
    assert mod3.__package__ == ""
    assert mod3.__name__ == "assert_package"


def test_invalid_source_file_exception():
    with pytest.raises(InvalidError, match="Invalid Modal source filename: 'foo.bar.py'"):
        import_file_or_module(ImportRef("path/to/foo.bar.py", use_module_mode=False))


def test_list_cli_commands():
    class FakeModule:
        app = App()
        other_app = App()

    @FakeModule.app.function(serialized=True, name="foo")
    def foo():
        pass

    @FakeModule.app.cls(serialized=True)
    class Cls:
        @method()
        def method_1(self):
            pass

        @web_server(8000)
        def web_method(self):
            pass

    def non_modal_func():
        pass

    FakeModule.non_modal_func = non_modal_func  # type: ignore[attr-defined]
    FakeModule.foo = foo  # type: ignore[attr-defined]
    FakeModule.Cls = Cls  # type: ignore[attr-defined]

    res = list_cli_commands(cast(types.ModuleType, FakeModule))

    assert res == [
        CLICommand(["foo", "app.foo"], foo, False, priority=AutoRunPriority.MODULE_FUNCTION),  # type: ignore
        CLICommand(
            ["Cls.method_1", "app.Cls.method_1"],
            MethodReference(Cls, "method_1"),  # type: ignore
            False,
            priority=AutoRunPriority.MODULE_FUNCTION,
        ),
        CLICommand(
            ["Cls.web_method", "app.Cls.web_method"],
            MethodReference(Cls, "web_method"),  # type: ignore
            True,
            priority=AutoRunPriority.MODULE_FUNCTION,
        ),
    ]


================================================
File: test/cli_test.py
================================================
# Copyright Modal Labs 2022-2023
import asyncio
import contextlib
import json
import os
import platform
import pytest
import re
import subprocess
import sys
import tempfile
import threading
import traceback
from pathlib import Path
from pickle import dumps
from unittest import mock
from unittest.mock import MagicMock

import click
import click.testing
import toml

from modal import App, Sandbox
from modal._serialization import serialize
from modal._utils.grpc_testing import InterceptionContext
from modal.cli.entry_point import entrypoint_cli
from modal.exception import InvalidError
from modal_proto import api_pb2

from . import helpers
from .supports.skip import skip_windows

dummy_app_file = """
import modal

import other_module

app = modal.App("my_app", include_source=True)  # TODO: remove include_source=True)

# Sanity check that the module is imported properly
import sys
mod = sys.modules[__name__]
assert mod.app == app
"""

dummy_other_module_file = "x = 42"


def _run(args: list[str], expected_exit_code: int = 0, expected_stderr: str = "", expected_error: str = ""):
    runner = click.testing.CliRunner(mix_stderr=False)
    # DEBUGGING TIP: this runs the CLI in a separate subprocess, and output from it is not echoed by default,
    # including from the mock fixtures. Print res.stdout and res.stderr for debugging tests.
    with mock.patch.object(sys, "argv", args):
        res = runner.invoke(entrypoint_cli, args)
    if res.exit_code != expected_exit_code:
        print("stdout:", repr(res.stdout))
        print("stderr:", repr(res.stderr))
        traceback.print_tb(res.exc_info[2])
        print(res.exception, file=sys.stderr)
        assert res.exit_code == expected_exit_code
    if expected_stderr:
        assert re.search(expected_stderr, res.stderr), "stderr does not match expected string"
    if expected_error:
        assert re.search(expected_error, str(res.exception)), "exception message does not match expected string"
    return res


def test_app_deploy_success(servicer, mock_dir, set_env_client):
    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
        # Deploy as a script in cwd
        _run(["deploy", "myapp.py"])

        # Deploy as a module
        _run(["deploy", "myapp"])

        # Deploy as a script with an absolute path
        _run(["deploy", os.path.abspath("myapp.py")])

    assert "my_app" in servicer.deployed_apps


def test_app_deploy_with_name(servicer, mock_dir, set_env_client):
    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
        _run(["deploy", "myapp.py", "--name", "my_app_foo"])

    assert "my_app_foo" in servicer.deployed_apps


def test_secret_create(servicer, set_env_client):
    # fail without any keys
    _run(["secret", "create", "foo"], 2, None)

    _run(["secret", "create", "foo", "bar=baz"])
    assert len(servicer.secrets) == 1

    # Creating the same one again should fail
    _run(["secret", "create", "foo", "bar=baz"], expected_exit_code=1)

    # But it should succeed with --force
    _run(["secret", "create", "foo", "bar=baz", "--force"])


def test_secret_list(servicer, set_env_client):
    res = _run(["secret", "list"])
    assert "dummy-secret-0" not in res.stdout

    _run(["secret", "create", "foo", "bar=baz"])
    _run(["secret", "create", "bar", "baz=buz"])
    _run(["secret", "create", "eric", "baz=bu 123z=b\n\t\r #(Q)JO5️⃣5️⃣😤WMLE🔧:GWam "])

    res = _run(["secret", "list"])
    assert "dummy-secret-0" in res.stdout
    assert "dummy-secret-1" in res.stdout
    assert "dummy-secret-2" in res.stdout
    assert "dummy-secret-3" not in res.stdout


def test_app_token_new(servicer, set_env_client, server_url_env, modal_config):
    servicer.required_creds = {"abc": "xyz"}
    with modal_config() as config_file_path:
        _run(["token", "new", "--profile", "_test"])
        assert "_test" in toml.load(config_file_path)


def test_app_setup(servicer, set_env_client, server_url_env, modal_config):
    servicer.required_creds = {"abc": "xyz"}
    with modal_config() as config_file_path:
        _run(["setup", "--profile", "_test"])
        assert "_test" in toml.load(config_file_path)


app_file = Path("app_run_tests") / "default_app.py"
app_module = "app_run_tests.default_app"
file_with_entrypoint = Path("app_run_tests") / "local_entrypoint.py"


@pytest.mark.parametrize(
    ("run_command", "expected_exit_code", "expected_output"),
    [
        ([f"{app_file}"], 0, ""),
        ([f"{app_file}::app"], 0, ""),
        ([f"{app_file}::foo"], 0, ""),
        ([f"{app_file}::bar"], 1, ""),
        ([f"{file_with_entrypoint}"], 0, ""),
        ([f"{file_with_entrypoint}::main"], 0, ""),
        ([f"{file_with_entrypoint}::app.main"], 0, ""),
        ([f"{file_with_entrypoint}::foo"], 0, ""),
    ],
)
def test_run(servicer, set_env_client, supports_dir, monkeypatch, run_command, expected_exit_code, expected_output):
    monkeypatch.chdir(supports_dir)
    res = _run(["run"] + run_command, expected_exit_code=expected_exit_code)
    if expected_output:
        assert re.search(expected_output, res.stdout) or re.search(expected_output, res.stderr), (
            "output does not match expected string"
        )


def test_run_warns_without_module_flag(
    servicer, set_env_client, supports_dir, recwarn, monkeypatch, disable_auto_mount
):
    monkeypatch.chdir(supports_dir)
    _run(["run", "-m", f"{app_module}::foo"])
    assert not len(recwarn)

    with pytest.warns(match=" -m "):
        _run(["run", f"{app_module}::foo"])


def test_run_stub(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "app_was_once_stub.py"
    with pytest.warns(match="App"):
        _run(["run", app_file.as_posix() + "::foo"])


def test_run_async(servicer, set_env_client, test_dir):
    sync_fn = test_dir / "supports" / "app_run_tests" / "local_entrypoint.py"
    res = _run(["run", sync_fn.as_posix()])
    assert "called locally" in res.stdout

    async_fn = test_dir / "supports" / "app_run_tests" / "local_entrypoint_async.py"
    res = _run(["run", async_fn.as_posix()])
    assert "called locally (async)" in res.stdout


def test_run_generator(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "generator.py"
    result = _run(["run", app_file.as_posix()], expected_exit_code=1)
    assert "generator functions" in str(result.exception)


def test_help_message_unspecified_function(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "app_with_multiple_functions.py"
    result = _run(["run", app_file.as_posix()], expected_exit_code=1, expected_stderr=None)

    # should suggest available functions on the app:
    assert "foo" in result.stderr
    assert "bar" in result.stderr

    result = _run(
        ["run", app_file.as_posix(), "--help"], expected_exit_code=1, expected_stderr=None
    )  # TODO: help should not return non-zero
    # help should also available functions on the app:
    assert "foo" in result.stderr
    assert "bar" in result.stderr


def test_run_states(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "default_app.py"
    _run(["run", app_file.as_posix()])
    assert servicer.app_state_history["ap-1"] == [
        api_pb2.APP_STATE_INITIALIZING,
        api_pb2.APP_STATE_EPHEMERAL,
        api_pb2.APP_STATE_STOPPED,
    ]


def test_run_detach(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "default_app.py"
    _run(["run", "--detach", app_file.as_posix()])
    assert servicer.app_state_history["ap-1"] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DETACHED]


def test_run_quiet(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "default_app.py"
    # Just tests that the command runs without error for now (tests end up defaulting to `show_progress=False` anyway,
    # without a TTY).
    _run(["run", "--quiet", app_file.as_posix()])


def test_run_class_hierarchy(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "class_hierarchy.py"
    _run(["run", app_file.as_posix() + "::Wrapped.defined_on_base"])
    _run(["run", app_file.as_posix() + "::Wrapped.overridden_on_wrapped"])


def test_run_write_result(servicer, set_env_client, test_dir):
    # Note that this test only exercises local entrypoint functions,
    # because the servicer doesn't appear to mock remote execution faithfully?
    app_file = (test_dir / "supports" / "app_run_tests" / "returns_data.py").as_posix()

    with tempfile.TemporaryDirectory() as tmpdir:
        _run(["run", "--write-result", result_file := f"{tmpdir}/result.txt", f"{app_file}::returns_str"])
        with open(result_file, "rt") as f:
            assert f.read() == "Hello!"

        _run(["run", "-w", result_file := f"{tmpdir}/result.bin", f"{app_file}::returns_bytes"])
        with open(result_file, "rb") as f:
            assert f.read().decode("utf8") == "Hello!"

        _run(
            ["run", "-w", result_file := f"{tmpdir}/result.bin", f"{app_file}::returns_int"],
            expected_exit_code=1,
            expected_error="Function must return str or bytes when using `--write-result`; got int.",
        )


@pytest.mark.parametrize(
    ["args", "success", "expected_warning"],
    [
        (["--name=deployment_name", str(app_file)], True, ""),
        (["--name=deployment_name", app_module], True, f"modal deploy -m {app_module}"),
        (["--name=deployment_name", "-m", app_module], True, ""),
    ],
)
def test_deploy(
    servicer, set_env_client, supports_dir, monkeypatch, args, success, expected_warning, disable_auto_mount, recwarn
):
    monkeypatch.chdir(supports_dir)
    _run(["deploy"] + args, expected_exit_code=0 if success else 1)
    if success:
        assert servicer.app_state_history["ap-1"] == [api_pb2.APP_STATE_INITIALIZING, api_pb2.APP_STATE_DEPLOYED]
    else:
        assert api_pb2.APP_STATE_DEPLOYED not in servicer.app_state_history["ap-1"]
    if expected_warning:
        assert len(recwarn) == 1
        assert expected_warning in str(recwarn[0].message)


def test_run_custom_app(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "custom_app.py"
    res = _run(["run", app_file.as_posix() + "::app"], expected_exit_code=1, expected_stderr=None)
    assert "Specify a Modal Function or local entrypoint to run" in res.stderr
    assert "foo / my_app.foo" in res.stderr
    res = _run(["run", app_file.as_posix() + "::app.foo"], expected_exit_code=1, expected_stderr=None)
    assert "Specify a Modal Function or local entrypoint" in res.stderr
    assert "foo / my_app.foo" in res.stderr

    _run(["run", app_file.as_posix() + "::foo"])


def test_run_aiofunc(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "async_app.py"
    _run(["run", app_file.as_posix()])
    assert len(servicer.client_calls) == 1


def test_run_local_entrypoint(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "local_entrypoint.py"

    res = _run(["run", app_file.as_posix() + "::app.main"])  # explicit name
    assert "called locally" in res.stdout
    assert len(servicer.client_calls) == 2

    res = _run(["run", app_file.as_posix()])  # only one entry-point, no name needed
    assert "called locally" in res.stdout
    assert len(servicer.client_calls) == 4


def test_run_local_entrypoint_invalid_with_app_run(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "local_entrypoint_invalid.py"

    res = _run(["run", app_file.as_posix()], expected_exit_code=1)
    assert "app is already running" in str(res.exception.__cause__).lower()
    assert "unreachable" not in res.stdout
    assert len(servicer.client_calls) == 0


def test_run_parse_args_entrypoint(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "cli_args.py"
    res = _run(["run", app_file.as_posix()], expected_exit_code=1, expected_stderr=None)
    assert "Specify a Modal Function or local entrypoint to run" in res.stderr

    valid_call_args = [
        (
            [
                "run",
                f"{app_file.as_posix()}::app.dt_arg",
                "--dt",
                "2022-10-31",
            ],
            "the day is 31",
        ),
        (["run", f"{app_file.as_posix()}::dt_arg", "--dt=2022-10-31"], "the day is 31"),
        (["run", f"{app_file.as_posix()}::int_arg", "--i=200"], "200 <class 'int'>"),
        (["run", f"{app_file.as_posix()}::default_arg"], "10 <class 'int'>"),
        (["run", f"{app_file.as_posix()}::unannotated_arg", "--i=2022-10-31"], "'2022-10-31' <class 'str'>"),
        (["run", f"{app_file.as_posix()}::unannotated_default_arg"], "10 <class 'int'>"),
        (["run", f"{app_file.as_posix()}::optional_arg", "--i=20"], "20 <class 'int'>"),
        (["run", f"{app_file.as_posix()}::optional_arg"], "None <class 'NoneType'>"),
        (["run", f"{app_file.as_posix()}::optional_arg_postponed"], "None <class 'NoneType'>"),
    ]
    if sys.version_info >= (3, 10):
        valid_call_args.extend(
            [
                (["run", f"{app_file.as_posix()}::optional_arg_pep604", "--i=20"], "20 <class 'int'>"),
                (["run", f"{app_file.as_posix()}::optional_arg_pep604"], "None <class 'NoneType'>"),
            ]
        )
    for args, expected in valid_call_args:
        res = _run(args)
        assert expected in res.stdout
        assert len(servicer.client_calls) == 0

    if sys.version_info >= (3, 10):
        res = _run(["run", f"{app_file.as_posix()}::unparseable_annot", "--i=20"], expected_exit_code=1)
        assert "Parameter `i` has unparseable annotation: typing.Union[int, str]" in str(res.exception)

    if sys.version_info <= (3, 10):
        res = _run(["run", f"{app_file.as_posix()}::optional_arg_pep604"], expected_exit_code=1)
        assert "Unable to generate command line interface for app entrypoint." in str(res.exception)


def test_run_parse_args_function(servicer, set_env_client, test_dir, recwarn, disable_auto_mount):
    app_file = test_dir / "supports" / "app_run_tests" / "cli_args.py"
    res = _run(["run", app_file.as_posix()], expected_exit_code=1, expected_stderr=None)
    assert "Specify a Modal Function or local entrypoint to run" in res.stderr

    # HACK: all the tests use the same arg, i.
    @servicer.function_body
    def print_type(i):
        print(repr(i), type(i))

    valid_call_args = [
        (["run", f"{app_file.as_posix()}::int_arg_fn", "--i=200"], "200 <class 'int'>"),
        (["run", f"{app_file.as_posix()}::ALifecycle.some_method", "--i=hello"], "'hello' <class 'str'>"),
        (["run", f"{app_file.as_posix()}::ALifecycle.some_method_int", "--i=42"], "42 <class 'int'>"),
        (["run", f"{app_file.as_posix()}::optional_arg_fn"], "None <class 'NoneType'>"),
    ]
    for args, expected in valid_call_args:
        res = _run(args)
        assert expected in res.stdout

    if len(recwarn):
        print("Unexpected warnings:", [str(w) for w in recwarn])
    assert len(recwarn) == 0


def test_run_user_script_exception(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "raises_error.py"
    res = _run(["run", app_file.as_posix()], expected_exit_code=1)
    assert res.exc_info[1].user_source == str(app_file.resolve())


@pytest.fixture
def fresh_main_thread_assertion_module(test_dir):
    modules_to_unload = [n for n in sys.modules.keys() if "main_thread_assertion" in n]
    assert len(modules_to_unload) <= 1
    for mod in modules_to_unload:
        sys.modules.pop(mod)
    yield test_dir / "supports" / "app_run_tests" / "main_thread_assertion.py"


def test_no_user_code_in_synchronicity_run(servicer, set_env_client, test_dir, fresh_main_thread_assertion_module):
    pytest._did_load_main_thread_assertion = False  # type: ignore
    _run(["run", fresh_main_thread_assertion_module.as_posix()])
    assert pytest._did_load_main_thread_assertion  # type: ignore
    print()


def test_no_user_code_in_synchronicity_deploy(servicer, set_env_client, test_dir, fresh_main_thread_assertion_module):
    pytest._did_load_main_thread_assertion = False  # type: ignore
    _run(["deploy", "--name", "foo", fresh_main_thread_assertion_module.as_posix()])
    assert pytest._did_load_main_thread_assertion  # type: ignore


def test_serve(servicer, set_env_client, server_url_env, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "webhook.py"
    _run(["serve", app_file.as_posix(), "--timeout", "3"], expected_exit_code=0)


@pytest.fixture
def mock_shell_pty(servicer):
    servicer.shell_prompt = b"TEST_PROMPT# "

    def mock_get_pty_info(shell: bool) -> api_pb2.PTYInfo:
        rows, cols = (64, 128)
        return api_pb2.PTYInfo(
            enabled=True,
            winsz_rows=rows,
            winsz_cols=cols,
            env_term=os.environ.get("TERM"),
            env_colorterm=os.environ.get("COLORTERM"),
            env_term_program=os.environ.get("TERM_PROGRAM"),
            pty_type=api_pb2.PTYInfo.PTY_TYPE_SHELL,
        )

    captured_out = []
    fake_stdin = [b"echo foo\n", b"exit\n"]

    async def write_to_fd(fd: int, data: bytes):
        nonlocal captured_out
        captured_out.append((fd, data))

    @contextlib.asynccontextmanager
    async def fake_stream_from_stdin(handle_input, use_raw_terminal=False):
        async def _write():
            message_index = 0
            while True:
                if message_index == len(fake_stdin):
                    break
                data = fake_stdin[message_index]
                await handle_input(data, message_index)
                message_index += 1

        write_task = asyncio.create_task(_write())
        yield
        write_task.cancel()

    with (
        mock.patch("rich.console.Console.is_terminal", True),
        mock.patch("modal.cli.container.get_pty_info", mock_get_pty_info),
        mock.patch("modal._pty.get_pty_info", mock_get_pty_info),
        mock.patch("modal.runner.get_pty_info", mock_get_pty_info),
        mock.patch("modal._utils.shell_utils.stream_from_stdin", fake_stream_from_stdin),
        mock.patch("modal.container_process.stream_from_stdin", fake_stream_from_stdin),
        mock.patch("modal.container_process.write_to_fd", write_to_fd),
    ):
        yield fake_stdin, captured_out


app_file = Path("app_run_tests") / "default_app.py"
app_file_as_module = "app_run_tests.default_app"
webhook_app_file = Path("app_run_tests") / "webhook.py"
cls_app_file = Path("app_run_tests") / "cls.py"


@skip_windows("modal shell is not supported on Windows.")
@pytest.mark.parametrize(
    ["flags", "rel_file", "suffix"],
    [
        ([], app_file, "::foo"),  # Function is explicitly specified
        (["-m"], app_file_as_module, "::foo"),  # Function is explicitly specified - module mode
        ([], webhook_app_file, "::foo"),  # Function is explicitly specified
        ([], webhook_app_file, ""),  # Function must be inferred
        # TODO: fix modal shell auto-detection of a single class, even if it has multiple methods
        # ([], cls_app_file, ""),  # Class must be inferred
        # ([], cls_app_file, "AParametrized"),  # class name
        ([], cls_app_file, "::AParametrized.some_method"),  # method name
    ],
)
def test_shell(servicer, set_env_client, mock_shell_pty, suffix, monkeypatch, supports_dir, rel_file, flags):
    monkeypatch.chdir(supports_dir)
    fake_stdin, captured_out = mock_shell_pty

    fake_stdin.clear()
    fake_stdin.extend([b'echo "Hello World"\n', b"exit\n"])

    shell_prompt = servicer.shell_prompt

    _run(["shell"] + flags + [str(rel_file) + suffix])

    # first captured message is the empty message the mock server sends
    assert captured_out == [(1, shell_prompt), (1, b"Hello World\n")]
    captured_out.clear()


@skip_windows("modal shell is not supported on Windows.")
def test_shell_cmd(servicer, set_env_client, test_dir, mock_shell_pty):
    app_file = test_dir / "supports" / "app_run_tests" / "default_app.py"
    _, captured_out = mock_shell_pty
    shell_prompt = servicer.shell_prompt
    _run(["shell", "--cmd", "pwd", app_file.as_posix() + "::foo"])
    expected_output = subprocess.run(["pwd"], capture_output=True, check=True).stdout
    assert captured_out == [(1, shell_prompt), (1, expected_output)]


@skip_windows("modal shell is not supported on Windows.")
def test_shell_preserve_token(servicer, set_env_client, mock_shell_pty, monkeypatch):
    monkeypatch.setenv("MODAL_TOKEN_ID", "my-token-id")

    fake_stdin, captured_out = mock_shell_pty
    shell_prompt = servicer.shell_prompt

    fake_stdin.clear()
    fake_stdin.extend([b'echo "$MODAL_TOKEN_ID"\n', b"exit\n"])
    _run(["shell"])

    expected_output = b"my-token-id\n"
    assert captured_out == [(1, shell_prompt), (1, expected_output)]


def test_shell_unsuported_cmds_fails_on_windows(servicer, set_env_client, mock_shell_pty):
    expected_exit_code = 1 if platform.system() == "Windows" else 0
    res = _run(["shell"], expected_exit_code=expected_exit_code)

    if expected_exit_code != 0:
        assert re.search("Windows", str(res.exception)), "exception message does not match expected string"


def test_app_descriptions(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "prints_desc_app.py"
    _run(["run", "--detach", app_file.as_posix() + "::foo"])

    create_reqs = [s for s in servicer.requests if isinstance(s, api_pb2.AppCreateRequest)]
    assert len(create_reqs) == 1
    assert create_reqs[0].app_state == api_pb2.APP_STATE_DETACHED
    description = create_reqs[0].description
    assert "prints_desc_app.py::foo" in description
    assert "run --detach " not in description

    _run(["serve", "--timeout", "0.0", app_file.as_posix()])
    create_reqs = [s for s in servicer.requests if isinstance(s, api_pb2.AppCreateRequest)]
    assert len(create_reqs) == 2
    description = create_reqs[1].description
    assert "prints_desc_app.py" in description
    assert "serve" not in description
    assert "--timeout 0.0" not in description


def test_logs(servicer, server_url_env, set_env_client, mock_dir):
    async def app_done(self, stream):
        await stream.recv_message()
        log = api_pb2.TaskLogs(data="hello\n", file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT)
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="1", items=[log]))
        await stream.send_message(api_pb2.TaskLogsBatch(app_done=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("AppGetLogs", app_done)

        # TODO Fix the mock servicer to use "real" App IDs so this does not get misconstrued as a name
        # res = _run(["app", "logs", "ap-123"])
        # assert res.stdout == "hello\n"

        with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
            res = _run(["deploy", "myapp.py", "--name", "my-app", "--stream-logs"])
            assert res.stdout.endswith("hello\n")

        res = _run(["app", "logs", "my-app"])
        assert res.stdout == "hello\n"

    _run(
        ["app", "logs", "does-not-exist"],
        expected_exit_code=1,
        expected_error="Could not find a deployed app named 'does-not-exist'",
    )


def test_app_stop(servicer, mock_dir, set_env_client):
    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
        # Deploy as a module
        _run(["deploy", "myapp"])

    res = _run(["app", "list"])
    assert re.search("my_app .+ deployed", res.stdout)

    _run(["app", "stop", "my_app"])

    # Note that the mock servicer doesn't report "stopped" app statuses
    # so we just check that it's not reported as deployed
    res = _run(["app", "list"])
    assert not re.search("my_app .+ deployed", res.stdout)


def test_nfs_get(set_env_client, servicer):
    nfs_name = "my-shared-nfs"
    _run(["nfs", "create", nfs_name])
    with tempfile.TemporaryDirectory() as tmpdir:
        upload_path = os.path.join(tmpdir, "upload.txt")
        with open(upload_path, "w") as f:
            f.write("foo bar baz")
            f.flush()
        _run(["nfs", "put", nfs_name, upload_path, "test.txt"])

        _run(["nfs", "get", nfs_name, "test.txt", tmpdir])
        with open(os.path.join(tmpdir, "test.txt")) as f:
            assert f.read() == "foo bar baz"


def test_nfs_create_delete(servicer, server_url_env, set_env_client):
    name = "test-delete-nfs"
    _run(["nfs", "create", name])
    assert name in _run(["nfs", "list"]).stdout
    _run(["nfs", "delete", "--yes", name])
    assert name not in _run(["nfs", "list"]).stdout


def test_volume_cli(set_env_client):
    _run(["volume", "--help"])


def test_volume_get(servicer, set_env_client):
    vol_name = "my-test-vol"
    _run(["volume", "create", vol_name])
    file_path = "test.txt"
    file_contents = b"foo bar baz"
    with tempfile.TemporaryDirectory() as tmpdir:
        upload_path = os.path.join(tmpdir, "upload.txt")
        with open(upload_path, "wb") as f:
            f.write(file_contents)
            f.flush()
        _run(["volume", "put", vol_name, upload_path, file_path])

        _run(["volume", "get", vol_name, file_path, tmpdir])
        with open(os.path.join(tmpdir, file_path), "rb") as f:
            assert f.read() == file_contents

        download_path = os.path.join(tmpdir, "download.txt")
        _run(["volume", "get", vol_name, file_path, download_path])
        with open(download_path, "rb") as f:
            assert f.read() == file_contents

    with tempfile.TemporaryDirectory() as tmpdir2:
        _run(["volume", "get", vol_name, "/", tmpdir2])
        with open(os.path.join(tmpdir2, file_path), "rb") as f:
            assert f.read() == file_contents


def test_volume_put_force(servicer, set_env_client):
    vol_name = "my-test-vol"
    _run(["volume", "create", vol_name])
    file_path = "test.txt"
    file_contents = b"foo bar baz"
    with tempfile.TemporaryDirectory() as tmpdir:
        upload_path = os.path.join(tmpdir, "upload.txt")
        with open(upload_path, "wb") as f:
            f.write(file_contents)
            f.flush()

        # File upload
        _run(["volume", "put", vol_name, upload_path, file_path])  # Seed the volume
        with servicer.intercept() as ctx:
            _run(["volume", "put", vol_name, upload_path, file_path], expected_exit_code=2, expected_stderr=None)
            assert ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files

            _run(["volume", "put", vol_name, upload_path, file_path, "--force"])
            assert not ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files

        # Dir upload
        _run(["volume", "put", vol_name, tmpdir])  # Seed the volume
        with servicer.intercept() as ctx:
            _run(["volume", "put", vol_name, tmpdir], expected_exit_code=2, expected_stderr=None)
            assert ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files

            _run(["volume", "put", vol_name, tmpdir, "--force"])
            assert not ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files


def test_volume_rm(servicer, set_env_client):
    vol_name = "my-test-vol"
    _run(["volume", "create", vol_name])
    file_path = "test.txt"
    file_contents = b"foo bar baz"
    with tempfile.TemporaryDirectory() as tmpdir:
        upload_path = os.path.join(tmpdir, "upload.txt")
        with open(upload_path, "wb") as f:
            f.write(file_contents)
            f.flush()
        _run(["volume", "put", vol_name, upload_path, file_path])

        _run(["volume", "get", vol_name, file_path, tmpdir])
        with open(os.path.join(tmpdir, file_path), "rb") as f:
            assert f.read() == file_contents

        _run(["volume", "rm", vol_name, file_path])
        _run(["volume", "get", vol_name, file_path], expected_exit_code=1, expected_stderr=None)


def test_volume_ls(servicer, set_env_client):
    vol_name = "my-test-vol"
    _run(["volume", "create", vol_name])

    fnames = ["a", "b", "c"]
    with tempfile.TemporaryDirectory() as tmpdir:
        for fname in fnames:
            src_path = os.path.join(tmpdir, f"{fname}.txt")
            with open(src_path, "w") as f:
                f.write(fname * 5)
            _run(["volume", "put", vol_name, src_path, f"data/{fname}.txt"])

    res = _run(["volume", "ls", vol_name])
    assert "data" in res.stdout

    res = _run(["volume", "ls", vol_name, "data"])
    for fname in fnames:
        assert f"{fname}.txt" in res.stdout

    res = _run(["volume", "ls", vol_name, "data", "--json"])
    res_dict = json.loads(res.stdout)
    assert len(res_dict) == len(fnames)
    for entry, fname in zip(res_dict, fnames):
        assert entry["Filename"] == f"data/{fname}.txt"
        assert entry["Type"] == "file"


def test_volume_create_delete(servicer, server_url_env, set_env_client):
    vol_name = "test-delete-vol"
    _run(["volume", "create", vol_name])
    assert vol_name in _run(["volume", "list"]).stdout
    _run(["volume", "delete", "--yes", vol_name])
    assert vol_name not in _run(["volume", "list"]).stdout


def test_volume_rename(servicer, server_url_env, set_env_client):
    old_name, new_name = "foo-vol", "bar-vol"
    _run(["volume", "create", old_name])
    _run(["volume", "rename", "--yes", old_name, new_name])
    assert new_name in _run(["volume", "list"]).stdout
    assert old_name not in _run(["volume", "list"]).stdout


@pytest.mark.parametrize("command", [["run"], ["deploy"], ["serve", "--timeout=1"], ["shell"]])
@pytest.mark.usefixtures("set_env_client", "mock_shell_pty")
@skip_windows("modal shell is not supported on Windows.")
def test_environment_flag(test_dir, servicer, command):
    @servicer.function_body
    def nothing(
        arg=None,
    ):  # hacky - compatible with both argless modal run and interactive mode which always sends an arg...
        pass

    app_file = test_dir / "supports" / "app_run_tests" / "app_with_lookups.py"
    with servicer.intercept() as ctx:
        ctx.add_response(
            "MountGetOrCreate",
            api_pb2.MountGetOrCreateResponse(
                mount_id="mo-123",
                handle_metadata=api_pb2.MountHandleMetadata(content_checksum_sha256_hex="abc123"),
            ),
            request_filter=lambda req: req.deployment_name.startswith("modal-client-mount")
            and req.namespace == api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
        )  # built-in client lookup
        ctx.add_response(
            "SharedVolumeGetOrCreate",
            api_pb2.SharedVolumeGetOrCreateResponse(shared_volume_id="sv-123"),
            request_filter=lambda req: req.deployment_name == "volume_app" and req.environment_name == "staging",
        )
        _run(command + ["--env=staging", str(app_file)])

    app_create: api_pb2.AppCreateRequest = ctx.pop_request("AppCreate")
    assert app_create.environment_name == "staging"


@pytest.mark.parametrize("command", [["run"], ["deploy"], ["serve", "--timeout=1"], ["shell"]])
@pytest.mark.usefixtures("set_env_client", "mock_shell_pty")
@skip_windows("modal shell is not supported on Windows.")
def test_environment_noflag(test_dir, servicer, command, monkeypatch):
    monkeypatch.setenv("MODAL_ENVIRONMENT", "some_weird_default_env")

    @servicer.function_body
    def nothing(
        arg=None,
    ):  # hacky - compatible with both argless modal run and interactive mode which always sends an arg...
        pass

    app_file = test_dir / "supports" / "app_run_tests" / "app_with_lookups.py"
    with servicer.intercept() as ctx:
        ctx.add_response(
            "MountGetOrCreate",
            api_pb2.MountGetOrCreateResponse(
                mount_id="mo-123",
                handle_metadata=api_pb2.MountHandleMetadata(content_checksum_sha256_hex="abc123"),
            ),
            request_filter=lambda req: req.deployment_name.startswith("modal-client-mount")
            and req.namespace == api_pb2.DEPLOYMENT_NAMESPACE_GLOBAL,
        )  # built-in client lookup
        ctx.add_response(
            "SharedVolumeGetOrCreate",
            api_pb2.SharedVolumeGetOrCreateResponse(shared_volume_id="sv-123"),
            request_filter=lambda req: req.deployment_name == "volume_app"
            and req.environment_name == "some_weird_default_env",
        )
        _run(command + [str(app_file)])

    app_create: api_pb2.AppCreateRequest = ctx.pop_request("AppCreate")
    assert app_create.environment_name == "some_weird_default_env"


def test_cls(servicer, set_env_client, test_dir):
    app_file = test_dir / "supports" / "app_run_tests" / "cls.py"

    print(_run(["run", app_file.as_posix(), "--x", "42", "--y", "1000"]))
    _run(["run", f"{app_file.as_posix()}::AParametrized.some_method", "--x", "42", "--y", "1000"])


def test_profile_list(servicer, server_url_env, modal_config):
    config = """
    [test-profile]
    token_id = "ak-abc"
    token_secret = "as-xyz"

    [other-profile]
    token_id = "ak-123"
    token_secret = "as-789"
    active = true
    """

    with modal_config(config):
        servicer.required_creds = {"ak-abc": "as-xyz", "ak-123": "as-789"}
        res = _run(["profile", "list"])
        table_rows = res.stdout.split("\n")
        assert re.search("Profile .+ Workspace", table_rows[1])
        assert re.search("test-profile .+ test-username", table_rows[3])
        assert re.search("other-profile .+ test-username", table_rows[4])

        res = _run(["profile", "list", "--json"])
        json_data = json.loads(res.stdout)
        assert json_data[0]["name"] == "test-profile"
        assert json_data[0]["workspace"] == "test-username"
        assert json_data[1]["name"] == "other-profile"
        assert json_data[1]["workspace"] == "test-username"

        orig_env_token_id = os.environ.get("MODAL_TOKEN_ID")
        orig_env_token_secret = os.environ.get("MODAL_TOKEN_SECRET")
        os.environ["MODAL_TOKEN_ID"] = "ak-abc"
        os.environ["MODAL_TOKEN_SECRET"] = "as-xyz"
        servicer.required_creds = {"ak-abc": "as-xyz"}
        try:
            res = _run(["profile", "list"])
            assert "Using test-username workspace based on environment variables" in res.stdout
        finally:
            if orig_env_token_id:
                os.environ["MODAL_TOKEN_ID"] = orig_env_token_id
            else:
                del os.environ["MODAL_TOKEN_ID"]
            if orig_env_token_secret:
                os.environ["MODAL_TOKEN_SECRET"] = orig_env_token_secret
            else:
                del os.environ["MODAL_TOKEN_SECRET"]


def test_config_show(servicer, server_url_env, modal_config):
    config = """
    [test-profile]
    token_id = "ak-abc"
    token_secret = "as-xyz"
    active = true
    """
    with modal_config(config):
        res = _run(["config", "show"])
        assert "'token_id': 'ak-abc'" in res.stdout
        assert "'token_secret': '***'" in res.stdout


def test_app_list(servicer, mock_dir, set_env_client):
    res = _run(["app", "list"])
    assert "my_app_foo" not in res.stdout

    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
        _run(["deploy", "myapp.py", "--name", "my_app_foo"])

    res = _run(["app", "list"])
    assert "my_app_foo" in res.stdout

    res = _run(["app", "list", "--json"])
    assert json.loads(res.stdout)

    _run(["volume", "create", "my-vol"])
    res = _run(["app", "list"])
    assert "my-vol" not in res.stdout


def test_app_history(servicer, mock_dir, set_env_client):
    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
        _run(["deploy", "myapp.py", "--name", "my_app_foo"])

    # app should be deployed once it exists
    res = _run(["app", "history", "my_app_foo"])
    assert "v1" in res.stdout, res.stdout

    res = _run(["app", "history", "my_app_foo", "--json"])
    assert json.loads(res.stdout)

    # re-deploying an app should result in a new row in the history table
    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
        _run(["deploy", "myapp.py", "--name", "my_app_foo"])

    res = _run(["app", "history", "my_app_foo"])
    assert "v1" in res.stdout
    assert "v2" in res.stdout, f"{res.stdout=}"

    # can't fetch history for stopped apps
    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
        _run(["app", "stop", "my_app_foo"])

    res = _run(["app", "history", "my_app_foo", "--json"], expected_exit_code=1)


def test_app_rollback(servicer, mock_dir, set_env_client):
    with mock_dir({"myapp.py": dummy_app_file, "other_module.py": dummy_other_module_file}):
        # Deploy multiple times
        for _ in range(4):
            _run(["deploy", "myapp.py", "--name", "my_app"])
    _run(["app", "rollback", "my_app"])
    app_id = servicer.deployed_apps.get("my_app")
    assert servicer.app_deployment_history[app_id][-1]["rollback_version"] == 3

    _run(["app", "rollback", "my_app", "v2"])
    app_id = servicer.deployed_apps.get("my_app")
    assert servicer.app_deployment_history[app_id][-1]["rollback_version"] == 2

    _run(["app", "rollback", "my_app", "2"], expected_exit_code=2)


def test_dict_create_list_delete(servicer, server_url_env, set_env_client):
    _run(["dict", "create", "foo-dict"])
    _run(["dict", "create", "bar-dict"])
    res = _run(["dict", "list"])
    assert "foo-dict" in res.stdout
    assert "bar-dict" in res.stdout

    _run(["dict", "delete", "bar-dict", "--yes"])
    res = _run(["dict", "list"])
    assert "foo-dict" in res.stdout
    assert "bar-dict" not in res.stdout


def test_dict_show_get_clear(servicer, server_url_env, set_env_client):
    # Kind of hacky to be modifying the attributes on the servicer like this
    key = ("baz-dict", api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, os.environ.get("MODAL_ENVIRONMENT", "main"))
    dict_id = "di-abc123"
    servicer.deployed_dicts[key] = dict_id
    servicer.dicts[dict_id] = {dumps("a"): dumps(123), dumps("b"): dumps("blah")}

    res = _run(["dict", "items", "baz-dict"])
    assert re.search(r" Key .+ Value", res.stdout)
    assert re.search(r" a .+ 123 ", res.stdout)
    assert re.search(r" b .+ blah ", res.stdout)

    res = _run(["dict", "items", "baz-dict", "1"])
    assert re.search(r"\.\.\. .+ \.\.\.", res.stdout)
    assert "blah" not in res.stdout

    res = _run(["dict", "items", "baz-dict", "2"])
    assert "..." not in res.stdout

    res = _run(["dict", "items", "baz-dict", "--json"])
    assert '"Key": "a"' in res.stdout
    assert '"Value": 123' in res.stdout
    assert "..." not in res.stdout

    assert _run(["dict", "get", "baz-dict", "a"]).stdout == "123\n"
    assert _run(["dict", "get", "baz-dict", "b"]).stdout == "blah\n"

    res = _run(["dict", "clear", "baz-dict", "--yes"])
    assert servicer.dicts[dict_id] == {}


def test_queue_create_list_delete(servicer, server_url_env, set_env_client):
    _run(["queue", "create", "foo-queue"])
    _run(["queue", "create", "bar-queue"])
    res = _run(["queue", "list"])
    assert "foo-queue" in res.stdout
    assert "bar-queue" in res.stdout

    _run(["queue", "delete", "bar-queue", "--yes"])

    res = _run(["queue", "list"])
    assert "foo-queue" in res.stdout
    assert "bar-queue" not in res.stdout


def test_queue_peek_len_clear(servicer, server_url_env, set_env_client):
    # Kind of hacky to be modifying the attributes on the servicer like this
    name = "queue-who"
    key = (name, api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, os.environ.get("MODAL_ENVIRONMENT", "main"))
    queue_id = "qu-abc123"
    servicer.deployed_queues[key] = queue_id
    servicer.queue = {b"": [dumps("a"), dumps("b"), dumps("c")], b"alt": [dumps("x"), dumps("y")]}

    assert _run(["queue", "peek", name]).stdout == "a\n"
    assert _run(["queue", "peek", name, "-p", "alt"]).stdout == "x\n"
    assert _run(["queue", "peek", name, "3"]).stdout == "a\nb\nc\n"
    assert _run(["queue", "peek", name, "3", "--partition", "alt"]).stdout == "x\ny\n"

    assert _run(["queue", "len", name]).stdout == "3\n"
    assert _run(["queue", "len", name, "--partition", "alt"]).stdout == "2\n"
    assert _run(["queue", "len", name, "--total"]).stdout == "5\n"

    _run(["queue", "clear", name, "--yes"])
    assert _run(["queue", "len", name]).stdout == "0\n"
    assert _run(["queue", "peek", name, "--partition", "alt"]).stdout == "x\n"

    _run(["queue", "clear", name, "--all", "--yes"])
    assert _run(["queue", "len", name, "--total"]).stdout == "0\n"
    assert _run(["queue", "peek", name, "--partition", "alt"]).stdout == ""


@pytest.mark.parametrize("name", [".main", "_main", "'-main'", "main/main", "main:main"])
def test_create_environment_name_invalid(servicer, set_env_client, name):
    assert isinstance(
        _run(
            ["environment", "create", name],
            1,
        ).exception,
        InvalidError,
    )


@pytest.mark.parametrize("name", ["main", "main_-123."])
def test_create_environment_name_valid(servicer, set_env_client, name):
    assert (
        "Environment created"
        in _run(
            ["environment", "create", name],
            0,
        ).stdout
    )


@pytest.mark.parametrize(("name", "set_name"), (("main", "main/main"), ("main", "'-main'")))
def test_update_environment_name_invalid(servicer, set_env_client, name, set_name):
    assert isinstance(
        _run(
            ["environment", "update", name, "--set-name", set_name],
            1,
        ).exception,
        InvalidError,
    )


@pytest.mark.parametrize(("name", "set_name"), (("main", "main_-123."), ("main:main", "main2")))
def test_update_environment_name_valid(servicer, set_env_client, name, set_name):
    assert (
        "Environment updated"
        in _run(
            ["environment", "update", name, "--set-name", set_name],
            0,
        ).stdout
    )


def test_call_update_environment_suffix(servicer, set_env_client):
    _run(["environment", "update", "main", "--set-web-suffix", "_"])


def _run_subprocess(cli_cmd: list[str]) -> helpers.PopenWithCtrlC:
    p = helpers.PopenWithCtrlC(
        [sys.executable, "-m", "modal"] + cli_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding="utf8"
    )
    return p


@pytest.mark.timeout(10)
def test_keyboard_interrupt_during_app_load(servicer, server_url_env, token_env, supports_dir):
    ctx: InterceptionContext
    creating_function = threading.Event()

    async def stalling_function_create(servicer, req):
        creating_function.set()
        await asyncio.sleep(10)

    with servicer.intercept() as ctx:
        ctx.set_responder("FunctionCreate", stalling_function_create)

        p = _run_subprocess(["run", f"{supports_dir / 'hello.py'}::hello"])
        creating_function.wait()
        p.send_ctrl_c()
        out, err = p.communicate(timeout=5)
        print(out)
        assert "Traceback" not in err
        assert "Aborting app initialization..." in out


@pytest.mark.timeout(10)
def test_keyboard_interrupt_during_app_run(servicer, server_url_env, token_env, supports_dir):
    ctx: InterceptionContext
    waiting_for_output = threading.Event()

    async def stalling_function_get_output(servicer, req):
        waiting_for_output.set()
        await asyncio.sleep(10)

    with servicer.intercept() as ctx:
        ctx.set_responder("FunctionGetOutputs", stalling_function_get_output)

        p = _run_subprocess(["run", f"{supports_dir / 'hello.py'}::hello"])
        waiting_for_output.wait()
        p.send_ctrl_c()
        out, err = p.communicate(timeout=5)
        assert "App aborted. View run at https://modaltest.com/apps/ap-123" in out
        assert "Traceback" not in err


@pytest.mark.timeout(10)
def test_keyboard_interrupt_during_app_run_detach(servicer, server_url_env, token_env, supports_dir):
    ctx: InterceptionContext
    waiting_for_output = threading.Event()

    async def stalling_function_get_output(servicer, req):
        waiting_for_output.set()
        await asyncio.sleep(10)

    with servicer.intercept() as ctx:
        ctx.set_responder("FunctionGetOutputs", stalling_function_get_output)

        p = _run_subprocess(["run", "--detach", f"{supports_dir / 'hello.py'}::hello"])
        waiting_for_output.wait()
        p.send_ctrl_c()
        out, err = p.communicate(timeout=5)
        print(out)
        assert "Shutting down Modal client." in out
        assert "The detached app keeps running. You can track its progress at:" in out
        assert "Traceback" not in err


@pytest.fixture
def app(client):
    app = App()
    with app.run(client):
        yield app


@skip_windows("modal shell is not supported on Windows.")
def test_container_exec(servicer, set_env_client, mock_shell_pty, app):
    sb = Sandbox.create("bash", "-c", "sleep 10000", app=app)

    fake_stdin, captured_out = mock_shell_pty

    fake_stdin.clear()
    fake_stdin.extend([b'echo "Hello World"\n', b"exit\n"])

    shell_prompt = servicer.shell_prompt

    _run(["container", "exec", "--pty", sb.object_id, "/bin/bash"])
    assert captured_out == [(1, shell_prompt), (1, b"Hello World\n")]
    captured_out.clear()

    sb.terminate()


def test_can_run_all_listed_functions_with_includes(supports_on_path, monkeypatch, set_env_client, disable_auto_mount):
    monkeypatch.setenv("TERM", "dumb")  # prevents looking at ansi escape sequences

    res = _run(["run", "multifile_project.main"], expected_exit_code=1)
    print("err", res.stderr)
    # there are no runnables directly in the target module, so references need to go via the app
    func_listing = res.stderr.split("functions and local entrypoints:")[1]

    listed_runnables = set(re.findall(r"\b[\w.]+\b", func_listing))

    expected_runnables = {
        "app.a_func",
        "app.b_func",
        "app.c_func",
        "app.main_function",
        "main_function",
        "Cls.method_on_other_app_class",
        "other_app.Cls.method_on_other_app_class",
    }
    assert listed_runnables == expected_runnables

    for runnable in expected_runnables:
        assert runnable in res.stderr
        _run(["run", f"multifile_project.main::{runnable}"], expected_exit_code=0)


def test_modal_launch_vscode(monkeypatch, set_env_client, servicer):
    mock_open = MagicMock()
    monkeypatch.setattr("webbrowser.open", mock_open)
    with servicer.intercept() as ctx:
        ctx.add_response("QueueGet", api_pb2.QueueGetResponse(values=[serialize(("http://dummy", "tok"))]))
        ctx.add_response("QueueGet", api_pb2.QueueGetResponse(values=[serialize("done")]))
        _run(["launch", "vscode"])

    assert mock_open.call_count == 1


def test_run_file_with_global_lookups(servicer, set_env_client, supports_dir):
    # having module-global Function/Cls objects from .from_name constructors shouldn't
    # cause issues, and they shouldn't be runnable via CLI (for now)
    with servicer.intercept() as ctx:
        _run(["run", str(supports_dir / "app_run_tests" / "file_with_global_lookups.py")])

    (req,) = ctx.get_requests("FunctionCreate")
    assert req.function.function_name == "local_f"
    assert len(ctx.get_requests("FunctionMap")) == 1
    assert len(ctx.get_requests("FunctionGet")) == 0


def test_run_auto_infer_prefer_target_module(servicer, supports_dir, set_env_client, monkeypatch, disable_auto_mount):
    monkeypatch.syspath_prepend(supports_dir / "app_run_tests")
    res = _run(["run", "multifile.util"])
    assert "ran util\nmain func" in res.stdout


@pytest.mark.parametrize("func", ["va_entrypoint", "va_function", "VaClass.va_method"])
def test_cli_run_variadic_args(servicer, set_env_client, test_dir, func, disable_auto_mount):
    app_file = test_dir / "supports" / "app_run_tests" / "variadic_args.py"

    @servicer.function_body
    def print_args(*args):
        print(f"args: {args}")

    res = _run(["run", f"{app_file.as_posix()}::{func}"])
    assert "args: ()" in res.stdout

    res = _run(["run", f"{app_file.as_posix()}::{func}", "abc", "--foo=123", "--bar=456"])
    assert "args: ('abc', '--foo=123', '--bar=456')" in res.stdout

    _run(["run", f"{app_file.as_posix()}::{func}_invalid", "--foo=123"], expected_exit_code=1)


================================================
File: test/client_test.py
================================================
# Copyright Modal Labs 2022
import platform
import pytest
import subprocess
import sys

from google.protobuf.empty_pb2 import Empty
from grpclib import GRPCError, Status

from modal import Client
from modal.exception import AuthError, ConnectionError, DeprecationError, InvalidError, ServerWarning
from modal_proto import api_pb2

from .supports.skip import skip_windows, skip_windows_unix_socket

TEST_TIMEOUT = 4.0  # align this with the container client timeout in client.py


def test_client_type(servicer, client):
    assert len(servicer.requests) == 0
    client.hello()
    assert len(servicer.requests) == 1
    assert isinstance(servicer.requests[0], Empty)
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CLIENT)


def test_client_platform_string(servicer, client):
    client.hello()
    platform_str = servicer.last_metadata["x-modal-platform"]
    system, release, machine = platform_str.split("-")
    if platform.system() == "Darwin":
        assert system == "macOS"
        assert release == platform.mac_ver()[0].replace("-", "_")
    else:
        assert system == platform.system().replace("-", "_")
        assert release == platform.release().replace("-", "_")
    assert machine == platform.machine().replace("-", "_")


@pytest.mark.asyncio
async def test_container_client_type(servicer, container_client):
    await container_client.hello.aio()
    assert len(servicer.requests) == 1
    assert isinstance(servicer.requests[0], Empty)
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CONTAINER)


@pytest.mark.asyncio
@pytest.mark.timeout(TEST_TIMEOUT)
async def test_client_dns_failure():
    with pytest.raises(ConnectionError) as excinfo:
        async with Client("https://xyz.invalid", api_pb2.CLIENT_TYPE_CONTAINER, None):
            pass
    assert excinfo.value


@pytest.mark.asyncio
@pytest.mark.timeout(TEST_TIMEOUT)
@skip_windows("Windows test crashes on connection failure")
async def test_client_connection_failure():
    with pytest.raises(ConnectionError) as excinfo:
        async with Client("https://localhost:443", api_pb2.CLIENT_TYPE_CONTAINER, None):
            pass
    assert excinfo.value


@pytest.mark.asyncio
@pytest.mark.timeout(TEST_TIMEOUT)
@skip_windows_unix_socket
async def test_client_connection_failure_unix_socket():
    with pytest.raises(ConnectionError) as excinfo:
        async with Client("unix:/tmp/xyz.txt", api_pb2.CLIENT_TYPE_CONTAINER, None):
            pass
    assert excinfo.value


@pytest.mark.asyncio
async def test_client_old_version(servicer, credentials):
    async with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials, version="0.0.0") as client:
        with pytest.raises(GRPCError) as excinfo:
            await client.hello.aio()
        assert excinfo.value.status == Status.FAILED_PRECONDITION
        assert excinfo.value.message == "Old client"


@pytest.mark.asyncio
async def test_client_deprecated(servicer, credentials):
    async with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials, version="deprecated") as client:
        with pytest.warns(ServerWarning):
            await client.hello.aio()


@pytest.mark.asyncio
async def test_client_unauthenticated(servicer):
    with pytest.raises(AuthError):
        async with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, None, version="unauthenticated") as client:
            await client.hello.aio()


def client_from_env(client_addr, credentials):
    token_id, token_secret = credentials
    _override_config = {
        "server_url": client_addr,
        "token_id": token_id,
        "token_secret": token_secret,
        "task_id": None,
        "task_secret": None,
    }
    client = Client.from_env(_override_config=_override_config)
    client.hello()
    return client


def test_client_from_env_client(servicer, credentials):
    client_1 = client_from_env(servicer.client_addr, credentials)
    client_2 = client_from_env(servicer.client_addr, credentials)
    assert isinstance(client_1, Client)
    assert isinstance(client_2, Client)
    assert client_1 == client_2


def test_client_from_env_failing(servicer, credentials):
    with pytest.raises(ConnectionError):
        client_from_env("https://foo.invalid", credentials)


def test_client_from_env_reset(servicer, credentials):
    client_1 = client_from_env(servicer.client_addr, credentials)
    Client.set_env_client(None)
    client_2 = client_from_env(servicer.client_addr, credentials)
    assert client_1 != client_2


def test_client_token_auth_in_sandbox(servicer, credentials, monkeypatch) -> None:
    """Ensure that clients can connect with token credentials inside a sandbox.

    This test is needed so that modal.com/playground works, since it relies on
    running a sandbox with token credentials. Also, `modal shell` uses this to
    preserve its auth context inside the shell.
    """
    monkeypatch.setenv("MODAL_TASK_ID", "ta-123")
    _client = client_from_env(servicer.client_addr, credentials)
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CLIENT)


def test_multiple_profile_error(servicer, modal_config):
    config = """
    [prof-1]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    active = true

    [prof-2]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    active = true
    """
    with modal_config(config):
        with pytest.raises(InvalidError, match="More than one Modal profile is active"):
            Client.from_env()


def test_implicit_default_profile_warning(servicer, modal_config, server_url_env):
    config = """
    [default]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'

    [other]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    """
    with modal_config(config):
        with pytest.raises(DeprecationError, match="Support for using an implicit 'default' profile is deprecated."):
            Client.from_env()

    config = """
    [default]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    """
    with modal_config(config):
        servicer.required_creds = {"ak-abc": "as_xyz"}
        # A single profile should be fine, even if not explicitly active and named 'default'
        Client.from_env()


def test_import_modal_from_thread(supports_dir):
    # this mainly ensures that we don't make any assumptions about which thread *imports* modal
    # For example, in Python <3.10, creating loop-bound asyncio primitives in global scope would
    # trigger an exception if there is no event loop in the thread (and it's not the main thread)
    subprocess.check_call([sys.executable, supports_dir / "import_modal_from_thread.py"])


def test_from_env_container(servicer, container_env):
    servicer.required_creds = {}  # Disallow default client creds
    client = Client.from_env()
    client.hello()
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CONTAINER)


def test_from_env_container_with_tokens(servicer, container_env, token_env):
    # Even if MODAL_TOKEN_ID and MODAL_TOKEN_SECRET are set, if we're in a containers, ignore those
    servicer.required_creds = {}  # Disallow default client creds
    with pytest.warns(match="token"):
        client = Client.from_env()
    client.hello()
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CONTAINER)


def test_from_credentials_client(servicer, set_env_client, server_url_env, token_env):
    # Note: this explicitly uses a lot of fixtures to make sure those are ignored
    token_id = "ak-foo-1"
    token_secret = "as-bar"
    servicer.required_creds = {token_id: token_secret}
    client = Client.from_credentials(token_id, token_secret)
    client.hello()
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CLIENT)


def test_from_credentials_container(servicer, container_env):
    token_id = "ak-foo-2"
    token_secret = "as-bar"
    servicer.required_creds = {token_id: token_secret}
    client = Client.from_credentials(token_id, token_secret)
    client.hello()
    assert servicer.last_metadata["x-modal-client-type"] == str(api_pb2.CLIENT_TYPE_CLIENT)


def test_client_verify(servicer, client):
    token_id = "ak-foo-2"
    token_secret = "as-bar"
    servicer.required_creds = {token_id: token_secret}

    assert len(servicer.requests) == 0
    client.verify(servicer.client_addr, (token_id, token_secret))
    assert len(servicer.requests) == 1

    with pytest.raises(AuthError):
        client.verify(servicer.client_addr, ("foo", "bar"))

    with pytest.raises(ConnectionError):
        client.verify("https://localhost:443", ("foo", "bar"))


================================================
File: test/cloud_bucket_mount_test.py
================================================
# Copyright Modal Labs 2024
import modal


def dummy():
    pass


def test_volume_mount(client, servicer):
    app = modal.App()
    secret = modal.Secret.from_dict({"AWS_ACCESS_KEY_ID": "1", "AWS_SECRET_ACCESS_KEY": "2"})
    cld_bckt_mnt = modal.CloudBucketMount(
        bucket_name="foo",
        key_prefix="dir/",
        bucket_endpoint_url="https://1234.r2.cloudflarestorage.com",
        secret=secret,
        read_only=False,
    )

    _ = app.function(volumes={"/root/foo": cld_bckt_mnt})(dummy)

    with app.run(client=client):
        pass


================================================
File: test/cls_test.py
================================================
# Copyright Modal Labs 2022
import inspect
import pytest
import subprocess
import sys
import threading
import typing
from typing import TYPE_CHECKING

from typing_extensions import assert_type

import modal.partial_function
from modal import App, Cls, Function, Image, Queue, build, enter, exit, method
from modal._partial_function import (
    _find_partial_methods_for_user_cls,
    _PartialFunction,
    _PartialFunctionFlags,
)
from modal._serialization import deserialize, deserialize_params, serialize
from modal._utils.async_utils import synchronizer
from modal._utils.deprecation import PendingDeprecationError
from modal._utils.function_utils import FunctionInfo
from modal.exception import DeprecationError, ExecutionError, InvalidError, NotFoundError
from modal.partial_function import (
    PartialFunction,
    asgi_app,
    web_endpoint,
)
from modal.runner import deploy_app
from modal.running_app import RunningApp
from modal_proto import api_pb2

from .supports.base_class import BaseCls2

app = App("app", include_source=True)


@pytest.fixture(autouse=True)
def auto_use_set_env_client(set_env_client):
    # TODO(elias): remove set_env_client fixture here if/when possible - this is required only since
    #  Client.from_env happens to inject an unused client when loading the
    #  parametrized function
    return


@app.cls()
class NoParamsCls:
    @method()
    def bar(self, x):
        return x**3

    @method()
    def baz(self, x):
        return x**2


@app.cls(cpu=42)
class Foo:
    @method()
    def bar(self, x: int) -> float:
        return x**3

    @method()
    def baz(self, y: int) -> float:
        return y**4


def test_run_class(client, servicer):
    assert len(servicer.precreated_functions) == 0
    assert servicer.n_functions == 0
    with app.run(client=client):
        method_handle_object_id = Foo._get_class_service_function().object_id  # type: ignore
        assert isinstance(Foo, Cls)
        assert isinstance(NoParamsCls, Cls)
        class_id = Foo.object_id
        class_id2 = NoParamsCls.object_id
        app_id = app.app_id

    assert len(servicer.classes) == 2 and set(servicer.classes) == {class_id, class_id2}
    assert servicer.n_functions == 2
    objects = servicer.app_objects[app_id]
    class_function_id = objects["Foo.*"]
    class_function_id2 = objects["NoParamsCls.*"]
    assert servicer.precreated_functions == {class_function_id, class_function_id2}
    assert method_handle_object_id == class_function_id  # method handle object id will probably go away
    assert len(objects) == 4  # two classes + two class service function
    assert objects["Foo"] == class_id
    assert class_function_id.startswith("fu-")
    assert servicer.app_functions[class_function_id].is_class
    assert servicer.app_functions[class_function_id].method_definitions == {
        "bar": api_pb2.MethodDefinition(
            function_name="Foo.bar",
            function_type=api_pb2.Function.FunctionType.FUNCTION_TYPE_FUNCTION,
        ),
        "baz": api_pb2.MethodDefinition(
            function_name="Foo.baz",
            function_type=api_pb2.Function.FunctionType.FUNCTION_TYPE_FUNCTION,
        ),
    }


def test_call_class_sync(client, servicer, set_env_client):
    with servicer.intercept() as ctx:
        with app.run(client=client):
            assert len(ctx.get_requests("FunctionCreate")) == 2  # one for Foo, one for NoParamsCls
            foo: NoParamsCls = NoParamsCls()
            assert len(ctx.get_requests("FunctionCreate")) == 2
            assert len(ctx.get_requests("FunctionBindParams")) == 0  # no binding, yet
            ret: float = foo.bar.remote(42)
            assert ret == 1764
            assert (
                len(ctx.get_requests("FunctionBindParams")) == 0
            )  # reuse class base function when class has no params

    function_creates_requests: list[api_pb2.FunctionCreateRequest] = ctx.get_requests("FunctionCreate")
    assert len(function_creates_requests) == 2
    assert len(ctx.get_requests("ClassCreate")) == 2
    function_creates = {fc.function.function_name: fc for fc in function_creates_requests}
    assert function_creates.keys() == {"Foo.*", "NoParamsCls.*"}
    service_function_id = servicer.app_objects["ap-1"]["NoParamsCls.*"]
    (function_map_request,) = ctx.get_requests("FunctionMap")
    assert function_map_request.function_id == service_function_id


def test_class_with_options(client, servicer):
    unhydrated_volume = modal.Volume.from_name("some_volume", create_if_missing=True)
    unhydrated_secret = modal.Secret.from_dict({"foo": "bar"})
    with servicer.intercept() as ctx:
        foo = Foo.with_options(  # type: ignore
            cpu=48, retries=5, volumes={"/vol": unhydrated_volume}, secrets=[unhydrated_secret]
        )()
        assert len(ctx.calls) == 0  # no rpcs in with_options

    with app.run(client=client):
        with servicer.intercept() as ctx:
            res = foo.bar.remote(2)
            function_bind_params: api_pb2.FunctionBindParamsRequest
            (function_bind_params,) = ctx.get_requests("FunctionBindParams")
            assert function_bind_params.function_options.retry_policy.retries == 5
            assert function_bind_params.function_options.resources.milli_cpu == 48000

            assert len(ctx.get_requests("VolumeGetOrCreate")) == 1
            assert len(ctx.get_requests("SecretGetOrCreate")) == 1

        with servicer.intercept() as ctx:
            res = foo.bar.remote(2)
            assert len(ctx.get_requests("FunctionBindParams")) == 0  # no need to rebind
            assert len(ctx.get_requests("VolumeGetOrCreate")) == 0  # no need to rehydrate
            assert len(ctx.get_requests("SecretGetOrCreate")) == 0  # no need to rehydrate

        assert res == 4
        assert len(servicer.function_options) == 1
        options: api_pb2.FunctionOptions = list(servicer.function_options.values())[0]
        assert options.resources.milli_cpu == 48_000
        assert options.retry_policy.retries == 5

        with pytest.warns(PendingDeprecationError, match="max_containers"):
            Foo.with_options(concurrency_limit=10)()  # type: ignore


def test_with_options_from_name(servicer):
    unhydrated_volume = modal.Volume.from_name("some_volume", create_if_missing=True)
    unhydrated_secret = modal.Secret.from_dict({"foo": "bar"})

    with servicer.intercept() as ctx:
        SomeClass = modal.Cls.from_name("some_app", "SomeClass")
        OptionedClass = SomeClass.with_options(cpu=10, secrets=[unhydrated_secret], volumes={"/vol": unhydrated_volume})
        inst = OptionedClass(x=10)
        assert len(ctx.calls) == 0

    with servicer.intercept() as ctx:
        ctx.add_response("VolumeGetOrCreate", api_pb2.VolumeGetOrCreateResponse(volume_id="vo-123"))
        ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
        ctx.add_response("ClassGet", api_pb2.ClassGetResponse(class_id="cs-123"))
        ctx.add_response(
            "FunctionGet",
            api_pb2.FunctionGetResponse(
                function_id="fu-123",
                handle_metadata=api_pb2.FunctionHandleMetadata(
                    method_handle_metadata={
                        "some_method": api_pb2.FunctionHandleMetadata(
                            use_function_id="fu-123",
                            use_method_name="some_method",
                            function_name="SomeClass.some_method",
                        )
                    }
                ),
            ),
        )
        ctx.add_response("FunctionBindParams", api_pb2.FunctionBindParamsResponse(bound_function_id="fu-124"))
        inst.some_method.remote()

    function_bind_params: api_pb2.FunctionBindParamsRequest
    (function_bind_params,) = ctx.get_requests("FunctionBindParams")
    assert len(function_bind_params.function_options.volume_mounts) == 1
    function_map: api_pb2.FunctionMapRequest
    (function_map,) = ctx.get_requests("FunctionMap")
    assert function_map.function_id == "fu-124"  # the bound function


# Reusing the app runs into an issue with stale function handles.
# TODO (akshat): have all the client tests use separate apps, and throw
# an exception if the user tries to reuse an app.
app_remote = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app_remote.cls(cpu=42)
class FooRemote:
    def __init__(self, x: int, y: str) -> None:
        self.x = x
        self.y = y

    @method()
    def bar(self, z: int):
        return z**3


def test_call_cls_remote_sync(client):
    with app_remote.run(client=client):
        foo_remote: FooRemote = FooRemote(3, "hello")
        ret: float = foo_remote.bar.remote(8)
        assert ret == 64  # Mock servicer just squares the argument


def test_call_cls_remote_invalid_type(client):
    with app_remote.run(client=client):

        def my_function():
            print("Hello, world!")

        with pytest.raises(ValueError) as excinfo:
            FooRemote(42, my_function)  # type: ignore

        exc = excinfo.value
        assert "function" in str(exc)


def test_call_cls_remote_modal_type(client):
    with app_remote.run(client=client):
        with Queue.ephemeral(client) as q:
            FooRemote(42, q)  # type: ignore


app_2 = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app_2.cls(cpu=42)
class Bar:
    @method()
    def baz(self, x):
        return x**3


@pytest.mark.asyncio
async def test_call_class_async(client, servicer):
    async with app_2.run(client=client):
        bar = Bar()
        assert await bar.baz.remote.aio(42) == 1764


def test_run_class_serialized(client, servicer):
    app_ser = App()

    @app_ser.cls(cpu=42, serialized=True)
    class FooSer:
        @method()
        def bar(self, x):
            return x**3

    assert servicer.n_functions == 0
    with app_ser.run(client=client):
        pass

    assert servicer.n_functions == 1
    class_function = servicer.function_by_name("FooSer.*")
    assert class_function.definition_type == api_pb2.Function.DEFINITION_TYPE_SERIALIZED
    user_cls = deserialize(class_function.class_serialized, client)

    # Create bound method
    obj = user_cls()
    bound_bar = user_cls.bar.__get__(obj)
    # Make sure it's callable
    assert bound_bar(100) == 1000000


app_remote_2 = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app_remote_2.cls(cpu=42)
class BarRemote:
    def __init__(self, x: int, y: str) -> None:
        self.x = x
        self.y = y

    @method()
    def baz(self, z: int):
        return z**3


@pytest.mark.asyncio
async def test_call_cls_remote_async(client):
    async with app_remote_2.run(client=client):
        bar_remote = BarRemote(3, "hello")
        assert await bar_remote.baz.remote.aio(8) == 64  # Mock servicer just squares the argument


app_local = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app_local.cls(cpu=42, enable_memory_snapshot=True)
class FooLocal:
    def __init__(self):
        self.side_effects = ["__init__"]

    @enter(snap=True)
    def presnap(self):
        self.side_effects.append("presnap")

    @enter()
    def postsnap(self):
        self.side_effects.append("postsnap")

    @method()
    def bar(self, x):
        return x**3

    @method()
    def baz(self, y):
        return self.bar.local(y + 1)


def test_can_call_locally(client):
    foo = FooLocal()
    assert foo.bar.local(4) == 64
    assert foo.baz.local(4) == 125
    with app_local.run(client=client):
        assert foo.baz.local(2) == 27
        assert foo.side_effects == ["__init__", "presnap", "postsnap"]


def test_can_call_remotely_from_local(client):
    with app_local.run(client=client):
        foo = FooLocal()
        # remote calls use the mockservicer func impl
        # which just squares the arguments
        assert foo.bar.remote(8) == 64
        assert foo.baz.remote(9) == 81


app_remote_3 = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app_remote_3.cls(cpu=42)
class NoArgRemote:
    def __init__(self) -> None:
        pass

    @method()
    def baz(self, z: int) -> float:
        return z**3.0


def test_call_cls_remote_no_args(client):
    with app_remote_3.run(client=client):
        foo_remote = NoArgRemote()
        assert foo_remote.baz.remote(8) == 64  # Mock servicer just squares the argument


if TYPE_CHECKING:
    # Check that type annotations carry through to the decorated classes
    assert_type(NoParamsCls(), NoParamsCls)
    # can't use assert_type with named arguments, as it will diff in the name
    # vs the anonymous argument in the assertion type
    # assert_type(Foo().bar, Function[[int], float])


def test_lookup(client, servicer):
    # basically same test as test_from_name_lazy_method_resolve, but assumes everything is hydrated
    deploy_app(app, "my-cls-app", client=client)

    with pytest.warns(DeprecationError, match="Cls.lookup"):
        cls: Cls = Cls.lookup("my-cls-app", "Foo", client=client)

    # objects are resolved
    assert cls.object_id.startswith("cs-")
    assert cls._get_class_service_function().object_id.startswith("fu-")

    # Check that function properties are preserved
    assert cls().bar.is_generator is False

    # Make sure we can instantiate the class
    with servicer.intercept() as ctx:
        obj = cls("foo", 234)
        assert len(ctx.calls) == 0  # no rpc requests for class instantiation

        # Make sure we can call methods
        # (mock servicer just returns the sum of the squares of the args)
        assert obj.bar.remote(42) == 1764
        assert len(ctx.get_requests("FunctionBindParams")) == 1  # bind params

        assert obj.baz.remote(41) == 1681
        assert len(ctx.get_requests("FunctionBindParams")) == 1  # call to other method shouldn't need a bind

    # Not allowed for remote classes:
    with pytest.raises(NotFoundError, match="can't be accessed for remote classes"):
        assert obj.a == "foo"

    # Make sure local calls fail
    with pytest.raises(ExecutionError):
        assert obj.bar.local(1, 2)


def test_from_name_lazy_method_hydration(client, servicer):
    deploy_app(app, "my-cls-app", client=client)
    cls: Cls = Cls.from_name("my-cls-app", "Foo")

    # Make sure we can instantiate the class
    obj = cls("foo", 234)

    # Check that function properties are preserved
    with servicer.intercept() as ctx:
        assert obj.bar.is_generator is False
        assert len(ctx.get_requests("FunctionBindParams")) == 1  # to determine this attribute, hydration is needed

    # Make sure we can methods
    # (mock servicer just returns the sum of the squares of the args)
    with servicer.intercept() as ctx:
        assert obj.bar.remote(42) == 1764
        assert len(ctx.get_requests("FunctionBindParams")) == 0

    with servicer.intercept() as ctx:
        assert obj.baz.remote(42) == 1764
        assert len(ctx.get_requests("FunctionBindParams")) == 0  # other method shouldn't rebind

    with pytest.raises(NotFoundError, match="can't be accessed for remote classes"):
        assert obj.a == 234

    # Make sure local calls fail
    with pytest.raises(ExecutionError, match="locally"):
        assert obj.bar.local(1, 2)

    # Make sure that non-existing methods fail
    with pytest.raises(NotFoundError):
        obj.non_exist.remote("hello")


def test_lookup_lazy_remote(client, servicer):
    # See #972 (PR) and #985 (revert PR): adding unit test to catch regression
    deploy_app(app, "my-cls-app", client=client)
    cls: Cls = Cls.from_name("my-cls-app", "Foo").hydrate(client=client)
    obj = cls("foo", 234)
    assert obj.bar.remote(42, 77) == 7693


def test_lookup_lazy_spawn(client, servicer):
    # See #1071
    deploy_app(app, "my-cls-app", client=client)
    cls: Cls = Cls.from_name("my-cls-app", "Foo").hydrate(client=client)
    obj = cls("foo", 234)
    function_call = obj.bar.spawn(42, 77)
    assert function_call.get() == 7693


baz_app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@baz_app.cls()
class Baz:
    def __init__(self, x):
        self.x = x

    def not_modal_method(self, y: int) -> int:
        return self.x * y


def test_call_not_modal_method():
    baz: Baz = Baz(5)
    assert baz.x == 5
    assert baz.not_modal_method(7) == 35


cls_with_enter_app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


def get_thread_id():
    return threading.current_thread().name


@cls_with_enter_app.cls()
class ClsWithEnter:
    def __init__(self, thread_id):
        self.inited = True
        self.entered = False
        self.thread_id = thread_id
        assert get_thread_id() == self.thread_id

    @enter()
    def enter(self):
        self.entered = True
        assert get_thread_id() == self.thread_id

    def not_modal_method(self, y: int) -> int:
        return y**2

    @method()
    def modal_method(self, y: int) -> int:
        return y**2


def test_dont_enter_on_local_access():
    obj = ClsWithEnter(get_thread_id())
    with pytest.raises(AttributeError):
        obj.doesnt_exist  # type: ignore
    assert obj.inited
    assert not obj.entered


def test_dont_enter_on_local_non_modal_call():
    obj = ClsWithEnter(get_thread_id())
    assert obj.not_modal_method(7) == 49
    assert obj.inited
    assert not obj.entered


def test_enter_on_local_modal_call():
    obj = ClsWithEnter(get_thread_id())
    assert obj.modal_method.local(7) == 49
    assert obj.inited
    assert obj.entered


@cls_with_enter_app.cls()
class ClsWithAsyncEnter:
    def __init__(self):
        self.inited = True
        self.entered = False

    @enter()
    async def enter(self):
        self.entered = True

    @method()
    async def modal_method(self, y: int) -> int:
        return y**2


@pytest.mark.skip("this doesn't actually work - but issue was hidden by `entered` being an obj property")
@pytest.mark.asyncio
async def test_async_enter_on_local_modal_call():
    obj = ClsWithAsyncEnter()
    assert await obj.modal_method.local(7) == 49
    assert obj.inited
    assert obj.entered


inheritance_app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


class BaseCls:
    @enter()
    def enter(self):
        self.x = 2

    @method()
    def run(self, y):
        return self.x * y


@inheritance_app.cls()
class DerivedCls(BaseCls):
    pass


def test_derived_cls(client, servicer):
    with inheritance_app.run(client=client):
        # default servicer fn just squares the number
        assert DerivedCls().run.remote(3) == 9


inheritance_app_2 = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@inheritance_app_2.cls()
class DerivedCls2(BaseCls2):
    pass


def test_derived_cls_external_file(client, servicer):
    with inheritance_app_2.run(client=client):
        # default servicer fn just squares the number
        assert DerivedCls2().run.remote(3) == 9


def test_rehydrate(client, servicer, reset_container_app):
    # Issue introduced in #922 - brief description in #931

    # Sanity check that local calls work
    obj = NoParamsCls()
    assert obj.bar.local(7) == 343

    # Deploy app to get an app id
    app_id = deploy_app(app, "my-cls-app", client=client).app_id

    # Initialize a container
    container_app = RunningApp(app_id)

    # Associate app with app
    app._init_container(client, container_app)

    # Hydration shouldn't overwrite local function definition
    obj = NoParamsCls()
    assert obj.bar.local(7) == 343


app_unhydrated = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app_unhydrated.cls()
class FooUnhydrated:
    @method()
    def bar(self, x): ...


def test_unhydrated():
    foo = FooUnhydrated()
    with pytest.raises(ExecutionError, match="hydrated"):
        foo.bar.remote(42)


app_method_args = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app_method_args.cls(min_containers=5)
class XYZ:
    @method()
    def foo(self): ...

    @method()
    def bar(self): ...


def test_method_args(servicer, client):
    with app_method_args.run(client=client):
        funcs = servicer.app_functions.values()
        assert {f.function_name for f in funcs} == {"XYZ.*"}
        warm_pools = {f.function_name: f.autoscaler_settings.min_containers for f in funcs}
        assert warm_pools == {"XYZ.*": 5}


def test_keep_warm_depr(client, set_env_client):
    app = App()

    with pytest.warns(PendingDeprecationError, match="keep_warm"):

        @app.cls(serialized=True)
        class ClsWithKeepWarmMethod:
            @method(keep_warm=2)
            def foo(self): ...

            @method()
            def bar(self): ...

    with app.run(client=client):
        with pytest.raises(modal.exception.InvalidError, match="keep_warm"):
            ClsWithKeepWarmMethod().bar.keep_warm(2)  # should not be usable on methods


def test_cls_keep_warm(client, servicer):
    app = App()

    @app.cls(serialized=True)
    class ClsWithMethod:
        def __init__(self, arg=None):
            self.arg = arg

        @method()
        def bar(self): ...

    with app.run(client=client):
        assert len(servicer.app_functions) == 1  # only class service function
        cls_service_fun = servicer.function_by_name("ClsWithMethod.*")
        assert cls_service_fun.is_class
        assert cls_service_fun.warm_pool_size == 0

        ClsWithMethod().keep_warm(2)  # type: ignore  # Python can't do type intersection
        assert cls_service_fun.warm_pool_size == 2

        ClsWithMethod("other-instance").keep_warm(5)  # type: ignore  # Python can't do type intersection
        instance_service_function = servicer.function_by_name("ClsWithMethod.*", params=((("other-instance",), {})))
        assert len(servicer.app_functions) == 2  # + instance service function
        assert cls_service_fun.warm_pool_size == 2
        assert instance_service_function.warm_pool_size == 5


with pytest.warns(DeprecationError, match="@modal.build"):

    class ClsWithHandlers:
        @build()
        def my_build(self):
            pass

        @enter(snap=True)
        def my_memory_snapshot(self):
            pass

        @enter()
        def my_enter(self):
            pass

        @build()
        @enter()
        def my_build_and_enter(self):
            pass

        @exit()
        def my_exit(self):
            pass


def test_handlers():
    pfs: dict[str, _PartialFunction]

    pfs = _find_partial_methods_for_user_cls(ClsWithHandlers, _PartialFunctionFlags.BUILD)
    assert list(pfs.keys()) == ["my_build", "my_build_and_enter"]

    pfs = _find_partial_methods_for_user_cls(ClsWithHandlers, _PartialFunctionFlags.ENTER_PRE_SNAPSHOT)
    assert list(pfs.keys()) == ["my_memory_snapshot"]

    pfs = _find_partial_methods_for_user_cls(ClsWithHandlers, _PartialFunctionFlags.ENTER_POST_SNAPSHOT)
    assert list(pfs.keys()) == ["my_enter", "my_build_and_enter"]

    pfs = _find_partial_methods_for_user_cls(ClsWithHandlers, _PartialFunctionFlags.EXIT)
    assert list(pfs.keys()) == ["my_exit"]


web_app_app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@web_app_app.cls()
class WebCls:
    @web_endpoint()
    def endpoint(self):
        pass

    @asgi_app()
    def asgi(self):
        pass


def test_web_cls(client):
    with web_app_app.run(client=client):
        c = WebCls()
        assert c.endpoint.web_url == "http://endpoint.internal"
        assert c.asgi.web_url == "http://asgi.internal"


handler_app = App("handler-app", include_source=True)


image = Image.debian_slim().pip_install("xyz")


with pytest.warns(DeprecationError, match="@modal.build"):

    @handler_app.cls(image=image)
    class ClsWithBuild:
        @build()
        def build(self):
            pass

        @method()
        def method(self):
            pass


def test_build_image(client, servicer):
    with handler_app.run(client=client):
        service_function = servicer.function_by_name("ClsWithBuild.*")
        # The function image should have added a new layer with original image as the parent
        f_image = servicer.images[service_function.image_id]
        assert f_image.base_images[0].image_id == image.object_id
        assert servicer.force_built_images == []


other_handler_app = App("other-handler-app", include_source=True)


with pytest.warns(DeprecationError, match="@modal.build"):

    @other_handler_app.cls(image=image)
    class ClsWithForceBuild:
        @build(force=True)
        def build(self):
            pass

        @method()
        def method(self):
            pass


def test_force_build_image(client, servicer):
    with other_handler_app.run(client=client):
        service_function = servicer.function_by_name("ClsWithForceBuild.*")
        # The function image should have added a new layer with original image as the parent
        f_image = servicer.images[service_function.image_id]
        assert f_image.base_images[0].image_id == image.object_id
        assert servicer.force_built_images == ["im-3"]


build_timeout_handler_app = App("build-timeout-handler-app", include_source=True)


with pytest.warns(DeprecationError, match="@modal.build"):

    @build_timeout_handler_app.cls(image=image)
    class ClsWithBuildTimeout:
        @build(timeout=123)
        def timeout_build(self):
            pass

        @build()
        def default_timeout_build(self):
            pass

        @method()
        def method(self):
            pass


def test_build_timeout_image(client, servicer):
    with build_timeout_handler_app.run(client=client):
        service_function = servicer.function_by_name("ClsWithBuildTimeout.timeout_build")
        assert service_function.timeout_secs == 123

        service_function = servicer.function_by_name("ClsWithBuildTimeout.default_timeout_build")
        assert service_function.timeout_secs == 86400


@pytest.mark.parametrize("decorator", [enter, exit])
def test_disallow_lifecycle_decorators_with_method(decorator):
    name = decorator.__name__.split("_")[-1]  # remove synchronicity prefix
    with pytest.raises(InvalidError, match=f"Cannot use `@{name}` decorator with `@method`."):

        class ClsDecoratorMethodStack:
            @decorator()
            @method()
            def f(self):
                pass


class HasSnapMethod:
    @enter(snap=True)
    def enter(self):
        pass

    @method()
    def f(self):
        pass


def test_snap_method_without_snapshot_enabled():
    with pytest.raises(InvalidError, match="A class must have `enable_memory_snapshot=True`"):
        app.cls(enable_memory_snapshot=False)(HasSnapMethod)


def test_partial_function_descriptors(client):
    class Foo:
        def __init__(self):
            pass

        @modal.enter()
        def enter_method(self):
            pass

        @modal.method()
        def bar(self):
            return "a"

        @modal.web_endpoint()
        def web(self):
            pass

    assert isinstance(Foo.bar, PartialFunction)

    assert Foo().bar() == "a"  # type: ignore   # edge case - using a non-decorated class should just return the bound original method
    assert inspect.ismethod(Foo().bar)
    app = modal.App()

    modal_foo_class = app.cls(serialized=True)(Foo)

    wrapped_method = modal_foo_class().bar
    assert isinstance(wrapped_method, Function)

    serialized_class = serialize(Foo)
    revived_class = deserialize(serialized_class, client)

    assert (
        revived_class().bar() == "a"
    )  # this instantiates the underlying "user_cls", so it should work basically like a normal Python class
    assert isinstance(
        revived_class.bar, PartialFunction
    )  # but it should be a PartialFunction, so it keeps associated metadata!

    # ensure that webhook metadata is kept
    web_partial_function: _PartialFunction = synchronizer._translate_in(revived_class.web)  # type: ignore
    assert web_partial_function.webhook_config
    assert web_partial_function.webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION


def test_cross_process_userclass_serde(supports_dir):
    res = subprocess.check_output([sys.executable, supports_dir / "serialize_class.py"])
    assert len(res) < 2000  # should be ~1300 bytes as of 2024-06-05
    revived_cls = deserialize(res, None)
    method_without_descriptor_protocol = revived_cls.__dict__["method"]
    assert isinstance(method_without_descriptor_protocol, modal.partial_function.PartialFunction)
    assert revived_cls().method() == "a"  # this should be bound to the object


app2 = App("app2", include_source=True)


@app2.cls()
class UsingAnnotationParameters:
    a: int = modal.parameter()
    b: str = modal.parameter(default="hello")
    c: float = modal.parameter(init=False)

    @method()
    def get_value(self):
        return self.a


init_side_effects = []


@app2.cls()
class UsingCustomConstructor:
    # might want to deprecate this soon
    a: int

    def __init__(self, a: int):
        self._a = a
        init_side_effects.append("did_run")

    @method()
    def get_value(self):
        return self._a


def test_implicit_constructor(client, set_env_client):
    c = UsingAnnotationParameters(a=10)

    assert c.a == 10
    assert c.get_value.local() == 10
    assert c.b == "hello"

    d = UsingAnnotationParameters(a=11, b="goodbye")
    assert d.b == "goodbye"

    with pytest.raises(ValueError, match="Missing required parameter: a"):
        with app2.run(client=client):
            UsingAnnotationParameters().get_value.remote()  # type: ignore

    # check that implicit constructors trigger strict parametrization
    function_info: FunctionInfo = synchronizer._translate_in(UsingAnnotationParameters)._class_service_function._info  # type: ignore
    assert function_info.class_parameter_info().format == api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PROTO


def test_custom_constructor():
    d = UsingCustomConstructor(10)
    assert not init_side_effects

    assert d._a == 10  # lazily run constructor when accessing non-method attributes (!)
    assert init_side_effects == ["did_run"]

    d2 = UsingCustomConstructor(11)
    assert d2.get_value.local() == 11  # run constructor before running locally
    # check that explicit constructors trigger pickle parametrization
    function_info: FunctionInfo = synchronizer._translate_in(UsingCustomConstructor)._class_service_function._info  # type: ignore
    assert function_info.class_parameter_info().format == api_pb2.ClassParameterInfo.PARAM_SERIALIZATION_FORMAT_PICKLE


class ParametrizedClass1:
    def __init__(self, a):
        pass


class ParametrizedClass1Implicit:
    a: int = modal.parameter()


class ParametrizedClass2:
    def __init__(self, a: int = 1):
        pass


class ParametrizedClass2Implicit:
    a: int = modal.parameter(default=1)


class ParametrizedClass3:
    def __init__(self):
        pass


app_batched = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


def test_batched_method_duplicate_error(client):
    with pytest.raises(
        InvalidError, match="Modal class BatchedClass_1 with a modal batched function cannot have other modal methods."
    ):

        @app_batched.cls(serialized=True)
        class BatchedClass_1:
            @modal.method()
            def method(self):
                pass

            @modal.batched(max_batch_size=2, wait_ms=0)
            def batched_method(self):
                pass

    with pytest.raises(InvalidError, match="Modal class BatchedClass_2 can only have one batched function."):

        @app_batched.cls(serialized=True)
        class BatchedClass_2:
            @modal.batched(max_batch_size=2, wait_ms=0)
            def batched_method_1(self):
                pass

            @modal.batched(max_batch_size=2, wait_ms=0)
            def batched_method_2(self):
                pass


def test_cls_with_both_constructor_and_parameters_is_invalid():
    with pytest.raises(InvalidError, match="constructor"):

        @app.cls(serialized=True)
        class A:
            a: int = modal.parameter()

            def __init__(self, a):
                self.a = a


def test_unannotated_parameters_are_invalid():
    with pytest.raises(InvalidError, match="annotated"):

        @app.cls(serialized=True)
        class B:
            b = modal.parameter()  # type: ignore


def test_unsupported_type_parameters_raise_errors():
    with pytest.raises(InvalidError, match="float"):

        @app.cls(serialized=True)
        class C:
            c: float = modal.parameter()


def test_unsupported_function_decorators_on_methods():
    with pytest.raises(InvalidError, match="cannot be used on class methods"):

        @app.cls(serialized=True)
        class M:
            @app.function(serialized=True)
            @modal.web_endpoint()
            def f(self):
                pass

    with pytest.raises(InvalidError, match="cannot be used on class methods"):

        @app.cls(serialized=True)
        class D:
            @app.function(serialized=True)
            def f(self):
                pass


def test_modal_object_param_uses_wrapped_type(servicer, set_env_client, client):
    with servicer.intercept() as ctx:
        with modal.Dict.ephemeral() as dct:
            with baz_app.run():
                # create bound instance:
                typing.cast(modal.Cls, Baz(x=dct)).keep_warm(1)

    req: api_pb2.FunctionBindParamsRequest = ctx.pop_request("FunctionBindParams")
    function_def: api_pb2.Function = servicer.app_functions[req.function_id]

    _client = typing.cast(modal.client._Client, synchronizer._translate_in(client))
    container_params = deserialize_params(req.serialized_params, function_def, _client)
    args, kwargs = container_params
    assert type(kwargs["x"]) is type(dct)


def test_using_method_on_uninstantiated_cls(recwarn, disable_auto_mount):
    app = App()

    @app.cls(serialized=True)
    class C:
        @method()
        def method(self):
            pass

    assert len(recwarn) == 0
    with pytest.raises(AttributeError):
        C.blah  # type: ignore   # noqa
    assert len(recwarn) == 0

    assert isinstance(C().method, Function)  # should be fine to access on an instance of the class
    assert len(recwarn) == 0

    # The following should warn since it's accessed on the class directly
    C.method  # noqa  # triggers a deprecation warning
    # TODO: this will be an AttributeError or return a non-modal unbound function in the future:
    assert len(recwarn) == 1
    warning_string = str(recwarn[0].message)
    assert "instantiate classes before using methods" in warning_string
    assert "C().method instead of C.method" in warning_string


def test_method_on_cls_access_warns():
    with pytest.warns(match="instantiate classes before using methods"):
        print(Foo.bar)


================================================
File: test/config_test.py
================================================
# Copyright Modal Labs 2022
import os
import pathlib
import pytest
import subprocess
import sys

import toml

import modal
from modal._utils.async_utils import synchronize_api
from modal.config import Config, _lookup_workspace, config
from modal.exception import InvalidError


class CLIException(Exception):
    pass


def _cli(args, env={}):
    lib_dir = pathlib.Path(modal.__file__).parent.parent
    args = [sys.executable, "-m", "modal.cli.entry_point"] + args
    env = {
        **os.environ,
        **env,
        # For windows
        "PYTHONUTF8": "1",
    }
    ret = subprocess.run(args, cwd=lib_dir, env=env, capture_output=True)
    stdout = ret.stdout.decode()
    stderr = ret.stderr.decode()
    if ret.returncode != 0:
        raise CLIException(f"Failed with {ret.returncode} stdout: {stdout} stderr: {stderr}")
    return stdout


def _get_config(env={}):
    stdout = _cli(["config", "show", "--no-redact"], env=env)
    return eval(stdout)


def test_config():
    config = _get_config()
    assert config["server_url"]


def test_config_env_override():
    config = _get_config(env={"MODAL_SERVER_URL": "xyz.corp"})
    assert config["server_url"] == "xyz.corp"


def test_config_store_user(servicer, modal_config):
    with modal_config(show_on_error=True) as config_file_path:
        env = {"MODAL_SERVER_URL": servicer.client_addr}

        servicer.required_creds = {"abc": "xyz", "foo": "bar1", "foo2": "bar2", "ABC": "XYZ", "foo3": "bar3"}

        # No token by default
        config = _get_config(env=env)
        assert config["token_id"] is None

        # Set creds to abc / xyz
        _cli(["token", "set", "--token-id", "abc", "--token-secret", "xyz"], env=env)

        # Make sure an incorrect token fails
        with pytest.raises(CLIException):
            _cli(["token", "set", "--token-id", "abc", "--token-secret", "incorrect"], env=env)

        # Set creds to foo / bar1 for the prof_1 profile
        _cli(
            ["token", "set", "--token-id", "foo", "--token-secret", "bar1", "--profile", "prof_1", "--no-activate"],
            env=env,
        )

        # Set creds to foo2 / bar2 for the prof_2 profile (given as an env var)
        _cli(
            ["token", "set", "--token-id", "foo2", "--token-secret", "bar2", "--no-activate"],
            env={"MODAL_PROFILE": "prof_2", **env},
        )

        # Now these should be stored in the user's home directory
        config = _get_config(env=env)
        assert config["token_id"] == "abc"
        assert config["token_secret"] == "xyz"

        # Make sure it can be overridden too
        config = _get_config(env={"MODAL_TOKEN_ID": "foo", **env})
        assert config["token_id"] == "foo"
        assert config["token_secret"] == "xyz"

        # Check that the profile is named after the workspace username by default
        config = _get_config(env={"MODAL_PROFILE": "test-username", **env})
        assert config["token_id"] == "abc"
        assert config["token_secret"] == "xyz"

        # Check that we can get the prof_1 env creds too
        config = _get_config(env={"MODAL_PROFILE": "prof_1", **env})
        assert config["token_id"] == "foo"
        assert config["token_secret"] == "bar1"

        # Check that we can get the prof_2 env creds too
        config = _get_config(env={"MODAL_PROFILE": "prof_2", **env})
        assert config["token_id"] == "foo2"
        assert config["token_secret"] == "bar2"

        # Check that an empty string falls back to the active profile
        config = _get_config(env={"MODAL_PROFILE": "", **env})
        assert config["token_secret"] == "xyz"

        # Test that only the first profile was explicitly activated
        for profile, profile_config in toml.load(config_file_path).items():
            if profile == "test-username":
                assert profile_config["active"] is True
            else:
                assert "active" not in profile_config

        # Check that we can overwrite the default profile
        _cli(["token", "set", "--token-id", "ABC", "--token-secret", "XYZ"], env=env)
        assert toml.load(config_file_path)["test-username"]["token_id"] == "ABC"

        # Check that we activate a profile by default while setting a token
        _cli(
            ["token", "set", "--token-id", "foo3", "--token-secret", "bar3", "--profile", "prof_3"],
            env=env,
        )
        for profile, profile_config in toml.load(config_file_path).items():
            if profile == "prof_3":
                assert profile_config["active"] is True
            else:
                assert "active" not in profile_config


def test_config_env_override_arbitrary_env():
    """config.override_locally() replaces existing env var if not part of config."""
    key = "NVIDIA_VISIBLE_DEVICES"
    value = "0,1"

    # Place old value in memory.
    os.environ[key] = "none"

    # Expect value to be overwritten.
    config.override_locally(key, value)
    assert os.getenv(key) == value


@pytest.mark.asyncio
async def test_workspace_lookup(servicer, server_url_env):
    servicer.required_creds = {"ak-abc": "as-xyz"}
    resp = await synchronize_api(_lookup_workspace).aio(servicer.client_addr, "ak-abc", "as-xyz")
    assert resp.username == "test-username"


@pytest.mark.parametrize("automount", ["false", "'false'", "'False'", "'0'", 0, "''"])
def test_config_boolean(modal_config, automount):
    modal_toml = f"""
    [prof-1]
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    automount = {automount}
    """
    with modal_config(modal_toml):
        assert not Config().get("automount", "prof-1")


def test_malformed_config_better(modal_config):
    modal_toml = """
    token_id = 'ak-abc'
    token_secret = 'as_xyz'
    """
    with pytest.raises(InvalidError, match="must contain table sections"):
        with modal_config(modal_toml):
            pass

    modal_toml = """
    [default]
    token_id
    """
    with pytest.raises(InvalidError, match="Key name found without value"):
        with modal_config(modal_toml):
            pass


================================================
File: test/container_app_test.py
================================================
# Copyright Modal Labs 2022
import json
import os
import pytest
import time
from contextlib import contextmanager
from unittest import mock

from google.protobuf.empty_pb2 import Empty
from google.protobuf.message import Message

from modal import App, interact
from modal._runtime.container_io_manager import ContainerIOManager
from modal._utils.async_utils import synchronize_api
from modal._utils.grpc_utils import retry_transient_errors
from modal.exception import InvalidError
from modal.running_app import RunningApp
from modal_proto import api_pb2


def my_f_1(x):
    pass


def temp_restore_path(tmpdir):
    # Write out a restore file so that snapshot+restore will complete
    restore_path = tmpdir.join("fake-restore-state.json")
    restore_path.write_text(
        json.dumps(
            {
                "task_id": "ta-i-am-restored",
                "task_secret": "ts-i-am-restored",
                "function_id": "fu-i-am-restored",
            }
        ),
        encoding="utf-8",
    )
    return restore_path


@pytest.mark.asyncio
async def test_container_function_lazily_imported(container_client):
    function_ids: dict[str, str] = {
        "my_f_1": "fu-123",
    }
    object_handle_metadata: dict[str, Message] = {
        "fu-123": api_pb2.FunctionHandleMetadata(),
    }
    container_app = RunningApp("ap-123", function_ids=function_ids, object_handle_metadata=object_handle_metadata)
    app = App()

    # This is normally done in _container_entrypoint
    app._init_container(container_client, container_app)

    # Now, let's create my_f after the app started running and make sure it works
    my_f_container = app.function()(my_f_1)
    assert await my_f_container.remote.aio(42) == 1764  # type: ignore


def square(x):
    pass


@synchronize_api
async def stop_app(client, app_id):
    # helper to ensur we run the rpc from the synchronicity loop - otherwise we can run into weird deadlocks
    return await retry_transient_errors(client.stub.AppStop, api_pb2.AppStopRequest(app_id=app_id))


@contextmanager
def set_env_vars(restore_path, container_addr):
    with mock.patch.dict(
        os.environ,
        {
            "MODAL_RESTORE_STATE_PATH": str(restore_path),
            "MODAL_SERVER_URL": container_addr,
            "MODAL_TASK_ID": "ta-123",
            "MODAL_IS_REMOTE": "1",
        },
    ):
        yield


@pytest.mark.asyncio
async def test_container_snapshot_reference_capture(container_client, tmpdir, servicer, client):
    app = App()
    from modal import Function
    from modal.runner import deploy_app

    app.function()(square)
    app_name = "my-app"
    app_id = deploy_app(app, app_name, client=container_client).app_id
    f = Function.from_name(app_name, "square").hydrate(container_client)
    assert f.object_id == "fu-1"
    await f.remote.aio()
    assert f.object_id == "fu-1"
    io_manager = ContainerIOManager(api_pb2.ContainerArguments(checkpoint_id="ch-123"), container_client)
    restore_path = temp_restore_path(tmpdir)
    with set_env_vars(restore_path, servicer.container_addr):
        io_manager.memory_snapshot()

    # Stop the App, invalidating the fu- ID stored in `f`.
    stop_app(client, app_id)
    # After snapshot-restore the previously looked-up Function should get refreshed and have the
    # new fu- ID. ie. the ID should not be stale and invalid.
    new_app_id = deploy_app(app, app_name, client=client).app_id
    assert new_app_id != app_id
    await f.remote.aio()
    assert f.object_id == "fu-2"
    # Purposefully break FunctionGet to check the hydration is cached.
    del servicer.app_objects[new_app_id]
    await f.remote.aio()  # remote call succeeds because it didn't re-hydrate Function
    assert f.object_id == "fu-2"


def test_container_snapshot_restore_heartbeats(tmpdir, servicer, container_client):
    io_manager = ContainerIOManager(api_pb2.ContainerArguments(checkpoint_id="ch-123"), container_client)
    restore_path = temp_restore_path(tmpdir)

    # Ensure that heartbeats only run after the snapshot
    heartbeat_interval_secs = 0.01
    with io_manager.heartbeats(True):
        with set_env_vars(restore_path, servicer.container_addr):
            with mock.patch("modal.runner.HEARTBEAT_INTERVAL", heartbeat_interval_secs):
                time.sleep(heartbeat_interval_secs * 2)
                assert not list(
                    filter(lambda req: isinstance(req, api_pb2.ContainerHeartbeatRequest), servicer.requests)
                )
                io_manager.memory_snapshot()
                time.sleep(heartbeat_interval_secs * 2)
                assert list(filter(lambda req: isinstance(req, api_pb2.ContainerHeartbeatRequest), servicer.requests))


@pytest.mark.asyncio
async def test_container_debug_snapshot(container_client, tmpdir, servicer):
    # Get an IO manager, where restore takes place
    io_manager = ContainerIOManager(api_pb2.ContainerArguments(checkpoint_id="ch-123"), container_client)
    restore_path = tmpdir.join("fake-restore-state.json")
    # Write the restore file to start a debugger
    restore_path.write_text(
        json.dumps({"snapshot_debug": "1"}),
        encoding="utf-8",
    )

    # Test that the breakpoint was called
    test_breakpoint = mock.Mock()
    with mock.patch("sys.breakpointhook", test_breakpoint):
        with set_env_vars(restore_path, servicer.container_addr):
            io_manager.memory_snapshot()
            test_breakpoint.assert_called_once()


@pytest.mark.asyncio
async def test_rpc_wrapping_restores(container_client, servicer, tmpdir):
    import modal

    io_manager = ContainerIOManager(api_pb2.ContainerArguments(checkpoint_id="ch-123"), container_client)
    restore_path = temp_restore_path(tmpdir)

    d = modal.Dict.from_name("my-amazing-dict", {"xyz": 123}, create_if_missing=True).hydrate(container_client)
    d["abc"] = 42

    with set_env_vars(restore_path, servicer.container_addr):
        io_manager.memory_snapshot()

    # TODO(Jonathon): These RPC wrappers are tested directly because I could not
    # find a way to construct in this test a UnaryStreamWrapper with a stale snapshotted client.
    @synchronize_api
    async def exercise_rpcs():
        n = 0
        # Test UnaryStreamWrapper
        async for _ in container_client.stub.DictContents.unary_stream(
            api_pb2.DictContentsRequest(dict_id=d.object_id, keys=True)
        ):
            n += 1
        assert n == 2
        # Test UnaryUnaryWrapper
        await container_client.stub.DictClear(api_pb2.DictClearRequest(dict_id=d.object_id))

    await exercise_rpcs.aio()


def test_interact(container_client, servicer):
    # Initialize container singleton
    function_def = api_pb2.Function(pty_info=api_pb2.PTYInfo(pty_type=api_pb2.PTYInfo.PTY_TYPE_SHELL))
    ContainerIOManager(api_pb2.ContainerArguments(function_def=function_def), container_client)
    with servicer.intercept() as ctx:
        ctx.add_response("FunctionStartPtyShell", Empty())
        interact()


def test_interact_no_pty_error(container_client, servicer):
    # Initialize container singleton
    ContainerIOManager(api_pb2.ContainerArguments(), container_client)
    with pytest.raises(InvalidError, match=r"modal.interact\(\) without running Modal in interactive mode"):
        interact()


================================================
File: test/container_buffer_test.py
================================================
# Copyright Modal Labs 2024
from modal import App

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function(
    buffer_containers=10,
)
def f1():
    pass


def test_fn_container_buffer(servicer, client):
    with app.run(client=client):
        assert len(servicer.app_functions) == 1
        fn1 = servicer.app_functions["fu-1"]  # f1
        # Test forward / backward compatibility
        assert fn1._experimental_buffer_containers == fn1.autoscaler_settings.buffer_containers == 10


================================================
File: test/decorator_test.py
================================================
# Copyright Modal Labs 2023
import pytest

from modal import App, asgi_app, batched, method, web_endpoint, wsgi_app
from modal.exception import InvalidError


def test_local_entrypoint_forgot_parentheses():
    app = App()

    with pytest.raises(InvalidError, match="local_entrypoint()"):

        @app.local_entrypoint  # type: ignore
        def f():
            pass


def test_function_forgot_parentheses():
    app = App()

    with pytest.raises(InvalidError, match="function()"):

        @app.function  # type: ignore
        def f():
            pass


def test_cls_forgot_parentheses():
    app = App()

    with pytest.raises(InvalidError, match="cls()"):

        @app.cls  # type: ignore
        class XYZ:
            pass


def test_method_forgot_parentheses():
    app = App()

    with pytest.raises(InvalidError, match="method()"):

        @app.cls()
        class XYZ:
            @method  # type: ignore
            def f(self):
                pass


def test_invalid_web_decorator_usage():
    app = App()

    with pytest.raises(InvalidError, match="web_endpoint()"):

        @app.function()  # type: ignore
        @web_endpoint  # type: ignore
        def my_handle():
            pass

    with pytest.raises(InvalidError, match="asgi_app()"):

        @app.function()  # type: ignore
        @asgi_app  # type: ignore
        def my_handle_asgi():
            pass

    with pytest.raises(InvalidError, match="wsgi_app()"):

        @app.function()  # type: ignore
        @wsgi_app  # type: ignore
        def my_handle_wsgi():
            pass


def test_web_endpoint_method():
    app = App()

    with pytest.raises(InvalidError, match="remove the `@method`"):

        @app.cls()
        class Container:
            @method()  # type: ignore
            @web_endpoint()
            def generate(self):
                pass


def test_batch_method():
    app = App()

    with pytest.raises(InvalidError, match="remove the `@method`"):

        @app.cls()
        class Container:
            @method()  # type: ignore
            @batched(max_batch_size=2, wait_ms=0)
            def generate(self):
                pass


================================================
File: test/deprecation_test.py
================================================
# Copyright Modal Labs 2022
import inspect
import pytest

from modal._utils.deprecation import renamed_parameter
from modal.exception import DeprecationError

from .supports.functions import deprecated_function

# Not a pytest unit test, but an extra assertion that we catch issues in global scope too
# See #2228
exc = None
try:
    deprecated_function(42)
except Exception as e:
    exc = e
finally:
    assert isinstance(exc, DeprecationError)  # If you see this, try running `pytest client/client_test`


def test_deprecation():
    # See conftest.py in the root of the repo
    # All deprecation warnings in modal during tests will trigger exceptions
    with pytest.raises(DeprecationError):
        deprecated_function(42)

    # With this context manager, it doesn't raise an exception, but we record
    # the warning. This is the normal behavior outside of pytest.
    with pytest.warns(DeprecationError) as record:
        res = deprecated_function(42)
        assert res == 1764

    # Make sure it raises in the right file
    from .supports import functions

    assert record[0].filename == functions.__file__


@renamed_parameter((2024, 12, 1), "foo", "bar")
def my_func(bar: int) -> int:
    return bar**2


def test_renamed_parameter():
    message = "The 'foo' parameter .+ has been renamed to 'bar'"
    with pytest.warns(DeprecationError, match=message):
        res = my_func(foo=2)  # type: ignore
        assert res == 4
    assert my_func(bar=3) == 9
    assert my_func(4) == 16

    sig = inspect.signature(my_func)
    assert "bar" in sig.parameters
    assert "foo" not in sig.parameters


================================================
File: test/dict_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import time

from modal import Dict
from modal.exception import InvalidError, NotFoundError


def test_dict_app(servicer, client):
    d = Dict.from_name("my-amazing-dict", {"xyz": 123}, create_if_missing=True).hydrate(client)
    d["foo"] = 42
    d["foo"] += 5
    assert d["foo"] == 47
    assert d.len() == 2

    assert sorted(d.keys()) == ["foo", "xyz"]
    assert sorted(d.values()) == [47, 123]
    assert sorted(d.items()) == [("foo", 47), ("xyz", 123)]

    d.clear()
    assert d.len() == 0
    with pytest.raises(KeyError):
        _ = d["foo"]

    assert d.get("foo", default=True)
    d["foo"] = None
    assert d["foo"] is None

    Dict.delete("my-amazing-dict", client=client)
    with pytest.raises(NotFoundError):
        Dict.from_name("my-amazing-dict").hydrate(client)


def test_dict_ephemeral(servicer, client):
    assert servicer.n_dict_heartbeats == 0
    with Dict.ephemeral({"bar": 123}, client=client, _heartbeat_sleep=1) as d:
        d["foo"] = 42
        assert d.len() == 2
        assert d["foo"] == 42
        assert d["bar"] == 123
        time.sleep(1.5)  # Make time for 2 heartbeats
    assert servicer.n_dict_heartbeats == 2


def test_dict_lazy_hydrate_named(set_env_client, servicer):
    with servicer.intercept() as ctx:
        d = Dict.from_name("foo", create_if_missing=True)
        assert len(ctx.get_requests("DictGetOrCreate")) == 0  # sanity check that the get request is lazy
        d["foo"] = 42
        assert d["foo"] == 42
        assert len(ctx.get_requests("DictGetOrCreate")) == 1  # just sanity check that object is only hydrated once...


@pytest.mark.parametrize("name", ["has space", "has/slash", "a" * 65])
def test_invalid_name(servicer, client, name):
    with pytest.raises(InvalidError, match="Invalid Dict name"):
        Dict.from_name(name).hydrate(client)


================================================
File: test/docker_utils_test.py
================================================
# Copyright Modal Labs 2024
import pytest
from pathlib import Path
from tempfile import NamedTemporaryFile, TemporaryDirectory

from modal._utils.docker_utils import extract_copy_command_patterns, find_dockerignore_file


@pytest.mark.parametrize(
    ("copy_commands", "expected_patterns"),
    [
        (
            ["CoPY files/dir1 ./smth_copy"],
            {"files/dir1"},
        ),
        (
            ["COPY files/*.txt /dest/", "COPY files/**/*.py /dest/"],
            {"files/*.txt", "files/**/*.py"},
        ),
        (
            ["COPY files/special/file[[]1].txt /dest/"],
            {"files/special/file[[]1].txt"},
        ),
        (
            ["COPY files/*.txt files/**/*.py /dest/"],
            {"files/*.txt", "files/**/*.py"},
        ),
        (
            [
                "copy --from=a b",
                "copy ./smth \\",
                "./foo.py \\",
                "# this is a comment",
                "./bar.py \\",
                "/x",
            ],
            {"./smth", "./foo.py", "./bar.py"},
        ),
        (
            [
                "COPY --from=a b",
            ],
            set(),
        ),
    ],
)
def test_extract_copy_command_patterns(copy_commands, expected_patterns):
    copy_command_sources = set(extract_copy_command_patterns(copy_commands))
    assert copy_command_sources == expected_patterns


@pytest.mark.usefixtures("tmp_cwd")
def test_image_dockerfile_copy_messy():
    with TemporaryDirectory(dir="./") as tmp_dir:
        dockerfile = NamedTemporaryFile("w", delete=False)
        dockerfile.write(
            f"""
FROM python:3.12-slim

WORKDIR /my-app

RUN ls

# COPY simple directory
    CoPY {tmp_dir}/dir1 ./smth_copy

RUN ls -la

# COPY multiple sources
        COPY {tmp_dir}/test.py {tmp_dir}/file10.txt /

RUN ls \\
    -l

# COPY multiple lines
copy {tmp_dir}/dir2 \\
    {tmp_dir}/file1.txt \\
# this is a comment
    {tmp_dir}/file2.txt \\
    /x

        RUN ls
        """
        )
        dockerfile.close()

        with open(dockerfile.name) as f:
            lines = f.readlines()

        assert sorted(extract_copy_command_patterns(lines)) == sorted(
            [
                f"{tmp_dir}/dir1",
                f"{tmp_dir}/test.py",
                f"{tmp_dir}/file10.txt",
                f"{tmp_dir}/dir2",
                f"{tmp_dir}/file1.txt",
                f"{tmp_dir}/file2.txt",
            ]
        )


@pytest.mark.usefixtures("tmp_cwd")
def test_find_generic_cwd_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()

        dockerfile_path = dir1 / "Dockerfile"
        dockerignore_path = tmp_path / ".dockerignore"
        dockerignore_path.write_text("**/*")
        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) == dockerignore_path


@pytest.mark.usefixtures("tmp_cwd")
def test_dont_find_specific_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()

        dockerfile_path = dir1 / "foo"
        dockerignore_path = tmp_path / "foo.dockerignore"
        dockerignore_path.write_text("**/*")
        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) is None


@pytest.mark.usefixtures("tmp_cwd")
def test_prefer_specific_cwd_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()

        dockerfile_path = tmp_path / "Dockerfile"
        generic_dockerignore_path = tmp_path / ".dockerignore"
        generic_dockerignore_path.write_text("**/*.py")
        specific_dockerignore_path = tmp_path / "Dockerfile.dockerignore"
        specific_dockerignore_path.write_text("**/*")
        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) == specific_dockerignore_path
        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) != generic_dockerignore_path


@pytest.mark.usefixtures("tmp_cwd")
def test_dont_find_nested_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()
        dir2 = dir1 / "dir2"
        dir2.mkdir()

        dockerfile_path = dir1 / "Dockerfile"
        dockerfile_path.write_text("COPY . /dummy")

        # should ignore parent ones
        generic_dockerignore_path = tmp_path / ".dockerignore"
        generic_dockerignore_path.write_text("**/*")
        specific_dockerignore_path = tmp_path / "Dockerfile.dockerignore"
        specific_dockerignore_path.write_text("**/*")

        # should ignore nested ones
        nested_generic_dockerignore_path = dir2 / ".dockerignore"
        nested_generic_dockerignore_path.write_text("**/*")
        nested_specific_dockerignore_path = dir2 / "Dockerfile.dockerignore"
        nested_specific_dockerignore_path.write_text("**/*")

        assert find_dockerignore_file(dir1, dockerfile_path) is None


@pytest.mark.usefixtures("tmp_cwd")
def test_find_next_to_dockerfile_dockerignore_file():
    test_cwd = Path.cwd()
    with TemporaryDirectory(dir=test_cwd) as tmp_dir:
        tmp_path = Path(tmp_dir)
        dir1 = tmp_path / "dir1"
        dir1.mkdir()

        dockerfile_path = dir1 / "Dockerfile"
        dockerignore_path = tmp_path / ".dockerignore"
        dockerignore_path.write_text("**/*")

        assert find_dockerignore_file(test_cwd / tmp_dir, dockerfile_path) == dockerignore_path


================================================
File: test/e2e_test.py
================================================
# Copyright Modal Labs 2022
import os
import pathlib
import subprocess
import sys


def _cli(args, server_url, credentials, extra_env={}, check=True) -> tuple[int, str, str]:
    lib_dir = pathlib.Path(__file__).parent.parent
    args = [sys.executable] + args
    token_id, token_secret = credentials
    env = {
        "MODAL_SERVER_URL": server_url,
        "MODAL_TOKEN_ID": token_id,
        "MODAL_TOKEN_SECRET": token_secret,
        **os.environ,
        "PYTHONUTF8": "1",  # For windows
        **extra_env,
    }
    ret = subprocess.run(args, cwd=lib_dir, env=env, capture_output=True)

    stdout = ret.stdout.decode()
    stderr = ret.stderr.decode()
    if check and ret.returncode != 0:
        raise Exception(f"Failed with {ret.returncode} stdout: {stdout} stderr: {stderr}")
    return ret.returncode, stdout, stderr


def test_run_e2e(servicer, credentials):
    _, _, err = _cli(["-m", "test.supports.script"], servicer.client_addr, credentials)
    assert err == ""


def test_run_progress_info(servicer, credentials):
    returncode, stdout, stderr = _cli(["-m", "test.supports.progress_info"], servicer.client_addr, credentials)
    assert returncode == 0
    assert stderr == ""
    lines = stdout.splitlines()
    assert "Initialized. View run at https://modaltest.com/apps/ap-123" in lines[0]
    assert "App completed" in lines[-1]


def test_run_profiler(servicer, credentials):
    _cli(["-m", "cProfile", "-m", "test.supports.script"], servicer.client_addr, credentials)


def test_run_unconsumed_map(servicer, credentials):
    _, _, err = _cli(["-m", "test.supports.unconsumed_map"], servicer.client_addr, credentials)
    assert "map" in err
    assert "for-loop" in err

    _, _, err = _cli(["-m", "test.supports.consumed_map"], servicer.client_addr, credentials)
    assert "map" not in err
    assert "for-loop" not in err


def test_auth_failure_last_line(servicer, credentials):
    returncode, out, err = _cli(
        ["-m", "test.supports.script"],
        servicer.client_addr,
        credentials,
        extra_env={"MODAL_TOKEN_ID": "bad", "MODAL_TOKEN_SECRET": "bad"},
        check=False,
    )
    try:
        assert returncode != 0
        assert "token" in err.strip().split("\n")[-1]  # err msg should be on the last line
    except Exception:
        print("out:", repr(out))
        print("err:", repr(err))
        raise


================================================
File: test/error_test.py
================================================
# Copyright Modal Labs 2022
from modal import Error
from modal.exception import NotFoundError


def test_modal_errors():
    assert issubclass(NotFoundError, Error)


================================================
File: test/file_io_test.py
================================================
# Copyright Modal Labs 2024
import json
import pytest

from grpclib import Status
from grpclib.exceptions import GRPCError

from modal.file_io import (  # type: ignore
    WRITE_CHUNK_SIZE,
    WRITE_FILE_SIZE_LIMIT,
    FileIO,
    FileWatchEvent,
    FileWatchEventType,
    delete_bytes,
    replace_bytes,
)
from modal_proto import api_pb2

OPEN_EXEC_ID = "exec-open-123"
READ_EXEC_ID = "exec-read-123"
READLINE_EXEC_ID = "exec-readline-123"
READLINES_EXEC_ID = "exec-readlines-123"
WRITE_EXEC_ID = "exec-write-123"
FLUSH_EXEC_ID = "exec-flush-123"
SEEK_EXEC_ID = "exec-seek-123"
WRITE_REPLACE_EXEC_ID = "exec-write-replace-123"
DELETE_EXEC_ID = "exec-delete-123"
CLOSE_EXEC_ID = "exec-close-123"
WATCH_EXEC_ID = "exec-watch-123"
LS_EXEC_ID = "exec-ls-123"
MKDIR_EXEC_ID = "exec-mkdir-123"
RM_EXEC_ID = "exec-rm-123"


async def container_filesystem_exec(servicer, stream):
    req = await stream.recv_message()

    if req.HasField("file_open_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(exec_id=OPEN_EXEC_ID, file_descriptor="fd-123")
        )
    elif req.HasField("file_read_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=READ_EXEC_ID,
            )
        )
    elif req.HasField("file_read_line_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=READLINE_EXEC_ID,
            )
        )
    elif req.HasField("file_write_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=WRITE_EXEC_ID,
            )
        )
    elif req.HasField("file_write_replace_bytes_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=WRITE_REPLACE_EXEC_ID,
            )
        )
    elif req.HasField("file_delete_bytes_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=DELETE_EXEC_ID,
            )
        )
    elif req.HasField("file_seek_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=SEEK_EXEC_ID,
            )
        )
    elif req.HasField("file_flush_request"):
        await stream.send_message(
            api_pb2.ContainerFilesystemExecResponse(
                exec_id=FLUSH_EXEC_ID,
            )
        )
    elif req.HasField("file_close_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=CLOSE_EXEC_ID))
    elif req.HasField("file_watch_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=WATCH_EXEC_ID))
    elif req.HasField("file_ls_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=LS_EXEC_ID))
    elif req.HasField("file_mkdir_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=MKDIR_EXEC_ID))
    elif req.HasField("file_rm_request"):
        await stream.send_message(api_pb2.ContainerFilesystemExecResponse(exec_id=RM_EXEC_ID))


def test_file_read(servicer, client):
    """Test file reading."""
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "r", client, "task-123")
        assert f.read() == content
        f.close()


def test_file_write(servicer, client):
    """Test file writing."""
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write(content)
        assert f.read() == content
        f.close()


def test_file_write_large(servicer, client):
    """Test file write chunking logic."""
    content = "A" * WRITE_FILE_SIZE_LIMIT
    write_counter = 0

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal write_counter
        req = await stream.recv_message()
        if req.exec_id == WRITE_EXEC_ID:
            write_counter += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write(content)
        assert write_counter == WRITE_FILE_SIZE_LIMIT // WRITE_CHUNK_SIZE
        f.close()


def test_file_write_too_large(servicer, client):
    """Test that writing a file larger than WRITE_FILE_SIZE_LIMIT raises an error."""
    content = "A" * (WRITE_FILE_SIZE_LIMIT + 1)

    async def container_filesystem_exec_get_output(servicer, stream):
        await stream.recv_message()
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        with pytest.raises(ValueError):
            FileIO.create("/test.txt", "a+", client, "task-123").write(content)


def test_file_readline(servicer, client):
    """Test file reading line by line."""
    lines = ["foo\n", "bar\n", "baz\n", "end"]
    content = "".join(lines)
    index = 0

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == READLINE_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[lines[index].encode()]))
            index += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.readline() == "foo\n"
        assert f.readline() == "bar\n"
        assert f.readline() == "baz\n"
        assert f.readline() == "end"
        f.close()


def test_file_readlines_no_newline_end(servicer, client):
    """Test file reading lines."""
    lines = ["foo\n", "bar\n", "baz\n", "end"]
    content = "".join(lines)

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.readlines() == lines
        f.close()


def test_file_readlines_newline_end(servicer, client):
    """Test file reading lines."""
    lines = ["foo\n", "bar\n", "baz\n", "end\n"]
    content = "".join(lines)

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.readlines() == lines
        f.close()


def test_file_readlines_multiple_newline_end(servicer, client):
    """Test file reading lines."""
    lines = ["foo\n", "bar\n", "baz\n", "end\n", "\n", "\n"]
    content = "".join(lines)

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.readlines() == lines
        f.close()


def test_file_flush(servicer, client):
    """Test file flushing."""

    async def container_filesystem_exec_get_output(servicer, stream):
        await stream.recv_message()
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write("foo")
        f.flush()
        f.close()


def test_file_seek(servicer, client):
    """Test file seeking."""
    index = 0
    expected_outputs = ["foo\nbar\nbaz\n", "bar\nbaz\n", "baz\n", ""]

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[expected_outputs[index].encode()]))
            index += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write("foo\nbar\nbaz\n")
        f.close()
        f = FileIO.create("/test.txt", "r", client, "task-123")
        for i in range(4):
            f.seek(3)
            assert f.read() == expected_outputs[i]
        f.close()


def test_file_write_replace_bytes(servicer, client):
    """Test file write replace bytes."""
    index = 0
    expected_outputs = ["foo\nbar\nbaz\n", "foo\nbarbar\nbaz\n"]

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[expected_outputs[index].encode()]))
            index += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write("foo\nbar\nbaz\n")
        assert f.read() == expected_outputs[0]
        replace_bytes(f, data=b"barbar", start=4, end=7)
        assert f.read() == expected_outputs[1]
        f.close()


def test_file_delete_bytes(servicer, client):
    """Test file delete bytes."""
    index = 0
    expected_outputs = ["foo\nbar\nbaz\n", "foo\nbaz\n"]

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[expected_outputs[index].encode()]))
            index += 1
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "a+", client, "task-123")
        f.write("foo\nbar\nbaz\n")
        assert f.read() == expected_outputs[0]
        delete_bytes(f, start=4, end=7)
        assert f.read() == expected_outputs[1]
        f.close()


def test_invalid_mode(servicer, client):
    """Test a variety of invalid modes."""
    invalid_modes = [
        "",  # empty mode
        "invalid",  # invalid mode
        "rr",  # duplicate letters
        "rab",  # too many modes
        "r++",  # too many modes
        "+",  # plus without read/write mode
        "x+r",  # too many modes
        "wx",  # too many modes
        "rbb",  # too many binary flags
        " r",  # whitespace
        "r ",  # whitespace
        "R",  # uppercase
        "W",  # uppercase
        "\n",  # newline
    ]
    for mode in invalid_modes:
        with pytest.raises(ValueError):
            FileIO.create("/test.txt", mode, client, "task-123")  # type: ignore


def test_client_retry(servicer, client):
    """Test client retry."""
    retries = 5
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal retries
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            if retries > 0:
                retries -= 1
                raise GRPCError(Status.UNAVAILABLE, "test")
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        f = FileIO.create("/test.txt", "w+", client, "task-123")
        f.write(content)
        assert f.read() == content
        f.close()


def test_file_watch(servicer, client):
    """Test file watching."""
    expected_events = [
        FileWatchEvent(paths=["/foo.txt"], type=FileWatchEventType.Access),
        FileWatchEvent(paths=["/bar.txt"], type=FileWatchEventType.Create),
        FileWatchEvent(paths=["/baz.txt", "/baz/foo.txt"], type=FileWatchEventType.Modify),
    ]

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == WATCH_EXEC_ID:
            for event in expected_events:
                await stream.send_message(
                    api_pb2.FilesystemRuntimeOutputBatch(
                        output=[
                            f'{{"paths": {json.dumps(event.paths)}, "event_type": "{event.type.value}"}}\n\n'.encode()
                        ]
                    )
                )
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        events = FileIO.watch("/test.txt", client, "task-123")
        seen_events: list[FileWatchEvent] = []
        for event in events:
            seen_events.append(event)
        assert len(seen_events) == len(expected_events)
        for e, se in zip(expected_events, seen_events):
            assert e.paths == se.paths
            assert e.type == se.type


def test_file_watch_with_filter(servicer, client):
    """Test file watching with filter."""
    expected_events = [
        FileWatchEvent(paths=["/foo.txt"], type=FileWatchEventType.Access),
        FileWatchEvent(paths=["/bar.txt"], type=FileWatchEventType.Create),
        FileWatchEvent(paths=["/baz.txt", "/baz/foo.txt"], type=FileWatchEventType.Modify),
    ]

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == WATCH_EXEC_ID:
            for event in expected_events:
                await stream.send_message(
                    api_pb2.FilesystemRuntimeOutputBatch(
                        output=[
                            f'{{"paths": {json.dumps(event.paths)}, "event_type": "{event.type.value}"}}\n\n'.encode()
                        ]
                    )
                )
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        events = FileIO.watch("/test.txt", client, "task-123", filter=[FileWatchEventType.Access])
        seen_events: list[FileWatchEvent] = []
        for event in events:
            seen_events.append(event)
        assert len(seen_events) == 1
        assert seen_events[0].paths == expected_events[0].paths
        assert seen_events[0].type == expected_events[0].type


def test_file_watch_ignore_invalid_events(servicer, client):
    """Test file watching ignores invalid events."""
    index = 0
    expected_events = [
        FileWatchEvent(paths=["/foo.txt"], type=FileWatchEventType.Access),
        FileWatchEvent(paths=["/bar.txt"], type=FileWatchEventType.Create),
        FileWatchEvent(paths=["/baz.txt", "/baz/foo.txt"], type=FileWatchEventType.Modify),
    ]
    raw_events = []
    for i, event in enumerate(expected_events):
        raw_events.append(f'{{"paths": {json.dumps(event.paths)}, "event_type": "{event.type.value}"}}\n\n'.encode())
        if i % 2 == 0:
            # interweave invalid events to test that they are ignored
            raw_events.append(b"invalid\n\n")

    async def container_filesystem_exec_get_output(servicer, stream):
        nonlocal index
        req = await stream.recv_message()
        if req.exec_id == WATCH_EXEC_ID:
            for event in raw_events:
                await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[event]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        events = FileIO.watch("/test.txt", client, "task-123")
        seen_events: list[FileWatchEvent] = []
        for event in events:
            seen_events.append(event)
        assert len(seen_events) == len(expected_events)
        for e, se in zip(expected_events, seen_events):
            assert e.paths == se.paths
            assert e.type == se.type


@pytest.mark.asyncio
async def test_file_io_async_context_manager(servicer, client):
    """Test file io context manager."""
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        async with FileIO.create("/test.txt", "w+", client, "task-123") as f:
            await f.write.aio(content)
            assert await f.read.aio() == content


def test_file_io_sync_context_manager(servicer, client):
    """Test file io context manager."""
    content = "foo\nbar\nbaz\n"

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == READ_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(output=[content.encode()]))
        await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        with FileIO.create("/test.txt", "w+", client, "task-123") as f:
            f.write(content)
            assert f.read() == content


def test_ls(servicer, client):
    """Test ls."""

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == LS_EXEC_ID:
            await stream.send_message(
                api_pb2.FilesystemRuntimeOutputBatch(output=[b'{"paths": ["foo", "bar", "baz"]}'])
            )
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))
        else:
            raise Exception("Unexpected exec_id: " + req.exec_id)

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        files = FileIO.ls("/test.txt", client, "task-123")
        assert files == ["foo", "bar", "baz"]


def test_mkdir(servicer, client):
    """Test mkdir."""

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == MKDIR_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))
        else:
            raise Exception("Unexpected exec_id: " + req.exec_id)

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        FileIO.mkdir("/test.txt", client, "task-123")


def test_rm(servicer, client):
    """Test rm."""

    async def container_filesystem_exec_get_output(servicer, stream):
        req = await stream.recv_message()
        if req.exec_id == RM_EXEC_ID:
            await stream.send_message(api_pb2.FilesystemRuntimeOutputBatch(eof=True))
        else:
            raise Exception("Unexpected exec_id: " + req.exec_id)

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerFilesystemExec", container_filesystem_exec)
        ctx.set_responder("ContainerFilesystemExecGetOutput", container_filesystem_exec_get_output)

        FileIO.rm("/test.txt", client, "task-123")


================================================
File: test/file_pattern_matcher_test.py
================================================
# Copyright Modal Labs 2024

"""Tests for file_pattern_matcher.py.

These are ported from the original patternmatcher Go library.
"""

import contextlib
import os
import os.path
import platform
import pytest
from pathlib import Path

from modal import FilePatternMatcher


def test_wildcard_matches():
    assert FilePatternMatcher("*")._matches("fileutils.go")


def test_pattern_matches():
    assert FilePatternMatcher("*.go")._matches("fileutils.go")


def test_exclusion_pattern_matches_pattern_before():
    assert FilePatternMatcher("!fileutils.go", "*.go")._matches("fileutils.go")


def test_pattern_matches_folder_exclusions():
    assert not FilePatternMatcher("docs", "!docs/README.md")._matches("docs/README.md")


def test_pattern_matches_folder_with_slash_exclusions():
    assert not FilePatternMatcher("docs/", "!docs/README.md")._matches("docs/README.md")


def test_pattern_matches_folder_wildcard_exclusions():
    assert not FilePatternMatcher("docs/*", "!docs/README.md")._matches("docs/README.md")


def test_exclusion_pattern_matches_pattern_after():
    assert not FilePatternMatcher("*.go", "!fileutils.go")._matches("fileutils.go")


def test_exclusion_pattern_matches_whole_directory():
    assert not FilePatternMatcher("*.go")._matches(".")


def test_single_exclamation_error():
    try:
        FilePatternMatcher("!")
    except ValueError as e:
        assert str(e) == 'Illegal exclusion pattern: "!"'


def test_matches_with_no_patterns():
    assert not FilePatternMatcher()._matches("/any/path/there")


def test_matches_with_malformed_patterns():
    try:
        FilePatternMatcher("[")
    except ValueError as e:
        assert str(e) == "Bad pattern: ["


def test_matches():
    tests = [
        ("**", "file", True),
        ("**", "file/", True),
        ("**/", "file", True),  # weird one
        ("**/", "file/", True),
        ("**", "/", True),
        ("**/", "/", True),
        ("**", "dir/file", True),
        ("**/", "dir/file", True),
        ("**", "dir/file/", True),
        ("**/", "dir/file/", True),
        ("**/**", "dir/file", True),
        ("**/**", "dir/file/", True),
        ("dir/**", "dir/file", True),
        ("dir/**", "dir/file/", True),
        ("dir/**", "dir/dir2/file", True),
        ("dir/**", "dir/dir2/file/", True),
        ("**/dir", "dir", True),
        ("**/dir", "dir/file", True),
        ("**/dir2/*", "dir/dir2/file", True),
        ("**/dir2/*", "dir/dir2/file/", True),
        ("**/dir2/**", "dir/dir2/dir3/file", True),
        ("**/dir2/**", "dir/dir2/dir3/file/", True),
        ("**file", "file", True),
        ("**file", "dir/file", True),
        ("**/file", "dir/file", True),
        ("**file", "dir/dir/file", True),
        ("**/file", "dir/dir/file", True),
        ("**/file*", "dir/dir/file", True),
        ("**/file*", "dir/dir/file.txt", True),
        ("**/file*txt", "dir/dir/file.txt", True),
        ("**/file*.txt", "dir/dir/file.txt", True),
        ("**/file*.txt*", "dir/dir/file.txt", True),
        ("**/**/*.txt", "dir/dir/file.txt", True),
        ("**/**/*.txt2", "dir/dir/file.txt", False),
        ("**/*.txt", "file.txt", True),
        ("**/**/*.txt", "file.txt", True),
        ("a**/*.txt", "a/file.txt", True),
        ("a**/*.txt", "a/dir/file.txt", True),
        ("a**/*.txt", "a/dir/dir/file.txt", True),
        ("a/*.txt", "a/dir/file.txt", False),
        ("a/*.txt", "a/file.txt", True),
        ("a/*.txt**", "a/file.txt", True),
        ("a[b-d]e", "ae", False),
        ("a[b-d]e", "ace", True),
        ("a[b-d]e", "aae", False),
        ("a[^b-d]e", "aze", True),
        (".*", ".foo", True),
        (".*", "foo", False),
        ("abc.def", "abcdef", False),
        ("abc.def", "abc.def", True),
        ("abc.def", "abcZdef", False),
        ("abc?def", "abcZdef", True),
        ("abc?def", "abcdef", False),
        ("a\\\\", "a\\", True),
        ("**/foo/bar", "foo/bar", True),
        ("**/foo/bar", "dir/foo/bar", True),
        ("**/foo/bar", "dir/dir2/foo/bar", True),
        ("abc/**", "abc", False),
        ("abc/**", "abc/def", True),
        ("abc/**", "abc/def/ghi", True),
        ("**/.foo", ".foo", True),
        ("**/.foo", "bar.foo", False),
        ("a(b)c/def", "a(b)c/def", True),
        ("a(b)c/def", "a(b)c/xyz", False),
        ("a.|)$(}+{bc", "a.|)$(}+{bc", True),
        (
            "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl",
            "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl",
            True,
        ),
        ("dist/*.whl", "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl", True),
    ]

    multi_pattern_tests = [
        (["**", "!util/docker/web"], "util/docker/web/foo", False),
        (["**", "!util/docker/web", "util/docker/web/foo"], "util/docker/web/foo", True),
        (
            ["**", "!dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl"],
            "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl",
            False,
        ),
        (["**", "!dist/*.whl"], "dist/proxy.py-2.4.0rc3.dev36+g08acad9-py3-none-any.whl", False),
    ]

    for pattern, text, expected in tests:
        assert FilePatternMatcher(pattern)._matches(text) is expected

    for patterns, text, expected in multi_pattern_tests:
        assert FilePatternMatcher(*patterns)._matches(text) is expected


def test_clean_patterns():
    patterns = ["docs", "config"]
    pm = FilePatternMatcher(*patterns)
    cleaned = pm.patterns
    assert len(cleaned) == 2


def test_clean_patterns_strip_empty_patterns():
    patterns = ["docs", "config", ""]
    pm = FilePatternMatcher(*patterns)
    cleaned = pm.patterns
    assert len(cleaned) == 2


def test_clean_patterns_exception_flag():
    patterns = ["docs", "!docs/README.md"]
    pm = FilePatternMatcher(*patterns)
    assert any(p.exclusion for p in pm.patterns)


def test_clean_patterns_leading_space_trimmed():
    patterns = ["docs", "  !docs/README.md"]
    pm = FilePatternMatcher(*patterns)
    assert any(p.exclusion for p in pm.patterns)


def test_clean_patterns_trailing_space_trimmed():
    patterns = ["docs", "!docs/README.md  "]
    pm = FilePatternMatcher(*patterns)
    assert any(p.exclusion for p in pm.patterns)


def test_clean_patterns_error_single_exception():
    patterns = ["!"]
    try:
        FilePatternMatcher(*patterns)
    except ValueError as e:
        assert str(e) == 'Illegal exclusion pattern: "!"'


def test_match():
    match_tests = [
        ("abc", "abc", True, None),
        ("*", "abc", True, None),
        ("*c", "abc", True, None),
        ("a*", "a", True, None),
        ("a*", "abc", True, None),
        ("a*", "ab/c", True, None),
        ("a*/b", "abc/b", True, None),
        ("a*/b", "a/c/b", False, None),
        ("a*b*c*d*e*/f", "axbxcxdxe/f", True, None),
        ("a*b*c*d*e*/f", "axbxcxdxexxx/f", True, None),
        ("a*b*c*d*e*/f", "axbxcxdxe/xxx/f", False, None),
        ("a*b*c*d*e*/f", "axbxcxdxexxx/fff", False, None),
        ("a*b?c*x", "abxbbxdbxebxczzx", True, None),
        ("a*b?c*x", "abxbbxdbxebxczzy", False, None),
        ("ab[c]", "abc", True, None),
        ("ab[b-d]", "abc", True, None),
        ("ab[e-g]", "abc", False, None),
        ("ab[^c]", "abc", False, None),
        ("ab[^b-d]", "abc", False, None),
        ("ab[^e-g]", "abc", True, None),
        ("a\\*b", "a*b", True, None),
        ("a\\*b", "ab", False, None),
        ("a?b", "a☺b", True, None),
        ("a[^a]b", "a☺b", True, None),
        ("a???b", "a☺b", False, None),
        ("a[^a][^a][^a]b", "a☺b", False, None),
        ("[a-ζ]*", "α", True, None),
        ("*[a-ζ]", "A", False, None),
        ("a?b", "a/b", False, None),
        ("a*b", "a/b", False, None),
        ("[\\]a]", "]", True, None),
        ("[\\-]", "-", True, None),
        ("[x\\-]", "x", True, None),
        ("[x\\-]", "-", True, None),
        ("[x\\-]", "z", False, None),
        ("[\\-x]", "x", True, None),
        ("[\\-x]", "-", True, None),
        ("[\\-x]", "a", False, None),
        # These do not return errors because the Python re.compile() method does
        # not raise an error on invalid syntax like Go does. We can omit the
        # tests though since it doesn't affect behavior on _correct_ syntax.
        #
        # ("[]a]", "]", False, ValueError),
        # ("[-]", "-", False, ValueError),
        # ("[x-]", "x", False, ValueError),
        # ("[x-]", "-", False, ValueError),
        # ("[x-]", "z", False, ValueError),
        # ("[-x]", "x", False, ValueError),
        # ("[-x]", "-", False, ValueError),
        # ("[-x]", "a", False, ValueError),
        # ("\\", "a", False, ValueError),
        # ("[a-b-c]", "a", False, ValueError),
        # ("[", "a", False, ValueError),
        # ("[^", "a", False, ValueError),
        # ("[^bc", "a", False, ValueError),
        # ("a[", "a", False, ValueError),
        # ("a[", "ab", False, ValueError),
        ("*x", "xxx", True, None),
    ]

    for pattern, text, expected, error in match_tests:
        if platform.system() == "Windows":
            if "\\" in pattern:
                # No escape allowed on Windows.
                continue
            pattern = os.path.normpath(pattern)
            text = os.path.normpath(text)

        with pytest.raises(error) if error else contextlib.nullcontext():
            assert FilePatternMatcher(pattern)._matches(text) is expected


def __helper_get_file_paths(tmp_path: Path) -> list[Path]:
    file_paths = []
    for root, _, files in os.walk(tmp_path):
        for file in files:
            file_paths.append(Path(os.path.join(root, file)))
    return file_paths


def test_against_paths(tmp_path_with_content):
    tmp_path = tmp_path_with_content
    file_paths = __helper_get_file_paths(tmp_path)

    # match everything that's not ignored
    lff = FilePatternMatcher("**/*", "!**/module")

    for file_path in file_paths:
        if "module" in str(file_path):
            assert not lff(file_path)
        else:
            assert lff(file_path)

    lff = FilePatternMatcher("**/*.py")

    for file_path in file_paths:
        if str(file_path).endswith(".py"):
            assert lff(file_path)
        else:
            assert not lff(file_path)


def test_empty_patterns(tmp_path_with_content):
    tmp_path = tmp_path_with_content
    file_paths = __helper_get_file_paths(tmp_path)
    lff = FilePatternMatcher()

    for file_path in file_paths:
        assert not lff(file_path)


def test_invert_patterns(tmp_path_with_content):
    tmp_path = tmp_path_with_content
    file_paths = __helper_get_file_paths(tmp_path)

    # match everything that's not ignored
    lff = ~FilePatternMatcher("**/*", "!**/module")

    for file_path in file_paths:
        if "module" in str(file_path):
            assert lff(file_path)
        else:
            assert not lff(file_path)

    # empty patterns should match nothing
    # inverted empty patterns should match everything
    lff = ~FilePatternMatcher()
    for file_path in file_paths:
        assert lff(file_path)

    # single negative pattern should match nothing
    lff = FilePatternMatcher("!**/*.txt")
    for file_path in file_paths:
        assert not lff(file_path)


@pytest.mark.usefixtures("tmp_cwd")
@pytest.mark.parametrize("as_type", [Path, str])
def test_from_file(as_type):
    rel_top_dir = Path("top")
    rel_top_dir.mkdir()
    ignore_file = rel_top_dir / "pattern_file"
    ignore_file.write_text("**/*.txt")

    lff = FilePatternMatcher.from_file(as_type(ignore_file))
    assert lff(Path("top/data.txt"))
    assert not lff(Path("top/data.py"))


================================================
File: test/fork_test.py
================================================
# Copyright Modal Labs 2024
import subprocess
import sys

from test.supports.skip import skip_windows


@skip_windows("fork not supported on windows")
def test_process_fork(supports_dir, server_url_env, token_env):
    output = subprocess.check_output([sys.executable, supports_dir / "forking.py"], timeout=2, encoding="utf8")

    success_pids = {int(x) for x in output.split()}
    assert len(success_pids) == 2


================================================
File: test/function_retry_test.py
================================================
# Copyright Modal Labs 2024
import pytest

import modal
from modal import App
from modal.retries import RetryManager
from modal_proto import api_pb2

function_call_count = 0


@pytest.fixture(autouse=True)
def reset_function_call_count(monkeypatch):
    # Set default retry delay to something small so we don't slow down tests
    monkeypatch.setattr("modal.retries.MIN_INPUT_RETRY_DELAY_MS", 0.00001)
    global function_call_count
    function_call_count = 0


class FunctionCallCountException(Exception):
    """
    An exception which lets us report to the test how many times a function was called.
    """

    def __init__(self, function_call_count):
        self.function_call_count = function_call_count


def counting_function(attempt_to_return_success: int):
    """
    A function that updates the global function_call_count counter each time it is called.

    """
    global function_call_count
    function_call_count += 1
    if function_call_count < attempt_to_return_success:
        raise FunctionCallCountException(function_call_count)
    return function_call_count


@pytest.fixture
def setup_app_and_function(servicer):
    app = App()
    servicer.function_body(counting_function)
    retries = modal.Retries(
        max_retries=3,
        backoff_coefficient=1.0,
        initial_delay=0,
    )
    f = app.function(retries=retries)(counting_function)
    return app, f


def test_all_retries_fail_raises_error(client, setup_app_and_function, monkeypatch):
    monkeypatch.setenv("MODAL_CLIENT_RETRIES", "true")
    app, f = setup_app_and_function
    with app.run(client=client):
        with pytest.raises(FunctionCallCountException) as exc_info:
            # The client should give up after the 4th call.
            f.remote(5)
        # Assert the function was called 4 times - the original call plus 3 retries
        assert exc_info.value.function_call_count == 4


def test_failures_followed_by_success(client, setup_app_and_function, monkeypatch):
    monkeypatch.setenv("MODAL_CLIENT_RETRIES", "true")
    app, f = setup_app_and_function
    with app.run(client=client):
        function_call_count = f.remote(3)
        assert function_call_count == 3


def test_no_retries_when_first_call_succeeds(client, setup_app_and_function, monkeypatch):
    monkeypatch.setenv("MODAL_CLIENT_RETRIES", "true")
    app, f = setup_app_and_function
    with app.run(client=client):
        function_call_count = f.remote(1)
        assert function_call_count == 1


def test_retry_dealy_ms():
    with pytest.raises(ValueError):
        RetryManager._retry_delay_ms(0, api_pb2.FunctionRetryPolicy())

    retry_policy = api_pb2.FunctionRetryPolicy(retries=2, backoff_coefficient=3, initial_delay_ms=2000)
    assert RetryManager._retry_delay_ms(1, retry_policy) == 2000

    retry_policy = api_pb2.FunctionRetryPolicy(retries=2, backoff_coefficient=3, initial_delay_ms=2000)
    assert RetryManager._retry_delay_ms(2, retry_policy) == 6000


def test_lost_inputs_retried(client, setup_app_and_function, monkeypatch, servicer):
    monkeypatch.setenv("MODAL_CLIENT_RETRIES", "true")
    app, f = setup_app_and_function
    # The client should retry if it receives a internal failure status.
    servicer.failure_status = api_pb2.GenericResult.GENERIC_STATUS_INTERNAL_FAILURE

    with app.run(client=client):
        f.remote(10)
        # Assert the function was called 10 times
        assert function_call_count == 10


================================================
File: test/function_serialization_test.py
================================================
# Copyright Modal Labs 2023
import pytest

from modal import App
from modal._serialization import deserialize


@pytest.mark.asyncio
async def test_serialize_deserialize_function(servicer, client):
    app = App()

    @app.function(serialized=True, name="foo")
    def foo():
        2 * foo.remote()

    assert not foo.is_hydrated
    with pytest.raises(Exception):
        foo.object_id  # noqa

    with app.run(client=client):
        object_id = foo.object_id

    assert object_id is not None
    assert {object_id} == servicer.precreated_functions

    foo_def = servicer.app_functions[object_id]

    assert len(servicer.client_calls) == 0

    deserialized_function_body = deserialize(foo_def.function_serialized, client)
    deserialized_function_body()  # call locally as if in container, this should trigger a "remote" foo() call
    assert len(servicer.client_calls) == 1
    function_call_id = list(servicer.client_calls.keys())[0]
    assert servicer.function_id_for_function_call[function_call_id] == object_id


================================================
File: test/function_test.py
================================================
# Copyright Modal Labs 2022
import asyncio
import inspect
import os
import pytest
import time
import typing
from contextlib import contextmanager

from synchronicity.exceptions import UserCodeException

import modal
from modal import App, Image, NetworkFileSystem, Proxy, asgi_app, batched, web_endpoint
from modal._utils.async_utils import synchronize_api
from modal._vendor import cloudpickle
from modal.exception import DeprecationError, ExecutionError, InvalidError
from modal.functions import Function, FunctionCall
from modal.runner import deploy_app
from modal_proto import api_pb2
from test.helpers import deploy_app_externally

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


if os.environ.get("GITHUB_ACTIONS") == "true":
    TIME_TOLERANCE = 0.25
else:
    TIME_TOLERANCE = 0.05


@app.function()
def foo(p, q):
    return p + q + 11  # not actually used in test (servicer returns sum of square of all args)


@app.function()
async def async_foo(p, q):
    return p + q + 12


def dummy():
    pass  # not actually used in test (servicer returns sum of square of all args)


def test_run_function(client, servicer):
    assert len(servicer.cleared_function_calls) == 0
    with app.run(client=client):
        assert foo.remote(2, 4) == 20
        assert len(servicer.cleared_function_calls) == 1


def test_single_input_function_call_uses_single_rpc(client, servicer):
    with app.run(client=client):
        with servicer.intercept() as ctx:
            assert foo.remote(2, 4) == 20
        assert len(ctx.calls) == 2
        (msg1_type, msg1), (msg2_type, msg2) = ctx.calls
        assert msg1_type == "FunctionMap"
        assert msg2_type == "FunctionGetOutputs"


@pytest.mark.asyncio
async def test_call_function_locally(client, servicer):
    assert foo.local(22, 44) == 77  # call it locally
    assert await async_foo.local(22, 44) == 78

    with app.run(client=client):
        assert foo.remote(2, 4) == 20
        assert async_foo.remote(2, 4) == 20
        assert await async_foo.remote.aio(2, 4) == 20


@pytest.mark.parametrize("slow_put_inputs", [False, True])
@pytest.mark.timeout(120)
def test_map(client, servicer, slow_put_inputs):
    servicer.slow_put_inputs = slow_put_inputs

    app = App()
    dummy_modal = app.function()(dummy)

    assert len(servicer.cleared_function_calls) == 0
    with app.run(client=client):
        assert list(dummy_modal.map([5, 2], [4, 3])) == [41, 13]
        assert len(servicer.cleared_function_calls) == 1
        assert set(dummy_modal.map([5, 2], [4, 3], order_outputs=False)) == {13, 41}
        assert len(servicer.cleared_function_calls) == 2


@pytest.mark.asyncio
async def test_map_async_generator(client):
    app = App()
    dummy_modal = app.function()(dummy)

    async def gen_num():
        yield 2
        yield 3

    async with app.run(client=client):
        res = [num async for num in dummy_modal.map.aio(gen_num())]
        assert res == [4, 9]


def _pow2(x: int):
    return x**2


@contextmanager
def synchronicity_loop_delay_tracker():
    done = False

    async def _track_eventloop_blocking():
        max_dur = 0.0
        BLOCK_TIME = 0.01
        while not done:
            t0 = time.perf_counter()
            await asyncio.sleep(BLOCK_TIME)
            max_dur = max(max_dur, time.perf_counter() - t0)
        return max_dur - BLOCK_TIME  # if it takes exactly BLOCK_TIME we would have zero delay

    track_eventloop_blocking = synchronize_api(_track_eventloop_blocking)
    yield track_eventloop_blocking(_future=True)
    done = True


def test_map_blocking_iterator_blocking_synchronicity_loop(client):
    app = App()
    SLEEP_DUR = 0.5

    def blocking_iter():
        yield 1
        time.sleep(SLEEP_DUR)
        yield 2

    pow2 = app.function()(_pow2)

    with app.run(client=client):
        t0 = time.monotonic()
        with synchronicity_loop_delay_tracker() as max_delay:
            for _ in pow2.map(blocking_iter()):
                pass
        dur = time.monotonic() - t0
    assert dur >= SLEEP_DUR
    assert max_delay.result() < TIME_TOLERANCE  # should typically be much smaller than this


@pytest.mark.asyncio
async def test_map_blocking_iterator_blocking_synchronicity_loop_async(client):
    app = App()
    SLEEP_DUR = 0.5

    def blocking_iter():
        yield 1
        time.sleep(SLEEP_DUR)
        yield 2

    pow2 = app.function()(_pow2)

    async with app.run(client=client):
        t0 = time.monotonic()
        with synchronicity_loop_delay_tracker() as max_delay:
            async for _ in pow2.map.aio(blocking_iter()):
                pass
        dur = time.monotonic() - t0
    assert dur >= SLEEP_DUR
    assert max_delay.result() < TIME_TOLERANCE  # should typically be much smaller than this


_side_effect_count = 0


def side_effect(_):
    global _side_effect_count
    _side_effect_count += 1


def test_for_each(client, servicer):
    app = App()
    servicer.function_body(side_effect)
    side_effect_modal = app.function()(side_effect)
    assert _side_effect_count == 0
    with app.run(client=client):
        side_effect_modal.for_each(range(10))

    assert _side_effect_count == 10


def custom_function(x):
    if x % 2 == 0:
        return x


def test_map_none_values(client, servicer):
    app = App()
    servicer.function_body(custom_function)
    custom_function_modal = app.function()(custom_function)

    with app.run(client=client):
        assert list(custom_function_modal.map(range(4))) == [0, None, 2, None]


def test_starmap(client):
    app = App()

    dummy_modal = app.function()(dummy)
    with app.run(client=client):
        assert list(dummy_modal.starmap([[5, 2], [4, 3]])) == [29, 25]


def test_function_memory_request(client):
    app = App()
    app.function(memory=2048)(dummy)


def test_function_memory_limit(client):
    app = App()
    f = app.function(memory=(2048, 4096))(dummy)

    with app.run(client=client):
        f.remote()

    g = app.function(memory=(2048, 2048 - 1))(custom_function)
    with pytest.raises(InvalidError), app.run(client=client):
        g.remote(0)


def test_function_cpu_request(client, servicer):
    app = App()
    f = app.function(cpu=2.0)(dummy)

    with app.run(client=client):
        f.remote()
        assert servicer.app_functions["fu-1"].resources.milli_cpu == 2000
        assert servicer.app_functions["fu-1"].resources.milli_cpu_max == 0
    assert f.spec.cpu == 2.0

    app = App()
    g = app.function(cpu=7)(dummy)

    with app.run(client=client):
        g.remote()
        assert servicer.app_functions["fu-2"].resources.milli_cpu == 7000
        assert servicer.app_functions["fu-2"].resources.milli_cpu_max == 0
    assert g.spec.cpu == 7


def test_function_cpu_limit(client, servicer):
    app = App()
    f = app.function(cpu=(1, 3))(dummy)
    assert f.spec.cpu == (1, 3)

    with app.run(client=client):
        f.remote()
        assert servicer.app_functions["fu-1"].resources.milli_cpu == 1000
        assert servicer.app_functions["fu-1"].resources.milli_cpu_max == 3000

    g = app.function(cpu=(1, 0.5))(custom_function)
    with pytest.raises(InvalidError), app.run(client=client):
        g.remote(0)


def test_function_disk_request(client):
    app = App()
    app.function(ephemeral_disk=1_000_000)(dummy)


def test_scaledown_window_must_be_positive():
    app = App()
    with pytest.raises(InvalidError, match="must be > 0"):
        app.function(scaledown_window=0)(dummy)


def later():
    return "hello"


def test_function_future(client, servicer):
    app = App()

    servicer.function_body(later)
    later_modal = app.function()(later)
    with app.run(client=client):
        future = later_modal.spawn()
        assert isinstance(future, FunctionCall)

        servicer.function_is_running = True
        assert future.object_id == "fc-1"

        with pytest.raises(TimeoutError):
            future.get(0.01)

        servicer.function_is_running = False
        assert future.get(0.01) == "hello"
        assert future.object_id not in servicer.cleared_function_calls

        future = later_modal.spawn()

        servicer.function_is_running = True
        assert future.object_id == "fc-2"

        future.cancel()
        assert "fc-2" in servicer.cancelled_calls

        assert future.object_id not in servicer.cleared_function_calls

        with pytest.raises(Exception, match="Cannot iterate"):
            next(future.get_gen())


@pytest.mark.asyncio
async def test_function_future_async(client, servicer):
    app = App()

    servicer.function_body(later)
    later_modal = app.function()(later)

    async with app.run(client=client):
        future = await later_modal.spawn.aio()
        servicer.function_is_running = True

        with pytest.raises(TimeoutError):
            await future.get.aio(0.01)

        servicer.function_is_running = False
        assert await future.get.aio(0.01) == "hello"
        assert future.object_id not in servicer.cleared_function_calls  # keep results around a bit longer for futures


def later_gen():
    yield "foo"


async def async_later_gen():
    yield "foo"


@pytest.mark.asyncio
async def test_generator(client, servicer):
    app = App()

    later_gen_modal = app.function()(later_gen)

    def dummy():
        yield "bar"
        yield "baz"
        yield "boo"

    servicer.function_body(dummy)

    assert len(servicer.cleared_function_calls) == 0
    with app.run(client=client):
        assert later_gen_modal.is_generator
        res: typing.Generator = later_gen_modal.remote_gen()  # type: ignore
        # Generators fulfil the *iterator protocol*, which requires both these methods.
        # https://docs.python.org/3/library/stdtypes.html#typeiter
        assert hasattr(res, "__iter__")  # strangely inspect.isgenerator returns false
        assert hasattr(res, "__next__")
        assert next(res) == "bar"
        assert list(res) == ["baz", "boo"]
        assert len(servicer.cleared_function_calls) == 1


def test_generator_map_invalid(client, servicer):
    app = App()

    later_gen_modal = app.function()(later_gen)

    def dummy(x):
        yield x

    servicer.function_body(dummy)

    with app.run(client=client):
        with pytest.raises(InvalidError, match="A generator function cannot be called with"):
            # Support for .map() on generators was removed in version 0.57
            for _ in later_gen_modal.map([1, 2, 3]):
                pass

        with pytest.raises(InvalidError, match="A generator function cannot be called with"):
            later_gen_modal.for_each([1, 2, 3])


@pytest.mark.asyncio
async def test_generator_async(client, servicer):
    app = App()

    later_gen_modal = app.function()(async_later_gen)

    async def async_dummy():
        yield "bar"
        yield "baz"

    servicer.function_body(async_dummy)

    assert len(servicer.cleared_function_calls) == 0
    async with app.run(client=client):
        assert later_gen_modal.is_generator
        res = later_gen_modal.remote_gen.aio()
        # Async generators fulfil the *asynchronous iterator protocol*, which requires both these methods.
        # https://peps.python.org/pep-0525/#support-for-asynchronous-iteration-protocol
        assert hasattr(res, "__aiter__")
        assert hasattr(res, "__anext__")
        # TODO(Jonathon): This works outside of testing, but here gives:
        # `TypeError: cannot pickle 'async_generator' object`
        # await res.__anext__() == "bar"
        # assert len(servicer.cleared_function_calls) == 1


@pytest.mark.asyncio
async def test_generator_future(client, servicer):
    app = App()

    servicer.function_body(later_gen)
    later_modal = app.function()(later_gen)
    with app.run(client=client):
        with pytest.raises(DeprecationError):
            later_modal.spawn()


async def slo1(sleep_seconds):
    # need to use async function body in client test to run stuff in parallel
    # but calling interface is still non-asyncio
    await asyncio.sleep(sleep_seconds)
    return sleep_seconds


def test_sync_parallelism(client, servicer):
    app = App()

    servicer.function_body(slo1)
    slo1_modal = app.function()(slo1)
    with app.run(client=client):
        t0 = time.time()
        # NOTE tests breaks in macOS CI if the smaller time is smaller than ~300ms
        res = FunctionCall.gather(slo1_modal.spawn(0.31), slo1_modal.spawn(0.3))
        t1 = time.time()
        assert res == [0.31, 0.3]  # results should be ordered as inputs, not by completion time
        assert t1 - t0 < 0.6  # less than the combined runtime, make sure they run in parallel


def test_proxy(client, servicer):
    app = App()

    app.function(proxy=Proxy.from_name("my-proxy"))(dummy)
    with app.run(client=client):
        pass


class CustomException(Exception):
    pass


def failure():
    raise CustomException("foo!")


def test_function_exception(client, servicer):
    app = App()

    servicer.function_body(failure)
    failure_modal = app.function()(failure)
    with app.run(client=client):
        with pytest.raises(CustomException) as excinfo:
            failure_modal.remote()
        assert "foo!" in str(excinfo.value)


@pytest.mark.asyncio
async def test_function_exception_async(client, servicer):
    app = App()

    servicer.function_body(failure)
    failure_modal = app.function()(failure)
    async with app.run(client=client):
        with pytest.raises(CustomException) as excinfo:
            coro = failure_modal.remote.aio()
            # mostly for mypy, since output could technically be an async generator which
            # isn't awaitable in the same sense
            assert inspect.isawaitable(coro)
            await coro
        assert "foo!" in str(excinfo.value)


def custom_exception_function(x):
    if x == 4:
        raise CustomException("bad")
    return x * x


def test_map_exceptions(client, servicer):
    app = App()

    servicer.function_body(custom_exception_function)
    custom_function_modal = app.function()(custom_exception_function)

    with app.run(client=client):
        assert list(custom_function_modal.map(range(4))) == [0, 1, 4, 9]

        with pytest.raises(CustomException) as excinfo:
            list(custom_function_modal.map(range(6)))
        assert "bad" in str(excinfo.value)

        res = list(custom_function_modal.map(range(6), return_exceptions=True))
        assert res[:4] == [0, 1, 4, 9] and res[5] == 25
        assert type(res[4]) is UserCodeException and "bad" in str(res[4])


def import_failure():
    raise ImportError("attempted relative import with no known parent package")


def test_function_relative_import_hint(client, servicer):
    app = App()

    servicer.function_body(import_failure)
    import_failure_modal = app.function()(import_failure)

    with app.run(client=client):
        with pytest.raises(ImportError) as excinfo:
            import_failure_modal.remote()
        assert "HINT" in str(excinfo.value)


def test_nonglobal_function():
    app = App()

    with pytest.raises(InvalidError) as excinfo:

        @app.function()
        def f():
            pass

    assert "global scope" in str(excinfo.value)


def test_non_global_serialized_function():
    app = App()

    @app.function(serialized=True)
    def f():
        pass


def test_closure_valued_serialized_function(client, servicer):
    app = App()

    def make_function(s):
        @app.function(name=f"ret_{s}", serialized=True)
        def returner():
            return s

    for s in ["foo", "bar"]:
        make_function(s)

    with app.run(client=client):
        pass

    functions = {}
    for func in servicer.app_functions.values():
        functions[func.function_name] = cloudpickle.loads(func.function_serialized)

    assert len(functions) == 2
    assert functions["ret_foo"]() == "foo"
    assert functions["ret_bar"]() == "bar"


def test_new_hydrated_internal(client, servicer):
    obj: FunctionCall[typing.Any] = FunctionCall._new_hydrated("fc-123", client, None)
    assert obj.object_id == "fc-123"


def test_from_id(client, servicer):
    app = App()

    @app.function(serialized=True)
    def foo():
        pass

    deploy_app(app, "dummy", client=client)

    function_id = foo.object_id
    assert function_id

    function_call = foo.spawn()
    assert function_call.object_id
    # Used in a few examples to construct FunctionCall objects
    rehydrated_function_call = FunctionCall.from_id(function_call.object_id, client)
    assert rehydrated_function_call.object_id == function_call.object_id


def test_local_execution_on_web_endpoint(client, servicer):
    app = App()

    @app.function(serialized=True)
    @web_endpoint()
    def foo(x: str):
        return f"{x}!"

    deploy_app(app, "dummy", client=client)

    function_id = foo.object_id
    assert function_id
    assert foo.web_url

    res = foo.local("hello")
    assert res == "hello!"


def test_local_execution_on_asgi_app(client, servicer):
    from fastapi import FastAPI

    app = App()

    @app.function(serialized=True)
    @asgi_app()
    def foo():
        from fastapi import FastAPI

        web_app = FastAPI()

        @web_app.get("/bar")
        def bar(arg="world"):
            return {"hello": arg}

        return web_app

    deploy_app(app, "dummy", client=client)

    function_id = foo.object_id
    assert function_id
    assert foo.web_url

    res = foo.local()
    assert type(res) is FastAPI


@pytest.mark.parametrize("remote_executor", ["remote", "remote_gen", "spawn"])
def test_invalid_remote_executor_on_web_endpoint(client, servicer, remote_executor):
    app = App()

    @app.function(serialized=True)
    @web_endpoint()
    def foo():
        pass

    deploy_app(app, "dummy", client=client)

    function_id = foo.object_id
    assert function_id
    assert foo.web_url

    with pytest.raises(InvalidError) as excinfo:
        f = getattr(foo, remote_executor)
        res = f()
        if inspect.isgenerator(res):
            next(res)

    assert "webhook" in str(excinfo.value) and remote_executor in str(excinfo.value)


@pytest.mark.parametrize("remote_executor", ["remote", "remote_gen", "spawn"])
def test_invalid_remote_executor_on_asgi_app(client, servicer, remote_executor):
    app = App()

    @app.function(serialized=True)
    @asgi_app()
    def foo():
        from fastapi import FastAPI

        web_app = FastAPI()

        @web_app.get("/foo")
        def foo(arg="world"):
            return {"hello": arg}

        return web_app

    deploy_app(app, "dummy", client=client)

    function_id = foo.object_id
    assert function_id
    assert foo.web_url

    with pytest.raises(InvalidError) as excinfo:
        f = getattr(foo, remote_executor)
        res = f()
        if inspect.isgenerator(res):
            next(res)

    assert "webhook" in str(excinfo.value) and remote_executor in str(excinfo.value)


@pytest.mark.parametrize("is_generator", [False, True])
def test_from_id_iter_gen(client, servicer, is_generator):
    app = App()

    f = later_gen if is_generator else later

    servicer.function_body(f)
    later_modal = app.function()(f)
    with app.run(client=client):
        if is_generator:
            with pytest.raises(DeprecationError):
                later_modal.spawn()
            return

        future = later_modal.spawn()
        assert isinstance(future, FunctionCall)

    assert future.object_id
    rehydrated_function_call = FunctionCall.from_id(future.object_id, client, is_generator=is_generator)
    assert rehydrated_function_call.object_id == future.object_id

    if is_generator:
        assert next(rehydrated_function_call.get_gen()) == "foo"
    else:
        assert rehydrated_function_call.get() == "hello"


lc_app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@lc_app.function()
def f(x):
    return x**2


def test_allow_cross_region_volumes(client, servicer):
    app = App()
    vol1 = NetworkFileSystem.from_name("xyz-1", create_if_missing=True)
    vol2 = NetworkFileSystem.from_name("xyz-2", create_if_missing=True)
    # Should pass flag for all the function's NetworkFileSystemMounts
    app.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(dummy)

    with app.run(client=client):
        assert len(servicer.app_functions) == 1
        for func in servicer.app_functions.values():
            assert len(func.shared_volume_mounts) == 2
            for svm in func.shared_volume_mounts:
                assert svm.allow_cross_region


def test_allow_cross_region_volumes_webhook(client, servicer):
    # TODO(erikbern): this test seems a bit redundant
    app = App()
    vol1 = NetworkFileSystem.from_name("xyz-1", create_if_missing=True)
    vol2 = NetworkFileSystem.from_name("xyz-2", create_if_missing=True)
    # Should pass flag for all the function's NetworkFileSystemMounts
    app.function(network_file_systems={"/sv-1": vol1, "/sv-2": vol2}, allow_cross_region_volumes=True)(
        web_endpoint()(dummy)
    )

    with app.run(client=client):
        assert len(servicer.app_functions) == 1
        for func in servicer.app_functions.values():
            assert len(func.shared_volume_mounts) == 2
            for svm in func.shared_volume_mounts:
                assert svm.allow_cross_region


def test_serialize_deserialize_function_handle(servicer, client):
    from modal._serialization import deserialize, serialize

    app = App()

    @app.function(serialized=True)
    @web_endpoint()
    def my_handle():
        pass

    with pytest.raises(InvalidError, match="hasn't been hydrated"):
        serialize(my_handle)  # handle is not "live" yet! should not be serializable yet

    with app.run(client=client):
        blob = serialize(my_handle)

        rehydrated_function_handle = deserialize(blob, client)
        assert rehydrated_function_handle.object_id == my_handle.object_id
        assert isinstance(rehydrated_function_handle, Function)
        assert rehydrated_function_handle.web_url == "http://xyz.internal"


def test_default_cloud_provider(client, servicer, monkeypatch):
    app = App()

    monkeypatch.setenv("MODAL_DEFAULT_CLOUD", "xyz")
    app.function()(dummy)
    with app.run(client=client):
        object_id: str = app.registered_functions["dummy"].object_id
        f = servicer.app_functions[object_id]

    assert f.cloud_provider == api_pb2.CLOUD_PROVIDER_UNSPECIFIED  # No longer sent
    assert f.cloud_provider_str == "xyz"


def test_autoscaler_settings(client, servicer):
    app = App()

    kwargs: dict[str, typing.Any] = dict(  # No idea why we need that type hint
        min_containers=2,
        max_containers=10,
        scaledown_window=60,
    )
    f = app.function(**kwargs)(dummy)

    with app.run(client=client):
        defn = servicer.app_functions[f.object_id]
        # Test both backwards and forwards compatibility
        settings = defn.autoscaler_settings
        assert settings.min_containers == defn.warm_pool_size == kwargs["min_containers"]
        assert settings.max_containers == defn.concurrency_limit == kwargs["max_containers"]
        assert settings.scaledown_window == defn.task_idle_timeout_secs == kwargs["scaledown_window"]


def test_not_hydrated():
    with pytest.raises(ExecutionError):
        assert foo.remote(2, 4) == 20


def test_invalid_large_serialization(client):
    big_data = b"1" * 500000

    def f():
        return big_data

    with pytest.warns(UserWarning, match="larger than the recommended limit"):
        app = App()
        app.function(serialized=True)(f)
        with app.run(client=client):
            pass

    bigger_data = b"1" * 50000000

    def g():
        return bigger_data

    with pytest.raises(InvalidError):
        app = App()
        app.function(serialized=True)(g)
        with app.run(client=client):
            pass


def test_call_unhydrated_function():
    with pytest.raises(ExecutionError, match="hydrated"):
        foo.remote(123, 456)


def test_deps_explicit(client, servicer):
    app = App()

    image = Image.debian_slim()
    nfs_1 = NetworkFileSystem.from_name("nfs-1", create_if_missing=True)
    nfs_2 = NetworkFileSystem.from_name("nfs-2", create_if_missing=True)

    app.function(image=image, network_file_systems={"/nfs_1": nfs_1, "/nfs_2": nfs_2})(dummy)

    with app.run(client=client):
        object_id: str = app.registered_functions["dummy"].object_id
        f = servicer.app_functions[object_id]

    dep_object_ids = {d.object_id for d in f.object_dependencies}
    assert dep_object_ids == {image.object_id, nfs_1.object_id, nfs_2.object_id}


def assert_is_wrapped_dict(some_arg):
    assert type(some_arg) is modal.Dict  # this should not be a modal._Dict unwrapped instance!
    return some_arg


def test_calls_should_not_unwrap_modal_objects(servicer, client):
    app = App()
    foo = app.function()(assert_is_wrapped_dict)
    servicer.function_body(assert_is_wrapped_dict)

    # make sure the serialized object is an actual Dict and not a _Dict in all user code contexts
    with app.run(client=client), modal.Dict.ephemeral(client=client) as some_modal_object:
        assert type(foo.remote(some_modal_object)) is modal.Dict
        fc = foo.spawn(some_modal_object)
        assert type(fc.get()) is modal.Dict
        for ret in foo.map([some_modal_object]):
            assert type(ret) is modal.Dict
        for ret in foo.starmap([[some_modal_object]]):
            assert type(ret) is modal.Dict
        foo.for_each([some_modal_object])

    assert len(servicer.client_calls) == 5


def assert_is_wrapped_dict_gen(some_arg):
    assert type(some_arg) is modal.Dict  # this should not be a modal._Dict unwrapped instance!
    yield some_arg


def test_calls_should_not_unwrap_modal_objects_gen(servicer, client):
    app = App()
    foo = app.function()(assert_is_wrapped_dict_gen)
    servicer.function_body(assert_is_wrapped_dict_gen)

    # make sure the serialized object is an actual Dict and not a _Dict in all user code contexts
    with app.run(client=client), modal.Dict.ephemeral(client=client) as some_modal_object:
        assert type(next(foo.remote_gen(some_modal_object))) is modal.Dict
        with pytest.raises(DeprecationError):
            foo.spawn(some_modal_object)

    assert len(servicer.client_calls) == 1


def test_function_deps_have_ids(client, servicer, monkeypatch, test_dir, set_env_client, disable_auto_mount):
    monkeypatch.syspath_prepend(test_dir / "supports")
    app = App()
    app.function(
        image=modal.Image.debian_slim().add_local_python_source("pkg_a"),
        volumes={"/vol": modal.Volume.from_name("vol", create_if_missing=True)},
        network_file_systems={"/vol": modal.NetworkFileSystem.from_name("nfs", create_if_missing=True)},
        secrets=[modal.Secret.from_dict({"foo": "bar"})],
    )(dummy)

    with servicer.intercept() as ctx:
        with app.run(client=client):
            pass

    function_create = ctx.pop_request("FunctionCreate")
    assert len(function_create.function.mount_ids) == 3  # client mount, explicit mount, entrypoint mount
    for mount_id in function_create.function.mount_ids:
        assert mount_id

    for dep in function_create.function.object_dependencies:
        assert dep.object_id


def test_no_state_reuse(client, servicer, supports_dir, disable_auto_mount):
    # two separate instances of the same mount content - triggers deduplication logic

    img = (
        Image.debian_slim()
        .add_local_file(supports_dir / "pyproject.toml", "/root/")
        .add_local_file(supports_dir / "pyproject.toml", "/root/")
    )
    app = App("reuse-mount-app")
    app.function(image=img)(dummy)

    with servicer.intercept() as ctx:
        deploy_app(app, client=client)
        func_create = ctx.pop_request("FunctionCreate")
        first_deploy_mounts = set(func_create.function.mount_ids)
        assert len(first_deploy_mounts) == 3  # client mount, one of the explicit mounts, entrypoint mount

    with servicer.intercept() as ctx:
        deploy_app(app, client=client)
        func_create = ctx.pop_request("FunctionCreate")
        second_deploy_mounts = set(func_create.function.mount_ids)
        assert len(second_deploy_mounts) == 3  # client mount, one of the explicit mounts, entrypoint mount

    # mount ids should not overlap between first and second deploy, except for client mount
    assert first_deploy_mounts & second_deploy_mounts == {servicer.default_published_client_mount}


@pytest.mark.asyncio
async def test_map_large_inputs(client, servicer, monkeypatch, blob_server):
    # TODO: tests making use of mock blob server currently have to be async, since the
    #  blob server runs as an async pytest fixture which will have its event loop blocked
    #  by the test itself otherwise... Should move to its own thread.
    monkeypatch.setattr("modal._utils.function_utils.MAX_OBJECT_SIZE_BYTES", 1)
    servicer.use_blob_outputs = True
    app = App()
    dummy_modal = app.function()(dummy)

    _, blobs = blob_server
    async with app.run.aio(client=client):
        assert len(blobs) == 0
        assert [a async for a in dummy_modal.map.aio(range(100))] == [i**2 for i in range(100)]
        assert len(servicer.cleared_function_calls) == 1

    assert len(blobs) == 200  # inputs + outputs


@pytest.mark.asyncio
async def test_non_aio_map_in_async_caller_error(client):
    dummy_function = app.function()(dummy)

    with app.run(client=client):
        with pytest.raises(InvalidError, match=".map.aio"):
            for _ in dummy_function.map([1, 2, 3]):
                pass

        # using .aio should be ok:
        res = [r async for r in dummy_function.map.aio([1, 2, 3])]
        assert res == [1, 4, 9]

        # we might want to deprecate this syntax (async for ... in map without .aio),
        # but we support it for backwards compatibility for now:
        res = [r async for r in dummy_function.map([1, 2, 4])]
        assert res == [1, 4, 16]


def test_warn_on_local_volume_mount(client, servicer):
    vol = modal.Volume.from_name("my-vol")
    dummy_function = app.function(volumes={"/foo": vol})(dummy)

    assert modal.is_local()
    with pytest.warns(match="local"):
        dummy_function.local()


class X:
    def f(self): ...


def test_function_decorator_on_method():
    app = modal.App()

    with pytest.raises(InvalidError, match="@app.cls"):
        app.function()(X.f)


def test_batch_function_invalid_error():
    app = App()

    with pytest.raises(InvalidError, match="must be a positive integer"):
        app.function(batched(max_batch_size=0, wait_ms=1))(dummy)

    with pytest.raises(InvalidError, match="must be a non-negative integer"):
        app.function(batched(max_batch_size=1, wait_ms=-1))(dummy)

    with pytest.raises(InvalidError, match="must be less than"):
        app.function(batched(max_batch_size=1000, wait_ms=1))(dummy)

    with pytest.raises(InvalidError, match="must be less than"):
        app.function(batched(max_batch_size=1, wait_ms=10 * 60 * 1000))(dummy)

    with pytest.raises(InvalidError, match="cannot return generators"):

        @app.function(serialized=True)
        @batched(max_batch_size=1, wait_ms=1)
        def f(x):
            yield [x_i**2 for x_i in x]

    with pytest.raises(InvalidError, match="does not accept default arguments"):

        @app.function(serialized=True)
        @batched(max_batch_size=1, wait_ms=1)
        def g(x=1):
            return [x_i**2 for x_i in x]


def test_experimental_spawn(client, servicer):
    app = App()
    dummy_modal = app.function()(dummy)

    with servicer.intercept() as ctx:
        with app.run(client=client):
            dummy_modal._experimental_spawn(1, 2)

    # Verify the correct invocation type is set
    function_map = ctx.pop_request("FunctionMap")
    assert function_map.function_call_invocation_type == api_pb2.FUNCTION_CALL_INVOCATION_TYPE_ASYNC


def test_from_name_web_url(servicer, set_env_client):
    f = Function.from_name("dummy-app", "func")

    with servicer.intercept() as ctx:
        ctx.add_response(
            "FunctionGet",
            api_pb2.FunctionGetResponse(
                function_id="fu-1", handle_metadata=api_pb2.FunctionHandleMetadata(web_url="test.internal")
            ),
        )
        assert f.web_url == "test.internal"


@pytest.mark.parametrize(
    ["config_automount", "app_constructor_value", "function_decorator_value", "expected_mounts"],
    [
        (None, None, None, 2),  # default with no options: entrypoint + first party (automount)
        ("0", None, None, 1),  # automount=0 in config - entrypoint only. Warn about config being deprecated
        ("1", None, None, 2),  # automount=1 explicit in config. Warn about config based automount=1 going away
        ("1", "False", None, 0),  # automount=1 explicit in config. Warn about config based automount=1 going away
        ("0", "False", None, 0),
        (None, "False", None, 0),
        (None, "False", "True", 1),
        (None, "True", "False", 0),
        # "legacy" mode is currently not enabled except as the default value
        # (None, "False", "'legacy'", 2),
        # (None, "True", "'legacy'", 2),
    ],
)
def test_include_source_mode(
    app_constructor_value,
    function_decorator_value,
    config_automount,
    expected_mounts,
    servicer,
    credentials,
    tmp_path,
    monkeypatch,
):
    # a little messy since it tests the "end to end" mounting behavior for the app
    app_constructor_value = "None" if app_constructor_value is None else app_constructor_value
    function_decorator_value = "None" if function_decorator_value is None else function_decorator_value
    src = f"""
import modal
import mod  # mod.py needs to be added for this file to load, so it needs to be included as source

app = modal.App(include_source={app_constructor_value})

@app.function(include_source={function_decorator_value})
def f():
    pass
"""
    entrypoint_file = tmp_path / "main.py"
    (tmp_path / "mod.py").touch()  # some file
    entrypoint_file.write_text(src)

    monkeypatch.delenv("MODAL_AUTOMOUNT")
    if config_automount is not None:
        env = {**os.environ, "MODAL_AUTOMOUNT": config_automount}
    else:
        env = {**os.environ}
    output = deploy_app_externally(servicer, credentials, str(entrypoint_file), env=env)
    print(output)
    mounts = servicer.mounts_excluding_published_client()

    assert len(mounts) == expected_mounts


================================================
File: test/function_utils_test.py
================================================
# Copyright Modal Labs 2023
import functools
import pytest
import time

from grpclib import Status

from modal import method, web_endpoint
from modal._serialization import serialize_data_format
from modal._utils import async_utils
from modal._utils.function_utils import (
    FunctionInfo,
    _stream_function_call_data,
    callable_has_non_self_non_default_params,
    callable_has_non_self_params,
)
from modal_proto import api_pb2

GLOBAL_VARIABLE = "whatever"


def hasarg(a): ...


def noarg(): ...


def defaultarg(a="hello"): ...


def wildcard_args(*wildcard_list, **wildcard_dict): ...


def test_is_nullary():
    assert not FunctionInfo(hasarg).is_nullary()
    assert FunctionInfo(noarg).is_nullary()
    assert FunctionInfo(defaultarg).is_nullary()
    assert FunctionInfo(wildcard_args).is_nullary()


class Cls:
    def f1(self):
        pass

    def f2(self, x):
        pass

    def f3(self, *args):
        pass

    def f4(self, x=1):
        pass


def f5():
    pass


def f6(x):
    pass


def f7(x=1):
    pass


def test_callable_has_non_self_params():
    assert not callable_has_non_self_params(Cls.f1)
    assert not callable_has_non_self_params(Cls().f1)
    assert callable_has_non_self_params(Cls.f2)
    assert callable_has_non_self_params(Cls().f2)
    assert callable_has_non_self_params(Cls.f3)
    assert callable_has_non_self_params(Cls().f3)
    assert callable_has_non_self_params(Cls.f4)
    assert callable_has_non_self_params(Cls().f4)
    assert not callable_has_non_self_params(f5)
    assert callable_has_non_self_params(f6)
    assert callable_has_non_self_params(f7)


def test_callable_has_non_self_non_default_params():
    assert not callable_has_non_self_non_default_params(Cls.f1)
    assert not callable_has_non_self_non_default_params(Cls().f1)
    assert callable_has_non_self_non_default_params(Cls.f2)
    assert callable_has_non_self_non_default_params(Cls().f2)
    assert callable_has_non_self_non_default_params(Cls.f3)
    assert callable_has_non_self_non_default_params(Cls().f3)
    assert not callable_has_non_self_non_default_params(Cls.f4)
    assert not callable_has_non_self_non_default_params(Cls().f4)
    assert not callable_has_non_self_non_default_params(f5)
    assert callable_has_non_self_non_default_params(f6)
    assert not callable_has_non_self_non_default_params(f7)


class Foo:
    def __init__(self):
        pass

    @method()
    def bar(self):
        return "hello"

    @web_endpoint()
    def web(self):
        pass


# run test on same event loop as servicer
@async_utils.synchronize_api
async def test_stream_function_call_data(servicer, client):
    req = api_pb2.FunctionCallPutDataRequest(
        function_call_id="fc-bar",
        data_chunks=[
            api_pb2.DataChunk(
                data_format=api_pb2.DATA_FORMAT_PICKLE,
                data=serialize_data_format("hello", api_pb2.DATA_FORMAT_PICKLE),
                index=1,
            ),
            api_pb2.DataChunk(
                data_format=api_pb2.DATA_FORMAT_PICKLE,
                data=serialize_data_format("world", api_pb2.DATA_FORMAT_PICKLE),
                index=2,
            ),
        ],
    )
    await client.stub.FunctionCallPutDataOut(req)

    t0 = time.time()
    gen = _stream_function_call_data(client, "fc-bar", "data_out")
    servicer.fail_get_data_out = [Status.INTERNAL] * 3
    assert await gen.__anext__() == "hello"
    elapsed = time.time() - t0
    assert 0.111 <= elapsed < 1.0

    assert await gen.__anext__() == "world"


def decorator(f):
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        return f

    return wrapper


def has_global_ref():
    assert GLOBAL_VARIABLE


@pytest.mark.parametrize("func", [has_global_ref, decorator(has_global_ref)])
def test_global_variable_extraction(func):
    info = FunctionInfo(func)
    assert info.get_globals().get("GLOBAL_VARIABLE") == GLOBAL_VARIABLE


================================================
File: test/gpu_fallbacks_test.py
================================================
# Copyright Modal Labs 2024
from modal import App
from modal_proto import api_pb2

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function(gpu=["a10g"])
def f1():
    pass


@app.function(gpu=["a10g", "t4:2"])
def f2():
    pass


@app.function(gpu=["h100:2", "a100-80gb:2"])
def f3():
    pass


def test_gpu_fallback(servicer, client):
    with app.run(client=client):
        assert len(servicer.app_functions) == 3

        a10_1 = api_pb2.Resources(
            gpu_config=api_pb2.GPUConfig(
                gpu_type="A10G",
                count=1,
            )
        )
        t4_2 = api_pb2.Resources(
            gpu_config=api_pb2.GPUConfig(
                gpu_type="T4",
                count=2,
            )
        )
        h100_2 = api_pb2.Resources(
            gpu_config=api_pb2.GPUConfig(
                gpu_type="H100",
                count=2,
            )
        )
        a100_80gb_2 = api_pb2.Resources(
            gpu_config=api_pb2.GPUConfig(type=api_pb2.GPU_TYPE_A100_80GB, count=2, gpu_type="A100-80GB")
        )

        fn1 = servicer.app_functions["fu-1"]  # f1
        assert len(fn1.ranked_functions) == 1
        assert fn1.ranked_functions[0].function.resources.gpu_config.gpu_type == a10_1.gpu_config.gpu_type
        assert fn1.ranked_functions[0].function.resources.gpu_config.count == a10_1.gpu_config.count

        fn2 = servicer.app_functions["fu-2"]  # f2
        assert len(fn2.ranked_functions) == 2
        assert fn2.ranked_functions[0].function.resources.gpu_config.gpu_type == a10_1.gpu_config.gpu_type
        assert fn2.ranked_functions[0].function.resources.gpu_config.count == a10_1.gpu_config.count
        assert fn2.ranked_functions[1].function.resources.gpu_config.gpu_type == t4_2.gpu_config.gpu_type
        assert fn2.ranked_functions[1].function.resources.gpu_config.count == t4_2.gpu_config.count

        fn3 = servicer.app_functions["fu-3"]  # f3
        assert len(fn3.ranked_functions) == 2
        assert fn3.ranked_functions[0].function.resources.gpu_config.gpu_type == h100_2.gpu_config.gpu_type
        assert fn3.ranked_functions[0].function.resources.gpu_config.count == h100_2.gpu_config.count
        assert fn3.ranked_functions[1].function.resources.gpu_config.gpu_type == a100_80gb_2.gpu_config.gpu_type
        assert fn3.ranked_functions[1].function.resources.gpu_config.count == a100_80gb_2.gpu_config.count
        assert fn3.ranked_functions[1].function.resources.gpu_config.gpu_type == a100_80gb_2.gpu_config.gpu_type


================================================
File: test/gpu_test.py
================================================
# Copyright Modal Labs 2022
import pytest

import modal.gpu
from modal import App
from modal.exception import InvalidError
from modal_proto import api_pb2


def dummy():
    pass  # not actually used in test (servicer returns sum of square of all args)


def test_gpu_any_function(client, servicer):
    app = App()

    app.function(gpu="any")(dummy)
    with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 1
    func_def = next(iter(servicer.app_functions.values()))
    assert func_def.resources.gpu_config.count == 1


@pytest.mark.parametrize(
    "gpu_arg,gpu_type,count",
    [
        ("A100-40GB", "A100-40GB", 1),
        ("a100-40gb", "A100-40GB", 1),
        ("a10g", "A10G", 1),
        ("t4:7", "T4", 7),
        ("a100-80GB:5", "A100-80GB", 5),
        ("l40s:2", "L40S", 2),
    ],
)
def test_gpu_string_config(client, servicer, gpu_arg, gpu_type, count):
    app = App()

    app.function(gpu=gpu_arg)(dummy)
    with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 1
    func_def = next(iter(servicer.app_functions.values()))
    assert func_def.resources.gpu_config.gpu_type == gpu_type
    assert func_def.resources.gpu_config.count == count


@pytest.mark.parametrize("gpu_arg", ["foo", "a10g:hello", "nonexistent:2"])
def test_invalid_gpu_string_config(client, servicer, gpu_arg):
    app = App()

    # Invalid enum value.
    with pytest.raises(InvalidError):
        app.function(gpu=gpu_arg)(dummy)
        with app.run(client=client):
            pass


def test_gpu_config_function(client, servicer):
    app = App()

    with pytest.warns(match='gpu="A100-40GB"'):
        app.function(gpu=modal.gpu.A100())(dummy)
    with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 1
    func_def = next(iter(servicer.app_functions.values()))
    assert func_def.resources.gpu_config.count == 1


def test_gpu_config_function_more(client, servicer):
    # Make sure some other GPU types also throw warnings
    with pytest.warns(match='gpu="A100-80GB"'):
        modal.gpu.A100(size="80GB")
    with pytest.warns(match='gpu="T4:7"'):
        modal.gpu.T4(count=7)


def test_cloud_provider_selection(client, servicer):
    app = App()

    app.function(gpu="A100", cloud="gcp")(dummy)
    with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 1
    func_def = next(iter(servicer.app_functions.values()))
    assert func_def.cloud_provider == api_pb2.CLOUD_PROVIDER_UNSPECIFIED  # No longer set
    assert func_def.cloud_provider_str == "gcp"

    assert func_def.resources.gpu_config.gpu_type == "A100"
    assert func_def.resources.gpu_config.count == 1


def test_invalid_cloud_provider_selection(client, servicer):
    app = App()

    # Invalid enum value.
    with pytest.raises(InvalidError):
        app.function(cloud="foo")(dummy)
        with app.run(client=client):
            pass


@pytest.mark.parametrize(
    "memory_arg,gpu_type",
    [
        ("40GB", "A100-40GB"),
        ("80GB", "A100-80GB"),
    ],
)
def test_memory_selection_gpu_variant(client, servicer, memory_arg, gpu_type):
    app = App()
    with pytest.warns(match='gpu="A100'):
        app.function(gpu=modal.gpu.A100(size=memory_arg))(dummy)

    with app.run(client=client):
        pass

    func_def = next(iter(servicer.app_functions.values()))

    assert func_def.resources.gpu_config.count == 1
    assert func_def.resources.gpu_config.gpu_type == gpu_type


def test_gpu_unsupported_config():
    app = App()

    with pytest.raises(ValueError, match="size='20GB' is invalid"):
        app.function(gpu=modal.gpu.A100(size="20GB"))(dummy)


@pytest.mark.parametrize("count", [1, 2, 3, 4])
def test_gpu_type_selection_from_count(client, servicer, count):
    app = App()

    # Task type does not change when user asks more than 1 GPU on an A100.
    app.function(gpu=f"A100:{count}")(dummy)
    with app.run(client=client):
        pass

    func_def = next(iter(servicer.app_functions.values()))

    assert func_def.resources.gpu_config.count == count


================================================
File: test/grpc_utils_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import time

from grpclib import GRPCError, Status

from modal._utils.async_utils import synchronize_api
from modal._utils.grpc_utils import connect_channel, create_channel, retry_transient_errors
from modal_proto import api_grpc, api_pb2

from .supports.skip import skip_windows_unix_socket


@pytest.mark.asyncio
async def test_http_channel(servicer, credentials):
    token_id, token_secret = credentials
    metadata = {
        "x-modal-client-type": str(api_pb2.CLIENT_TYPE_CLIENT),
        "x-modal-python-version": "3.12.1",
        "x-modal-client-version": "0.99",
        "x-modal-token-id": token_id,
        "x-modal-token-secret": token_secret,
    }
    assert servicer.client_addr.startswith("http://")
    channel = create_channel(servicer.client_addr)
    client_stub = api_grpc.ModalClientStub(channel)

    req = api_pb2.BlobCreateRequest()
    resp = await client_stub.BlobCreate(req, metadata=metadata)
    assert resp.blob_id

    channel.close()


@skip_windows_unix_socket
@pytest.mark.asyncio
async def test_unix_channel(servicer):
    metadata = {
        "x-modal-client-type": str(api_pb2.CLIENT_TYPE_CONTAINER),
        "x-modal-python-version": "3.12.1",
        "x-modal-client-version": "0.99",
    }
    assert servicer.container_addr.startswith("unix://")
    channel = create_channel(servicer.container_addr)
    client_stub = api_grpc.ModalClientStub(channel)

    req = api_pb2.BlobCreateRequest()
    resp = await client_stub.BlobCreate(req, metadata=metadata)
    assert resp.blob_id

    channel.close()


@pytest.mark.asyncio
async def test_http_broken_channel():
    ch = create_channel("https://xyz.invalid")
    with pytest.raises(OSError):
        await connect_channel(ch)


@pytest.mark.asyncio
async def test_retry_transient_errors(servicer, client):
    client_stub = client.stub

    @synchronize_api
    async def wrapped_blob_create(req, **kwargs):
        return await retry_transient_errors(client_stub.BlobCreate, req, **kwargs)

    # Use the BlobCreate request for retries
    req = api_pb2.BlobCreateRequest()

    # Fail 3 times -> should still succeed
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 3
    await wrapped_blob_create.aio(req)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "3"

    # Fail 4 times -> should fail
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 4
    with pytest.raises(GRPCError):
        await wrapped_blob_create.aio(req)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "3"

    # Fail 5 times, but set max_retries to infinity
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 5
    assert await wrapped_blob_create.aio(req, max_retries=None, base_delay=0)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "5"

    # Not a transient error.
    servicer.fail_blob_create = [Status.PERMISSION_DENIED]
    with pytest.raises(GRPCError):
        assert await wrapped_blob_create.aio(req, max_retries=None, base_delay=0)
    assert servicer.blob_create_metadata.get("x-idempotency-key")
    assert servicer.blob_create_metadata.get("x-retry-attempt") == "0"

    # Make sure to respect total_timeout
    t0 = time.time()
    servicer.fail_blob_create = [Status.UNAVAILABLE] * 99
    with pytest.raises(GRPCError):
        assert await wrapped_blob_create.aio(req, max_retries=None, total_timeout=3)
    total_time = time.time() - t0
    assert total_time <= 3.1


================================================
File: test/helpers.py
================================================
# Copyright Modal Labs 2023
import os
import pathlib
import signal
import subprocess
import sys
from typing import Optional


def deploy_app_externally(
    servicer,
    credentials: tuple[str, str],
    file_or_module: str,
    app_variable: Optional[str] = None,
    deployment_name="Deployment",
    cwd=None,
    env={},
    capture_output=True,
) -> Optional[str]:
    # deploys an app from another interpreter to prevent leaking state from client into a container process
    # (apart from what goes through the servicer) also has the advantage that no modules imported by the
    # test files themselves will be added to sys.modules and included in mounts etc.
    windows_support: dict[str, str] = {}

    if sys.platform == "win32":
        windows_support = {
            **os.environ.copy(),
            **{"PYTHONUTF8": "1"},
        }  # windows apparently needs a bunch of env vars to start python...

    token_id, token_secret = credentials
    env = {
        **windows_support,
        "MODAL_SERVER_URL": servicer.client_addr,
        "MODAL_TOKEN_ID": token_id,
        "MODAL_TOKEN_SECRET": token_secret,
        "MODAL_ENVIRONMENT": "main",
        **env,
    }
    if cwd is None:
        cwd = pathlib.Path(__file__).parent.parent

    app_ref = file_or_module if app_variable is None else f"{file_or_module}::{app_variable}"

    p = subprocess.Popen(
        [sys.executable, "-m", "modal.cli.entry_point", "deploy", app_ref, "--name", deployment_name],
        cwd=cwd,
        env=env,
        stderr=subprocess.STDOUT,
        stdout=subprocess.PIPE if capture_output else None,
    )
    stdout_b, stderr_b = p.communicate()
    stdout_s, stderr_s = (b.decode() if b is not None else None for b in (stdout_b, stderr_b))
    if p.returncode != 0:
        print(f"Deploying app failed!\n### stdout ###\n{stdout_s}\n### stderr ###\n{stderr_s}")
        raise Exception("Test helper failed to deploy app")
    return stdout_s


class PopenWithCtrlC(subprocess.Popen):
    def __init__(self, *args, creationflags=0, **kwargs):
        if sys.platform == "win32":
            # needed on windows to separate ctrl-c lifecycle of subprocess from parent:
            creationflags = creationflags | subprocess.CREATE_NEW_CONSOLE  # type: ignore

        super().__init__(*args, **kwargs, creationflags=creationflags)

    def send_ctrl_c(self):
        # platform independent way to replicate the behavior of Ctrl-C:ing a cli app
        if sys.platform == "win32":
            # windows doesn't support sigint, and subprocess.CTRL_C_EVENT has a bunch
            # of gotchas since it's bound to a console which is the same for the parent
            # process by default, and can't be sent using the python standard library
            # to a separate process's console
            import console_ctrl

            console_ctrl.send_ctrl_c(self.pid)  # noqa [E731]
        else:
            self.send_signal(signal.SIGINT)


================================================
File: test/i6pn_clustered_test.py
================================================
# Copyright Modal Labs 2024
import modal
import modal.experimental
from modal import App

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function()
@modal.experimental.clustered(size=2)
def f1():
    pass


@app.function()
def f2():
    pass


@app.function(i6pn=True)
def f3():
    pass


def test_experimental_cluster(servicer, client):
    with app.run(client=client):
        assert len(servicer.app_functions) == 3

        fn1 = servicer.app_functions["fu-1"]  # f1
        assert fn1._experimental_group_size == 2
        assert fn1.i6pn_enabled is True

        fn2 = servicer.app_functions["fu-2"]  # f2
        assert not fn2._experimental_group_size
        assert fn2.i6pn_enabled is False

        fn3 = servicer.app_functions["fu-3"]  # f3
        assert not fn3._experimental_group_size
        assert fn3.i6pn_enabled is True


def test_run_experimental_cluster(client, servicer, monkeypatch):
    with app.run(client=client):
        # The servicer returns the sum of the squares of all arguments
        assert f1.remote(2, 4) == 2**2 + 4**2


================================================
File: test/io_streams_test.py
================================================
# Copyright Modal Labs 2024
import pytest

from grpclib import Status
from grpclib.exceptions import GRPCError

from modal import enable_output
from modal._utils.async_utils import aclosing, sync_or_async_iter
from modal.io_streams import StreamReader
from modal_proto import api_pb2


def test_stream_reader(servicer, client):
    """Tests that the stream reader works with clean inputs."""
    lines = ["foo\n", "bar\n", "baz\n"]

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        for line in lines:
            log = api_pb2.TaskLogs(data=line, file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT)
            await stream.send_message(api_pb2.TaskLogsBatch(entry_id=line, items=[log]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="sb-123",
                object_type="sandbox",
                client=client,
            )

            out = []
            for line in stdout:
                out.append(line)

            assert out == lines


def test_stream_reader_processed(servicer, client):
    """Tests that the stream reader with logs by line works with clean inputs."""
    lines = ["foo\n", "bar\n", "baz\n"]

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        for line in lines:
            log = api_pb2.TaskLogs(data=line, file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT)
            await stream.send_message(api_pb2.TaskLogsBatch(entry_id=line, items=[log]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="sb-123",
                object_type="sandbox",
                client=client,
                by_line=True,
            )

            out = []
            for line in stdout:
                out.append(line)

            assert out == lines


def test_stream_reader_processed_multiple(servicer, client):
    """Tests that the stream reader with logs by line splits multiple lines."""

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        log = api_pb2.TaskLogs(
            data="foo\nbar\nbaz",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="0", items=[log]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="sb-123",
                object_type="sandbox",
                client=client,
                by_line=True,
            )

            out = []
            for line in stdout:
                out.append(line)

            assert out == ["foo\n", "bar\n", "baz"]


def test_stream_reader_processed_partial_lines(servicer, client):
    """Test that the stream reader with logs by line joins partial lines together."""

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        log1 = api_pb2.TaskLogs(
            data="foo",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="0", items=[log1]))

        log2 = api_pb2.TaskLogs(
            data="bar\n",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="1", items=[log2]))

        log3 = api_pb2.TaskLogs(
            data="baz",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="2", items=[log3]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="sb-123",
                object_type="sandbox",
                client=client,
                by_line=True,
            )

            out = []
            for line in stdout:
                out.append(line)

            assert out == ["foobar\n", "baz"]


@pytest.mark.asyncio
async def test_stream_reader_bytes_mode(servicer, client):
    """Test that the stream reader works in bytes mode."""

    async def container_exec_get_output(servicer, stream):
        await stream.recv_message()

        await stream.send_message(
            api_pb2.RuntimeOutputBatch(batch_index=0, items=[api_pb2.RuntimeOutputMessage(message_bytes=b"foo\n")])
        )

        await stream.send_message(api_pb2.RuntimeOutputBatch(exit_code=0))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerExecGetOutput", container_exec_get_output)

        with enable_output():
            stdout: StreamReader[bytes] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="tp-123",
                object_type="container_process",
                client=client,
                text=False,
            )

            assert await stdout.read.aio() == b"foo\n"


def test_stream_reader_line_buffered_bytes(servicer, client):
    """Test that using line-buffering with bytes mode fails."""

    with pytest.raises(ValueError):
        StreamReader(
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
            object_id="tp-123",
            object_type="container_process",
            client=client,
            by_line=True,
            text=False,
        )


@pytest.mark.asyncio
async def test_stream_reader_async_iter(servicer, client):
    """Test that StreamReader behaves as a proper async iterator."""

    async def sandbox_get_logs(servicer, stream):
        await stream.recv_message()

        log1 = api_pb2.TaskLogs(
            data="foo",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="0", items=[log1]))

        log2 = api_pb2.TaskLogs(
            data="bar",
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
        )
        await stream.send_message(api_pb2.TaskLogsBatch(entry_id="1", items=[log2]))

        # send EOF
        await stream.send_message(api_pb2.TaskLogsBatch(eof=True))

    with servicer.intercept() as ctx:
        ctx.set_responder("SandboxGetLogs", sandbox_get_logs)

        expected = "foobar"

        stdout: StreamReader[str] = StreamReader(
            file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
            object_id="sb-123",
            object_type="sandbox",
            client=client,
            by_line=True,
        )

        out = ""
        async with aclosing(sync_or_async_iter(stdout)) as stream:
            async for line in stream:
                out += line

        assert out == expected


@pytest.mark.asyncio
async def test_stream_reader_container_process_retry(servicer, client):
    """Test that StreamReader handles container process stream failures and retries."""

    batch_idx = 0

    async def container_exec_get_output(servicer, stream):
        nonlocal batch_idx
        await stream.recv_message()

        for _ in range(3):
            await stream.send_message(
                api_pb2.RuntimeOutputBatch(
                    batch_index=batch_idx,
                    items=[api_pb2.RuntimeOutputMessage(message_bytes=f"msg{batch_idx}\n".encode())],
                )
            )
            batch_idx += 1

        # Simulate failure on the first connection
        if batch_idx == 3:
            raise GRPCError(Status.INTERNAL, "internal error")

        await stream.send_message(api_pb2.RuntimeOutputBatch(exit_code=0))

    with servicer.intercept() as ctx:
        ctx.set_responder("ContainerExecGetOutput", container_exec_get_output)

        with enable_output():
            stdout: StreamReader[str] = StreamReader(
                file_descriptor=api_pb2.FILE_DESCRIPTOR_STDOUT,
                object_id="tp-123",
                object_type="container_process",
                client=client,
                by_line=True,
            )

            output = []
            async for line in stdout:
                output.append(line)

            assert output == [f"msg{i}\n" for i in range(6)]


================================================
File: test/live_reload_test.py
================================================
# Copyright Modal Labs 2023
import asyncio
import pytest
import threading
import time
from unittest import mock

from modal import Function, enable_output
from modal.cli.import_refs import ImportRef
from modal.serving import serve_app

from .supports.app_run_tests.webhook import app
from .supports.skip import skip_windows


@pytest.fixture
def import_ref(test_dir):
    return ImportRef(str(test_dir / "supports" / "app_run_tests" / "webhook.py"), use_module_mode=False)


@pytest.mark.asyncio
async def test_live_reload(import_ref, server_url_env, token_env, servicer):
    async with serve_app.aio(app, import_ref):
        await asyncio.sleep(3.0)
    assert servicer.app_publish_count == 1
    assert servicer.app_client_disconnect_count == 1
    assert servicer.app_get_logs_initial_count == 0


@pytest.mark.asyncio
async def test_live_reload_with_logs(import_ref, server_url_env, token_env, servicer):
    with enable_output():
        async with serve_app.aio(app, import_ref):
            await asyncio.sleep(3.0)
    assert servicer.app_publish_count == 1
    assert servicer.app_client_disconnect_count == 1
    assert servicer.app_get_logs_initial_count == 1


@skip_windows("live-reload not supported on windows")
def test_file_changes_trigger_reloads(import_ref, server_url_env, token_env, servicer, capfd):
    watcher_done = threading.Event()

    async def fake_watch():
        for i in range(3):
            yield {"/some/file"}
        watcher_done.set()

    with serve_app(app, import_ref, _watcher=fake_watch()):
        watcher_done.wait()  # wait until watcher loop is done
        foo: Function = app.registered_functions["foo"]
        assert foo.web_url.startswith("http://")

    stderr = capfd.readouterr().err
    print(stderr)
    assert "Traceback" not in stderr
    # TODO ideally we would assert the specific expected number here, but this test
    # is consistently flaking in CI and I cannot reproduce locally to debug.
    # I'm relaxing the assertion for now to stop the test from blocking deployments.
    # assert servicer.app_publish_count == 4  # 1 + number of file changes
    assert servicer.app_publish_count > 1
    assert servicer.app_client_disconnect_count == 1


@pytest.mark.asyncio
async def test_no_change(import_ref, server_url_env, token_env, servicer):
    async def fake_watch():
        # Iterator that returns immediately, yielding nothing
        if False:
            yield

    async with serve_app.aio(app, import_ref, _watcher=fake_watch()):
        pass

    assert servicer.app_publish_count == 1  # Should create the initial app once
    assert servicer.app_client_disconnect_count == 1


@pytest.mark.asyncio
async def test_heartbeats(import_ref, server_url_env, token_env, servicer):
    with mock.patch("modal.runner.HEARTBEAT_INTERVAL", 1):
        t0 = time.time()
        async with serve_app.aio(app, import_ref):
            await asyncio.sleep(3.1)
        total_secs = int(time.time() - t0)

    apps = list(servicer.app_heartbeats.keys())
    assert len(apps) == 1
    # Typically [0s, 1s, 2s, 3s], but asyncio.sleep may lag.
    actual_heartbeats = servicer.app_heartbeats[apps[0]]
    assert abs(actual_heartbeats - (total_secs + 1)) <= 1


================================================
File: test/lookup_test.py
================================================
# Copyright Modal Labs 2023
import pytest

from modal import App, Function, Volume, web_endpoint
from modal.exception import ExecutionError, NotFoundError, ServerWarning
from modal.runner import deploy_app
from modal_proto import api_pb2


def test_persistent_object(servicer, client):
    volume_id = Volume.create_deployed("my-volume", client=client)

    v = Volume.from_name("my-volume").hydrate(client)
    assert v.object_id == volume_id

    with pytest.raises(NotFoundError):
        Volume.from_name("bazbazbaz").hydrate(client)


def square(x):
    # This function isn't deployed anyway
    pass


def test_lookup_function(servicer, client):
    app = App()

    app.function()(square)
    deploy_app(app, "my-function", client=client)

    f = Function.from_name("my-function", "square").hydrate(client)
    assert f.object_id == "fu-1"

    # Call it using two arguments
    f = Function.from_name("my-function", "square").hydrate(client)
    assert f.object_id == "fu-1"
    with pytest.raises(NotFoundError):
        f = Function.from_name("my-function", "cube").hydrate(client)

    # Make sure we can call this function
    assert f.remote(2, 4) == 20
    assert [r for r in f.map([5, 2], [4, 3])] == [41, 13]

    # Make sure the new-style local calls raise an error
    with pytest.raises(ExecutionError):
        assert f.local(2, 4) == 20


def test_webhook_lookup(servicer, client):
    app = App()
    app.function()(web_endpoint(method="POST")(square))
    deploy_app(app, "my-webhook", client=client)

    f = Function.from_name("my-webhook", "square").hydrate(client)
    assert f.web_url


def test_deploy_exists(servicer, client):
    with pytest.raises(NotFoundError):
        Volume.from_name("my-volume").hydrate(client)
    Volume.create_deployed("my-volume", client=client)
    v1 = Volume.from_name("my-volume").hydrate(client)
    v2 = Volume.from_name("my-volume").hydrate(client)
    assert v1.object_id == v2.object_id


def test_create_if_missing(servicer, client):
    v1 = Volume.from_name("my-volume", create_if_missing=True).hydrate(client)
    v2 = Volume.from_name("my-volume").hydrate(client)
    assert v1.object_id == v2.object_id


def test_lookup_server_warnings(servicer, client):
    app = App()

    app.function()(square)
    deploy_app(app, "my-function", client=client)

    servicer.function_get_server_warnings = [
        api_pb2.Warning(
            type=api_pb2.Warning.WARNING_TYPE_CLIENT_DEPRECATION,
            message="xyz",
        )
    ]
    with pytest.warns(ServerWarning, match="xyz"):
        Function.from_name("my-function", "square").hydrate(client)

    servicer.function_get_server_warnings = [api_pb2.Warning(message="abc")]
    with pytest.warns(ServerWarning, match="abc"):
        Function.from_name("my-function", "square").hydrate(client)


================================================
File: test/mdmd_test.py
================================================
# Copyright Modal Labs 2023
import importlib
import os
from enum import IntEnum

from modal_docs.mdmd import mdmd


def test_simple_function():
    def foo():
        pass

    assert (
        mdmd.function_str("bar", foo)
        == """```python
def bar():
```\n\n"""
    )


def test_simple_async_function():
    async def foo():
        pass

    assert (
        mdmd.function_str("bar", foo)
        == """```python
async def bar():
```\n\n"""
    )


def test_async_gen_function():
    async def foo():
        yield

    assert (
        mdmd.function_str("bar", foo)
        == """```python
async def bar():
```\n\n"""
    )


def test_complex_function_signature():
    def foo(a: str, *args, **kwargs):
        pass

    assert (
        mdmd.function_str("foo", foo)
        == """```python
def foo(a: str, *args, **kwargs):
```\n\n"""
    )


def test_function_has_docstring():
    def foo():
        """short description

        longer description"""

    assert (
        mdmd.function_str("foo", foo)
        == """```python
def foo():
```

short description

longer description
"""
    )


def test_simple_class_with_docstring():
    class Foo:
        """The all important Foo"""

        def bar(self, baz: str):
            """Bars the foo with the baz"""

    assert (
        mdmd.class_str("Foo", Foo)
        == """```python
class Foo(object)
```

The all important Foo

### bar

```python
def bar(self, baz: str):
```

Bars the foo with the baz
"""
    )


def test_enum():
    class Eee(IntEnum):
        FOO = 1
        BAR = 2
        XYZ = 3

    expected = """```python
class bar(enum.IntEnum)
```

An enumeration.

The possible values are:

* `FOO`
* `BAR`
* `XYZ`
"""

    assert mdmd.class_str("bar", Eee) == expected


def test_class_with_classmethod():
    class Foo:
        @classmethod
        def create_foo(cls, some_arg):
            pass

    assert (
        mdmd.class_str("Foo", Foo)
        == """```python
class Foo(object)
```

### create_foo

```python
@classmethod
def create_foo(cls, some_arg):
```

"""
    )


def test_class_with_baseclass_includes_base_methods():
    class Foo:
        def foo(self):
            pass

    class Bar(Foo):
        def bar(self):
            pass

    out = mdmd.class_str("Bar", Bar)
    assert "def foo(self):" in out


def test_module(monkeypatch):
    test_data_dir = os.path.join(os.path.dirname(__file__), "mdmd_data")
    monkeypatch.chdir(test_data_dir)
    monkeypatch.syspath_prepend(test_data_dir)
    test_module = importlib.import_module("foo")
    expected_output = open("./foo-expected.md").read()
    assert mdmd.module_str("foo", test_module) == expected_output


def test_docstring_format_reindents_code():
    assert (
        mdmd.format_docstring(
            """```python
        foo
            bar
        ```"""
        )
        == """```python
foo
    bar
```
"""
    )


def test_synchronicity_async_and_blocking_interfaces():
    from synchronicity import Synchronizer

    class Foo:
        """docky mcdocface"""

        async def foo(self):
            pass

        def bar(self):
            pass

    s = Synchronizer()
    BlockingFoo = s.create_blocking(Foo, "BlockingFoo")

    assert (
        mdmd.class_str("BlockingFoo", BlockingFoo)
        == """```python
class BlockingFoo(object)
```

docky mcdocface

### foo

```python
def foo(self):
```

### bar

```python
def bar(self):
```

"""
    )


def test_synchronicity_constructors():
    from synchronicity import Synchronizer

    class Foo:
        """docky mcdocface"""

        def __init__(self):
            """constructy mcconstructorface"""

    s = Synchronizer()
    BlockingFoo = s.create_blocking(Foo, "BlockingFoo")

    assert (
        mdmd.class_str("BlockingFoo", BlockingFoo)
        == """```python
class BlockingFoo(object)
```

docky mcdocface

```python
def __init__(self):
```

constructy mcconstructorface
"""
    )


def test_get_all_signature_comments():
    def foo(
        # prefix comment
        one,  # one comment
        two,  # two comment
        # postfix comment
    ) -> str:  # return value comment
        pass

    assert (
        mdmd.function_str("foo", foo)
        == """```python
def foo(
    # prefix comment
    one,  # one comment
    two,  # two comment
    # postfix comment
) -> str:  # return value comment
```

"""
    )


def test_get_decorators():
    BLA = 1

    def my_deco(arg):
        def wrapper(f):
            return f

        return wrapper

    @my_deco(BLA)
    def foo():
        pass

    assert (
        mdmd.function_str("foo", foo)
        == """```python
@my_deco(BLA)
def foo():
```

"""
    )


================================================
File: test/mount_test.py
================================================
# Copyright Modal Labs 2022
import hashlib
import os
import platform
import pytest
from pathlib import Path, PurePosixPath

from modal import App, FilePatternMatcher
from modal._utils.blob_utils import LARGE_FILE_LIMIT
from modal.mount import Mount, module_mount_condition, module_mount_ignore_condition
from test.helpers import deploy_app_externally


@pytest.mark.asyncio
async def test_get_files(servicer, client, tmpdir):
    small_content = b"# not much here"
    large_content = b"a" * (LARGE_FILE_LIMIT + 1)

    tmpdir.join("small.py").write(small_content)
    tmpdir.join("large.py").write(large_content)
    tmpdir.join("fluff").write("hello")

    files = {}
    m = Mount._from_local_dir(Path(tmpdir), remote_path="/", condition=lambda fn: fn.endswith(".py"), recursive=True)
    async for upload_spec in Mount._get_files.aio(m.entries):
        files[upload_spec.mount_filename] = upload_spec

    os.umask(umask := os.umask(0o022))  # Get the current umask
    expected_mode = 0o644 if platform.system() == "Windows" else 0o666 - umask

    assert "/small.py" in files
    assert "/large.py" in files
    assert "/fluff" not in files
    assert files["/small.py"].use_blob is False
    assert files["/small.py"].content == small_content
    assert files["/small.py"].sha256_hex == hashlib.sha256(small_content).hexdigest()
    assert files["/small.py"].mode == expected_mode

    assert files["/large.py"].use_blob is True
    assert files["/large.py"].content is None
    assert files["/large.py"].sha256_hex == hashlib.sha256(large_content).hexdigest()
    assert files["/large.py"].mode == expected_mode

    await m._deploy.aio("my-mount", client=client)
    blob_id = max(servicer.blobs.keys())  # last uploaded one
    assert len(servicer.blobs[blob_id]) == len(large_content)
    assert servicer.blobs[blob_id] == large_content

    assert servicer.files_sha2data[files["/large.py"].sha256_hex] == {"data": b"", "data_blob_id": blob_id}
    assert servicer.files_sha2data[files["/small.py"].sha256_hex] == {
        "data": small_content,
        "data_blob_id": "",
    }


def test_create_mount(servicer, client):
    local_dir, cur_filename = os.path.split(__file__)

    def condition(fn):
        return fn.endswith(".py")

    m = Mount._from_local_dir(local_dir, remote_path="/foo", condition=condition)

    m._deploy("my-mount", client=client)

    assert m.object_id == "mo-1"
    assert f"/foo/{cur_filename}" in servicer.files_name2sha
    sha256_hex = servicer.files_name2sha[f"/foo/{cur_filename}"]
    assert sha256_hex in servicer.files_sha2data
    assert servicer.files_sha2data[sha256_hex]["data"] == open(__file__, "rb").read()
    assert repr(Path(local_dir)) in repr(m)


def test_create_mount_file_errors(servicer, tmpdir, client):
    m = Mount._from_local_dir(Path(tmpdir) / "xyz", remote_path="/xyz")
    with pytest.raises(FileNotFoundError):
        m._deploy("my-mount", client=client)

    with open(tmpdir / "abc", "w"):
        pass
    m = Mount._from_local_dir(Path(tmpdir) / "abc", remote_path="/abc")
    with pytest.raises(NotADirectoryError):
        m._deploy("my-mount", client=client)


def dummy():
    pass


def test_from_local_python_packages(servicer, client, test_dir, monkeypatch):
    app = App()

    monkeypatch.syspath_prepend((test_dir / "supports").as_posix())

    app.function(mounts=[Mount._from_local_python_packages("pkg_a", "pkg_b", "standalone_file")])(dummy)

    with app.run(client=client):
        files = set(servicer.files_name2sha.keys())
        expected_files = {
            "/root/pkg_a/a.py",
            "/root/pkg_a/b/c.py",
            "/root/pkg_b/f.py",
            "/root/pkg_b/g/h.py",
            "/root/standalone_file.py",
        }
        assert expected_files.issubset(files)

        assert "/root/pkg_c/i.py" not in files
        assert "/root/pkg_c/j/k.py" not in files


def test_chained_entries(test_dir):
    # TODO: remove when public Mount is deprecated
    a_txt = str(test_dir / "a.txt")
    b_txt = str(test_dir / "b.txt")
    with open(a_txt, "w") as f:
        f.write("A")
    with open(b_txt, "w") as f:
        f.write("B")
    mount = Mount._from_local_file(a_txt).add_local_file(b_txt)
    entries = mount.entries
    assert len(entries) == 2
    files = [file for file in Mount._get_files(entries)]
    assert len(files) == 2
    files.sort(key=lambda file: file.source_description)
    assert files[0].source_description.name == "a.txt"
    assert files[0].mount_filename.endswith("/a.txt")
    assert files[0].content == b"A"
    m = hashlib.sha256()
    m.update(b"A")
    assert files[0].sha256_hex == m.hexdigest()
    assert files[0].use_blob is False


def test_module_mount_condition():
    condition = module_mount_condition(Path("/a/.venv/site-packages/mymod"))
    ignore_condition = module_mount_ignore_condition(Path("/a/.venv/site-packages/mymod"))

    include_paths = [
        Path("/a/.venv/site-packages/mymod/foo.py"),
        Path("/a/my_mod/config/foo.txt"),
        Path("/a/my_mod/config/foo.py"),
    ]
    exclude_paths = [
        Path("/a/site-packages/mymod/foo.pyc"),
        Path("/a/site-packages/mymod/__pycache__/foo.py"),
        Path("/a/my_mod/.config/foo.py"),
    ]
    for path in include_paths:
        assert condition(path)
        assert not ignore_condition(path)
    for path in exclude_paths:
        assert not condition(path)
        assert ignore_condition(path)


def test_mount_from_local_dir_ignore(test_dir, tmp_path_with_content):
    ignore = FilePatternMatcher("**/*.txt", "**/module", "!**/*.txt", "!**/*.py")
    expected = {
        "/foo/module/sub.py",
        "/foo/module/sub/sub.py",
        "/foo/data/sub",
        "/foo/module/__init__.py",
        "/foo/data.txt",
        "/foo/module/sub/__init__.py",
    }

    mount = Mount._add_local_dir(tmp_path_with_content, PurePosixPath("/foo"), ignore=ignore)

    file_names = [file.mount_filename for file in Mount._get_files(entries=mount.entries)]
    assert set(file_names) == expected


def test_missing_python_source_warning(servicer, credentials, supports_dir, monkeypatch):
    # should warn if function doesn't have an imported non-third-party package attached
    # either through add OR copy mode, unless automount=False mode is used
    monkeypatch.delenv("MODAL_AUTOMOUNT")  # prevent autoused disable_automount fixture

    def has_warning(output: str):
        return '.add_local_python_source("pkg_a")' in output

    output = deploy_app_externally(servicer, credentials, "pkg_d.main", cwd=supports_dir, capture_output=True)
    assert has_warning(output)

    # adding the source to the image should make the warning disappear
    output = deploy_app_externally(
        servicer, credentials, "pkg_d.main", cwd=supports_dir, capture_output=True, env={"ADD_SOURCE": "add"}
    )
    assert not has_warning(output)

    # *copying* the source to the image should make the warning disappear too
    output = deploy_app_externally(
        servicer, credentials, "pkg_d.main", cwd=supports_dir, capture_output=True, env={"ADD_SOURCE": "copy"}
    )
    assert not has_warning(output)

    # disabling auto-mount explicitly should make warning disappear
    output = deploy_app_externally(
        servicer, credentials, "pkg_d.main", cwd=supports_dir, capture_output=True, env={"MODAL_AUTOMOUNT": "0"}
    )
    assert not has_warning(output)


================================================
File: test/mount_utils_test.py
================================================
# Copyright Modal Labs 2024
import pytest

from modal._utils.mount_utils import validate_mount_points, validate_network_file_systems, validate_volumes
from modal.exception import InvalidError
from modal.network_file_system import _NetworkFileSystem
from modal.volume import _Volume


def test_validate_mount_points():
    # valid mount points
    dict_input = {"/foo/bar": _NetworkFileSystem.from_name("_NetworkFileSystem", create_if_missing=False)}
    validate_mount_points("_NetworkFileSystem", dict_input)  # type: ignore

    # invalid list input, should be dicts
    list_input = [_NetworkFileSystem.from_name("_NetworkFileSystem", create_if_missing=False)]

    with pytest.raises(InvalidError, match="volume_likes"):
        validate_mount_points("_NetworkFileSystem", list_input)  # type: ignore


@pytest.mark.parametrize("path", ["/", "/root", "/tmp", "foo/bar"])
def test_validate_mount_points_invalid_paths(path):
    validated_mount_points = {path: _NetworkFileSystem.from_name("_NetworkFileSystem", create_if_missing=False)}
    with pytest.raises(InvalidError, match="_NetworkFileSystem"):
        validate_mount_points("_NetworkFileSystem", validated_mount_points)


def test_validate_network_file_systems(client, servicer):
    # valid network_file_systems input
    network_file_systems = {"/my/path": _NetworkFileSystem.from_name("foo", create_if_missing=False)}
    validate_network_file_systems(network_file_systems)  # type: ignore

    # invalid non network_file_systems input
    not_network_file_systems = {"/my/path": _Volume.from_name("foo", create_if_missing=False)}
    with pytest.raises(InvalidError, match="_Volume"):
        validate_network_file_systems(not_network_file_systems)  # type: ignore


def test_validate_volumes(client, servicer):
    # valid volume input
    volumes = {"/my/path": _Volume.from_name("foo", create_if_missing=False)}
    validate_volumes(volumes)  # type: ignore

    # invalid non volume input
    not_volumes = {"/my/path": _NetworkFileSystem.from_name("foo", create_if_missing=False)}
    with pytest.raises(InvalidError, match="_NetworkFileSystem"):
        validate_volumes(not_volumes)  # type: ignore

    # invalid attempt mount volume twice
    vol = _Volume.from_name("foo", create_if_missing=False)
    bad_path_volumes = {"/my/path": vol, "/my/other/path": vol}
    with pytest.raises(InvalidError, match="Volume"):
        validate_volumes(bad_path_volumes)  # type: ignore


================================================
File: test/mounted_files_test.py
================================================
# Copyright Modal Labs 2022
import os
import pytest
import subprocess
import sys
from pathlib import Path

import pytest_asyncio

import modal
from modal import Mount
from modal.mount import get_sys_modules_mounts

from . import helpers
from .supports.skip import skip_windows


@pytest.fixture
def venv_path(tmp_path, repo_root):
    venv_path = tmp_path
    args = [sys.executable, "-m", "venv", venv_path, "--system-site-packages"]
    if sys.platform == "win32":
        # --copies appears to be broken on Python 3.13.0
        # but I believe it is a no-op on non-windows platforms anyway?
        args.append("--copies")
    subprocess.run(args, check=True)
    # Install Modal and a tiny package in the venv.
    subprocess.run([venv_path / "bin" / "python", "-m", "pip", "install", "-e", repo_root], check=True)
    subprocess.run([venv_path / "bin" / "python", "-m", "pip", "install", "--force-reinstall", "six"], check=True)
    yield venv_path


@pytest.fixture
def path_with_symlinked_files(tmp_path):
    src = tmp_path / "foo.txt"
    src.write_text("Hello")
    trg = tmp_path / "bar.txt"
    trg.symlink_to(src)
    return tmp_path, {src, trg}


script_path = "pkg_a/script.py"


def f():
    pass


@pytest_asyncio.fixture
async def env_mount_files():
    # If something is installed using pip -e, it will be bundled up as a part of the environment.
    # Those are env-specific so we ignore those as a part of the test
    filenames = []
    for mount in get_sys_modules_mounts().values():
        async for file_info in mount._get_files(mount.entries):
            filenames.append(file_info.mount_filename)

    return filenames


def test_mounted_files_script(servicer, credentials, supports_dir, env_mount_files, server_url_env, monkeypatch):
    monkeypatch.setenv("MODAL_AUTOMOUNT", "1")  # re-enable automount since that's what we test here
    print(helpers.deploy_app_externally(servicer, credentials, script_path, cwd=supports_dir))
    files = set(servicer.files_name2sha.keys()) - set(env_mount_files)

    # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c`:
    assert files == {
        "/root/a.py",
        "/root/b/c.py",
        "/root/b/e.py",
        "/root/pkg_b/__init__.py",
        "/root/pkg_b/f.py",
        "/root/pkg_b/g/h.py",
        "/root/script.py",
    }


serialized_fn_path = "pkg_a/serialized_fn.py"


def test_mounted_files_serialized(servicer, credentials, supports_dir, env_mount_files, server_url_env, monkeypatch):
    monkeypatch.delenv("MODAL_AUTOMOUNT")
    helpers.deploy_app_externally(servicer, credentials, serialized_fn_path, cwd=supports_dir)
    files = set(servicer.files_name2sha.keys()) - set(env_mount_files)

    # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c`:
    assert files == {
        # should serialized_fn be included? It's not needed to run the function,
        # but it's loaded into sys.modules at definition time...
        "/root/serialized_fn.py",
        # this is mounted under root since it's imported as `import b`
        # and not `import pkg_a.b` from serialized_fn.py
        "/root/b/c.py",
        "/root/b/e.py",  # same as above
        "/root/a.py",  # same as above
        "/root/pkg_b/__init__.py",
        "/root/pkg_b/f.py",
        "/root/pkg_b/g/h.py",
    }


def test_mounted_files_package(supports_dir, env_mount_files, servicer, server_url_env, token_env):
    p = subprocess.run(["modal", "run", "pkg_a.package"], cwd=supports_dir)
    assert p.returncode == 0

    files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
    # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c`:
    assert files == {
        "/root/pkg_a/__init__.py",
        "/root/pkg_a/a.py",
        "/root/pkg_a/b/c.py",
        "/root/pkg_a/d.py",
        "/root/pkg_a/b/e.py",
        "/root/pkg_a/script.py",
        "/root/pkg_a/serialized_fn.py",
        "/root/pkg_a/package.py",
    }


def test_mounted_files_package_with_automount(supports_dir, env_mount_files, servicer, server_url_env, token_env):
    p = subprocess.run(
        ["modal", "run", "pkg_a.package"],
        cwd=supports_dir,
        env={**os.environ, "MODAL_AUTOMOUNT": "1"},
    )
    assert p.returncode == 0
    files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
    assert files == {
        "/root/pkg_a/__init__.py",
        "/root/pkg_a/a.py",
        "/root/pkg_a/b/c.py",
        "/root/pkg_a/b/e.py",
        "/root/pkg_a/d.py",
        "/root/pkg_a/package.py",
        "/root/pkg_a/script.py",
        "/root/pkg_a/serialized_fn.py",
        "/root/pkg_b/__init__.py",
        "/root/pkg_b/f.py",
        "/root/pkg_b/g/h.py",
    }


@skip_windows("venvs behave differently on Windows.")
def test_mounted_files_sys_prefix(servicer, supports_dir, venv_path, env_mount_files, server_url_env, token_env):
    # Run with venv activated, so it's on sys.prefix, and modal is dev-installed in the VM
    subprocess.run(
        [venv_path / "bin" / "modal", "run", script_path], cwd=supports_dir, env={**os.environ, "MODAL_AUTOMOUNT": "1"}
    )
    files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
    # Assert we include everything from `pkg_a` and `pkg_b` but not `pkg_c` or `modal`
    assert files == {
        "/root/a.py",
        "/root/b/c.py",
        "/root/b/e.py",
        "/root/script.py",
        "/root/pkg_b/__init__.py",
        "/root/pkg_b/f.py",
        "/root/pkg_b/g/h.py",
    }


@pytest.fixture
def symlinked_python_installation_venv_path(tmp_path, repo_root):
    # sets up a symlink to the python *installation* (not just the python binary)
    # and initialize the virtualenv using a path via that symlink
    # This makes the file paths of any stdlib modules use the symlinked path
    # instead of the original, which is similar to what some tools do (e.g. mise)
    # and has the potential to break automounting behavior, so we keep this
    # test as a regression test for that
    venv_path = tmp_path / "venv"
    actual_executable = Path(sys.executable).resolve()
    assert actual_executable.parent.name == "bin"
    python_install_dir = actual_executable.parent.parent
    # create a symlink to the python install *root*
    symlink_python_install = tmp_path / "python-install"
    symlink_python_install.symlink_to(python_install_dir)

    # use a python executable specified via the above symlink
    symlink_python_executable = symlink_python_install / "bin" / actual_executable.name
    # create a new venv
    subprocess.check_call([symlink_python_executable, "-m", "venv", venv_path, "--copies"])
    # check that a builtin module, like ast, is indeed identified to be in the non-resolved install path
    # since this is the source of bugs that we want to assert we don't run into!
    ast_path = subprocess.check_output(
        [venv_path / "bin" / "python", "-c", "import ast; print(ast.__file__);"], encoding="utf8"
    )
    assert ast_path != Path(ast_path).resolve()

    # install modal from current dir
    subprocess.check_call([venv_path / "bin" / "pip", "install", repo_root])
    yield venv_path


@skip_windows("venvs behave differently on Windows.")
def test_mounted_files_symlinked_python_install(
    symlinked_python_installation_venv_path, supports_dir, server_url_env, token_env, servicer
):
    # TODO(elias): This test fails when run from a uv-managed virtualenv
    subprocess.check_call(
        [symlinked_python_installation_venv_path / "bin" / "modal", "run", supports_dir / "imports_ast.py"]
    )
    assert "/root/ast.py" not in servicer.files_name2sha


def test_mounted_files_config(servicer, supports_dir, env_mount_files, server_url_env, token_env):
    p = subprocess.run(
        ["modal", "run", "pkg_a/script.py"], cwd=supports_dir, env={**os.environ, "MODAL_AUTOMOUNT": "0"}
    )
    assert p.returncode == 0
    files = set(servicer.files_name2sha.keys()) - set(env_mount_files)
    assert files == {
        "/root/script.py",
    }


def test_e2e_modal_run_py_file_mounts(servicer, credentials, supports_dir):
    helpers.deploy_app_externally(servicer, credentials, "hello.py", cwd=supports_dir)
    # Reactivate the following mount assertions when we remove auto-mounting of dev-installed packages
    # assert len(servicer.files_name2sha) == 1
    # assert servicer.n_mounts == 1  # there should be a single mount
    # assert servicer.n_mount_files == 1
    assert "/root/hello.py" in servicer.files_name2sha


def test_e2e_modal_run_py_module_mounts(servicer, credentials, supports_dir):
    helpers.deploy_app_externally(servicer, credentials, "hello", cwd=supports_dir)
    # Reactivate the following mount assertions when we remove auto-mounting of dev-installed packages
    # assert len(servicer.files_name2sha) == 1
    # assert servicer.n_mounts == 1  # there should be a single mount
    # assert servicer.n_mount_files == 1
    assert "/root/hello.py" in servicer.files_name2sha


def foo():
    pass


def test_mounts_are_not_traversed_on_declaration(supports_dir, monkeypatch, client, server_url_env):
    # TODO: remove once Mount is fully deprecated (replaced by test_image_mounts_are_not_traversed_on_declaration)
    return_values = []
    original = modal.mount._MountDir.get_files_to_upload

    def mock_get_files_to_upload(self):
        r = list(original(self))
        return_values.append(r)
        return r

    monkeypatch.setattr("modal.mount._MountDir.get_files_to_upload", mock_get_files_to_upload)
    app = modal.App()
    mount_with_many_files = Mount._from_local_dir(supports_dir / "pkg_a", remote_path="/test")
    app.function(mounts=[mount_with_many_files])(foo)
    assert len(return_values) == 0  # ensure we don't look at the files yet

    with app.run(client=client):
        pass

    assert return_values  # at this point we should have gotten all the mount files
    # flatten inspected files
    files = set()
    for r in return_values:
        for fn, _ in r:
            files.add(fn)
    # sanity check - this test file should be included since we mounted the test dir
    assert Path(__file__) in files  # this test file should have been included


def test_image_mounts_are_not_traversed_on_declaration(supports_dir, monkeypatch, client, server_url_env):
    return_values = []
    original = modal.mount._MountDir.get_files_to_upload

    def mock_get_files_to_upload(self):
        r = list(original(self))
        return_values.append(r)
        return r

    monkeypatch.setattr("modal.mount._MountDir.get_files_to_upload", mock_get_files_to_upload)
    app = modal.App()
    image_mount_with_many_files = modal.Image.debian_slim().add_local_dir(supports_dir / "pkg_a", remote_path="/test")
    app.function(image=image_mount_with_many_files)(foo)
    assert len(return_values) == 0  # ensure we don't look at the files yet

    with app.run(client=client):
        pass

    assert return_values  # at this point we should have gotten all the mount files
    # flatten inspected files
    files = set()
    for r in return_values:
        for fn, _ in r:
            files.add(fn)
    # sanity check - this test file should be included since we mounted the test dir
    assert Path(__file__) in files  # this test file should have been included


def test_mount_dedupe(servicer, credentials, test_dir, server_url_env):
    supports_dir = test_dir / "supports"
    normally_not_included_file = supports_dir / "pkg_a" / "normally_not_included.pyc"
    normally_not_included_file.touch(exist_ok=True)
    print(
        helpers.deploy_app_externally(
            # no explicit mounts, rely on auto-mounting
            servicer,
            credentials,
            "mount_dedupe.py",
            cwd=test_dir / "supports",
            env={**os.environ, "USE_EXPLICIT": "0", "MODAL_AUTOMOUNT": "1"},
        )
    )
    assert servicer.n_mounts == 2
    # the order isn't strictly defined here
    entrypoint_mount, pkg_a_mount = sorted(
        servicer.mounts_excluding_published_client().items(), key=lambda item: len(item[1])
    )
    assert entrypoint_mount[1].keys() == {"/root/mount_dedupe.py"}
    for fn in pkg_a_mount[1].keys():
        assert fn.startswith("/root/pkg_a")
    assert "/root/pkg_a/normally_not_included.pyc" not in pkg_a_mount[1].keys()


def test_mount_dedupe_explicit(servicer, credentials, supports_dir, server_url_env):
    normally_not_included_file = supports_dir / "pkg_a" / "normally_not_included.pyc"
    normally_not_included_file.touch(exist_ok=True)
    print(
        helpers.deploy_app_externally(
            # two explicit mounts of the same package
            servicer,
            credentials,
            "mount_dedupe.py",
            cwd=supports_dir,
            env={"USE_EXPLICIT": "1"},
        )
    )
    assert servicer.n_mounts == 3

    # mounts are loaded in parallel, but there
    mounted_files_sets = {frozenset(m.keys()) for m in servicer.mounts_excluding_published_client().values()}
    assert {"/root/mount_dedupe.py"} in mounted_files_sets
    mounted_files_sets.remove(frozenset({"/root/mount_dedupe.py"}))

    # find one mount that includes normally_not_included.py
    for mount_with_pyc in mounted_files_sets:
        if "/root/pkg_a/normally_not_included.pyc" in mount_with_pyc:
            break
    else:
        assert False, "could not find a mount with normally_not_included.pyc"
    mounted_files_sets.remove(mount_with_pyc)

    # and one without it
    remaining_mount = list(mounted_files_sets)[0]
    assert "/root/pkg_a/normally_not_included.pyc" not in remaining_mount
    for fn in remaining_mount:
        assert fn.startswith("/root/pkg_a")

    assert len(mount_with_pyc) == len(remaining_mount) + 1
    normally_not_included_file.unlink()  # cleanup


def test_mount_dedupe_relative_path_entrypoint(servicer, credentials, supports_dir, server_url_env, monkeypatch):
    workdir = supports_dir / "pkg_a"
    target_app = "../hello.py"  # in parent directory - requiring `..` expansion in path normalization

    helpers.deploy_app_externally(
        # two explicit mounts of the same package
        servicer,
        credentials,
        target_app,
        cwd=workdir,
    )
    # should be only one unique set of files in mounts
    mounted_files_sets = {frozenset(m.keys()) for m in servicer.mounts_excluding_published_client().values()}
    assert len(mounted_files_sets) == 1

    # but there should also be only one actual mount if deduplication works as expected
    assert len(servicer.mounts_excluding_published_client()) == 1


# @skip_windows("pip-installed pdm seems somewhat broken on windows")
# @skip_old_py("some weird issues w/ pdm and Python 3.9", min_version=(3, 10, 0))
@pytest.mark.skip(reason="currently broken on ubuntu github actions")
def test_pdm_cache_automount_exclude(tmp_path, monkeypatch, supports_dir, servicer, server_url_env, token_env):
    # check that `pdm`'s cached packages are not included in automounts
    project_dir = Path(__file__).parent.parent
    monkeypatch.chdir(tmp_path)
    subprocess.run(["pdm", "init", "-n"], check=True)
    subprocess.run(
        ["pdm", "add", "--dev", project_dir], check=True
    )  # install workdir modal into venv, not using cache...
    subprocess.run(["pdm", "config", "--local", "install.cache", "on"], check=True)
    subprocess.run(["pdm", "add", "six"], check=True)  # single file module
    subprocess.run(
        ["pdm", "run", "modal", "deploy", supports_dir / "imports_six.py"], check=True
    )  # deploy a basically empty function

    files = set(servicer.files_name2sha.keys())
    assert files == {
        "/root/imports_six.py",
    }


def test_mount_directory_with_symlinked_file(path_with_symlinked_files, servicer, client):
    path, files = path_with_symlinked_files
    mount = Mount._from_local_dir(path)
    mount._deploy("mo-1", client=client)
    pkg_a_mount = servicer.mount_contents["mo-1"]
    for src_f in files:
        assert any(mnt_f.endswith(src_f.name) for mnt_f in pkg_a_mount)


def test_module_with_dot_prefixed_parent_can_be_mounted(tmp_path, monkeypatch, servicer, client):
    # the typical usecase would be to have a `.venv` directory with a virualenv
    # that could possibly contain local site-packages that a user wants to mount

    # set up some dummy packages:
    # .parent
    #    |---- foo.py
    #    |---- bar
    #    |------|--baz.py
    #    |------|--.hidden_dir
    #    |------|------|-----mod.py
    #    |------|--.hidden_mod.py

    parent_dir = Path(tmp_path) / ".parent"
    parent_dir.mkdir()
    foo_py = parent_dir / "foo.py"
    foo_py.touch()
    bar_package = parent_dir / "bar"
    bar_package.mkdir()
    (bar_package / "__init__.py").touch()
    (bar_package / "baz.py").touch()
    (bar_package / ".hidden_dir").mkdir()
    (bar_package / ".hidden_dir" / "mod.py").touch()  # should be excluded
    (bar_package / ".hidden_mod.py").touch()  # should be excluded

    monkeypatch.syspath_prepend(parent_dir)
    foo_mount = Mount._from_local_python_packages("foo")
    foo_mount._deploy("mo-1", client=client)
    foo_mount_content = servicer.mount_contents["mo-1"]
    assert foo_mount_content.keys() == {"/root/foo.py"}

    bar_mount = Mount._from_local_python_packages("bar")
    bar_mount._deploy("mo-2", client=client)

    bar_mount_content = servicer.mount_contents["mo-2"]
    assert bar_mount_content.keys() == {"/root/bar/__init__.py", "/root/bar/baz.py"}


================================================
File: test/network_file_system_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import time
from io import BytesIO
from unittest import mock

import modal
from modal.exception import InvalidError, NotFoundError


def dummy():
    pass


def test_network_file_system_files(client, test_dir, servicer):
    app = modal.App()
    nfs = modal.NetworkFileSystem.from_name("xyz", create_if_missing=True)

    dummy_modal = app.function(network_file_systems={"/root/foo": nfs})(dummy)

    with app.run(client=client):
        dummy_modal.remote()


def test_network_file_system_bad_paths():
    app = modal.App()
    nfs = modal.NetworkFileSystem.from_name("xyz", create_if_missing=True)

    def _f():
        pass

    with pytest.raises(InvalidError):
        app.function(network_file_systems={"/root/../../foo": nfs})(dummy)

    with pytest.raises(InvalidError):
        app.function(network_file_systems={"/": nfs})(dummy)

    with pytest.raises(InvalidError):
        app.function(network_file_systems={"/tmp/": nfs})(dummy)


def test_network_file_system_handle_single_file(client, tmp_path, servicer):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
        nfs.add_local_file(local_file_path)
        nfs.add_local_file(local_file_path.as_posix(), remote_path="/foo/other_destination")
        object_id = nfs.object_id

    assert servicer.nfs_files[object_id].keys() == {
        "/some_file",
        "/foo/other_destination",
    }
    assert servicer.nfs_files[object_id]["/some_file"].data == b"hello world"
    assert servicer.nfs_files[object_id]["/foo/other_destination"].data == b"hello world"


@pytest.mark.asyncio
async def test_network_file_system_handle_dir(client, tmp_path, servicer):
    local_dir = tmp_path / "some_dir"
    local_dir.mkdir()
    (local_dir / "smol").write_text("###")

    subdir = local_dir / "subdir"
    subdir.mkdir()
    (subdir / "other").write_text("####")

    with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
        nfs.add_local_dir(local_dir)
        object_id = nfs.object_id

    assert servicer.nfs_files[object_id].keys() == {
        "/some_dir/smol",
        "/some_dir/subdir/other",
    }
    assert servicer.nfs_files[object_id]["/some_dir/smol"].data == b"###"
    assert servicer.nfs_files[object_id]["/some_dir/subdir/other"].data == b"####"


@pytest.mark.asyncio
async def test_network_file_system_handle_big_file(client, tmp_path, servicer, blob_server, *args):
    with mock.patch("modal.network_file_system.LARGE_FILE_LIMIT", 10):
        local_file_path = tmp_path / "bigfile"
        local_file_path.write_text("hello world, this is a lot of text")

        async with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
            await nfs.add_local_file.aio(local_file_path)
            object_id = nfs.object_id

        assert servicer.nfs_files[object_id].keys() == {"/bigfile"}
        assert servicer.nfs_files[object_id]["/bigfile"].data == b""
        assert servicer.nfs_files[object_id]["/bigfile"].data_blob_id == "bl-1"

        _, blobs = blob_server
        assert blobs["bl-1"] == b"hello world, this is a lot of text"


def test_read_file(client, tmp_path, servicer):
    with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
        with pytest.raises(FileNotFoundError):
            for _ in nfs.read_file("idontexist.txt"):
                ...


def test_write_file(client, tmp_path, servicer):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    with modal.NetworkFileSystem.ephemeral(client=client) as nfs:
        nfs.write_file("remote_path.txt", open(local_file_path, "rb"))

        # Make sure we can write through the provider too
        nfs.write_file("remote_path.txt", open(local_file_path, "rb"))


def test_persisted(servicer, client):
    # Lookup should fail since it doesn't exist
    with pytest.raises(NotFoundError):
        modal.NetworkFileSystem.from_name("xyz").hydrate(client)

    # Create it
    modal.NetworkFileSystem.from_name("xyz", create_if_missing=True).hydrate(client)

    # Lookup should succeed now
    modal.NetworkFileSystem.from_name("xyz").hydrate(client)


def test_nfs_ephemeral(servicer, client, tmp_path):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    assert servicer.n_nfs_heartbeats == 0
    with modal.NetworkFileSystem.ephemeral(client=client, _heartbeat_sleep=1) as nfs:
        assert nfs.listdir("/") == []
        nfs.write_file("xyz.txt", open(local_file_path, "rb"))
        (entry,) = nfs.listdir("/")
        assert entry.path == "xyz.txt"

        time.sleep(1.5)  # Make time for 2 heartbeats
    assert servicer.n_nfs_heartbeats == 2


def test_nfs_lazy_hydration_from_name(set_env_client):
    nfs = modal.NetworkFileSystem.from_name("nfs", create_if_missing=True)
    bio = BytesIO(b"content")
    nfs.write_file("blah", bio)


@pytest.mark.parametrize("name", ["has space", "has/slash", "a" * 65])
def test_invalid_name(name):
    with pytest.raises(InvalidError, match="Invalid NetworkFileSystem name"):
        modal.NetworkFileSystem.from_name(name)


def test_attempt_mount_volume(client, servicer):
    app = modal.App()
    modal.Volume.create_deployed("my-other-vol", client=client)
    vol = modal.NetworkFileSystem.from_name("my-other-vol", create_if_missing=False)
    f = app.function(network_file_systems={"/data": vol})(dummy)
    with pytest.raises(InvalidError, match="already exists as a Volume"):
        with app.run(client=client):
            f.remote()


================================================
File: test/notebook_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import warnings
from pathlib import Path

with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    import jupytext

try:
    from nbclient.exceptions import CellExecutionError
except (ModuleNotFoundError, DeprecationWarning):
    # TODO(erikbern): sometimes my local jupyter packages end up in a bad state,
    # but we don't want that to cause pytest to fail on startup.
    warnings.warn("failed importing nbclient")


@pytest.fixture
def notebook_runner(servicer):
    import nbformat
    from nbclient import NotebookClient

    def runner(notebook_path: Path):
        output_notebook_path = notebook_path.with_suffix(".output.ipynb")

        nb = jupytext.read(
            notebook_path,
        )

        parameter_cell = nb["cells"][0]
        assert "parameters" in parameter_cell["metadata"]["tags"]  # like in papermill
        parameter_cell["source"] = f'server_addr = "{servicer.client_addr}"'

        client = NotebookClient(nb)

        try:
            client.execute()
        except CellExecutionError:
            nbformat.write(nb, output_notebook_path)
            pytest.fail(
                f"""There was an error when executing the notebook.

Inspect the output notebook: {output_notebook_path}
"""
            )
        tagged_cells = {}
        for cell in nb["cells"]:
            for tag in cell["metadata"].get("tags", []):
                tagged_cells[tag] = cell

        return tagged_cells

    return runner


# for some reason this import is failing due to a circular import of IPython.terminal.embed
# but only when running in CI (sometimes?), causing these tests to fail:
# from IPython.terminal import interactiveshell


@pytest.mark.skip("temporarily disabled until IPython import issues in CI are resolved")
def test_notebook_outputs_status(notebook_runner, test_dir):
    input_notebook_path = test_dir / "supports" / "notebooks" / "simple.notebook.py"
    tagged_cells = notebook_runner(input_notebook_path)
    combined_output = "\n".join(c["data"]["text/plain"] for c in tagged_cells["main"]["outputs"])
    assert "Initialized" in combined_output
    assert "Created objects." in combined_output
    assert "App completed." in combined_output


================================================
File: test/object_test.py
================================================
# Copyright Modal Labs 2022
import pytest

from modal import Secret
from modal._object import _Object
from modal.dict import Dict, _Dict
from modal.exception import DeprecationError, InvalidError
from modal.queue import _Queue


def test_new_hydrated(client):
    assert isinstance(_Dict._new_hydrated("di-123", client, None), _Dict)
    assert isinstance(_Queue._new_hydrated("qu-123", client, None), _Queue)

    with pytest.raises(InvalidError):
        _Queue._new_hydrated("di-123", client, None)  # Wrong prefix for type

    assert isinstance(_Object._new_hydrated("qu-123", client, None), _Queue)
    assert isinstance(_Object._new_hydrated("di-123", client, None), _Dict)

    with pytest.raises(InvalidError):
        _Object._new_hydrated("xy-123", client, None)


def test_on_demand_hydration(client):
    obj = Dict.from_name("test-dict", create_if_missing=True).hydrate(client)
    assert obj.object_id is not None


def test_resolve_deprecation(client):
    obj = Dict.from_name("test-dict", create_if_missing=True)
    warning = r"Please use `Dict.hydrate\(\)` or `await Dict.hydrate.aio\(\)`"
    with pytest.warns(DeprecationError, match=warning):
        obj.resolve(client)
    assert obj.object_id is not None


def test_constructor():
    with pytest.raises(InvalidError) as excinfo:
        Secret({"foo": 123})

    assert "Secret" in str(excinfo.value)
    assert "constructor" in str(excinfo.value)


def test_types():
    assert _Object._get_type_from_id("di-123") == _Dict
    assert _Dict._is_id_type("di-123")
    assert not _Dict._is_id_type("qu-123")
    assert _Queue._is_id_type("qu-123")
    assert not _Queue._is_id_type("di-123")


================================================
File: test/package_utils_test.py
================================================
# Copyright Modal Labs 2022
import platform
import pytest

from modal._utils.package_utils import get_module_mount_info
from modal.exception import ModuleNotMountable


def test_get_module_mount_info():
    res = get_module_mount_info("modal")
    assert len(res) == 1
    assert res[0][0] == True

    res = get_module_mount_info("asyncio")
    assert len(res) == 1
    assert res[0][0] == True

    res = get_module_mount_info("six")
    assert len(res) == 1
    assert res[0][0] == False

    if platform.system() != "Windows":
        # TODO This assertion fails on windows; I assume that compiled file formats are different there?
        with pytest.raises(ModuleNotMountable, match="aiohttp can't be mounted because it contains binary file"):
            get_module_mount_info("aiohttp")


================================================
File: test/queue_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import queue
import time

from modal import Queue
from modal.exception import InvalidError, NotFoundError

from .supports.skip import skip_macos, skip_windows


def test_queue(servicer, client):
    q = Queue.from_name("some-random-queue", create_if_missing=True).hydrate(client)
    assert isinstance(q, Queue)
    assert q.len() == 0
    q.put(42)
    assert q.len() == 1
    assert q.get() == 42
    with pytest.raises(queue.Empty):
        q.get(timeout=0)
    assert q.len() == 0

    # test iter
    q.put_many([1, 2, 3])
    t0 = time.time()
    assert [v for v in q.iterate(item_poll_timeout=1.0)] == [1, 2, 3]
    assert 1.0 < time.time() - t0 < 2.0
    assert [v for v in q.iterate(item_poll_timeout=0.0)] == [1, 2, 3]

    Queue.delete("some-random-queue", client=client)
    with pytest.raises(NotFoundError):
        Queue.from_name("some-random-queue").hydrate(client)


def test_queue_ephemeral(servicer, client):
    with Queue.ephemeral(client=client, _heartbeat_sleep=1) as q:
        q.put("hello")
        assert q.len() == 1
        assert q.get() == "hello"
        time.sleep(1.5)  # enough to trigger two heartbeats

    assert servicer.n_queue_heartbeats == 2


@skip_macos("TODO(erikbern): this consistently fails on OSX. Unclear why.")
@skip_windows("TODO(Jonathon): figure out why timeouts don't occur on Windows.")
@pytest.mark.parametrize(
    ["put_timeout_secs", "min_queue_full_exc_count", "max_queue_full_exc_count"],
    [
        (0.02, 1, 100),  # a low timeout causes some exceptions
        (10.0, 0, 0),  # a high timeout causes zero exceptions
        (0.00, 1, 100),  # zero-len timeout causes some exceptions
        (None, 0, 0),  # no timeout causes zero exceptions
    ],
)
def test_queue_blocking_put(put_timeout_secs, min_queue_full_exc_count, max_queue_full_exc_count, servicer, client):
    import queue
    import threading

    producer_delay = 0.001
    consumer_delay = producer_delay * 5

    queue_full_exceptions = 0
    with Queue.ephemeral(client=client) as q:

        def producer():
            nonlocal queue_full_exceptions
            for i in range(servicer.queue_max_len * 2):
                item = f"Item {i}"
                try:
                    q.put(item, block=True, timeout=put_timeout_secs)  # type: ignore
                except queue.Full:
                    queue_full_exceptions += 1
                time.sleep(producer_delay)

        def consumer():
            while True:
                time.sleep(consumer_delay)
                item = q.get(block=True)  # type: ignore
                if item is None:
                    break  # Exit if a None item is received

        producer_thread = threading.Thread(target=producer)
        consumer_thread = threading.Thread(target=consumer)
        producer_thread.start()
        consumer_thread.start()
        producer_thread.join()
        # Stop the consumer by sending a None item
        q.put(None)  # type: ignore
        consumer_thread.join()

        assert queue_full_exceptions >= min_queue_full_exc_count
        assert queue_full_exceptions <= max_queue_full_exc_count


def test_queue_nonblocking_put(servicer, client):
    with Queue.ephemeral(client=client) as q:
        # Non-blocking PUTs don't tolerate a full queue and will raise exception.
        with pytest.raises(queue.Full) as excinfo:
            for i in range(servicer.queue_max_len + 1):
                q.put(i, block=False)  # type: ignore

    assert str(servicer.queue_max_len) in str(excinfo.value)
    assert i == servicer.queue_max_len


def test_queue_deploy(servicer, client):
    d = Queue.from_name("xyz", create_if_missing=True).hydrate(client)
    d.put(123)


def test_queue_lazy_hydrate_from_name(set_env_client):
    q = Queue.from_name("foo", create_if_missing=True)
    q.put(123)
    assert q.get() == 123


@pytest.mark.parametrize("name", ["has space", "has/slash", "a" * 65])
def test_invalid_name(name):
    with pytest.raises(InvalidError, match="Invalid Queue name"):
        Queue.from_name(name)


================================================
File: test/resolver_test.py
================================================
# Copyright Modal Labs 2023
import asyncio
import pytest
import time
from typing import Optional

from modal._object import _Object
from modal._resolver import Resolver


@pytest.mark.flaky(max_runs=2)
@pytest.mark.asyncio
async def test_multi_resolve_sequential_loads_once(client):
    resolver = Resolver(client, environment_name="", app_id=None)

    load_count = 0

    class _DumbObject(_Object, type_prefix="zz"):
        pass

    async def _load(self: _DumbObject, resolver: Resolver, existing_object_id: Optional[str]):
        nonlocal load_count
        load_count += 1
        self._hydrate("zz-123", resolver.client, None)
        await asyncio.sleep(0.1)

    obj = _DumbObject._from_loader(_load, "DumbObject()")

    t0 = time.monotonic()
    await resolver.load(obj)
    await resolver.load(obj)
    assert 0.08 < time.monotonic() - t0 < 0.15

    assert load_count == 1


@pytest.mark.asyncio
async def test_multi_resolve_concurrent_loads_once(client):
    resolver = Resolver(client, environment_name="", app_id=None)

    load_count = 0

    class _DumbObject(_Object, type_prefix="zz"):
        pass

    async def _load(self: _DumbObject, resolver: Resolver, existing_object_id: Optional[str]):
        nonlocal load_count
        load_count += 1
        self._hydrate("zz-123", resolver.client, None)
        await asyncio.sleep(0.1)

    obj = _DumbObject._from_loader(_load, "DumbObject()")
    t0 = time.monotonic()
    await asyncio.gather(resolver.load(obj), resolver.load(obj))
    assert 0.08 < time.monotonic() - t0 < 0.17
    assert load_count == 1


def test_resolver_without_rich(no_rich, client):
    resolver = Resolver(client, environment_name="", app_id=None)
    resolver.add_status_row()
    with resolver.display():
        pass


================================================
File: test/retries_test.py
================================================
# Copyright Modal Labs 2022
import pytest

import modal
from modal.exception import InvalidError


def default_retries_from_int():
    pass


def fixed_delay_retries():
    pass


def exponential_backoff():
    return 67


def exponential_with_max_delay():
    return 67


def dummy():
    pass


def zero_retries():
    pass


def test_retries(client):
    app = modal.App()

    default_retries_from_int_modal = app.function(retries=5)(default_retries_from_int)
    fixed_delay_retries_modal = app.function(retries=modal.Retries(max_retries=5, backoff_coefficient=1.0))(
        fixed_delay_retries
    )

    exponential_backoff_modal = app.function(
        retries=modal.Retries(max_retries=2, initial_delay=2.0, backoff_coefficient=2.0)
    )(exponential_backoff)

    exponential_with_max_delay_modal = app.function(
        retries=modal.Retries(max_retries=2, backoff_coefficient=2.0, max_delay=30.0)
    )(exponential_with_max_delay)

    zero_retries_modal = app.function(retries=0)(zero_retries)

    with pytest.raises(TypeError):
        # Reject no-args constructions, which is unreadable and harder to support long-term
        app.function(retries=modal.Retries())(dummy)  # type: ignore

    # Reject weird inputs:
    # Don't need server to detect and reject nonsensical input. Can do client-side.
    with pytest.raises(InvalidError):
        app.function(retries=modal.Retries(max_retries=-2))(dummy)

    with pytest.raises(InvalidError):
        app.function(retries=modal.Retries(max_retries=2, backoff_coefficient=0.0))(dummy)

    with app.run(client=client):
        default_retries_from_int_modal.remote()
        fixed_delay_retries_modal.remote()
        exponential_backoff_modal.remote()
        exponential_with_max_delay_modal.remote()
        zero_retries_modal.remote()


================================================
File: test/runner_test.py
================================================
# Copyright Modal Labs 2023
import pytest
import typing

import modal
from modal.client import Client
from modal.exception import AuthError
from modal.runner import deploy_app, run_app
from modal_proto import api_pb2

T = typing.TypeVar("T")


def test_run_app(servicer, client):
    dummy_app = modal.App()
    with servicer.intercept() as ctx:
        with run_app(dummy_app, client=client):
            pass

    ctx.pop_request("AppCreate")
    ctx.pop_request("AppPublish")
    ctx.pop_request("AppClientDisconnect")


def test_run_app_unauthenticated(servicer):
    dummy_app = modal.App()
    with Client.anonymous(servicer.client_addr) as client:
        with pytest.raises(AuthError):
            with run_app(dummy_app, client=client):
                pass


def dummy(): ...


def test_run_app_profile_env_with_refs(servicer, client, monkeypatch):
    monkeypatch.setenv("MODAL_ENVIRONMENT", "profile_env")
    with servicer.intercept() as ctx:
        dummy_app = modal.App()
        ref = modal.Secret.from_name("some_secret")
        dummy_app.function(secrets=[ref])(dummy)

    assert ctx.calls == []  # all calls should be deferred

    with servicer.intercept() as ctx:
        ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
        with run_app(dummy_app, client=client):
            pass

    with pytest.raises(Exception):
        ctx.pop_request("SecretCreate")  # should not create a new secret...

    app_create = ctx.pop_request("AppCreate")
    assert app_create.environment_name == "profile_env"

    secret_get_or_create = ctx.pop_request("SecretGetOrCreate")
    assert secret_get_or_create.environment_name == "profile_env"


def test_run_app_custom_env_with_refs(servicer, client, monkeypatch):
    monkeypatch.setenv("MODAL_ENVIRONMENT", "profile_env")
    dummy_app = modal.App()
    own_env_secret = modal.Secret.from_name("own_env_secret")
    other_env_secret = modal.Secret.from_name("other_env_secret", environment_name="third")  # explicit lookup

    dummy_app.function(secrets=[own_env_secret, other_env_secret])(dummy)

    with servicer.intercept() as ctx:
        ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-123"))
        ctx.add_response("SecretGetOrCreate", api_pb2.SecretGetOrCreateResponse(secret_id="st-456"))
        with run_app(dummy_app, client=client, environment_name="custom"):
            pass

    with pytest.raises(Exception):
        ctx.pop_request("SecretCreate")

    app_create = ctx.pop_request("AppCreate")
    assert app_create.environment_name == "custom"

    secret_get_or_create = ctx.pop_request("SecretGetOrCreate")
    assert secret_get_or_create.environment_name == "custom"

    secret_get_or_create_2 = ctx.pop_request("SecretGetOrCreate")
    assert secret_get_or_create_2.environment_name == "third"


def test_deploy_without_rich(servicer, client, no_rich):
    app = modal.App("dummy-app")
    app.function()(dummy)
    deploy_app(app, client=client)


================================================
File: test/sandbox_test.py
================================================
# Copyright Modal Labs 2022

import hashlib
import pytest
import time
from pathlib import Path

from modal import App, Image, Mount, NetworkFileSystem, Proxy, Sandbox, SandboxSnapshot, Secret
from modal.exception import DeprecationError, InvalidError
from modal.stream_type import StreamType
from modal_proto import api_pb2

from .supports.skip import skip_windows

skip_non_subprocess = skip_windows("Needs subprocess support")


@pytest.fixture
def app(client):
    app = App()
    with app.run(client):
        yield app


@skip_non_subprocess
def test_sandbox(app, servicer):
    sb = Sandbox.create("bash", "-c", "echo bye >&2 && sleep 1 && echo hi && exit 42", timeout=600, app=app)

    assert sb.poll() is None

    t0 = time.time()
    sb.wait()
    # Test that we actually waited for the sandbox to finish.
    assert time.time() - t0 > 0.3

    assert sb.stdout.read() == "hi\n"
    assert sb.stderr.read() == "bye\n"
    # read a second time
    assert sb.stdout.read() == ""
    assert sb.stderr.read() == ""

    assert sb.returncode == 42
    assert sb.poll() == 42


@skip_non_subprocess
def test_sandbox_mount(app, servicer, tmpdir):
    # TODO: remove once Mounts are fully deprecated (replaced by test_sandbox_mount_layer)
    tmpdir.join("a.py").write(b"foo")

    sb = Sandbox.create("echo", "hi", mounts=[Mount._from_local_dir(Path(tmpdir), remote_path="/m")], app=app)
    sb.wait()

    sha = hashlib.sha256(b"foo").hexdigest()
    assert servicer.files_sha2data[sha]["data"] == b"foo"


@skip_non_subprocess
def test_sandbox_mount_layer(app, servicer, tmpdir):
    tmpdir.join("a.py").write(b"foo")

    sb = Sandbox.create("echo", "hi", image=Image.debian_slim().add_local_dir(Path(tmpdir), remote_path="/m"), app=app)
    sb.wait()

    sha = hashlib.sha256(b"foo").hexdigest()
    assert servicer.files_sha2data[sha]["data"] == b"foo"


@skip_non_subprocess
def test_sandbox_image(app, servicer, tmpdir):
    tmpdir.join("a.py").write(b"foo")

    sb = Sandbox.create("echo", "hi", image=Image.debian_slim().pip_install("foo", "bar", "potato"), app=app)
    sb.wait()

    idx = max(servicer.images.keys())
    last_image = servicer.images[idx]

    assert all(c in last_image.dockerfile_commands[-1] for c in ["foo", "bar", "potato"])


@skip_non_subprocess
def test_sandbox_secret(app, servicer, tmpdir):
    sb = Sandbox.create("echo", "$FOO", secrets=[Secret.from_dict({"FOO": "BAR"})], app=app)
    sb.wait()

    assert len(servicer.sandbox_defs[0].secret_ids) == 1


@skip_non_subprocess
def test_sandbox_nfs(client, app, servicer, tmpdir):
    with NetworkFileSystem.ephemeral(client=client) as nfs:
        with pytest.raises(InvalidError):
            Sandbox.create("echo", "foo > /cache/a.txt", network_file_systems={"/": nfs}, app=app)

        Sandbox.create("echo", "foo > /cache/a.txt", network_file_systems={"/cache": nfs}, app=app)

    assert len(servicer.sandbox_defs[0].nfs_mounts) == 1


@skip_non_subprocess
def test_sandbox_from_id(app, client, servicer):
    sb = Sandbox.create("bash", "-c", "echo foo && exit 42", timeout=600, app=app)
    sb.wait()

    sb2 = Sandbox.from_id(sb.object_id, client=client)
    assert sb2.stdout.read() == "foo\n"
    assert sb2.returncode == 42


@skip_non_subprocess
def test_sandbox_terminate(app, servicer):
    sb = Sandbox.create("bash", "-c", "sleep 10000", app=app)
    sb.terminate()

    assert sb.returncode != 0


@skip_non_subprocess
@pytest.mark.asyncio
async def test_sandbox_stdin_async(app, servicer):
    sb = await Sandbox.create.aio("bash", "-c", "while read line; do echo $line; done && exit 13", app=app)

    sb.stdin.write(b"foo\n")
    sb.stdin.write(b"bar\n")

    sb.stdin.write_eof()

    await sb.stdin.drain.aio()

    await sb.wait.aio()

    assert await sb.stdout.read.aio() == "foo\nbar\n"
    assert sb.returncode == 13


@skip_non_subprocess
def test_sandbox_stdin(app, servicer):
    sb = Sandbox.create("bash", "-c", "while read line; do echo $line; done && exit 13", app=app)

    sb.stdin.write(b"foo\n")
    sb.stdin.write(b"bar\n")

    sb.stdin.write_eof()

    sb.stdin.drain()

    sb.wait()

    assert sb.stdout.read() == "foo\nbar\n"
    assert sb.returncode == 13


@skip_non_subprocess
def test_sandbox_stdin_write_str(app, servicer):
    sb = Sandbox.create("bash", "-c", "while read line; do echo $line; done && exit 13", app=app)

    sb.stdin.write("foo\n")
    sb.stdin.write("bar\n")

    sb.stdin.write_eof()

    sb.stdin.drain()

    sb.wait()

    assert sb.stdout.read() == "foo\nbar\n"
    assert sb.returncode == 13


@skip_non_subprocess
def test_sandbox_stdin_write_after_terminate(app, servicer):
    sb = Sandbox.create("bash", "-c", "echo foo", app=app)
    sb.wait()
    with pytest.raises(ValueError):
        sb.stdin.write(b"foo")
        sb.stdin.drain()


@skip_non_subprocess
def test_sandbox_stdin_write_after_eof(app, servicer):
    sb = Sandbox.create(app=app)
    sb.stdin.write_eof()
    with pytest.raises(ValueError):
        sb.stdin.write(b"foo")
    sb.terminate()


@skip_non_subprocess
def test_sandbox_stdout(app, servicer):
    """Test that reads from sandboxes are fully line-buffered, i.e.,
    that we don't read partial lines or multiple lines at once."""

    # normal sequence of reads
    sb = Sandbox.create("bash", "-c", "for i in $(seq 1 3); do echo foo $i; done", app=app)
    out = []
    for line in sb.stdout:
        out.append(line)
    assert out == ["foo 1\n", "foo 2\n", "foo 3\n"]

    # multiple newlines
    sb = Sandbox.create("bash", "-c", "echo 'foo 1\nfoo 2\nfoo 3'", app=app)
    out = []
    for line in sb.stdout:
        out.append(line)
    assert out == ["foo 1\n", "foo 2\n", "foo 3\n"]

    # partial lines
    sb = Sandbox.create("sleep", "infinity", app=app)
    cp = sb.exec("bash", "-c", "while read line; do echo $line; done")

    cp.stdin.write(b"foo 1\n")
    cp.stdin.write(b"foo 2")
    cp.stdin.write(b"foo 3\n")
    cp.stdin.write_eof()
    cp.stdin.drain()

    assert cp.stdout.read() == "foo 1\nfoo 2foo 3\n"


@skip_non_subprocess
@pytest.mark.asyncio
async def test_sandbox_async_for(app, servicer):
    sb = await Sandbox.create.aio("bash", "-c", "echo hello && echo world && echo bye >&2", app=app)

    out = ""

    async for message in sb.stdout:
        out += message
    assert out == "hello\nworld\n"

    # test streaming stdout a second time
    out2 = ""
    async for message in sb.stdout:
        out2 += message
    assert out2 == ""

    err = ""
    async for message in sb.stderr:
        err += message

    assert err == "bye\n"

    # test reading after receiving EOF
    assert await sb.stdout.read.aio() == ""
    assert await sb.stderr.read.aio() == ""


@skip_non_subprocess
def test_sandbox_exec_stdout_bytes_mode(app, servicer):
    """Test that the stream reader works in bytes mode."""

    sb = Sandbox.create(app=app)

    p = sb.exec("echo", "foo", text=False)
    assert p.stdout.read() == b"foo\n"

    p = sb.exec("echo", "foo", text=False)
    for line in p.stdout:
        assert line == b"foo\n"


@skip_non_subprocess
def test_app_sandbox(client, servicer):
    image = Image.debian_slim().pip_install("xyz").add_local_file(__file__, remote_path="/xyz")
    secret = Secret.from_dict({"FOO": "bar"})

    with pytest.raises(DeprecationError, match="Creating a `Sandbox` without an `App`"):
        Sandbox.create("bash", "-c", "echo bye >&2 && echo hi", image=image, secrets=[secret])

    app = App()
    with app.run(client):
        # Create sandbox
        with pytest.raises(DeprecationError, match="`App.spawn_sandbox` is deprecated"):
            app.spawn_sandbox("bash", "-c", "echo bye >&2 && echo hi", image=image, secrets=[secret])

        sb = Sandbox.create("bash", "-c", "echo bye >&2 && echo hi", image=image, secrets=[secret], app=app)
        sb.wait()
        assert sb.stderr.read() == "bye\n"
        assert sb.stdout.read() == "hi\n"


@skip_non_subprocess
def test_sandbox_exec(app, servicer):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("bash", "-c", "while read line; do echo $line; done")

    cp.stdin.write(b"foo\n")
    cp.stdin.write(b"bar\n")
    cp.stdin.write_eof()
    cp.stdin.drain()

    assert cp.stdout.read() == "foo\nbar\n"


@skip_non_subprocess
def test_sandbox_exec_wait(app, servicer):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("bash", "-c", "sleep 0.5 && exit 42")

    assert cp.poll() is None

    t0 = time.time()
    assert cp.wait() == 42
    assert time.time() - t0 > 0.2

    assert cp.poll() == 42


@skip_non_subprocess
def test_sandbox_create_and_exec_with_bad_args(app, servicer):
    with pytest.raises(InvalidError):
        too_big = 2_097_152 + 10
        single_arg_size = too_big // 10
        args = ["a" * single_arg_size for _ in range(10)]
        Sandbox.create(*args, app=app)

    sb = Sandbox.create("sleep", "infinity", app=app)
    with pytest.raises(InvalidError):
        sb.exec("echo", 1)  # type: ignore


@skip_non_subprocess
def test_sandbox_on_app_lookup(client, servicer):
    app = App.lookup("my-app", create_if_missing=True, client=client)
    sb = Sandbox.create("echo", "hi", app=app)
    sb.wait()
    assert sb.stdout.read() == "hi\n"
    assert servicer.sandbox_app_id == app.app_id


@skip_non_subprocess
def test_sandbox_list_env(app, client, servicer):
    sb = Sandbox.create("bash", "-c", "sleep 10000", app=app)
    assert len(list(Sandbox.list(client=client))) == 1
    sb.terminate()
    assert not list(Sandbox.list(client=client))


@skip_non_subprocess
def test_sandbox_list_app(client, servicer):
    image = Image.debian_slim().pip_install("xyz").add_local_file(__file__, "/xyz")
    secret = Secret.from_dict({"FOO": "bar"})

    app = App()

    with app.run(client):
        # Create sandbox
        sb = Sandbox.create("bash", "-c", "sleep 10000", image=image, secrets=[secret], app=app)
        assert len(list(Sandbox.list(app_id=app.app_id, client=client))) == 1
        sb.terminate()
        assert not list(Sandbox.list(app_id=app.app_id, client=client))


@skip_non_subprocess
def test_sandbox_list_tags(app, client, servicer):
    sb = Sandbox.create("bash", "-c", "sleep 10000", app=app)
    sb.set_tags({"foo": "bar", "baz": "qux"}, client=client)
    assert len(list(Sandbox.list(tags={"foo": "bar"}, client=client))) == 1
    assert not list(Sandbox.list(tags={"foo": "notbar"}, client=client))
    sb.terminate()
    assert not list(Sandbox.list(tags={"baz": "qux"}, client=client))


@skip_non_subprocess
def test_sandbox_network_access(app, servicer):
    with pytest.raises(InvalidError):
        Sandbox.create("echo", "test", block_network=True, cidr_allowlist=["10.0.0.0/8"], app=app)

    # Test that blocking works
    sb = Sandbox.create("echo", "test", block_network=True, app=app)
    assert (
        servicer.sandbox_defs[0].network_access.network_access_type == api_pb2.NetworkAccess.NetworkAccessType.BLOCKED
    )
    assert len(servicer.sandbox_defs[0].network_access.allowed_cidrs) == 0
    sb.terminate()

    # Test that allowlisting works
    sb = Sandbox.create("echo", "test", block_network=False, cidr_allowlist=["10.0.0.0/8"], app=app)
    assert (
        servicer.sandbox_defs[1].network_access.network_access_type == api_pb2.NetworkAccess.NetworkAccessType.ALLOWLIST
    )
    assert len(servicer.sandbox_defs[1].network_access.allowed_cidrs) == 1
    assert servicer.sandbox_defs[1].network_access.allowed_cidrs[0] == "10.0.0.0/8"
    sb.terminate()

    # Test that no rules means allow all
    sb = Sandbox.create("echo", "test", block_network=False, app=app)
    assert servicer.sandbox_defs[2].network_access.network_access_type == api_pb2.NetworkAccess.NetworkAccessType.OPEN
    assert len(servicer.sandbox_defs[2].network_access.allowed_cidrs) == 0
    sb.terminate()


@skip_non_subprocess
def test_sandbox_no_entrypoint(app, servicer):
    sb = Sandbox.create(app=app)

    p = sb.exec("echo", "hi")
    p.wait()
    assert p.returncode == 0
    assert p.stdout.read() == "hi\n"

    sb.terminate()


@skip_non_subprocess
def test_sandbox_gpu_fallbacks_support(client, servicer):
    with pytest.raises(InvalidError, match="do not support"):
        Sandbox.create(client=client, gpu=["t4", "a100"])  # type: ignore


@skip_non_subprocess
def test_sandbox_exec_stdout(app, servicer, capsys):
    sb = Sandbox.create("sleep", "infinity", app=app)

    cp = sb.exec("bash", "-c", "echo hi", stdout=StreamType.STDOUT)
    cp.wait()

    assert capsys.readouterr().out == "hi\n"

    with pytest.raises(InvalidError):
        cp.stdout.read()


@skip_non_subprocess
def test_sandbox_snapshot(app, client, servicer):
    sb = Sandbox.create(app=app, _experimental_enable_snapshot=True)
    sandbox_snapshot = sb._experimental_snapshot()
    snapshot_id = sandbox_snapshot.object_id
    assert snapshot_id == "sn-123"
    sb.terminate()

    sandbox_snapshot = SandboxSnapshot.from_id(snapshot_id, client=client)
    assert sandbox_snapshot.object_id == snapshot_id

    sb = Sandbox._experimental_from_snapshot(sandbox_snapshot, client=client)
    sb.terminate()


@skip_non_subprocess
def test_sandbox_snapshot_fs(app, servicer):
    sb = Sandbox.create(app=app)
    image = sb.snapshot_filesystem()
    sb.terminate()

    sb2 = Sandbox.create(image=image, app=app)
    sb2.terminate()

    assert image.object_id == "im-123"
    assert servicer.sandbox_defs[1].image_id == "im-123"


@skip_non_subprocess
def test_sandbox_cpu_request(app, servicer):
    _ = Sandbox.create(cpu=2.0, app=app)

    assert servicer.sandbox_defs[0].resources.milli_cpu == 2000
    assert servicer.sandbox_defs[0].resources.milli_cpu_max == 0


@skip_non_subprocess
def test_sandbox_cpu_limit(app, servicer):
    _ = Sandbox.create(cpu=(2, 4), app=app)

    assert servicer.sandbox_defs[0].resources.milli_cpu == 2000
    assert servicer.sandbox_defs[0].resources.milli_cpu_max == 4000


@skip_non_subprocess
def test_sandbox_proxy(app, servicer):
    _ = Sandbox.create(proxy=Proxy.from_name("my-proxy"), app=app)

    assert servicer.sandbox_defs[0].proxy_id == "pr-123"


================================================
File: test/schedule_test.py
================================================
# Copyright Modal Labs 2022
from modal import App, Period
from modal_proto import api_pb2

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function(schedule=Period(seconds=5))
def f():
    pass


def test_schedule(servicer, client):
    with app.run(client=client):
        assert servicer.function2schedule == {"fu-1": api_pb2.Schedule(period=api_pb2.Schedule.Period(seconds=5.0))}


================================================
File: test/scheduler_placement_test.py
================================================
# Copyright Modal Labs 2024
from modal import App, Sandbox, SchedulerPlacement
from modal_proto import api_pb2

from .supports.skip import skip_windows

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function(
    _experimental_scheduler_placement=SchedulerPlacement(
        region="us-east-1",
        zone="us-east-1a",
        spot=False,
        instance_type="g4dn.xlarge",
    ),
)
def f1():
    pass


@app.function(
    region="us-east-1",
)
def f2():
    pass


@app.function(
    region=["us-east-1", "us-west-2"],
)
def f3():
    pass


def test_fn_scheduler_placement(servicer, client):
    with app.run(client=client):
        assert len(servicer.app_functions) == 3
        fn1 = servicer.app_functions["fu-1"]  # f1
        assert fn1.scheduler_placement == api_pb2.SchedulerPlacement(
            regions=["us-east-1"],
            _zone="us-east-1a",
            _lifecycle="on-demand",
            _instance_types=["g4dn.xlarge"],
        )

        fn2 = servicer.app_functions["fu-2"]  # f2
        assert fn2.scheduler_placement == api_pb2.SchedulerPlacement(
            regions=["us-east-1"],
        )

        fn3 = servicer.app_functions["fu-3"]  # f3
        assert fn3.scheduler_placement == api_pb2.SchedulerPlacement(
            regions=["us-east-1", "us-west-2"],
        )


@skip_windows("needs subprocess")
def test_sandbox_scheduler_placement(client, servicer):
    with app.run(client):
        Sandbox.create(
            "bash",
            "-c",
            "echo bye >&2 && sleep 1 && echo hi && exit 42",
            timeout=600,
            region="us-east-1",
            app=app,
        )

        assert len(servicer.sandbox_defs) == 1
        sb_def = servicer.sandbox_defs[0]
        assert sb_def.scheduler_placement == api_pb2.SchedulerPlacement(
            regions=["us-east-1"],
        )


================================================
File: test/secret_test.py
================================================
# Copyright Modal Labs 2022
import os
import pytest
import tempfile
from unittest import mock

from modal import App, Secret
from modal.exception import InvalidError

from .supports.skip import skip_old_py


def dummy(): ...


def test_secret_from_dict(servicer, client):
    app = App()
    secret = Secret.from_dict({"FOO": "hello, world"})
    app.function(secrets=[secret])(dummy)
    with app.run(client=client):
        assert secret.object_id == "st-0"
        assert servicer.secrets["st-0"] == {"FOO": "hello, world"}


@skip_old_py("python-dotenv requires python3.8 or higher", (3, 8))
def test_secret_from_dotenv(servicer, client):
    with tempfile.TemporaryDirectory() as tmpdirname:
        with open(os.path.join(tmpdirname, ".env"), "w") as f:
            f.write("# My settings\nUSER=user\nPASSWORD=abc123\n")

        with open(os.path.join(tmpdirname, ".env-dev"), "w") as f:
            f.write("# My settings\nUSER=user2\nPASSWORD=abc456\n")

        app = App()
        secret = Secret.from_dotenv(tmpdirname)
        app.function(secrets=[secret])(dummy)
        with app.run(client=client):
            assert secret.object_id == "st-0"
            assert servicer.secrets["st-0"] == {"USER": "user", "PASSWORD": "abc123"}

        app = App()
        secret = Secret.from_dotenv(tmpdirname, filename=".env-dev")
        app.function(secrets=[secret])(dummy)
        with app.run(client=client):
            assert secret.object_id == "st-1"
            assert servicer.secrets["st-1"] == {"USER": "user2", "PASSWORD": "abc456"}


@mock.patch.dict(os.environ, {"FOO": "easy", "BAR": "1234"})
def test_secret_from_local_environ(servicer, client):
    app = App()
    secret = Secret.from_local_environ(["FOO", "BAR"])
    app.function(secrets=[secret])(dummy)
    with app.run(client=client):
        assert secret.object_id == "st-0"
        assert servicer.secrets["st-0"] == {"FOO": "easy", "BAR": "1234"}

    with pytest.raises(InvalidError, match="NOTFOUND"):
        Secret.from_local_environ(["FOO", "NOTFOUND"])


def test_init_types():
    with pytest.raises(InvalidError):
        Secret.from_dict({"foo": 1.0})  # type: ignore


def test_secret_from_dict_none(servicer, client):
    app = App()
    secret = Secret.from_dict({"FOO": os.getenv("xyz"), "BAR": os.environ.get("abc"), "BAZ": "baz"})
    app.function(secrets=[secret])(dummy)
    with app.run(client=client):
        assert servicer.secrets["st-0"] == {"BAZ": "baz"}


def test_secret_from_name(servicer, client):
    # Deploy secret
    secret_id = Secret.create_deployed("my-secret", {"FOO": "123"}, client=client)

    # Look up secret
    secret = Secret.from_name("my-secret").hydrate(client)
    assert secret.object_id == secret_id

    # Look up secret through app
    app = App()
    secret = Secret.from_name("my-secret")
    app.function(secrets=[secret])(dummy)
    with app.run(client=client):
        assert secret.object_id == secret_id


================================================
File: test/serialization_test.py
================================================
# Copyright Modal Labs 2022
import pytest
import random

from modal import Queue
from modal._serialization import (
    deserialize,
    deserialize_data_format,
    deserialize_proto_params,
    serialize,
    serialize_data_format,
    serialize_proto_params,
)
from modal._utils.rand_pb_testing import rand_pb
from modal.exception import DeserializationError
from modal_proto import api_pb2

from .supports.skip import skip_old_py


@pytest.mark.asyncio
async def test_roundtrip(servicer, client):
    async with Queue.ephemeral(client=client) as q:
        data = serialize(q)
        # TODO: strip synchronizer reference from synchronicity entities!
        assert len(data) < 350  # Used to be 93...
        # Note: if this blows up significantly, it's most likely because
        # cloudpickle can't find a class in the global scope. When this
        # happens, it tries to serialize the entire class along with the
        # object. The reason it doesn't find the class in the global scope
        # is most likely because the name doesn't match. To fix this, make
        # sure that cls.__name__ (which is something synchronicity sets)
        # is the same as the symbol defined in the global scope.
        q_roundtrip = deserialize(data, client)
        assert isinstance(q_roundtrip, Queue)
        assert q.object_id == q_roundtrip.object_id


@skip_old_py("random.randbytes() was introduced in python 3.9", (3, 9))
@pytest.mark.asyncio
async def test_asgi_roundtrip():
    rand = random.Random(42)
    for _ in range(10000):
        msg = rand_pb(api_pb2.Asgi, rand)
        buf = msg.SerializeToString()
        asgi_obj = deserialize_data_format(buf, api_pb2.DATA_FORMAT_ASGI, None)
        assert asgi_obj is None or (isinstance(asgi_obj, dict) and asgi_obj["type"])
        buf = serialize_data_format(asgi_obj, api_pb2.DATA_FORMAT_ASGI)
        asgi_obj_roundtrip = deserialize_data_format(buf, api_pb2.DATA_FORMAT_ASGI, None)
        assert asgi_obj == asgi_obj_roundtrip


def test_deserialization_error(client):
    # Curated object that we should not be able to deserialize
    obj = (
        b"\x80\x04\x95(\x00\x00\x00\x00\x00\x00\x00\x8c\x17"
        b"undeserializable_module\x94\x8c\x05Dummy\x94\x93\x94)\x81\x94."
    )
    with pytest.raises(DeserializationError, match="'undeserializable_module' .+ local environment"):
        deserialize(obj, client)


@pytest.mark.parametrize(
    ["pydict", "params", "expected_bytes"],
    [
        (
            {"foo": "bar", "i": 5},
            [
                api_pb2.ClassParameterSpec(name="foo", type=api_pb2.PARAM_TYPE_STRING),
                api_pb2.ClassParameterSpec(name="i", type=api_pb2.PARAM_TYPE_INT),
            ],
            # only update this byte sequence if you are aware of the consequences of changing
            # serialization byte output - it could invalidate existing container pools for users
            # on redeployment, and possibly cause startup crashes if new containers can't
            # deserialize old proto parameters.
            b"\n\x0c\n\x03foo\x10\x01\x1a\x03bar\n\x07\n\x01i\x10\x02 \x05",
        )
    ],
)
def test_proto_serde_params_success(pydict, params, expected_bytes):
    serialized_params = serialize_proto_params(pydict, params)
    # it's important that the serialization doesn't change, since the serialized params bytes
    # are used as a key for the container pooling of parameterized services (classes)
    assert serialized_params == expected_bytes
    reconstructed = deserialize_proto_params(serialized_params, params)
    assert reconstructed == pydict


def test_proto_serde_failure_incomplete_params():
    # construct an incorrect serialization:
    incomplete_proto_params = api_pb2.ClassParameterSet(
        parameters=[api_pb2.ClassParameterValue(name="a", type=api_pb2.PARAM_TYPE_STRING, string_value="b")]
    )
    encoded_params = incomplete_proto_params.SerializeToString(deterministic=True)
    with pytest.raises(AttributeError, match="Constructor arguments don't match"):
        deserialize_proto_params(encoded_params, [api_pb2.ClassParameterSpec(name="x", type=api_pb2.PARAM_TYPE_STRING)])

    # TODO: add test for incorrect types


================================================
File: test/shutdown_test.py
================================================
# Copyright Modal Labs 2024
import asyncio
import pytest
import threading
import time

import grpclib

import modal
from modal._utils.async_utils import synchronize_api
from modal.client import Client
from modal.exception import ClientClosed
from modal_proto import api_pb2


def close_client_soon(client):
    def cb():
        time.sleep(0.1)
        client._close()

    threading.Thread(target=cb).start()


@pytest.mark.timeout(5)
def test_client_shutdown_raises_client_closed(servicer, credentials):
    # Queue.get() loops rpc calls until it gets a response - make sure it shuts down
    # if the client is closed and doesn't stay in an indefinite retry loop
    with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        with modal.Queue.ephemeral(client=client) as q:
            close_client_soon(client)  # simulate an early shutdown of the client
            with pytest.raises(modal.exception.ClientClosed):
                # ensure that ongoing rcp calls are aborted
                q.get()

            with pytest.raises(modal.exception.ClientClosed):
                # ensure the client isn't doesn't allow for *new* connections
                # after shutdown either
                q.get()


@pytest.mark.timeout(5)
@pytest.mark.asyncio
async def test_client_shutdown_raises_client_closed_streaming(servicer, credentials, caplog):
    # Queue.get() loops rpc calls until it gets a response - make sure it shuts down
    # if the client is closed and doesn't stay in an indefinite retry loop

    async def _mocked_logs_loop(client: Client, app_id: str):
        request = api_pb2.AppGetLogsRequest(
            app_id=app_id,
            task_id="",
            timeout=55,
            last_entry_id="",
        )
        async for _ in client.stub.AppGetLogs.unary_stream(request):
            pass

    sync_log_loop = synchronize_api(_mocked_logs_loop)

    with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        t = asyncio.create_task(sync_log_loop.aio(client, "ap-1"))
        await asyncio.sleep(0.1)  # in loop

    with pytest.raises(ClientClosed):
        await t

    with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        t = asyncio.create_task(_mocked_logs_loop(client, "ap-1"))
        await asyncio.sleep(0.1)  # in loop

    with pytest.raises(grpclib.exceptions.StreamTerminatedError):
        await t
    assert len(caplog.records) == 3  # open, send and recv called outside of task context
    for rec in caplog.records:
        assert "made outside of task context" in rec.message


@pytest.mark.timeout(5)
@pytest.mark.asyncio
async def test_client_close_cancellation_context_only_used_in_correct_event_loop(servicer, credentials, caplog):
    with Client(servicer.client_addr, api_pb2.CLIENT_TYPE_CLIENT, credentials) as client:
        with modal.Queue.ephemeral(client=client) as q:
            request = api_pb2.QueueGetRequest(
                queue_id=q.object_id,
                partition_key=b"",
                timeout=10,
                n_values=1,
            )
            # this request should not use task context since it's not issued from the same loop
            # that the task context is triggered from, otherwise we'll get cross-event loop
            # waits/cancellations etc.
            t = asyncio.create_task(client.stub.QueueGet(request))
            await asyncio.sleep(0.1)
    with pytest.raises(grpclib.exceptions.StreamTerminatedError):
        await t
    expected_warnings = [msg for msg in caplog.messages if "QueueGet made outside of task context" in msg]
    assert len(expected_warnings) == 1


================================================
File: test/slow_dependencies_test.py
================================================
# Copyright Modal Labs 2024
import subprocess
import sys


def test_slow_dependencies_local(supports_dir):
    # Make sure that "import modal" doesn't load some big dependencies like aiohttp
    subprocess.check_output([sys.executable, supports_dir / "slow_dependencies_local.py"])


def test_slow_dependencies_container(supports_dir):
    # Make sure that "import modal._container_entrypoint" doesn't load some big dependencies like aiohttp
    subprocess.check_output([sys.executable, supports_dir / "slow_dependencies_container.py"])


================================================
File: test/static_types_test.py
================================================
# Copyright Modal Labs 2024
import pytest
import subprocess

from test.supports.skip import skip_old_py, skip_windows


@pytest.fixture(scope="module")
def generate_type_stubs():
    subprocess.check_call(["inv", "type-stubs"])


@skip_windows("Type tests fail on windows since they don't exclude non-windows features")
@skip_old_py("can't generate type stubs w/ Concatenate on <3.10", (3, 10))
@pytest.mark.usefixtures("generate_type_stubs")
def test_remote_call_keeps_original_return_value():
    subprocess.check_call(["mypy", "test/supports/type_assertions.py"])


@skip_windows("Type tests fail on windows since they don't exclude non-windows features")
@skip_old_py("can't generate type stubs w/ Concatenate on <3.10", (3, 10))
@pytest.mark.usefixtures("generate_type_stubs")
def test_negative_assertions():
    p = subprocess.Popen(
        ["mypy", "test/supports/type_assertions_negative.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        encoding="utf8",
    )
    stdout, _ = p.communicate()
    assert p.returncode == 1
    print(stdout)
    assert "Found 6 errors in 1 file" in stdout
    assert 'Unexpected keyword argument "b" for "__call__"' in stdout
    assert 'Argument "a" to "__call__" of "__remote_spec" has incompatible type "int"' in stdout
    assert 'Unexpected keyword argument "c" for "local" of "Function"' in stdout
    assert 'Argument "a" to "local" of "Function" has incompatible type "int"' in stdout
    assert 'Unexpected keyword argument "e" for "aio" of "__remote_spec"' in stdout
    assert 'Argument "a" to "aio" of "__remote_spec" has incompatible type "float"' in stdout


================================================
File: test/telemetry_test.py
================================================
# Copyright Modal Labs 2024

import json
import logging
import os
import pytest
import queue
import socket
import sys
import tempfile
import threading
import time
import typing
import uuid
from pathlib import Path
from struct import unpack

from modal._runtime.telemetry import (
    MESSAGE_HEADER_FORMAT,
    MESSAGE_HEADER_LEN,
    ImportInterceptor,
    instrument_imports,
    supported_platform,
)


class TelemetryConsumer:
    socket_filename: Path
    server: socket.socket
    connections: set[socket.socket]
    events: queue.Queue
    tmp: tempfile.TemporaryDirectory

    def __init__(self):
        self.stopped = False
        self.events = queue.Queue()

    def __enter__(self):
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()

    def start(self):
        self.tmp = tempfile.TemporaryDirectory()
        self.socket_filename = Path(self.tmp.name) / "telemetry.sock"
        self.connections = set()
        self.server = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.server.bind(self.socket_filename.as_posix())
        self.server.listen()
        listener = threading.Thread(target=self._listen, daemon=True)
        listener.start()

    def stop(self):
        self.stopped = True
        self.server.close()
        for conn in list(self.connections):
            conn.close()

    def _listen(self):
        while not self.stopped:
            try:
                conn, _ = self.server.accept()
                receiver = threading.Thread(target=self._recv, args=(conn,), daemon=True)
                receiver.start()
                self.connections.add(conn)
            except OSError as e:
                logging.debug(f"listener got exception, exiting: {e}")
                return

    def _recv(self, conn):
        try:
            buffer = bytearray()
            while not self.stopped:
                try:
                    data = conn.recv(1024)
                except OSError as e:
                    logging.debug(f"connection {conn} got exception, exiting: {e}")
                    return
                buffer.extend(data)
                while True:
                    if len(buffer) <= MESSAGE_HEADER_LEN:
                        break
                    message_len = unpack(MESSAGE_HEADER_FORMAT, buffer[0:MESSAGE_HEADER_LEN])[0]
                    if len(buffer) < message_len + MESSAGE_HEADER_LEN:
                        break
                    message_bytes = buffer[MESSAGE_HEADER_LEN : MESSAGE_HEADER_LEN + message_len]
                    buffer = buffer[MESSAGE_HEADER_LEN + message_len :]
                    message = message_bytes.decode("utf-8").strip()
                    message = json.loads(message)
                    self.events.put(message)
        finally:
            self.connections.remove(conn)


def test_import_tracing(monkeypatch):
    if not supported_platform():
        pytest.skip(f"unsupported platform: {sys.platform}")

    with TelemetryConsumer() as consumer, ImportInterceptor.connect(consumer.socket_filename.absolute().as_posix()):
        from .telemetry import tracing_module_1  # noqa

        expected_messages: list[dict[str, typing.Any]] = [
            {"event": "module_load_start", "attributes": {"name": "test.telemetry.tracing_module_1"}},
            {"event": "module_load_start", "attributes": {"name": "test.telemetry.tracing_module_2"}},
            {"event": "module_load_end", "attributes": {"name": "test.telemetry.tracing_module_2"}},
            {"event": "module_load_end", "attributes": {"name": "test.telemetry.tracing_module_1"}},
        ]

        for expected_message in expected_messages:
            m = consumer.events.get(timeout=30)
            # skip this test module - behavior seems to vary depending on timing and maybe python version etc
            while m["attributes"]["name"] == "test.telemetry":
                m = consumer.events.get(timeout=30)
            assert m["event"] == expected_message["event"]
            assert m["attributes"]["name"] == expected_message["attributes"]["name"]
            assert m["timestamp"] >= 0
            assert uuid.UUID(m["span_id"])
            if m["event"] == "module_load_end":
                assert m["attributes"]["latency"] >= 0


# For manual testing
def generate_import_telemetry(telemetry_socket):
    instrument_imports(telemetry_socket)
    t0 = time.monotonic()
    import kubernetes  # noqa

    return time.monotonic() - t0


# For manual testing
def main():
    telemetry_socket = os.environ.get("MODAL_TELEMETRY_SOCKET")
    if telemetry_socket:
        latency = generate_import_telemetry(telemetry_socket)
    else:
        with TelemetryConsumer() as consumer:
            latency = generate_import_telemetry(consumer.socket_filename.absolute().as_posix())

            while True:
                try:
                    m = consumer.events.get_nowait()
                    print(m)
                except queue.Empty:
                    break

    print(f"import kubernetes took {latency:.02}s")


if __name__ == "__main__":
    main()


================================================
File: test/token_flow_test.py
================================================
# Copyright Modal Labs 2023
import pytest

import aiohttp

from modal.token_flow import TokenFlow


@pytest.mark.asyncio
async def test_token_flow_server(servicer, client):
    tf = TokenFlow(client)
    async with tf.start() as (token_flow_id, _, _):
        # Make a request against the local web server and make sure it validates
        localhost_url = f"http://localhost:{servicer.token_flow_localhost_port}"
        async with aiohttp.ClientSession() as session:
            async with session.get(localhost_url) as resp:
                text = await resp.text()
                assert text == token_flow_id


================================================
File: test/traceback_test.py
================================================
# Copyright Modal Labs 2024
import pytest
from pathlib import Path
from traceback import extract_tb

from modal._traceback import (
    append_modal_tb,
    extract_traceback,
    reduce_traceback_to_user_code,
    traceback_contains_remote_call,
)
from modal._vendor import tblib

from .supports.raise_error import raise_error

SUPPORT_MODULE = "supports.raise_error"


def call_raise_error():
    raise_error()


def test_extract_traceback():
    task_id = "ta-123"
    try:
        call_raise_error()
    except Exception as exc:
        tb_dict, line_cache = extract_traceback(exc, task_id)

    test_path = Path(__file__)
    support_path = test_path.parent / (SUPPORT_MODULE.replace(".", "/") + ".py")

    frame = tb_dict["tb_frame"]
    assert tb_dict["tb_lineno"] == frame["f_lineno"] - 2
    assert frame["f_code"]["co_filename"] == f"<{task_id}>:{test_path}"
    assert frame["f_code"]["co_name"] == "test_extract_traceback"
    assert frame["f_globals"]["__file__"] == str(test_path)
    assert frame["f_globals"]["__name__"] == f"test.{test_path.name[:-3]}"
    assert frame["f_locals"] == {}

    frame = tb_dict["tb_next"]["tb_frame"]
    assert frame["f_code"]["co_filename"] == f"<{task_id}>:{test_path}"
    assert frame["f_code"]["co_name"] == "call_raise_error"
    assert frame["f_globals"]["__file__"] == str(test_path)
    assert frame["f_globals"]["__name__"] == f"test.{test_path.name[:-3]}"
    assert frame["f_locals"] == {}

    frame = tb_dict["tb_next"]["tb_next"]["tb_frame"]
    assert frame["f_code"]["co_filename"] == f"<{task_id}>:{support_path}"
    assert frame["f_code"]["co_name"] == "raise_error"
    assert frame["f_globals"]["__file__"] == str(support_path)
    assert frame["f_globals"]["__name__"] == f"test.{SUPPORT_MODULE}"
    assert frame["f_locals"] == {}

    assert tb_dict["tb_next"]["tb_next"]["tb_next"] is None

    line_cache_list = list(line_cache.items())
    assert line_cache_list[0][0][0] == str(test_path)
    assert line_cache_list[0][1] == "call_raise_error()"
    assert line_cache_list[1][0][0] == str(test_path)
    assert line_cache_list[1][1] == "raise_error()"
    assert line_cache_list[2][0][0] == str(support_path)
    assert line_cache_list[2][1] == 'raise RuntimeError("Boo!")'


def test_append_modal_tb():
    task_id = "ta-123"
    try:
        call_raise_error()
    except Exception as exc:
        tb_dict, line_cache = extract_traceback(exc, task_id)

    try:
        raise RuntimeError("Remote error")
    except Exception as exc:
        remote_exc = exc
        append_modal_tb(exc, tb_dict, line_cache)

    assert remote_exc.__line_cache__ == line_cache  # type: ignore
    frames = [f.name for f in extract_tb(remote_exc.__traceback__)]
    assert frames == ["test_append_modal_tb", "call_raise_error", "raise_error"]


def make_tb_stack(frames: list[tuple[str, str]]) -> list[dict]:
    """Given a minimal specification of (code filename, code name), return dict formatted for tblib."""
    tb_frames = []
    for lineno, (filename, name) in enumerate(frames):
        tb_frames.append(
            {
                "tb_lineno": lineno,
                "tb_frame": {
                    "f_lineno": lineno,
                    "f_globals": {},
                    "f_locals": {},
                    "f_code": {"co_filename": filename, "co_name": name},
                },
            }
        )
    return tb_frames


def tb_dict_from_stack_dicts(stack: list[dict]) -> dict:
    tb_root = tb = stack.pop(0)
    while stack:
        tb["tb_next"] = stack.pop(0)
        tb = tb["tb_next"]
    tb["tb_next"] = None
    return tb_root


@pytest.mark.parametrize("user_mode", ["script", "module"])
def test_reduce_traceback_to_user_code(user_mode):
    if user_mode == "script":
        user_source, user_filename, user_name = ("/root/user/ai.py", "/root/user/ai.py", "train")
    elif user_mode == "module":
        user_source, user_filename, user_name = ("ai.training", "/root/user/ai/training.py", "<module>")

    stack = [
        ("/modal/__main__.py", "main"),
        ("/modal/entrypoint.py", "run"),
        ("/site-packages/synchronicity/wizard.py", "magic"),
        (user_filename, user_name),
        ("/modal/function.py", "execute"),
        ("/site-packages/synchronicity/devil.py", "pitchfork"),
    ]

    tb_dict = tb_dict_from_stack_dicts(make_tb_stack(stack))
    tb = tblib.Traceback.from_dict(tb_dict)
    tb_out = reduce_traceback_to_user_code(tb, user_source)

    f = tb_out.tb_frame
    assert f.f_code.co_filename == user_filename
    assert f.f_code.co_name == user_name

    f = tb_out.tb_next.tb_frame
    assert f.f_code.co_filename == "/modal/function.py"
    assert f.f_code.co_name == "execute"

    assert tb_out.tb_next.tb_next is None


def test_traceback_contains_remote_call():
    stack = [
        ("/home/foobar/code/script.py", "f"),
        ("/usr/local/venv/modal.py", "local"),
    ]

    tb = tblib.Traceback.from_dict(tb_dict_from_stack_dicts(make_tb_stack(stack)))
    assert not traceback_contains_remote_call(tb)

    task_id = "ta-0123456789ABCDEFGHILJKMNOP"
    stack.extend(
        [
            (f"<{task_id}>:/usr/local/lib/python3.11/importlib/__init__.py", ""),
            ("/root/script.py", ""),
        ]
    )

    tb = tblib.Traceback.from_dict(tb_dict_from_stack_dicts(make_tb_stack(stack)))
    assert traceback_contains_remote_call(tb)


================================================
File: test/tunnel_test.py
================================================
# Copyright Modal Labs 2023

import pytest

from modal import forward
from modal.exception import InvalidError

from .supports.skip import skip_windows_unix_socket


def test_tunnel_outside_container(client):
    with pytest.raises(InvalidError):
        with forward(8000, client=client):
            pass


@skip_windows_unix_socket
def test_invalid_port_numbers(container_client):
    for port in (-1, 0, 65536):
        with pytest.raises(InvalidError):
            with forward(port, client=container_client):
                pass


@skip_windows_unix_socket
def test_create_tunnel(container_client):
    with forward(8000, client=container_client) as tunnel:
        assert tunnel.host == "8000.modal.test"
        assert tunnel.url == "https://8000.modal.test"


================================================
File: test/user_code_import_test.py
================================================
# Copyright Modal Labs 2024
from unittest.mock import MagicMock

from modal._runtime import user_code_imports
from modal.image import _Image
from modal_proto import api_pb2


def test_import_function(supports_dir, monkeypatch):
    monkeypatch.syspath_prepend(supports_dir)
    fun = api_pb2.Function(module_name="user_code_import_samples.func", function_name="f")
    service = user_code_imports.import_single_function_service(
        fun,
        None,
        None,
        None,
        None,
    )
    assert len(service.service_deps) == 1
    assert type(service.service_deps[0]) is _Image
    assert service.app

    assert service.user_cls_instance is None

    # TODO (elias): shouldn't have to pass the function definition again!
    io_manager = MagicMock()  # shouldn't actually be used except by web endpoints - indicates some need for refactoring
    finalized_funcs = service.get_finalized_functions(fun, container_io_manager=io_manager)
    assert len(finalized_funcs) == 1
    finalized_func = finalized_funcs[""]
    assert finalized_func.is_async is False
    assert finalized_func.is_generator is False
    assert finalized_func.data_format == api_pb2.DATA_FORMAT_PICKLE
    assert finalized_func.lifespan_manager is None
    container_callable = finalized_func.callable
    assert container_callable("world") == "hello world"


def test_import_function_undecorated(supports_dir, monkeypatch):
    monkeypatch.syspath_prepend(supports_dir)
    fun = api_pb2.Function(module_name="user_code_import_samples.func", function_name="undecorated_f")
    service = user_code_imports.import_single_function_service(
        fun,
        None,
        None,
        None,
        None,
    )
    assert service.service_deps is None  # undecorated - can't get code deps
    # can't reliably get app - this is deferred to a name based lookup later in the container entrypoint
    assert service.app is None


def test_import_class(monkeypatch, supports_dir):
    monkeypatch.syspath_prepend(supports_dir)
    fun = api_pb2.Function(
        module_name="user_code_import_samples.cls",
        function_name="C.*",
    )
    service = user_code_imports.import_class_service(
        fun,
        None,
        (),
        {},
    )
    assert len(service.service_deps) == 1
    assert type(service.service_deps[0]) is _Image

    assert service.app

    from user_code_import_samples.cls import UndecoratedC  # type: ignore

    assert isinstance(service.user_cls_instance, UndecoratedC)

    # TODO (elias): shouldn't have to pass the function definition again!
    io_manager = MagicMock()  # shouldn't actually be used except by web endpoints - indicates some need for refactoring
    finalized_funcs = service.get_finalized_functions(fun, container_io_manager=io_manager)
    io_manager.assert_not_called()
    assert len(finalized_funcs) == 3

    for finalized in finalized_funcs.values():
        assert finalized.is_async is False
        assert finalized.is_generator is False
        assert finalized.data_format == api_pb2.DATA_FORMAT_PICKLE
        assert finalized.lifespan_manager is None

    finalized_1, finalized_2 = finalized_funcs["f"], finalized_funcs["f2"]
    assert finalized_1.callable("world") == "hello world"
    assert finalized_2.callable("world") == "other world"


# TODO: add test cases for serialized functions, web endpoints, explicit/implicit generators etc.
#   with and without decorators in globals scope...


================================================
File: test/utils_test.py
================================================
# Copyright Modal Labs 2022
import asyncio
import hashlib
import io
import pytest

from modal._utils.bytes_io_segment_payload import BytesIOSegmentPayload
from modal._utils.name_utils import (
    check_object_name,
    is_valid_environment_name,
    is_valid_object_name,
    is_valid_subdomain_label,
    is_valid_tag,
)
from modal._utils.package_utils import parse_major_minor_version
from modal.exception import InvalidError


def test_subdomain_label():
    assert is_valid_subdomain_label("banana")
    assert is_valid_subdomain_label("foo-123-456")
    assert not is_valid_subdomain_label("BaNaNa")
    assert not is_valid_subdomain_label(" ")
    assert not is_valid_subdomain_label("ban/ana")


def test_object_name():
    assert is_valid_object_name("baNaNa")
    assert is_valid_object_name("foo-123_456")
    assert is_valid_object_name("a" * 64)
    assert not is_valid_object_name("hello world")
    assert not is_valid_object_name("a" * 65)
    assert not is_valid_object_name("ap-abcdefghABCDEFGH012345")
    with pytest.raises(InvalidError, match="Invalid Volume name: 'foo/bar'"):
        check_object_name("foo/bar", "Volume")


def test_environment_name():
    assert is_valid_object_name("a" * 64)
    assert not is_valid_object_name("a" * 65)
    assert not is_valid_environment_name("--help")
    assert not is_valid_environment_name(":env")
    assert not is_valid_environment_name("env:env")
    assert not is_valid_environment_name("/env")
    assert not is_valid_environment_name("env/env")
    assert not is_valid_environment_name("")


def test_tag():
    assert is_valid_tag("v1.0.0")
    assert is_valid_tag("a38298githash39920bk")
    assert not is_valid_tag("v1 .0.0-alpha")
    assert not is_valid_tag("$$$build")


@pytest.mark.asyncio
async def test_file_segment_payloads():
    data = io.BytesIO(b"abc123")
    data2 = io.BytesIO(data.getbuffer())

    class DummyOutput:  # AbstractStreamWriter
        def __init__(self):
            self.value = b""

        async def write(self, chunk: bytes):
            self.value += chunk

    out1 = DummyOutput()
    out2 = DummyOutput()
    p1 = BytesIOSegmentPayload(data, 0, 3)
    p2 = BytesIOSegmentPayload(data2, 3, 3)

    # "out of order" writes
    await p2.write(out2)  # type: ignore
    await p1.write(out1)  # type: ignore
    assert out1.value == b"abc"
    assert out2.value == b"123"
    assert p1.md5_checksum().digest() == hashlib.md5(b"abc").digest()
    assert p2.md5_checksum().digest() == hashlib.md5(b"123").digest()

    data = io.BytesIO(b"abc123")

    # test reset_on_error
    all_data = BytesIOSegmentPayload(data, 0, 6)

    class DummyExc(Exception):
        pass

    try:
        with all_data.reset_on_error():
            await all_data.write(DummyOutput())  # type: ignore
    except DummyExc:
        pass

    out = DummyOutput()
    await all_data.write(out)  # type: ignore
    assert out.value == b"abc123"


@pytest.mark.asyncio
async def test_file_segment_payloads_concurrency():
    data = io.BytesIO((b"123" * 1024 * 350)[: 1024 * 1024])  # 1 MiB
    data2 = io.BytesIO(data.getbuffer())

    class DummyOutput:  # AbstractStreamWriter
        def __init__(self):
            self.value = b""

        async def write(self, chunk: bytes):
            self.value += chunk

    out1 = DummyOutput()
    out2 = DummyOutput()
    p1 = BytesIOSegmentPayload(data, 0, len(data.getvalue()) // 2, chunk_size=100 * 1024)  # 100 KiB chunks
    p2 = BytesIOSegmentPayload(data2, len(data.getvalue()) // 2, len(data.getvalue()) // 2, chunk_size=100 * 1024)
    await asyncio.gather(p2.write(out2), p1.write(out1))  # type: ignore
    assert out1.value + out2.value == data.getvalue()


def test_parse_major_minor_version():
    assert parse_major_minor_version("3.11") == (3, 11)
    assert parse_major_minor_version("3.9.1") == (3, 9)
    assert parse_major_minor_version("3.10.1rc0") == (3, 10)
    with pytest.raises(ValueError, match="at least an 'X.Y' format"):
        parse_major_minor_version("123")
    with pytest.raises(ValueError, match="at least an 'X.Y' format with integral"):
        parse_major_minor_version("x.y")


================================================
File: test/version_test.py
================================================
# Copyright Modal Labs 2022
import pkg_resources

import modal


def test_version():
    mod_version = modal.__version__
    pkg_version = pkg_resources.require("modal")[0].version

    assert pkg_resources.parse_version(mod_version) > pkg_resources.parse_version("0.0.0")
    assert pkg_resources.parse_version(pkg_version) > pkg_resources.parse_version("0.0.0")

    assert mod_version == pkg_version


================================================
File: test/volume_test.py
================================================
# Copyright Modal Labs 2023
import asyncio
import io
import os
import platform
import pytest
import re
import sys
import time
from pathlib import Path
from unittest import mock

import modal
from modal.exception import InvalidError, NotFoundError, VolumeUploadTimeoutError
from modal.volume import _open_files_error_annotation
from modal_proto import api_pb2


def dummy():
    pass


def test_volume_mount(client, servicer):
    app = modal.App()
    vol = modal.Volume.from_name("xyz", create_if_missing=True)

    _ = app.function(volumes={"/root/foo": vol})(dummy)

    with app.run(client=client):
        pass


def test_volume_bad_paths():
    app = modal.App()
    vol = modal.Volume.from_name("xyz")

    with pytest.raises(InvalidError):
        app.function(volumes={"/root/../../foo": vol})(dummy)

    with pytest.raises(InvalidError):
        app.function(volumes={"/": vol})(dummy)

    with pytest.raises(InvalidError):
        app.function(volumes={"/tmp/": vol})(dummy)


def test_volume_duplicate_mount():
    app = modal.App()
    vol = modal.Volume.from_name("xyz")

    with pytest.raises(InvalidError):
        app.function(volumes={"/foo": vol, "/bar": vol})(dummy)


@pytest.mark.parametrize("skip_reload", [False, True])
def test_volume_commit(client, servicer, skip_reload):
    with servicer.intercept() as ctx:
        ctx.add_response("VolumeCommit", api_pb2.VolumeCommitResponse(skip_reload=skip_reload))
        ctx.add_response("VolumeCommit", api_pb2.VolumeCommitResponse(skip_reload=skip_reload))

        with modal.Volume.ephemeral(client=client) as vol:
            # Note that in practice this will not work unless run in a task.
            vol.commit()

            # Make sure we can commit through the provider too
            vol.commit()

            assert ctx.pop_request("VolumeCommit").volume_id == vol.object_id
            assert ctx.pop_request("VolumeCommit").volume_id == vol.object_id

            # commit should implicitly reload on successful commit if skip_reload=False
            assert servicer.volume_reloads[vol.object_id] == 0 if skip_reload else 2


@pytest.mark.asyncio
async def test_volume_get(servicer, client, tmp_path):
    await modal.Volume.create_deployed.aio("my-vol", client=client)
    vol = await modal.Volume.from_name("my-vol").hydrate.aio(client=client)

    file_contents = b"hello world"
    file_path = "foo.txt"
    local_file_path = tmp_path / file_path
    local_file_path.write_bytes(file_contents)

    async with vol.batch_upload() as batch:
        batch.put_file(local_file_path, file_path)

    data = b""
    for chunk in vol.read_file(file_path):
        data += chunk
    assert data == file_contents

    output = io.BytesIO()
    vol.read_file_into_fileobj(file_path, output)
    assert output.getvalue() == file_contents

    with pytest.raises(FileNotFoundError):
        for _ in vol.read_file("/abc/def/i-dont-exist-at-all"):
            ...


def test_volume_reload(client, servicer):
    with modal.Volume.ephemeral(client=client) as vol:
        # Note that in practice this will not work unless run in a task.
        vol.reload()

        assert servicer.volume_reloads[vol.object_id] == 1


@pytest.mark.asyncio
async def test_volume_batch_upload(servicer, client, tmp_path):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    local_dir = tmp_path / "some_dir"
    local_dir.mkdir()
    (local_dir / "smol").write_text("###")

    subdir = local_dir / "subdir"
    subdir.mkdir()
    (subdir / "other").write_text("####")

    async with modal.Volume.ephemeral(client=client) as vol:
        with open(local_file_path, "rb") as fp:
            with vol.batch_upload() as batch:
                batch.put_file(local_file_path, "/some_file")
                batch.put_directory(local_dir, "/some_dir")
                batch.put_file(io.BytesIO(b"data from a file-like object"), "/filelike", mode=0o600)
                batch.put_directory(local_dir, "/non-recursive", recursive=False)
                batch.put_file(fp, "/filelike2")
        object_id = vol.object_id

    assert servicer.volume_files[object_id].keys() == {
        "/some_file",
        "/some_dir/smol",
        "/some_dir/subdir/other",
        "/filelike",
        "/non-recursive/smol",
        "/filelike2",
    }
    assert servicer.volume_files[object_id]["/some_file"].data == b"hello world"
    assert servicer.volume_files[object_id]["/some_dir/smol"].data == b"###"
    assert servicer.volume_files[object_id]["/some_dir/subdir/other"].data == b"####"
    assert servicer.volume_files[object_id]["/filelike"].data == b"data from a file-like object"
    assert servicer.volume_files[object_id]["/filelike"].mode == 0o600
    assert servicer.volume_files[object_id]["/non-recursive/smol"].data == b"###"
    assert servicer.volume_files[object_id]["/filelike2"].data == b"hello world"
    assert servicer.volume_files[object_id]["/filelike2"].mode == 0o644


@pytest.mark.asyncio
async def test_volume_batch_upload_force(servicer, client, tmp_path):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    local_file_path2 = tmp_path / "some_file2"
    local_file_path2.write_text("overwritten")

    async with modal.Volume.ephemeral(client=client) as vol:
        with servicer.intercept() as ctx:
            # Seed the volume
            with vol.batch_upload() as batch:
                batch.put_file(local_file_path, "/some_file")
            assert ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files

            # Attempting to overwrite the file with force=False should result in an error
            with pytest.raises(FileExistsError):
                with vol.batch_upload(force=False) as batch:
                    batch.put_file(local_file_path, "/some_file")
            assert ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files
            assert servicer.volume_files[vol.object_id]["/some_file"].data == b"hello world"

            # Overwriting should work with force=True
            with vol.batch_upload(force=True) as batch:
                batch.put_file(local_file_path2, "/some_file")
            assert not ctx.pop_request("VolumePutFiles").disallow_overwrite_existing_files
            assert servicer.volume_files[vol.object_id]["/some_file"].data == b"overwritten"


@pytest.mark.asyncio
async def test_volume_upload_removed_file(servicer, client, tmp_path):
    local_file_path = tmp_path / "some_file"
    local_file_path.write_text("hello world")

    async with modal.Volume.ephemeral(client=client) as vol:
        with pytest.raises(FileNotFoundError):
            with vol.batch_upload() as batch:
                batch.put_file(local_file_path, "/dest")
                local_file_path.unlink()


@pytest.mark.asyncio
async def test_volume_upload_large_file(client, tmp_path, servicer, blob_server, *args):
    with mock.patch("modal._utils.blob_utils.LARGE_FILE_LIMIT", 10):
        local_file_path = tmp_path / "bigfile"
        local_file_path.write_text("hello world, this is a lot of text")

        async with modal.Volume.ephemeral(client=client) as vol:
            async with vol.batch_upload() as batch:
                batch.put_file(local_file_path, "/a")
            object_id = vol.object_id

        assert servicer.volume_files[object_id].keys() == {"/a"}
        assert servicer.volume_files[object_id]["/a"].data == b""
        assert servicer.volume_files[object_id]["/a"].data_blob_id == "bl-1"

        _, blobs = blob_server
        assert blobs["bl-1"] == b"hello world, this is a lot of text"


@pytest.mark.asyncio
async def test_volume_upload_large_stream(client, servicer, blob_server, *args):
    with mock.patch("modal._utils.blob_utils.LARGE_FILE_LIMIT", 10):
        stream = io.BytesIO(b"hello world, this is a lot of text")

        async with modal.Volume.ephemeral(client=client) as vol:
            async with vol.batch_upload() as batch:
                batch.put_file(stream, "/a")
            object_id = vol.object_id

        assert servicer.volume_files[object_id].keys() == {"/a"}
        assert servicer.volume_files[object_id]["/a"].data == b""
        assert servicer.volume_files[object_id]["/a"].data_blob_id == "bl-1"

        _, blobs = blob_server
        assert blobs["bl-1"] == b"hello world, this is a lot of text"


@pytest.mark.asyncio
async def test_volume_upload_file_timeout(client, tmp_path, servicer, blob_server, *args):
    call_count = 0

    async def mount_put_file(self, stream):
        await stream.recv_message()
        nonlocal call_count
        call_count += 1
        await stream.send_message(api_pb2.MountPutFileResponse(exists=False))

    with servicer.intercept() as ctx:
        ctx.set_responder("MountPutFile", mount_put_file)
        with mock.patch("modal._utils.blob_utils.LARGE_FILE_LIMIT", 10):
            with mock.patch("modal.volume.VOLUME_PUT_FILE_CLIENT_TIMEOUT", 0.5):
                local_file_path = tmp_path / "bigfile"
                local_file_path.write_text("hello world, this is a lot of text")

                async with modal.Volume.ephemeral(client=client) as vol:
                    with pytest.raises(VolumeUploadTimeoutError):
                        async with vol.batch_upload() as batch:
                            batch.put_file(local_file_path, "/dest")

                assert call_count > 2


@pytest.mark.asyncio
async def test_volume_copy_1(client, tmp_path, servicer):
    ## test 1: copy src path to dst path ##
    src_path = "original.txt"
    dst_path = "copied.txt"
    local_file_path = tmp_path / src_path
    local_file_path.write_text("test copy")

    async with modal.Volume.ephemeral(client=client) as vol:
        # add local file to volume
        async with vol.batch_upload() as batch:
            batch.put_file(local_file_path, src_path)
        object_id = vol.object_id

        # copy file from src_path to dst_path
        vol.copy_files([src_path], dst_path)

    assert servicer.volume_files[object_id].keys() == {src_path, dst_path}

    assert servicer.volume_files[object_id][src_path].data == b"test copy"
    assert servicer.volume_files[object_id][dst_path].data == b"test copy"


@pytest.mark.asyncio
async def test_volume_copy_2(client, tmp_path, servicer):
    ## test 2: copy multiple files into a directory ##
    file_paths = ["file1.txt", "file2.txt"]

    async with modal.Volume.ephemeral(client=client) as vol:
        for file_path in file_paths:
            local_file_path = tmp_path / file_path
            local_file_path.write_text("test copy")
            async with vol.batch_upload() as batch:
                batch.put_file(local_file_path, file_path)
            object_id = vol.object_id

        vol.copy_files(file_paths, "test_dir")

    returned_volume_files = [Path(file) for file in servicer.volume_files[object_id].keys()]
    expected_volume_files = [
        Path(file) for file in ["file1.txt", "file2.txt", "test_dir/file1.txt", "test_dir/file2.txt"]
    ]

    assert returned_volume_files == expected_volume_files

    returned_file_data = {
        Path(entry): servicer.volume_files[object_id][entry] for entry in servicer.volume_files[object_id]
    }
    assert returned_file_data[Path("test_dir/file1.txt")].data == b"test copy"
    assert returned_file_data[Path("test_dir/file2.txt")].data == b"test copy"


@pytest.mark.parametrize("delete_as_instance_method", [True, False])
def test_persisted(servicer, client, delete_as_instance_method):
    # Lookup should fail since it doesn't exist
    with pytest.raises(NotFoundError):
        modal.Volume.from_name("xyz").hydrate(client)

    # Create it
    modal.Volume.from_name("xyz", create_if_missing=True).hydrate(client)

    # Lookup should succeed now
    modal.Volume.from_name("xyz").hydrate(client)

    # Delete it
    modal.Volume.delete("xyz", client=client)

    # Lookup should fail again
    with pytest.raises(NotFoundError):
        modal.Volume.from_name("xyz").hydrate(client)


def test_ephemeral(servicer, client):
    assert servicer.n_vol_heartbeats == 0
    with modal.Volume.ephemeral(client=client, _heartbeat_sleep=1) as vol:
        assert vol.listdir("/") == []
        # TODO(erikbern): perform some operations
        time.sleep(1.5)  # Make time for 2 heartbeats
    assert servicer.n_vol_heartbeats == 2


def test_lazy_hydration_from_named(set_env_client):
    vol = modal.Volume.from_name("my-vol", create_if_missing=True)
    assert vol.listdir("/") == []


@pytest.mark.skipif(platform.system() != "Linux", reason="needs /proc")
@pytest.mark.asyncio
async def test_open_files_error_annotation(tmp_path):
    assert _open_files_error_annotation(tmp_path) is None

    # Current process keeps file open
    with (tmp_path / "foo.txt").open("w") as _f:
        assert _open_files_error_annotation(tmp_path) == "path foo.txt is open"

    # cwd of current process is inside volume
    cwd = os.getcwd()
    os.chdir(tmp_path)
    assert _open_files_error_annotation(tmp_path) == "cwd is inside volume"
    os.chdir(cwd)

    # Subprocess keeps open file
    open_path = tmp_path / "bar.txt"
    open_path.write_text("")
    proc = await asyncio.create_subprocess_exec("tail", "-f", open_path.as_posix())
    await asyncio.sleep(0.01)  # Give process some time to start
    assert _open_files_error_annotation(tmp_path) == f"path bar.txt is open from 'tail -f {open_path.as_posix()}'"
    proc.kill()
    await proc.wait()
    assert _open_files_error_annotation(tmp_path) is None

    # Subprocess cwd inside volume
    proc = await asyncio.create_subprocess_exec(
        sys.executable, "-c", f"import time; import os; os.chdir('{tmp_path}'); time.sleep(60)"
    )
    # Wait for process to chdir
    for _ in range(100):
        if os.readlink(f"/proc/{proc.pid}/cwd") == tmp_path.as_posix():
            break
        await asyncio.sleep(0.05)
    assert re.match(f"^cwd of '{sys.executable} -c .*' is inside volume$", _open_files_error_annotation(tmp_path))
    proc.kill()
    await proc.wait()
    assert _open_files_error_annotation(tmp_path) is None


@pytest.mark.parametrize("name", ["has space", "has/slash", "a" * 65])
def test_invalid_name(name):
    with pytest.raises(InvalidError, match="Invalid Volume name"):
        modal.Volume.from_name(name)


@pytest.fixture()
def unset_main_thread_event_loop():
    try:
        event_loop = asyncio.get_event_loop()
    except RuntimeError:
        event_loop = None

    asyncio.set_event_loop(None)
    try:
        yield
    finally:
        asyncio.set_event_loop(event_loop)  # reset so we don't break other tests


@pytest.mark.usefixtures("unset_main_thread_event_loop")
def test_lock_is_py39_safe(set_env_client):
    vol = modal.Volume.from_name("my_vol", create_if_missing=True)
    vol.reload()


================================================
File: test/watcher_test.py
================================================
# Copyright Modal Labs 2023
import pytest
import random
import string
from pathlib import Path

from watchfiles import Change

import modal
from modal import method
from modal._watcher import _watch_args_from_mounts
from modal.exception import ExecutionError
from modal.mount import _get_client_mount, _Mount


@pytest.mark.asyncio
async def test__watch_args_from_mounts(monkeypatch, test_dir):
    paths, watch_filter = _watch_args_from_mounts(
        mounts=[
            _Mount._from_local_file("/x/foo.py", remote_path="/foo.py"),
            _Mount._from_local_dir("/one/two/bucklemyshoe", remote_path="/"),
            _Mount._from_local_dir("/x/z", remote_path="/z"),
        ]
    )

    assert paths == {Path("/x").absolute(), Path("/one/two/bucklemyshoe").absolute(), Path("/x/z").absolute()}
    assert watch_filter(Change.modified, "/x/foo.py")
    assert not watch_filter(Change.modified, "/x/notwatched.py")
    assert not watch_filter(Change.modified, "/x/y/foo.py")
    assert watch_filter(Change.modified, "/x/z/bar.py")
    random_filename = "".join(random.choices(string.ascii_uppercase + string.digits, k=10))
    assert watch_filter(Change.modified, f"/one/two/bucklemyshoe/{random_filename}")
    assert not watch_filter(Change.modified, "/one/two/bucklemyshoe/.DS_Store")


def dummy():
    pass


def test_watch_mounts_requires_running_app():
    # Arguably a bit strange to test this, as the exception should never
    # happen unless there is a bug in the client, since _get_watch_mounts
    # is not a public function, and should only ever be called from "safe"
    # contexts...

    # requires running app to make sure the mounts have been loaded
    app = modal.App()
    with pytest.raises(ExecutionError):
        # _get_watch_mounts needs to be called on a hydrated app
        app._get_watch_mounts()


def test_watch_mounts_includes_function_mounts(client, supports_dir, monkeypatch, disable_auto_mount):
    # TODO: remove this test once public Mount constructions are fully deprecated
    monkeypatch.syspath_prepend(supports_dir)
    app = modal.App()
    pkg_a_mount = modal.Mount._from_local_python_packages("pkg_a")

    @app.function(mounts=[pkg_a_mount], serialized=True)
    def f():
        pass

    with app.run(client=client):
        watch_mounts = app._get_watch_mounts()
    assert watch_mounts == [pkg_a_mount]


def test_watch_mounts_includes_cls_mounts(client, supports_dir, monkeypatch, disable_auto_mount):
    # TODO: remove this test once public Mount constructions are fully deprecated
    monkeypatch.syspath_prepend(supports_dir)
    app = modal.App()
    pkg_a_mount = modal.Mount._from_local_python_packages("pkg_a")

    @app.cls(mounts=[pkg_a_mount], serialized=True)
    class A:
        @method()
        def foo(self):
            pass

    with app.run(client=client):
        watch_mounts = app._get_watch_mounts()
    assert watch_mounts == [pkg_a_mount]


def test_watch_mounts_includes_image_mounts(client, supports_dir, monkeypatch, disable_auto_mount):
    # TODO: remove this test once public Mount constructions are fully deprecated
    monkeypatch.syspath_prepend(supports_dir)
    app = modal.App()
    pkg_a_mount = modal.Mount._from_local_python_packages("pkg_a")
    image = modal.Image.debian_slim().copy_mount(pkg_a_mount)

    @app.function(image=image, serialized=True)
    def f():
        pass

    with app.run(client=client):
        watch_mounts = app._get_watch_mounts()
    assert watch_mounts == [pkg_a_mount]


def test_watch_mounts_ignore_non_local(disable_auto_mount, client, servicer):
    app = modal.App()

    # uses the published client mount - should not be included in watch items
    # serialized=True avoids auto-mounting the entrypoint
    @app.function(mounts=[_get_client_mount()], serialized=True)
    def dummy():
        pass

    with app.run(client=client):
        mounts = app._get_watch_mounts()

    assert len(mounts) == 0


def test_add_local_mount_included_in_serve_watchers(servicer, client, supports_on_path, disable_auto_mount):
    deb_slim = modal.Image.debian_slim()
    img = deb_slim.add_local_python_source("pkg_a")
    app = modal.App()

    @app.function(serialized=True, image=img)
    def f():
        pass

    with app.run(client=client):
        watch_mounts = app._get_watch_mounts()
    assert len(watch_mounts) == 1


================================================
File: test/web_server_proxy_test.py
================================================
# Copyright Modal Labs 2024
import asyncio
import contextlib
import pytest
import socket
from dataclasses import dataclass
from typing import Any

import pytest_asyncio
from aiohttp.web import Application
from aiohttp.web_runner import AppRunner, SockSite

import modal._runtime.asgi

# TODO: add more tests


@dataclass
class DummyHttpServer:
    host: str
    port: int
    event: asyncio.Event
    assertion_log: list[str]


@contextlib.asynccontextmanager
async def run_temporary_http_server(app: Application):
    # Allocates a random port, runs a server in a context manager
    sock = socket.socket()
    host = "127.0.0.1"
    sock.bind((host, 0))
    port = sock.getsockname()[1]
    runner = AppRunner(app)
    await runner.setup()
    site = SockSite(runner, sock=sock)
    await site.start()
    try:
        yield host, port
    finally:
        await runner.cleanup()


@pytest_asyncio.fixture()
async def http_dummy_server():
    from aiohttp import web

    assertion_log = []
    event = asyncio.Event()

    async def hello(request):
        assertion_log.append("request")
        try:
            await request.read()
        except asyncio.CancelledError:
            assertion_log.append("cancelled")
            raise
        except OSError:
            # disconnect
            assertion_log.append("disconnect")
            event.set()
            return

        return web.Response(text="Hello, world")

    app = web.Application()
    app.add_routes([web.post("/", hello)])
    async with run_temporary_http_server(app) as (host, port):
        yield DummyHttpServer(host=host, port=port, event=event, assertion_log=assertion_log)


@contextlib.asynccontextmanager
async def lifespan_ctx_manager(asgi_app):
    state: dict[str, Any] = {}

    lm = modal._runtime.asgi.LifespanManager(asgi_app, state)
    t = asyncio.create_task(lm.background_task())
    await lm.lifespan_startup()
    yield state
    await lm.lifespan_shutdown()

    t.cancel()


@pytest.mark.asyncio
async def test_web_server_wrapper_immediate_disconnect(http_dummy_server: DummyHttpServer):
    proxy_asgi_app = modal._runtime.asgi.web_server_proxy(http_dummy_server.host, http_dummy_server.port)

    async def recv():
        return {"type": "http.disconnect"}

    async def send(msg):
        print("msg", msg)

    async with lifespan_ctx_manager(proxy_asgi_app) as state:
        scope = {"type": "http", "method": "POST", "path": "/", "headers": [], "state": state}
        await proxy_asgi_app(scope, recv, send)
        await http_dummy_server.event.wait()
        assert http_dummy_server.assertion_log == ["request", "disconnect"]


def test_add_forwarded_for_header():
    # case 1:
    # X-Forwarded-For already exist in headers and is the same as client IP
    # should do nothing
    original_scope = {"headers": [(b"X-Forwarded-For", b"1.2.3.4")], "client": ("1.2.3.4", 80)}
    expected_scope = {"headers": [(b"X-Forwarded-For", b"1.2.3.4")], "client": ("1.2.3.4", 80)}
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope

    # case 2:
    # X-Forwarded-For already exist in headers but is not the same as client IP
    # should append client IP to X-Forwarded-For
    original_scope = {"headers": [(b"X-Forwarded-For", b"1.2.3.4")], "client": ("4.5.6.7", 80)}
    expected_scope = {"headers": [(b"X-Forwarded-For", b"4.5.6.7, 1.2.3.4")], "client": ("4.5.6.7", 80)}
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope

    # case 3:
    # X-Forwarded-For does not exist in headers
    # should add X-Forwarded-For with client IP
    original_scope = {"headers": [], "client": ("4.5.6.7", 80)}
    expected_scope = {"headers": [(b"X-Forwarded-For", b"4.5.6.7")], "client": ("4.5.6.7", 80)}
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope

    # case 4:
    # X-Forwarded-For exists multiple times in headers
    # but client IP is not in the list
    # should add client IP to the first one
    original_scope = {
        "headers": [(b"X-Forwarded-For", b"1.2.3.4"), (b"X-Forwarded-For", b"5.6.7.8")],
        "client": ("4.5.6.7", 80),
    }
    expected_scope = {
        "headers": [(b"X-Forwarded-For", b"4.5.6.7, 1.2.3.4"), (b"X-Forwarded-For", b"5.6.7.8")],
        "client": ("4.5.6.7", 80),
    }
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope

    # case 5:
    # X-Forwarded-For exists multiple times in headers
    # but client IP is already in the list
    # should do nothing
    original_scope = {
        "headers": [(b"X-Forwarded-For", b"1.2.3.4"), (b"X-Forwarded-For", b"5.6.7.8, 4.5.6.7")],
        "client": ("4.5.6.7", 80),
    }
    expected_scope = {
        "headers": [(b"X-Forwarded-For", b"1.2.3.4"), (b"X-Forwarded-For", b"5.6.7.8, 4.5.6.7")],
        "client": ("4.5.6.7", 80),
    }
    res = modal._runtime.asgi._add_forwarded_for_header(original_scope)
    assert res == expected_scope, res


================================================
File: test/webhook_test.py
================================================
# Copyright Modal Labs 2022
import pathlib
import pytest
import subprocess
import sys

from fastapi.testclient import TestClient

from modal import App, asgi_app, web_endpoint, wsgi_app
from modal._runtime.asgi import webhook_asgi_app
from modal.exception import DeprecationError, InvalidError
from modal.functions import Function
from modal.running_app import RunningApp
from modal_proto import api_pb2

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function(cpu=42)
@web_endpoint(method="PATCH", docs=True)
async def f(x):
    return {"square": x**2}


@pytest.mark.asyncio
async def test_webhook(servicer, client, reset_container_app):
    async with app.run(client=client):
        assert f.web_url

        assert servicer.app_functions["fu-1"].webhook_config.type == api_pb2.WEBHOOK_TYPE_FUNCTION
        assert servicer.app_functions["fu-1"].webhook_config.method == "PATCH"

        # Make sure we can call the webhooks
        # TODO: reinstate `.remote` check when direct webhook fn invocation is fixed.
        # assert await f.remote(10)
        assert await f.local(100) == {"square": 10000}

        # Make sure the container gets the app id as well
        container_app = RunningApp(app.app_id)
        app._init_container(client, container_app)
        assert isinstance(f, Function)
        assert f.web_url


def test_webhook_cors():
    def handler():
        return {"message": "Hello, World!"}

    app = webhook_asgi_app(handler, method="GET", docs=False)
    client = TestClient(app)
    resp = client.options(
        "/",
        headers={
            "Origin": "http://example.com",
            "Access-Control-Request-Method": "POST",
        },
    )
    assert resp.headers["Access-Control-Allow-Origin"] == "http://example.com"

    assert client.get("/").json() == {"message": "Hello, World!"}
    assert client.post("/").status_code == 405  # Method Not Allowed


@pytest.mark.asyncio
async def test_webhook_no_docs():
    # FastAPI automatically sets docs URLs for apps, which we disable by default because it
    # can be unexpected for users who are unfamilar with FastAPI.
    #
    # https://fastapi.tiangolo.com/tutorial/metadata/#docs-urls

    def handler():
        return {"message": "Hello, World!"}

    app = webhook_asgi_app(handler, method="GET", docs=False)
    client = TestClient(app)
    assert client.get("/docs").status_code == 404
    assert client.get("/redoc").status_code == 404
    assert client.get("/openapi.json").status_code == 404


@pytest.mark.asyncio
async def test_webhook_docs():
    # By turning on docs, we should get three new routes: /docs, /redoc, and /openapi.json
    def handler():
        return {"message": "Hello, docs!"}

    app = webhook_asgi_app(handler, method="GET", docs=True)
    client = TestClient(app)
    assert client.get("/docs").status_code == 200
    assert client.get("/redoc").status_code == 200
    assert client.get("/openapi.json").status_code == 200


def test_webhook_generator():
    app = App()

    with pytest.raises(InvalidError) as excinfo:

        @app.function(serialized=True)
        @web_endpoint()
        def web_gen():
            yield None

    assert "streaming" in str(excinfo.value).lower()


@pytest.mark.asyncio
async def test_webhook_forgot_function(servicer, client):
    lib_dir = pathlib.Path(__file__).parent.parent
    args = [sys.executable, "-m", "test.supports.webhook_forgot_function"]
    ret = subprocess.run(args, cwd=lib_dir, stderr=subprocess.PIPE)
    stderr = ret.stderr.decode()
    assert "absent_minded_function" in stderr
    assert "@app.function" in stderr


@pytest.mark.asyncio
async def test_webhook_decorator_in_wrong_order(servicer, client):
    app = App()

    with pytest.raises(InvalidError) as excinfo:

        @web_endpoint()  # type: ignore
        @app.function(serialized=True)
        async def g(x):
            pass

    assert "wrong order" in str(excinfo.value).lower()


@pytest.mark.asyncio
async def test_asgi_wsgi(servicer, client):
    app = App()

    @app.function(serialized=True)
    @asgi_app()
    def my_asgi():
        pass

    @app.function(serialized=True)
    @wsgi_app()
    def my_wsgi():
        pass

    with pytest.raises(InvalidError, match="can't have parameters"):

        @app.function(serialized=True)
        @asgi_app()
        def my_invalid_asgi(x):
            pass

    with pytest.raises(InvalidError, match="can't have parameters"):

        @app.function(serialized=True)
        @wsgi_app()
        def my_invalid_wsgi(x):
            pass

    with pytest.warns(DeprecationError, match="default parameters"):

        @app.function(serialized=True)
        @asgi_app()
        def my_deprecated_default_params_asgi(x=1):
            pass

    with pytest.warns(DeprecationError, match="default parameters"):

        @app.function(serialized=True)
        @wsgi_app()
        def my_deprecated_default_params_wsgi(x=1):
            pass

    with pytest.raises(InvalidError, match="async function"):

        @app.function(serialized=True)
        @asgi_app()
        async def my_async_asgi_function():
            pass

    with pytest.raises(InvalidError, match="async function"):

        @app.function(serialized=True)
        @wsgi_app()
        async def my_async_wsgi_function():
            pass

    async with app.run(client=client):
        pass

    assert len(servicer.app_functions) == 4
    assert servicer.app_functions["fu-1"].webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP
    assert servicer.app_functions["fu-2"].webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP
    assert servicer.app_functions["fu-3"].webhook_config.type == api_pb2.WEBHOOK_TYPE_ASGI_APP
    assert servicer.app_functions["fu-4"].webhook_config.type == api_pb2.WEBHOOK_TYPE_WSGI_APP


def test_positional_method(servicer, client):
    with pytest.raises(InvalidError, match="method="):
        web_endpoint("GET")
    with pytest.raises(InvalidError, match="label="):
        asgi_app("baz")
    with pytest.raises(InvalidError, match="label="):
        wsgi_app("baz")


================================================
File: test/mdmd_data/foo-expected.md
================================================
# foo

This module does cool stuff

## foo.Foo

```python
class Foo(object)
```

A class that foos

### bar

```python
def bar(self):
```

## foo.funky

```python
def funky():
```

funks the baz

**Usage**

```python
import foo
foo.funky()  # outputs something
```

Enjoy!


================================================
File: test/mdmd_data/foo.py
================================================
# Copyright Modal Labs 2023
"""This module does cool stuff"""

# global untyped objects are currently not documented
some_dict = {}  # type: ignore


class Foo:
    """A class that foos"""

    def bar(self):
        pass


def funky():
    """funks the baz

    **Usage**

    ```python
    import foo
    foo.funky()  # outputs something
    ```

    Enjoy!
    """
    pass


def hidden():
    """mdmd:hidden

    This is marked as hidden in docs and shouldn't be shown"""


================================================
File: test/supports/assert_package.py
================================================
# Copyright Modal Labs 2022
# See test in client_test/package_utils_test.py

name = "xyz"


================================================
File: test/supports/base_class.py
================================================
# Copyright Modal Labs 2022
from modal import enter, method


class BaseCls2:
    @enter()
    def enter(self):
        self.x = 2

    @method()
    def run(self, y):
        return self.x * y


================================================
File: test/supports/class_hierarchy.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App("class-hierarchy", include_source=True)  # TODO: remove include_source=True)


class Base:
    @modal.method()
    def defined_on_base(self):
        print("base")

    @modal.method()
    def overridden_on_wrapped(self):
        raise NotImplementedError()


@app.cls()
class Wrapped(Base):
    @modal.method()
    def overridden_on_wrapped(self):
        print("wrapped")


================================================
File: test/supports/class_with_image.py
================================================
# Copyright Modal Labs 2024
import modal

image = modal.Image.debian_slim()
app = modal.App(image=image)


@app.cls()
class ClassWithImage:
    @modal.method()
    def image_is_hydrated(self):
        return image.is_hydrated


================================================
File: test/supports/common.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
def f(x):
    # not actually used in test (servicer returns sum of square of all args)
    pass


================================================
File: test/supports/consumed_map.py
================================================
# Copyright Modal Labs 2022
from .common import app, f

if __name__ == "__main__":
    with app.run():
        for x in f.map([1, 2, 3]):  # type: ignore
            pass


================================================
File: test/supports/experimental.py
================================================
# Copyright Modal Labs 2022
from __future__ import annotations

import modal.experimental
from modal import (
    App,
    enter,
    method,
)

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.cls()
class StopFetching:
    @enter()
    def init(self):
        self.counter = 0

    @method()
    def after_two(self, x):
        self.counter += 1

        if self.counter >= 2:
            modal.experimental.stop_fetching_inputs()

        return x * x


================================================
File: test/supports/forking.py
================================================
# Copyright Modal Labs 2024
import os

from modal._utils.async_utils import synchronize_api
from modal.client import Client
from modal_proto import api_pb2


@synchronize_api
async def list_volumes(method):
    await method(api_pb2.VolumeListRequest(environment_name="main"))
    print(os.getpid())


def sub(api_stub):
    list_volumes(api_stub)


if __name__ == "__main__":
    client = Client.from_env()
    rpc_method = client.stub.VolumeList
    sub(rpc_method)
    if not (fork_pid := os.fork()):
        list_volumes(rpc_method)
    else:
        os.waitpid(fork_pid, 0)


================================================
File: test/supports/function_without_app.py
================================================
# Copyright Modal Labs 2024
from modal.app import App


def f(x):
    assert App._get_container_app()
    return 123


================================================
File: test/supports/functions.py
================================================
# Copyright Modal Labs 2022
import asyncio
import contextlib
import pytest
import threading
import time

from modal import (
    App,
    Sandbox,
    asgi_app,
    batched,
    build,
    current_function_call_id,
    current_input_id,
    enter,
    exit,
    is_local,
    method,
    web_endpoint,
    web_server,
    wsgi_app,
)
from modal._utils.deprecation import deprecation_warning
from modal.exception import DeprecationError
from modal.experimental import get_local_input_concurrency, set_local_input_concurrency

SLEEP_DELAY = 0.1

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function()
def square(x: int):
    return x * x


@app.function()
def ident(x):
    return x


@app.function()
def delay(t):
    time.sleep(t)
    return t


@app.function()
async def delay_async(t):
    await asyncio.sleep(t)
    return t


@app.function()
async def async_cancel_doesnt_reraise(t):
    try:
        await asyncio.sleep(t)
    except asyncio.CancelledError:
        pass


@app.function()
async def square_async(x):
    await asyncio.sleep(SLEEP_DELAY)
    return x * x


@app.function()
def raises(x):
    raise Exception("Failure!")


@app.function()
def raises_sysexit(x):
    raise SystemExit(1)


@app.function()
def raises_keyboardinterrupt(x):
    raise KeyboardInterrupt()


@app.function()
def gen_n(n):
    for i in range(n):
        yield i**2


@app.function()
def gen_n_fail_on_m(n, m):
    for i in range(n):
        if i == m:
            raise Exception("bad")
        yield i**2


def deprecated_function(x):
    deprecation_warning((2000, 1, 1), "This function is deprecated")
    return x**2


@app.function()
@web_endpoint()
def webhook(arg="world"):
    return {"hello": arg}


def stream():
    for i in range(10):
        time.sleep(SLEEP_DELAY)
        yield f"{i}..."


@app.function()
@web_endpoint()
def webhook_streaming():
    from fastapi.responses import StreamingResponse

    return StreamingResponse(stream())


async def stream_async():
    for i in range(10):
        await asyncio.sleep(SLEEP_DELAY)
        yield f"{i}..."


@app.function()
@web_endpoint()
async def webhook_streaming_async():
    from fastapi.responses import StreamingResponse

    return StreamingResponse(stream_async())


if __name__ == "__main__":
    raise Exception("This line is not supposed to be reachable")


def gen(n):
    for i in range(n):
        yield i**2


@app.function(is_generator=True)
def fun_returning_gen(n):
    return gen(n)


@app.function()
@asgi_app()
def fastapi_app():
    from fastapi import FastAPI

    web_app = FastAPI()

    @web_app.get("/foo")
    async def foo(arg="world"):
        return {"hello": arg}

    return web_app


@app.function()
@web_server(8765, startup_timeout=1)
def non_blocking_web_server():
    import subprocess

    subprocess.Popen(["python", "-m", "http.server", "-b", "0.0.0.0", "8765"])


lifespan_global_asgi_app_func: list[str] = []


@app.function()
@asgi_app()
def fastapi_app_with_lifespan():
    from fastapi import FastAPI, Request

    assert len(lifespan_global_asgi_app_func) == 0

    @contextlib.asynccontextmanager
    async def lifespan(wapp: FastAPI):
        lifespan_global_asgi_app_func.append("enter")
        yield {"foo": "this was set from state"}
        lifespan_global_asgi_app_func.append("exit")

    web_app = FastAPI(lifespan=lifespan)

    @web_app.get("/")
    async def foo(request: Request):
        lifespan_global_asgi_app_func.append("foo")
        return request.state.foo

    return web_app


@app.function()
@asgi_app()
def fastapi_app_with_lifespan_failing_startup():
    from fastapi import FastAPI

    @contextlib.asynccontextmanager
    async def lifespan(wapp: FastAPI):
        print("enter")
        raise Exception("Error while setting up asgi app")
        yield
        print("exit")

    web_app = FastAPI(lifespan=lifespan)

    @web_app.get("/")
    async def foo():
        print("foo")
        return "bar"

    return web_app


@app.function()
@asgi_app()
def fastapi_app_with_lifespan_failing_shutdown():
    from fastapi import FastAPI

    @contextlib.asynccontextmanager
    async def lifespan(wapp: FastAPI):
        print("enter")
        yield
        raise Exception("Error while setting up asgi app")
        print("exit")

    web_app = FastAPI(lifespan=lifespan)

    @web_app.get("/")
    async def foo():
        print("foo")
        return "bar"

    return web_app


lifespan_global_asgi_app_cls: list[str] = []


@app.cls(scaledown_window=300, max_containers=1, allow_concurrent_inputs=100)
class fastapi_class_multiple_asgi_apps_lifespans:
    def __init__(self):
        assert len(lifespan_global_asgi_app_cls) == 0

    @asgi_app()
    def my_app1(self):
        from fastapi import FastAPI

        @contextlib.asynccontextmanager
        async def lifespan1(wapp):
            lifespan_global_asgi_app_cls.append("enter1")
            yield
            lifespan_global_asgi_app_cls.append("exit1")

        web_app1 = FastAPI(lifespan=lifespan1)

        @web_app1.get("/")
        async def foo1():
            lifespan_global_asgi_app_cls.append("foo1")
            return "foo1"

        return web_app1

    @asgi_app()
    def my_app2(self):
        from fastapi import FastAPI

        @contextlib.asynccontextmanager
        async def lifespan2(wapp):
            lifespan_global_asgi_app_cls.append("enter2")
            yield
            lifespan_global_asgi_app_cls.append("exit2")

        web_app2 = FastAPI(lifespan=lifespan2)

        @web_app2.get("/")
        async def foo2():
            lifespan_global_asgi_app_cls.append("foo2")
            return "foo2"

        return web_app2

    @exit()
    def exit(self):
        lifespan_global_asgi_app_cls.append("exit")


lifespan_global_asgi_app_cls_fail: list[str] = []


@app.cls(scaledown_window=300, max_containers=1, allow_concurrent_inputs=100)
class fastapi_class_lifespan_shutdown_failure:
    def __init__(self):
        assert len(lifespan_global_asgi_app_cls_fail) == 0

    @asgi_app()
    def my_app1(self):
        from fastapi import FastAPI

        @contextlib.asynccontextmanager
        async def lifespan1(wapp):
            lifespan_global_asgi_app_cls_fail.append("enter")
            yield
            raise

        web_app1 = FastAPI(lifespan=lifespan1)

        @web_app1.get("/")
        async def foo():
            lifespan_global_asgi_app_cls_fail.append("foo")
            return "foo"

        return web_app1

    @exit()
    def exit(self):
        lifespan_global_asgi_app_cls_fail.append("lifecycle exit")


@app.function()
@asgi_app()
def asgi_app_with_slow_lifespan_wind_down():
    async def _asgi_app(scope, receive, send):
        if scope["type"] == "lifespan":
            while True:
                message = await receive()
                if message["type"] == "lifespan.startup":
                    await send({"type": "lifespan.startup.complete"})
                elif message["type"] == "lifespan.shutdown":
                    await send({"type": "lifespan.shutdown.complete"})
                await asyncio.sleep(1)  # take some time to shut down - this should either be cancelled or awaited
        else:
            # dummy response to other requests
            await send({"type": "http.response.start", "status": 200})
            await send({"type": "http.response.body", "body": b'{"some_result":"foo"}'})

    return _asgi_app


@app.function()
@asgi_app()
def non_lifespan_asgi():
    async def app(scope, receive, send):
        if not scope["type"] == "http":
            return

        await send(
            {
                "type": "http.response.start",
                "status": 200,
                "headers": [
                    (b"content-type", b"application/json"),
                ],
            }
        )

        await send(
            {
                "type": "http.response.body",
                "body": b'"foo"',
            }
        )

    return app


@app.function()
@asgi_app()
def error_in_asgi_setup():
    raise Exception("Error while setting up asgi app")


@app.function()
@wsgi_app()
def basic_wsgi_app():
    def simple_app(environ, start_response):
        status = "200 OK"
        headers = [("Content-type", "text/plain; charset=utf-8")]
        body = environ["wsgi.input"].read()

        start_response(status, headers)
        yield b"got body: " + body

    return simple_app


@app.cls()
class LifecycleCls:
    """Ensures that {sync,async} lifecycle hooks work with {sync,async} functions."""

    def __init__(
        self,
        print_at_exit: bool = False,
        sync_enter_duration=0,
        async_enter_duration=0,
        sync_exit_duration=0,
        async_exit_duration=0,
    ):
        self.events: list[str] = []
        self.sync_enter_duration = sync_enter_duration
        self.async_enter_duration = async_enter_duration
        self.sync_exit_duration = sync_exit_duration
        self.async_exit_duration = async_exit_duration
        if print_at_exit:
            self._print_at_exit()

    def _print_at_exit(self):
        import atexit

        atexit.register(lambda: print("[events:" + ",".join(self.events) + "]"))

    @enter()
    def enter_sync(self):
        self.events.append("enter_sync")
        time.sleep(self.sync_enter_duration)

    @enter()
    async def enter_async(self):
        self.events.append("enter_async")
        await asyncio.sleep(self.async_enter_duration)

    @exit()
    def exit_sync(self):
        self.events.append("exit_sync")
        time.sleep(self.sync_exit_duration)

    @exit()
    async def exit_async(self):
        self.events.append("exit_async")
        await asyncio.sleep(self.async_exit_duration)

    @method()
    def local(self):
        self.events.append("local")

    @method()
    def f_sync(self):
        self.events.append("f_sync")
        self.local.local()
        return self.events

    @method()
    async def f_async(self):
        self.events.append("f_async")
        self.local.local()
        return self.events

    @method()
    def delay(self, duration: float):
        self._print_at_exit()
        self.events.append("delay")
        time.sleep(duration)
        return self.events

    @method()
    async def delay_async(self, duration: float):
        self._print_at_exit()
        self.events.append("delay_async")
        await asyncio.sleep(duration)
        return self.events


@app.function(allow_concurrent_inputs=5)
def sleep_700_sync(x):
    time.sleep(0.7)
    return x * x, current_input_id(), current_function_call_id()


@app.function(allow_concurrent_inputs=5)
async def sleep_700_async(x):
    await asyncio.sleep(0.7)
    return x * x, current_input_id(), current_function_call_id()


@app.function()
@batched(max_batch_size=4, wait_ms=500)
def batch_function_sync(x: tuple[int], y: tuple[int]):
    outputs = []
    for x_i, y_i in zip(x, y):
        outputs.append(x_i / y_i)
    return outputs


@app.function()
@batched(max_batch_size=4, wait_ms=500)
def batch_function_outputs_not_list(x: tuple[int], y: tuple[int]):
    return str(x)


@app.function()
@batched(max_batch_size=4, wait_ms=500)
def batch_function_outputs_wrong_len(x: tuple[int], y: tuple[int]):
    return list(x) + [0]


@app.function()
@batched(max_batch_size=4, wait_ms=500)
async def batch_function_async(x: tuple[int], y: tuple[int]):
    outputs = []
    for x_i, y_i in zip(x, y):
        outputs.append(x_i / y_i)
    await asyncio.sleep(0.1)
    return outputs


def unassociated_function(x):
    return 100 - x


class BaseCls:
    @enter()
    def enter(self):
        self.x = 2

    @method()
    def run(self, y):
        return self.x * y


@app.cls()
class DerivedCls(BaseCls):
    pass


@app.function()
def cube(x):
    # Note: this ends up calling the servicer fixture,
    # which always just returns the sum of the squares of the inputs,
    # regardless of the actual funtion.
    assert square.is_hydrated
    return square.remote(x) * x


with pytest.warns(DeprecationError, match="@modal.build"):

    @app.cls()
    class BuildCls:
        def __init__(self):
            self._k = 1

        @enter()
        def enter1(self):
            self._k += 10

        @build()
        def build1(self):
            self._k += 100
            return self._k

        @build()
        def build2(self):
            self._k += 1000
            return self._k

        @exit()
        def exit1(self):
            raise Exception("exit called!")

        @method()
        def f(self, x):
            return self._k * x


@app.cls(enable_memory_snapshot=True)
class SnapshottingCls:
    def __init__(self):
        self._vals = []

    @enter(snap=True)
    def enter1(self):
        self._vals.append("A")

    @enter(snap=True)
    def enter2(self):
        self._vals.append("B")

    @enter()
    def enter3(self):
        self._vals.append("C")

    @method()
    def f(self, x):
        return "".join(self._vals) + x


@app.function(enable_memory_snapshot=True)
def snapshotting_square(x):
    return x * x


@app.cls()
class EventLoopCls:
    @enter()
    async def enter(self):
        self.loop = asyncio.get_running_loop()

    @method()
    async def f(self):
        return self.loop.is_running()


@app.function()
def sandbox_f(x):
    # TODO(erikbern): maybe inside containers, `app=app` should be automatic?
    sb = Sandbox.create("echo", str(x), app=app)
    return sb.object_id


@app.function()
def is_local_f(x):
    return is_local()


@app.function()
def raise_large_unicode_exception():
    byte_str = (b"k" * 120_000_000) + b"\x99"
    byte_str.decode("utf-8")


@app.function()
def get_input_concurrency(timeout: int):
    time.sleep(timeout)
    return get_local_input_concurrency()


@app.function()
def set_input_concurrency(start: float):
    set_local_input_concurrency(3)
    time.sleep(1)
    return time.time() - start


@app.function()
def check_container_app():
    # The container app should be associated with the app object
    assert App._get_container_app() == app


@app.function()
def get_running_loop(x):
    return asyncio.get_running_loop()


@app.function()
def is_main_thread_sync(x):
    return threading.main_thread() == threading.current_thread()


@app.function()
async def is_main_thread_async(x):
    return threading.main_thread() == threading.current_thread()


_import_thread_is_main_thread = threading.main_thread() == threading.current_thread()


@app.function()
def import_thread_is_main_thread(x):
    return _import_thread_is_main_thread


class CustomException(Exception):
    pass


@app.function()
def raises_custom_exception(x):
    raise CustomException("Failure!")


================================================
File: test/supports/hello.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


@app.function()
def hello():
    print("hello")
    return "hello"


@app.function()
def other():
    return "other"


================================================
File: test/supports/image_run_function.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App("a", include_source=True)  # TODO: remove include_source=True)
other = modal.App("b", include_source=True)  # TODO: remove include_source=True)


def builder_function():
    print("ran builder function")


image = modal.Image.debian_slim().run_function(builder_function)


@app.function(image=image)
def foo():
    pass


================================================
File: test/supports/import_and_filter_source.py
================================================
# Copyright Modal Labs 2025
from modal import App, asgi_app, method, web_endpoint

app_with_one_web_function = App(
    include_source=True
)  # TODO: remove include_source=True when automount is disabled by default


@app_with_one_web_function.function()
@web_endpoint()
def web1():
    pass


app_with_one_function_one_web_endpoint = App(
    include_source=True
)  # TODO: remove include_source=True when automount is disabled by default


@app_with_one_function_one_web_endpoint.function()
def f1():
    pass


@app_with_one_function_one_web_endpoint.function()
@web_endpoint()
def web2():
    pass


app_with_one_web_method = App(
    include_source=True
)  # TODO: remove include_source=True when automount is disabled by default


@app_with_one_web_method.cls()
class C1:
    @asgi_app()
    def web_3(self):
        pass


app_with_one_web_method_one_method = App(
    include_source=True
)  # TODO: remove include_source=True when automount is disabled by default


@app_with_one_web_method_one_method.cls()
class C2:
    @asgi_app()
    def web_4(self):
        pass

    @method()
    def f2(self):
        pass


app_with_local_entrypoint_and_function = App(
    include_source=True
)  # TODO: remove include_source=True when automount is disabled by default


@app_with_local_entrypoint_and_function.local_entrypoint()
def le_1():
    pass


@app_with_local_entrypoint_and_function.function()
def f3():
    pass


================================================
File: test/supports/import_modal_from_thread.py
================================================
# Copyright Modal Labs 2024
import threading

success = threading.Event()


def main():
    import modal  # noqa

    success.set()


if __name__ == "__main__":
    t = threading.Thread(target=main, daemon=True)
    t.start()
    was_success = success.wait(timeout=5)
    assert was_success


================================================
File: test/supports/imports_ast.py
================================================
# Copyright Modal Labs 2024
import ast  # noqa

import modal

app = modal.App("imports_ast", include_source=True)  # TODO: remove include_source=True)


@app.function()
def some_func():
    pass


================================================
File: test/supports/imports_six.py
================================================
# Copyright Modal Labs 2024
import six  # noqa

import modal

app = modal.App("imports_six", include_source=True)  # TODO: remove include_source=True)


@app.function()
def some_func():
    pass


================================================
File: test/supports/lazy_hydration.py
================================================
# Copyright Modal Labs 2024
from modal import App, Image, Queue, Volume

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default

image = Image.debian_slim().pip_install("xyz")
volume = Volume.from_name("my-vol")
queue = Queue.from_name("my-queue")


@app.function(image=image, volumes={"/tmp/xyz": volume})
def f(x):
    # These are hydrated by virtue of being dependencies
    assert image.is_hydrated
    assert volume.is_hydrated

    # This one should be hydrated lazily
    queue.put("123")
    assert queue.get() == "123"


================================================
File: test/supports/missing_main_conditional.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
def square(x):
    return x**2


# This should fail in a container
with app.run():
    print(square.remote(42))


================================================
File: test/supports/module_1.py
================================================
# Copyright Modal Labs 2022
def square(x):
    return x**2


================================================
File: test/supports/module_2.py
================================================
# Copyright Modal Labs 2022
def square(x):
    return x**2


================================================
File: test/supports/mount_dedupe.py
================================================
# Copyright Modal Labs 2023
import os

import modal

app = modal.App()
import pkg_a  # noqa


if int(os.environ["USE_EXPLICIT"]):
    image_1 = modal.Image.debian_slim().add_local_python_source("pkg_a")  # this should be reused
    # same as above, but different instance - should be app-deduplicated:
    image_2 = (
        modal.Image.debian_slim()
        .add_local_python_source("pkg_a")  # identical to first explicit mount and auto mounts
        .add_local_python_source(
            # custom ignore condition, include normally_not_included.pyc (but skip __pycache__)
            "pkg_a",
            ignore=["**/__pycache__"],
        )
    )
else:
    # only use automounting
    image_1 = modal.Image.debian_slim()
    image_2 = modal.Image.debian_slim()


@app.function(image=image_1)
def foo():
    pass


@app.function(image=image_2)
def bar():
    pass


================================================
File: test/supports/multiapp.py
================================================
# Copyright Modal Labs 2023
import modal

a = modal.App()


@a.function()
def a_func(i):
    assert a_func.is_hydrated
    assert not b_func.is_hydrated
    assert modal.App._get_container_app() == a


b = modal.App()


@b.function()
def b_func(i):
    assert b_func.is_hydrated
    assert not a_func.is_hydrated
    assert modal.App._get_container_app() == b


================================================
File: test/supports/multiapp_privately_decorated.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


def foo(i):
    return 1


foo_handle = app.function()(foo)  #  "privately" decorated, by not override the original function


other_app = modal.App()


@other_app.function()
def bar(i):
    return 2


================================================
File: test/supports/multiapp_privately_decorated_named_app.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App("dummy", include_source=True)  # TODO: remove include_source=True)


def foo(i):
    return 1


foo_handle = app.function()(foo)


other_app = modal.App()


@other_app.function()
def bar(i):
    return 2


================================================
File: test/supports/multiapp_same_name.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App("dummy", include_source=True)  # TODO: remove include_source=True)


def foo(i):
    return 1


foo_handle = app.function()(foo)


other_app = modal.App("dummy", include_source=True)  # TODO: remove include_source=True)


@other_app.function()
def bar(i):
    return 2


================================================
File: test/supports/multiapp_serialized_func.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


def foo(i):
    return 1


foo_handle = app.function(serialized=True)(foo)


other_app = modal.App()


@other_app.function()
def bar(i):
    return 2


================================================
File: test/supports/package_mount.py
================================================
# Copyright Modal Labs 2022
from modal import App, Image

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default

# just make sure that non-existing package doesn't cause this to crash in containers:
image = Image.debian_slim().add_local_python_source("non_existing_package_123154")


@app.function(image=image, serialized=True)
def dummy(_x):
    return 0


================================================
File: test/supports/progress_info.py
================================================
# Copyright Modal Labs 2022
from modal import enable_output

from .common import app, f

if __name__ == "__main__":
    with enable_output():
        with app.run():
            assert f.remote(2, 4) == 20  # type: ignore


================================================
File: test/supports/pyproject.toml
================================================
[foo]
bar = "baz"


================================================
File: test/supports/raise_error.py
================================================
# Copyright Modal Labs 2024
def raise_error():
    raise RuntimeError("Boo!")


================================================
File: test/supports/sandbox.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App()


@app.function()
def spawn_sandbox(x):
    modal.Sandbox.create("bash", "-c", "echo bar")


================================================
File: test/supports/script.py
================================================
# Copyright Modal Labs 2022
from .common import app, f

if __name__ == "__main__":
    with app.run():
        assert f.remote(2, 4) == 20  # type: ignore


================================================
File: test/supports/serialize_class.py
================================================
# Copyright Modal Labs 2024
import sys

import modal
from modal import enter, method, web_endpoint
from modal._serialization import serialize


class UserCls:
    @enter()
    def enter(self):
        pass

    @method()
    def method(self):
        return "a"

    @web_endpoint()
    def web_endpoint(self):
        pass


app = modal.App()
app.cls()(UserCls)  # avoid warnings about not turning methods into functions

sys.stdout.buffer.write(serialize(UserCls))


================================================
File: test/supports/sibling_hydration_app.py
================================================
# Copyright Modal Labs 2025
import modal
from modal import asgi_app, enter, method, web_endpoint

app = modal.App()


@app.function()
def square(x):
    return x * x


@app.function()
@asgi_app()
def fastapi_app():
    return None


def gen():
    yield


@app.function(is_generator=True)
def fun_returning_gen():
    return gen()


@app.function()
def function_calling_method(x, y, z):
    obj = ParamCls(x=x, y=y)
    return obj.f.remote(z)


@app.function()
def check_sibling_hydration(x):
    assert square.is_hydrated
    assert fastapi_app.is_hydrated
    assert fastapi_app.web_url
    assert fun_returning_gen.is_hydrated
    assert fun_returning_gen.is_generator

    # make sure the underlying service function for the class is hydrated:
    assert NonParamCls._get_class_service_function().is_hydrated  # type: ignore
    assert ParamCls._get_class_service_function().is_hydrated  # type: ignore

    # notably not hydrated at this point:
    # NonParamCls()  (instance of parameter-less class - note that hydration shouldn't require any roundtrips for this)
    # NonParamCls().f  (method of parameter-less class - note that hydration shouldn't require any roundtrips for this)
    # ParamCls(x=1, y=3)  (parameter-bound class instance)


@app.cls()
class NonParamCls:
    _k = 11  # not a parameter, just a static initial value

    @enter()
    def enter(self):
        self._k = 111

    @method()
    def f(self, x):
        return self._k * x

    @web_endpoint()
    def web(self, arg):
        return {"ret": arg * self._k}

    @asgi_app()
    def asgi_web(self):
        from fastapi import FastAPI

        k_at_construction = self._k  # expected to be 111
        hydrated_at_contruction = square.is_hydrated
        web_app = FastAPI()

        @web_app.get("/")
        def k(arg: str):
            return {
                "at_construction": k_at_construction,
                "at_runtime": self._k,
                "arg": arg,
                "other_hydrated": hydrated_at_contruction,
            }

        return web_app

    def _generator(self, x):
        yield x**3

    @method(is_generator=True)
    def generator(self, x):
        return self._generator(x)


@app.cls()
class ParamCls:
    x: int = modal.parameter()
    y: str = modal.parameter()

    @method()
    def f(self, z: int):
        return f"{self.x} {self.y} {z}"

    @method()
    def g(self, z):
        return self.f.local(z)


================================================
File: test/supports/skip.py
================================================
# Copyright Modal Labs 2022
import os
import platform
import pytest
import sys


def skip_windows(msg: str):
    return pytest.mark.skipif(platform.system() == "Windows", reason=msg)


def skip_macos(msg: str):
    return pytest.mark.skipif(platform.system() == "Darwin", reason=msg)


skip_windows_unix_socket = skip_windows("Windows doesn't have UNIX sockets")


def skip_old_py(msg: str, min_version: tuple):
    return pytest.mark.skipif(sys.version_info < min_version, reason=msg)


skip_github_non_linux = pytest.mark.skipif(
    (os.environ.get("GITHUB_ACTIONS") == "true" and platform.system() != "Linux"),
    reason="containers only have to run on linux.",
)


================================================
File: test/supports/slow_dependencies_container.py
================================================
# Copyright Modal Labs 2024
import sys

import modal._container_entrypoint  # noqa

assert "modal" in sys.modules

# This is a very heavy dependency that takes about 70-80ms to import
# Let's make sure it doesn't get imported in global scope
assert "aiohttp" not in sys.modules


================================================
File: test/supports/slow_dependencies_local.py
================================================
# Copyright Modal Labs 2024
import sys

import modal  # noqa

assert "modal" in sys.modules

# This is a very heavy dependency that takes about 70-80ms to import
# Let's make sure it doesn't get imported in global scope
assert "aiohttp" not in sys.modules


================================================
File: test/supports/special_poetry.lock
================================================
[foo]
bar = "baz"


================================================
File: test/supports/standalone_file.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/startup_failure.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App("hello-world", include_source=True)  # TODO: remove include_source=True)

if not modal.is_local():
    import nonexistent_package  # noqa


@app.function()
def f(i):
    pass


================================================
File: test/supports/test-conda-environment.yml
================================================
name: env1
channels:
  - pytorch
  - defaults
dependencies:
  - python=3.12.5
  - foo=1.0
  - pip:
      - bar=2.1


================================================
File: test/supports/test-dockerfile
================================================
FROM python:3.10-slim-bullseye
RUN pip install numpy


================================================
File: test/supports/test-pyproject.toml
================================================
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

[tool.mypy]
python_version = "3.9"
exclude = "build"
ignore_missing_imports = true
check_untyped_defs = true
no_strict_optional = true
namespace_packages = true

[project]
name = "foo"
description = "bar"
requires-python = ">=3.9"
dependencies = ["banana >=1.2.0", "potato >=0.1.0"]

[project.optional-dependencies]
dev = ["linting-tool >=0.0.0"]
test = ["pytest >=1.2.0"]
doc = ["mkdocs >=1.4.2"]


================================================
File: test/supports/test-requirements.txt
================================================
# some comment
banana~=1.2.3
apple # another comment
blueberry~=0.0.0; python_version >= '3.7'
git+https://github.com/modal-com/synchronicity.git#egg=synchronicity


# more
# comments


================================================
File: test/supports/type_assertions.py
================================================
# Copyright Modal Labs 2024
import typing

from typing_extensions import assert_type

import modal
from modal.partial_function import method

app = modal.App()


@app.function()
def typed_func(a: str) -> float:
    return 0.0


@app.function()
def other_func() -> str:
    return "foo"


ret = typed_func.remote(a="hello")
assert_type(ret, float)

ret2 = modal.FunctionCall.gather(typed_func.spawn("bar"), other_func.spawn())
# This assertion doesn't work in mypy (it infers the more generic list[object]), but does work in pyright/vscode:
# assert_type(ret2, typing.List[typing.Union[float, str]])
mypy_compatible_ret: typing.Sequence[object] = ret2  # mypy infers to the broader "object" type instead


should_be_float = typed_func.remote(a="hello")
assert_type(should_be_float, float)


@app.function()
async def async_typed_func(b: bool) -> str:
    return ""


async_typed_func

should_be_str = async_typed_func.remote(False)  # should be blocking without aio
assert_type(should_be_str, str)


@app.cls()
class Cls:
    @method()
    def foo(self, a: str) -> int:
        return 1

    @method()
    async def bar(self, a: str) -> int:
        return 1


instance = Cls()
should_be_int = instance.foo.remote("foo")
assert_type(should_be_int, int)

should_be_int = instance.bar.remote("bar")
assert_type(should_be_int, int)


async def async_block() -> None:
    should_be_str_2 = await async_typed_func.remote.aio(True)
    assert_type(should_be_str_2, str)
    should_also_be_str = await async_typed_func.local(False)  # local should be the original return type (!)
    assert_type(should_also_be_str, str)
    should_be_int = await instance.bar.local("bar")
    assert_type(should_be_int, int)


# check sandboxes
sandbox = modal.Sandbox.create("dummy")
assert_type(sandbox.stdout.read(), str)

for line_str in sandbox.stdout:
    assert_type(line_str, str)

cmd = sandbox.exec("other")
assert_type(cmd.stdout.read(), str)

for line_str in cmd.stdout:
    assert_type(line_str, str)

cmd2 = sandbox.exec("other_bin", text=False)
assert_type(cmd2.stdout.read(), bytes)

for line_bytes in cmd2.stdout:
    assert_type(line_bytes, bytes)

# check file_io
file_io = sandbox.open("foo", "w")
assert_type(file_io.read(), str)
assert_type(file_io.readline(), str)
assert_type(file_io.readlines(), typing.Sequence[str])

file_io2 = sandbox.open("foo", "rb")
assert_type(file_io2.read(), bytes)
assert_type(file_io2.readline(), bytes)
assert_type(file_io2.readlines(), typing.Sequence[bytes])


================================================
File: test/supports/type_assertions_negative.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App()


@app.function()
def typed_func(a: str) -> float:
    return 0.0


typed_func.remote(b="hello")  # wrong arg name
typed_func.remote(a=10)  # wrong arg type

typed_func.local(c="hello")  # wrong arg name
typed_func.local(a=10)  # wrong arg type


async def aio_calls() -> None:
    await typed_func.remote.aio(e="hello")  # wrong arg name
    await typed_func.remote.aio(a=10.5)  # wrong arg type


================================================
File: test/supports/unconsumed_map.py
================================================
# Copyright Modal Labs 2022
from .common import app, f

if __name__ == "__main__":
    with app.run():
        f.map([1, 2, 3])  # type: ignore


================================================
File: test/supports/volume_local.py
================================================
# Copyright Modal Labs 2024
from modal import App, Volume

app2 = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app2.function(volumes={"/foo": Volume.from_name("my-vol")})
def volume_func():
    pass


@app2.function()
def volume_func_outer():
    volume_func.local()


================================================
File: test/supports/webhook_forgot_function.py
================================================
# Copyright Modal Labs 2023
from modal import web_endpoint


@web_endpoint()
async def absent_minded_function(x):
    pass


================================================
File: test/supports/app_run_tests/app_was_once_stub.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.Stub()


@app.function()
def foo():
    print("foo")


================================================
File: test/supports/app_run_tests/app_with_lookups.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App("my-app", include_source=True)  # TODO: remove include_source=True)

nfs = modal.NetworkFileSystem.from_name("volume_app").hydrate()


@app.function()
def foo():
    print("foo")


================================================
File: test/supports/app_run_tests/app_with_multiple_functions.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


@app.function()
def foo():
    pass


@app.function()
def bar():
    pass


================================================
File: test/supports/app_run_tests/async_app.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
async def foo():
    pass


================================================
File: test/supports/app_run_tests/cli_args.py
================================================
# Copyright Modal Labs 2022
from datetime import datetime
from typing import Optional, Union

from modal import App, method

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.local_entrypoint()
def dt_arg(dt: datetime):
    print(f"the day is {dt.day}")


@app.local_entrypoint()
def int_arg(i: int):
    print(repr(i), type(i))


@app.local_entrypoint()
def default_arg(i: int = 10):
    print(repr(i), type(i))


@app.local_entrypoint()
def unannotated_arg(i):
    print(repr(i), type(i))


@app.local_entrypoint()
def unannotated_default_arg(i=10):
    print(repr(i), type(i))


@app.function()
def int_arg_fn(i: int):
    print(repr(i), type(i))


@app.cls()
class ALifecycle:
    @method()
    def some_method(self, i):
        print(repr(i), type(i))

    @method()
    def some_method_int(self, i: int):
        print(repr(i), type(i))


@app.local_entrypoint()
def optional_arg(i: Optional[int] = None):
    print(repr(i), type(i))


@app.local_entrypoint()
def optional_arg_pep604(i: "int | None" = None):
    print(repr(i), type(i))


@app.local_entrypoint()
def optional_arg_postponed(i: "Optional[int]" = None):
    print(repr(i), type(i))


@app.function()
def optional_arg_fn(i: Optional[int] = None):
    print(repr(i), type(i))


@app.local_entrypoint()
def unparseable_annot(i: Union[int, str]):
    pass


================================================
File: test/supports/app_run_tests/cls.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


@app.cls()
class AParametrized:
    def __init__(self, x: int):
        self._x = x

    @modal.method()
    def some_method(self, y: int): ...

    @modal.asgi_app()
    def other_method(self): ...


================================================
File: test/supports/app_run_tests/custom_app.py
================================================
# Copyright Modal Labs 2022
import modal

my_app = modal.App()


@my_app.function()
def foo():
    pass


================================================
File: test/supports/app_run_tests/default_app.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
def foo():
    print("foo")


================================================
File: test/supports/app_run_tests/file_with_global_lookups.py
================================================
# Copyright Modal Labs 2025
import modal

remote_func = modal.Function.from_name("app", "some_func")
remote_cls = modal.Cls.from_name("app", "some_class")

app = modal.App()


@app.function()
def local_f():
    pass


================================================
File: test/supports/app_run_tests/generator.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()


@app.function()
def foo():
    yield "xyz"


================================================
File: test/supports/app_run_tests/local_entrypoint.py
================================================
# Copyright Modal Labs 2022

import modal

app = modal.App()


@app.function()
def foo():
    pass


@app.local_entrypoint()
def main():
    print("called locally")
    foo.remote()
    foo.remote()


================================================
File: test/supports/app_run_tests/local_entrypoint_async.py
================================================
# Copyright Modal Labs 2022

import modal

app = modal.App()


@app.function()
def foo():
    pass


@app.local_entrypoint()
async def main():
    print("called locally (async)")
    await foo.remote.aio()
    await foo.remote.aio()


================================================
File: test/supports/app_run_tests/local_entrypoint_invalid.py
================================================
# Copyright Modal Labs 2023
import modal

app = modal.App()


@app.function()
def foo():
    pass


@app.local_entrypoint()
def main():
    with app.run():  # should error here
        print("unreachable")
        foo.remote()  # should not get here


================================================
File: test/supports/app_run_tests/main_thread_assertion.py
================================================
# Copyright Modal Labs 2022
import pytest
import threading

import modal

assert threading.current_thread() == threading.main_thread()

# can be checked to ensure module is loaded at all
pytest._did_load_main_thread_assertion = True  # type: ignore

app = modal.App()


@app.function()
def dummy():
    pass


================================================
File: test/supports/app_run_tests/prints_desc_app.py
================================================
# Copyright Modal Labs 2022
import modal

app = modal.App()

# This is in module scope, so will show what the `description`
# value is at import time, which may be different if some code
# changes the `description` post-import.
print(f"app.description: {app.description}")


@app.function()
def foo():
    pass


================================================
File: test/supports/app_run_tests/raises_error.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App()


@app.function(gpu="broken:gpu:string")
def f():
    pass


================================================
File: test/supports/app_run_tests/returns_data.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App()


@app.local_entrypoint()
def returns_str() -> str:
    return "Hello!"


@app.local_entrypoint()
def returns_bytes() -> bytes:
    return b"Hello!"


@app.local_entrypoint()
def returns_int() -> int:
    return 42


================================================
File: test/supports/app_run_tests/variadic_args.py
================================================
# Copyright Modal Labs 2022
from modal import App, method

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.cls()
class VaClass:
    @method()
    def va_method(self, *args):
        pass  # Set via @servicer.function_body

    @method()
    def va_method_invalid(self, x: int, *args):
        pass  # Set via @servicer.function_body


@app.function()
def va_function(*args):
    pass  # Set via @servicer.function_body


@app.function()
def va_function_invalid(x: int, *args):
    pass  # Set via @servicer.function_body


@app.local_entrypoint()
def va_entrypoint(*args):
    print(f"args: {args}")


@app.local_entrypoint()
def va_entrypoint_invalid(x: int, *args):
    print(f"args: {args}")


================================================
File: test/supports/app_run_tests/webhook.py
================================================
# Copyright Modal Labs 2022
from modal import App, web_endpoint

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function()
@web_endpoint()
def foo():
    return {"bar": "baz"}


================================================
File: test/supports/app_run_tests/multifile/__init__.py
================================================
# Copyright Modal Labs 2025


================================================
File: test/supports/app_run_tests/multifile/main.py
================================================
# Copyright Modal Labs 2025
import modal

app = modal.App()


@app.local_entrypoint()
def some_main_entrypoint():
    print("main entrypoint")


def main_func():
    print("main func")


================================================
File: test/supports/app_run_tests/multifile/util.py
================================================
# Copyright Modal Labs 2025
from .main import app, main_func


@app.local_entrypoint()
def run_this():
    print("ran util")
    main_func()


================================================
File: test/supports/multifile_project/__init__.py
================================================
# Copyright Modal Labs 2025


================================================
File: test/supports/multifile_project/a.py
================================================
# Copyright Modal Labs 2024
import modal

from . import c

app = modal.App()

d = modal.Dict.from_name("my-queue", create_if_missing=True)


@app.function()
def a_func():
    d["foo"] = "bar"


app.include(c.app)


================================================
File: test/supports/multifile_project/b.py
================================================
# Copyright Modal Labs 2024
import modal

from . import c

app = modal.App()


@app.function(secrets=[modal.Secret.from_dict({"foo": "bar"})])
def b_func():
    pass


app.include(c.app)


================================================
File: test/supports/multifile_project/c.py
================================================
# Copyright Modal Labs 2024
import modal

app = modal.App("c", include_source=True)  # TODO: remove include_source=True)


@app.function()
def c_func():
    pass


================================================
File: test/supports/multifile_project/main.py
================================================
# Copyright Modal Labs 2024
import modal
from modal import enter, method, web_endpoint

from . import a, b

app = modal.App()
app.include(a.app)
app.include(b.app)


@app.function()
def main_function():
    pass


@app.function()
@web_endpoint()
def web():
    pass


other_app = modal.App()


@other_app.cls()
class Cls:
    @enter()
    def startup(self):
        pass

    @method()
    def method_on_other_app_class(self):
        pass

    @web_endpoint()
    def web_endpoint_on_other_app(self):
        pass


================================================
File: test/supports/notebooks/simple.notebook.py
================================================
# ---
# jupyter:
#   jupytext:
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.14.1
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# + tags=["parameters"]
server_addr = None
# -

from modal.client import Client
from modal_proto import api_pb2

client = Client(server_addr, api_pb2.CLIENT_TYPE_CLIENT, ("foo-id", "foo-secret"))

# +
import modal

app = modal.App()


@app.function()
def hello():
    print("running")


# + tags=["main"]
with client:
    with app.run(client=client):
        hello.remote()
# -


================================================
File: test/supports/pkg_a/__init__.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_a/a.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_a/d.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_a/package.py
================================================
# Copyright Modal Labs 2022

import pkg_b.f  # noqa
import pkg_b.g.h  # noqa

import modal  # noqa

from .a import *  # noqa
from .b.c import *  # noqa


app = modal.App()


@app.function()
def f():
    pass


================================================
File: test/supports/pkg_a/script.py
================================================
# Copyright Modal Labs 2022
import a  # noqa
import b  # noqa
import b.c  # noqa
import pkg_b  # noqa
import six  # noqa

import modal  # noqa


app = modal.App()


@app.function()
def f():
    pass


================================================
File: test/supports/pkg_a/serialized_fn.py
================================================
# Copyright Modal Labs 2022
import a  # noqa
import b  # noqa
import b.c  # noqa
import pkg_b  # noqa
import six  # noqa

import modal  # noqa


app = modal.App()


@app.function(serialized=True)
def f():
    pass


================================================
File: test/supports/pkg_a/b/c.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_a/b/e.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_b/__init__.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_b/f.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_b/g/h.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_c/__init__.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_c/i.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_c/j/k.py
================================================
# Copyright Modal Labs 2022


================================================
File: test/supports/pkg_d/__init__.py
================================================
# Copyright Modal Labs 2025


================================================
File: test/supports/pkg_d/main.py
================================================
# Copyright Modal Labs 2025
import os

from pkg_a import a  # noqa  # this would cause an automount warning

import modal

app = modal.App()

image = modal.Image.debian_slim()

if os.environ.get("ADD_SOURCE") == "add":
    # intentionally makes add local not the last call, to make sure the added modules transfer to downstream layers
    image = image.add_local_python_source("pkg_a").add_local_file(__file__, "/tmp/blah")

elif os.environ.get("ADD_SOURCE") == "copy":
    # intentionally makes add local not the last call, to make sure the added modules transfer to downstream layers
    image = image.add_local_python_source("pkg_a", copy=True).run_commands("echo hello")


@app.function(image=image)
def f():
    pass


================================================
File: test/supports/pkg_d/sibling.py
================================================
# Copyright Modal Labs 2025


================================================
File: test/supports/user_code_import_samples/__init__.py
================================================
# Copyright Modal Labs 2024


================================================
File: test/supports/user_code_import_samples/cls.py
================================================
# Copyright Modal Labs 2024
import modal
from modal import App

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


class C:
    @modal.method()
    def f(self, arg):
        return f"hello {arg}"

    @modal.method()
    def f2(self, arg):
        return f"other {arg}"

    @modal.method()
    def calls_f_remote(self, arg):
        return self.f.remote(arg)


UndecoratedC = C  # keep a reference to original class before overwriting

C = app.cls()(C)  # type: ignore[misc]   # "decorator" of C


================================================
File: test/supports/user_code_import_samples/func.py
================================================
# Copyright Modal Labs 2024
from modal import App

app = App(include_source=True)  # TODO: remove include_source=True when automount is disabled by default


@app.function()
def f(arg):
    return f"hello {arg}"


def undecorated_f(arg):
    return f"hello {arg}"


================================================
File: test/telemetry/tracing_module_1.py
================================================
# Copyright Modal Labs 2022
from . import tracing_module_2  # noqa


def foo():
    pass


================================================
File: test/telemetry/tracing_module_2.py
================================================
# Copyright Modal Labs 2022
def bar():
    pass


================================================
File: .github/pull_request_template.md
================================================
## Describe your changes

- _Provide Linear issue reference (e.g. MOD-1234) if available._

<details> <summary>Backward/forward compatibility checks</summary>

---

Check these boxes or delete any item (or this section) if not relevant for this PR.

- [ ] Client+Server: this change is compatible with old servers
- [ ] Client forward compatibility: this change ensures client can accept data intended for later versions of itself

Note on protobuf: protobuf message changes in one place may have impact to
multiple entities (client, server, worker, database). See points above.

---

</details>

## Changelog

<!--
If relevant, include a brief user-facing description of what's new in this version.

Format the changelog updates using bullet points.
See https://modal.com/docs/reference/changelog for examples and try to use a consistent style.

Provide short code examples, indented under the relevant bullet point, if they would be helpful.
Cross-linking to relevant documentation is also encouraged.
-->


================================================
File: .github/actions/setup-cached-python/action.yml
================================================
name: setup-cached-python

inputs:
  version:
    description: Which Python version to install
    required: true
    default: "3.9"

runs:
  using: composite
  steps:
    - name: Install Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ inputs.version }}

    - name: Get cached python dependencies
      uses: actions/cache@v3
      with:
        path: ${{ env.pythonLocation }}
        key: |
          ${{ runner.os }}-${{ env.pythonLocation }}-${{ hashFiles('**/setup.cfg', 'requirements.dev.txt', 'pyproject.toml') }}
        restore-keys: ${{ runner.os }}-${{ env.pythonLocation }}-

    - name: Install Python packages
      shell: bash
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.dev.txt


================================================
File: .github/workflows/check.yml
================================================
name: Check

on:
  push:
    branches:
      - main
  pull_request:

# Cancel previous runs of the same PR but do not cancel previous runs on main
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  link:
    name: Ruff linting
    runs-on: ubuntu-20.04

    steps:
      - uses: actions/checkout@v3

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - run: inv lint

      - run: inv lint-protos

  type_check:
    name: Static type checks
    runs-on: ubuntu-20.04

    steps:
      - uses: actions/checkout@v3

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - run: inv protoc

      - run: pip install -e .  # gets all dependencies and the package itself into python env

      - name: Build type stubs
        run: inv type-stubs

      - run: inv type-check

  check-copyright:
    name: Check copyright
    runs-on: ubuntu-20.04

    steps:
      - uses: actions/checkout@v3

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - run: inv check-copyright


================================================
File: .github/workflows/ci-cd.yml
================================================
name: CI/CD

on:
  push:
    branches:
      - main
  pull_request:

# Cancel previous runs of the same PR but do not cancel previous runs on main
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

env:
  TERM: linux
  TERMINFO: /etc/terminfo
  PYTHONIOENCODING: utf-8

jobs:

  client-versioning:
    if: github.ref == 'refs/heads/main'
    name: Update changelog and client version
    concurrency: client-versioning
    runs-on: ubuntu-20.04
    env:
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    outputs:
      client-version: ${{ steps.version.outputs.client_version }}

    steps:
      - name: Generate token for Github PR Bot
        id: generate_token
        uses: tibdex/github-app-token@v1
        with:
          app_id: ${{ secrets.GH_PRBOT_APP_ID }}
          private_key: ${{ secrets.GH_PRBOT_APP_PRIVATE_KEY }}

      - uses: actions/checkout@v3
        with:
          token: ${{ steps.generate_token.outputs.token }}

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - name: Bump the version number
        run: inv update-build-number

      - name: Update the changelog
        run: inv update-changelog --sha=$GITHUB_SHA

      - name: Get the current client version
        id: version
        run: echo "client_version=`python -m modal_version`" >> "$GITHUB_OUTPUT"

      - uses: EndBug/add-and-commit@v9
        with:
          add: modal_version/_version_generated.py CHANGELOG.md
          tag: v${{ steps.version.outputs.client_version }}
          message: "[auto-commit] [skip ci] Bump the build number"
          pull: "--rebase --autostash"
          default_author: github_actions

      - name: Install the client
        run: |
          inv protoc
          pip install .

      - name: Publish client mount
        env:
          MODAL_ENVIRONMENT: main
          MODAL_LOGLEVEL: DEBUG
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: python -m modal_global_objects.mounts.modal_client_package


  client-test:
    name: Unit tests on ${{ matrix.python-version }} and ${{ matrix.os }} (protobuf=${{ matrix.proto-version }})
    timeout-minutes: 30

    strategy:
      fail-fast: false # run all variants across python versions/os to completion
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12", "3.13"]
        os: ["ubuntu-20.04"]
        proto-version: ["latest"]
        include:
          - os: "macos-13" # x86-64
            python-version: "3.10"
            proto-version: "latest"
          - os: "macos-14" # ARM64 (M1)
            python-version: "3.10"
            proto-version: "latest"
          - os: "windows-latest"
            python-version: "3.10"
            proto-version: "latest"
          - os: "ubuntu-20.04"
            python-version: "3.9"
            proto-version: "3.19"

    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v3

      - uses: ./.github/actions/setup-cached-python
        with:
          version: ${{ matrix.python-version }}

      - if: matrix.proto-version != 'latest'
        name: Install protobuf
        run: pip install protobuf==${{ matrix.proto-version }}

      - name: Build protobuf
        run: inv protoc

      - name: Build client package (installs all dependencies)
        run: pip install -e .

      - name: Run client tests
        run: pytest -v

      - name: Run docstring tests
        if: github.event.pull_request.head.repo.fork == false
        env:
          MODAL_ENVIRONMENT: client-doc-tests
          MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
          MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
        run: pytest -v --markdown-docs -m markdown-docs modal

  container-dependencies:
    name: Check minimal container dependencies for ${{ matrix.python-version }} / ${{ matrix.image-builder-version }}
    runs-on: ubuntu-20.04
    timeout-minutes: 4
    strategy:
      matrix:
        include:
          - image-builder-version: "2024.04"
            python-version: "3.9"
          - image-builder-version: "2024.04"
            python-version: "3.12"
          - image-builder-version: "2024.10"
            python-version: "3.9"
          - image-builder-version: "2024.10"
            python-version: "3.13"

    steps:
      - uses: actions/checkout@v3

      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          pip install -r modal/requirements/${{ matrix.image-builder-version }}.txt
          pip install synchronicity

      - name: Compile protos
        run: |
          python -m venv venv
          source venv/bin/activate
          if [ "${{ matrix.python-version }}" == "3.9" ]; then
            pip install grpcio-tools==1.48.2 grpclib==0.4.7;
          elif [ "${{ matrix.python-version }}" == "3.12" ]; then
            pip install grpcio-tools==1.59.2 grpclib==0.4.7;
          elif [ "${{ matrix.python-version }}" == "3.13" ]; then
            pip install grpcio-tools==1.66.2 grpclib==0.4.7;
          fi
          python -m grpc_tools.protoc --python_out=. --grpclib_python_out=. --grpc_python_out=. -I . modal_proto/api.proto modal_proto/options.proto
          python -m grpc_tools.protoc --plugin=protoc-gen-modal-grpclib-python=protoc_plugin/plugin.py --modal-grpclib-python_out=. -I . modal_proto/api.proto modal_proto/options.proto
          deactivate

      - name: Check entrypoint import
        run: |
          python -c 'import modal._container_entrypoint'
          if [ "${{ matrix.image-builder-version }}" == "2024.04" ]; then python -c 'import fastapi'; fi

  publish-client:
    name: Publish client package
    if: github.ref == 'refs/heads/main'
    needs: [client-versioning, client-test]
    runs-on: ubuntu-20.04
    concurrency: publish-client
    timeout-minutes: 5
    steps:

      - uses: actions/checkout@v3
        with:
          ref: v${{ needs.client-versioning.outputs.client-version}}

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.10"

      - name: Build protobuf
        run: inv protoc

      - name: Install all dependencies
        run: pip install -e .

      - name: Build type stubs
        run: inv type-stubs
      
      - name: Install build
        run: pip install build

      - name: Build package distributions (wheel and source)
        run: |
          python -m build

      - name: Upload to PyPI
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}
        run: twine upload dist/* --non-interactive

  publish-python-standalone:
    name: Publish Python standalone mounts
    if: github.ref == 'refs/heads/main'
    needs: [client-versioning, client-test, publish-client]
    runs-on: ubuntu-20.04
    timeout-minutes: 5
    env:
      MODAL_LOGLEVEL: DEBUG
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}

    steps:
      - uses: actions/checkout@v3
        with:
          ref: v${{ needs.client-versioning.outputs.client-version}}

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.11"

      - name: Build protobuf
        run: inv protoc

      - name: Build client package (installs all dependencies)
        run: pip install -e .

      - name: Publish mounts
        run: python -m modal_global_objects.mounts.python_standalone


  publish-base-images:
    name: |
      Publish base images for ${{ matrix.image-name }} ${{ matrix.image-builder-version }}
    if: github.ref == 'refs/heads/main'
    needs: [client-versioning, client-test, publish-client]
    runs-on: ubuntu-20.04
    timeout-minutes: 5
    env:
      MODAL_LOGLEVEL: DEBUG
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
    strategy:
      matrix:
        image-builder-version: ["2023.12", "2024.04", "2024.10"]
        image-name: ["debian_slim", "micromamba"]

    steps:
      - uses: actions/checkout@v3
        with:
          ref: v${{ needs.client-versioning.outputs.client-version}}

      - uses: ./.github/actions/setup-cached-python
        with:
          version: "3.11"

      - name: Build protobuf
        run: inv protoc

      - name: Build client package (installs all dependencies)
        run: pip install -e .

      - name: Set the Modal environment
        run: modal config set-environment main

      - name: Publish base images
        env:
          MODAL_IMAGE_BUILDER_VERSION: ${{ matrix.image-builder-version }}
          MODAL_IMAGE_ALLOW_GLOBAL_DEPLOYMENT: "1"
        run: |
          python -m modal_global_objects.images.base_images ${{ matrix.image-name }}


================================================
File: .github/workflows/docs.yml
================================================
name: Docs

on:
  push:
    branches:
      - main
  pull_request:

# Cancel previous runs of the same PR but do not cancel previous runs on main
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  doc-test:
    name: Doc generation tests
    timeout-minutes: 5
    runs-on: ubuntu-20.04
    steps:
      - uses: actions/checkout@v3

      - uses: ./.github/actions/setup-cached-python

      - name: Build protobuf
        run: inv protoc

      - name: Install package + deps
        run: pip install -e .  # Makes sure doc generation doesn't break on client imports etc.

      - name: Generate reference docs
        run: python -m modal_docs.gen_reference_docs reference_docs_output

      - name: Generate CLI docs
        run: python -m modal_docs.gen_cli_docs cli_docs


================================================
File: .github/workflows/sast-codeql.yml
================================================
# For most projects, this workflow file will not need changing; you simply need
# to commit it to your repository.
#
# You may wish to alter this file to override the set of languages analyzed,
# or to provide custom queries or build logic.
#
# ******** NOTE ********
# We have attempted to detect the languages in your repository. Please check
# the `language` matrix defined below to confirm you have the correct set of
# supported CodeQL languages.
#
name: "CodeQL"

on:
  push:
    branches: [ "main" ]
  pull_request:
    # The branches below must be a subset of the branches above
    branches: [ "main" ]
  schedule:
    - cron: '43 3 * * 1'

# Cancel previous runs of the same PR but do not cancel previous runs on main
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/main' }}

jobs:
  analyze:
    name: Analyze
    runs-on: ${{ (matrix.language == 'swift' && 'macos-latest') || 'ubuntu-latest' }}
    permissions:
      actions: read
      contents: read
      security-events: write

    strategy:
      fail-fast: false
      matrix:
        language: [ 'python' ]
        # CodeQL supports [ 'cpp', 'csharp', 'go', 'java', 'javascript', 'python', 'ruby' ]
        # Use only 'java' to analyze code written in Java, Kotlin or both
        # Use only 'javascript' to analyze code written in JavaScript, TypeScript or both
        # Learn more about CodeQL language support at https://aka.ms/codeql-docs/language-support

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    # Initializes the CodeQL tools for scanning.
    - name: Initialize CodeQL
      uses: github/codeql-action/init@v2
      with:
        languages: ${{ matrix.language }}
        # If you wish to specify custom queries, you can do so here or in a config file.
        # By default, queries listed here will override any specified in a config file.
        # Prefix the list here with "+" to use these queries and those in the config file.

        # For more details on CodeQL's query packs, refer to: https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/configuring-code-scanning#using-queries-in-ql-packs
        # queries: security-extended,security-and-quality


    # Autobuild attempts to build any compiled languages  (C/C++, C#, Go, or Java).
    # If this step fails, then you should remove it and run the build manually (see below)
    - name: Autobuild
      uses: github/codeql-action/autobuild@v2

    # ℹ️ Command-line programs to run using the OS shell.
    # 📚 See https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idstepsrun

    #   If the Autobuild fails above, remove it and uncomment the following three lines.
    #   modify them (or add more) to build your code if your project, please refer to the EXAMPLE below for guidance.

    # - run: |
    #     echo "Run, Build Application using script"
    #     ./location_of_script_within_repo/buildscript.sh

    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2
      with:
        category: "/language:${{matrix.language}}"


