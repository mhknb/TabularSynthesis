- [Home](https://baukebrenninkmeijer.github.io/table-evaluator/index.html)
- Metrics
- [View page source](https://baukebrenninkmeijer.github.io/table-evaluator/_sources/metrics.rst.txt)

* * *

# Metrics [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html\#metrics "Link to this heading")

table\_evaluator.metrics.column\_correlations( _dataset\_a:DataFrame_, _dataset\_b:DataFrame_, _categorical\_columns:list\[str\]\|None_, _theil\_u=True_) [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html#table_evaluator.metrics.column_correlations "Link to this definition")

Column-wise correlation calculation between `dataset_a` and `dataset_b`.

Parameters:

- **dataset\_a** ( _pd.DataFrame_) – First DataFrame

- **dataset\_b** ( _pd.DataFrame_) – Second DataFrame

- **categorical\_columns** ( _list_ _\[_ _str_ _\]_) – The columns containing categorical values

- **theil\_u** ( _bool_) – Whether to use Theil’s U. If False, use Cramer’s V.


Returns:

Mean correlation between all columns.

Return type:

float

table\_evaluator.metrics.euclidean\_distance( _y\_true:ndarray\|Series_, _y\_pred:ndarray\|Series_)→float [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html#table_evaluator.metrics.euclidean_distance "Link to this definition")

Returns the euclidean distance between y\_true and y\_pred.

Parameters:

- **y\_true** ( _numpy.ndarray_) – The ground truth values.

- **y\_pred** ( _numpy.ndarray_) – The predicted values.


Returns:

The mean absolute error.

Return type:

float

table\_evaluator.metrics.jensenshannon\_distance( _colname:str_, _real\_col:Series_, _fake\_col:Series_, _bins:int=25_)→Dict\[str,Any\] [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html#table_evaluator.metrics.jensenshannon_distance "Link to this definition")

Calculate the Jensen-Shannon distance between real and fake data columns.

This function bins the data, calculates probability distributions, and then
computes the Jensen-Shannon distance between these distributions.

Parameters:

- **colname** ( _str_) – Name of the column being analyzed.

- **real\_col** ( _pd.Series_) – Series containing the real data.

- **fake\_col** ( _pd.Series_) – Series containing the fake data.

- **bins** ( _int_ _,_ _optional_) – Number of bins to use for discretization. Defaults to 25.


Returns:

A dictionary containing:

- ’col\_name’: Name of the column.

- ’js\_distance’: The calculated Jensen-Shannon distance.


Return type:

Dict\[str, Any\]

Note

The number of bins is capped at the length of the real column to avoid empty bins.

table\_evaluator.metrics.js\_distance\_df( _real:DataFrame_, _fake:DataFrame_, _numerical\_columns:List\[str\]_)→DataFrame [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html#table_evaluator.metrics.js_distance_df "Link to this definition")

Calculate Jensen-Shannon distances between real and fake data for numerical columns.

This function computes the Jensen-Shannon distance for each numerical column
in parallel using joblib’s Parallel and delayed functions.

Parameters:

- **real** ( _pd.DataFrame_) – DataFrame containing the real data.

- **fake** ( _pd.DataFrame_) – DataFrame containing the fake data.

- **numerical\_columns** ( _List_ _\[_ _str_ _\]_) – List of column names to compute distances for.


Returns:

A DataFrame with column names as index and Jensen-Shannon

distances as values.

Return type:

pd.DataFrame

Raises:

**AssertionError** – If the columns in real and fake DataFrames are not identical.

table\_evaluator.metrics.kolmogorov\_smirnov\_test( _col\_name:str_, _real\_col:Series_, _fake\_col:Series_)→Dict\[str,Any\] [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html#table_evaluator.metrics.kolmogorov_smirnov_test "Link to this definition")

Perform Kolmogorov-Smirnov test on real and fake data columns.

Parameters:

- **col\_name** ( _str_) – Name of the column being tested.

- **real\_col** ( _pd.Series_) – Series containing the real data.

- **fake\_col** ( _pd.Series_) – Series containing the fake data.


Returns:

A dictionary containing:

- ’col\_name’: Name of the column.

- ’statistic’: The KS statistic.

- ’p-value’: The p-value of the test.

- ’equality’: ‘identical’ if p-value > 0.01, else ‘different’.


Return type:

Dict\[str, Any\]

table\_evaluator.metrics.mean\_absolute\_error( _y\_true:ndarray_, _y\_pred:ndarray_)→floating\[Any\] [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html#table_evaluator.metrics.mean_absolute_error "Link to this definition")

Returns the mean absolute error between y\_true and y\_pred.

Parameters:

- **y\_true** – NumPy.ndarray with the ground truth values.

- **y\_pred** – NumPy.ndarray with the ground predicted values.


Returns:

Mean absolute error (float).

table\_evaluator.metrics.mean\_absolute\_percentage\_error( _y\_true:ndarray\|Series_, _y\_pred:ndarray\|Series_) [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html#table_evaluator.metrics.mean_absolute_percentage_error "Link to this definition")

Returns the mean absolute percentage error between y\_true and y\_pred. Throws ValueError if y\_true contains zero
values.

Parameters:

- **y\_true** ( _numpy.ndarray_) – The ground truth values.

- **y\_pred** ( _numpy.ndarray_) – The predicted values.


Returns:

Mean absolute percentage error.

Return type:

float

table\_evaluator.metrics.rmse( _y\_true:ndarray\|Series_, _y\_pred:ndarray\|Series_)→ndarray\|Series [](https://baukebrenninkmeijer.github.io/table-evaluator/metrics.html#table_evaluator.metrics.rmse "Link to this definition")

Returns the root mean squared error between y\_true and y\_pred.

Parameters:

- **y\_true** – NumPy.ndarray with the ground truth values.

- **y\_pred** – NumPy.ndarray with the ground predicted values.


Returns:

root mean squared error (float).